{
    "tests/test_benchmark.py": [
        {
            "function": "test_benchmark_model_model_error",
            "error_type": "Failed",
            "error_details": "Failed: DID NOT RAISE <class 'RuntimeError'>",
            "line_number": "215",
            "code_snippet": "    with pytest.raises(RuntimeError, match=\"Model execution failed\"):",
            "captured_output": "DEBUG: Invoking benchmark_model\nStarting benchmark_model with parameters: batch_size=1, num_batches=1, device_type=cpu, precision=medium, model_checkpoint=None\nCollating batch of size: 1\nInput shapes: [torch.Size([1, 30, 30])]\nOutput shapes: [torch.Size([1, 30, 30])]\nMax dimensions: height=30, width=30\nPadded input shape: torch.Size([1, 1, 30, 30])\nPadded output shape: torch.Size([1, 1, 30, 30])\nProcessing batch 1/1\nInputs type: <class 'torch.Tensor'>\nInputs shape: torch.Size([1, 1, 30, 30])\nOutputs type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 30, 30])\nTask IDs: ['task_1']\nInputs shape: torch.Size([1, 1, 30, 30]), Outputs shape: torch.Size([1, 1, 30, 30]), Task IDs: ['task_1']\nBatch time: 0.00016307830810546875\nCollating batch of size: 1\nInput shapes: [torch.Size([1, 30, 30])]\nOutput shapes: [torch.Size([1, 30, 30])]\nMax dimensions: height=30, width=30\nPadded input shape: torch.Size([1, 1, 30, 30])\nPadded output shape: torch.Size([1, 1, 30, 30])\nBenchmark completed. Total time: 0.00016307830810546875, Total grids: 1\nBenchmark completed. Final results - avg_time: 0.00016307830810546875, avg_grids: 6132.023391812865",
            "captured_log": "DEBUG    benchmark:benchmark.py:106 Batch content before unpacking: [tensor([[[[-7.7192e-01, -8.1481e-01,  2.0562e-01,  8.6199e-02,  1.1263e+00,\n            1.5118e+00,  1.4748e+00,  2.6802e-01, -3.5737e-01,  4.4866e-01,\n           -8.8972e-01,  3.5953e-01,  6.4089e-02,  3.0900e-01,  6.8671e-01,\n            7.2572e-01,  2.5350e-01,  1.8825e+00,  1.1915e+00, -1.4544e+00,\n           -3.3731e-01, -3.4775e-01,  3.1726e-01, -7.2917e-01, -1.4512e+00,\n            3.1673e-01,  4.4322e-01,  1.2145e+00, -1.9389e+00, -1.0291e+00],\n          [ 8.0860e-01,  3.0482e-01,  2.3844e-01,  1.1162e+00,  1.1861e+00,\n           -1.2970e+00, -1.2660e+00, -1.3720e+00,  6.2189e-02,  2.3059e-01,\n            1.9681e-01, -1.0183e-01,  5.8064e-01,  7.4414e-01,  1.0791e-01,\n           -6.7509e-01,  8.8475e-01,  2.0427e-01,  1.3510e-02, -4.4697e-01,\n            6.4219e-01,  7.2997e-02, -1.0124e+00, -3.0014e-01, -2.0934e+00,\n            1.5055e-03, -8.2803e-01,  1.3003e+00, -7.5225e-01,  8.9464e-01],\n          [-4.2467e-01, -1.2051e+00, -6.6662e-01,  8.8799e-01,  2.0081e-02,\n           -1.9973e+00, -1.4197e+00, -4.9518e-01, -9.5745e-01, -1.3518e+00,\n            2.2503e+00, -1.6991e+00, -2.3621e-01, -3.1959e+00, -2.1351e+00,\n           -1.9779e+00,  1.0427e+00,  7.2265e-01,  2.3106e+00, -8.2342e-01,\n           -3.7336e-01,  1.9533e+00, -6.3327e-01,  1.8475e-01,  2.6780e-01,\n            5.4300e-01,  1.8409e-01,  5.0785e-01,  1.2708e+00,  1.5659e+00],\n          [-1.3002e+00,  1.1762e+00, -1.6560e-01, -7.7182e-01,  1.8576e+00,\n            9.3245e-01, -1.1386e-01,  4.1621e-01,  5.2001e-01,  1.0201e+00,\n            1.6295e+00, -1.4974e+00,  2.0616e-01,  5.5397e-01, -2.2808e-01,\n            4.6008e-01, -1.8022e+00,  9.0397e-01, -3.2189e-02, -7.2651e-01,\n           -1.1338e+00, -8.2402e-01,  1.3348e+00, -1.6583e+00, -2.9862e-01,\n            7.0618e-01, -9.5381e-01,  1.8541e+00, -4.4768e-02,  6.4470e-01],\n          [-2.1512e-01,  4.2179e-01, -6.5495e-01, -1.0596e+00, -1.2388e+00,\n           -3.9130e-01,  8.9024e-01,  7.6305e-01, -7.8634e-01, -1.4110e+00,\n           -1.9888e+00, -3.6939e-01,  4.1230e-01,  1.1105e+00, -1.4203e+00,\n            2.5425e-01,  5.9970e-01,  1.3127e+00, -9.8736e-01,  4.2049e-02,\n            4.1233e-01, -5.1514e-01,  2.0196e+00, -7.0892e-01,  6.0596e-01,\n           -1.1441e+00, -5.4774e-01, -5.5136e-01,  1.2821e+00, -8.5300e-01],\n          [-5.7426e-01, -6.1796e-01, -6.4943e-02,  2.8102e-01, -2.1656e+00,\n            1.4767e+00,  9.4674e-01, -3.2803e-01,  2.3051e+00, -1.0675e+00,\n           -4.4313e-01, -4.2923e-01,  1.6305e-01, -1.6557e+00,  7.3231e-02,\n           -4.3993e-01,  2.6382e-02,  1.1318e+00,  1.9583e+00, -1.6888e+00,\n            6.0944e-02, -1.4877e+00, -2.6262e-01, -2.4860e-01, -5.1321e-01,\n           -1.2098e+00,  8.1876e-01, -1.1674e-02,  6.4624e-02,  1.7071e+00],\n          [ 1.4604e+00,  6.1337e-01, -1.2897e-01, -7.6217e-01, -1.7969e+00,\n            8.3440e-01,  1.3111e+00, -3.5923e-01,  4.3390e-01, -1.7264e-01,\n            2.9961e-01,  5.8974e-01, -7.0587e-01,  3.2943e-01,  1.0439e+00,\n           -6.4489e-01,  4.9792e-01,  1.9698e+00,  6.4919e-01,  9.3630e-01,\n           -6.6464e-01,  8.1107e-01, -9.9875e-01,  4.2044e-01, -8.6365e-01,\n           -2.1107e-01, -2.2386e-01,  2.8307e-01,  1.0289e+00, -3.6265e-01],\n          [-6.0699e-02, -2.7136e-01, -4.3851e-01, -1.1576e+00, -7.0874e-01,\n           -8.5520e-01, -1.5363e-01, -3.9701e-01,  1.7520e+00,  1.1209e-01,\n           -3.9371e-01, -1.3852e+00,  9.5402e-01, -8.2645e-01,  5.5816e-01,\n            2.4261e-01, -1.0518e+00,  1.2572e-01, -2.0440e-01,  1.3356e-01,\n            1.7344e+00,  3.8822e-01,  1.1617e+00, -2.2900e+00, -1.2398e+00,\n           -1.1015e+00,  1.2804e+00,  2.0696e+00,  1.0855e+00, -8.4769e-01],\n          [ 3.2343e-02, -7.7463e-01, -5.7986e-01,  1.3584e-01, -5.3818e-01,\n           -8.2564e-01, -5.5283e-01,  7.9036e-01, -1.1016e+00, -4.6635e-02,\n           -8.0764e-01, -3.3111e-01, -5.1913e-01,  6.0298e-01,  3.3187e-02,\n           -1.1429e+00,  1.9073e-01, -9.4229e-01, -4.4944e-01,  1.6158e-01,\n            2.5677e-01,  1.2084e+00, -1.3742e+00,  5.5700e-01,  4.7082e-02,\n           -2.1745e-01,  6.1792e-02,  9.3925e-01,  1.1220e+00,  1.3978e-01],\n          [ 3.0132e-01,  1.3013e+00,  7.1064e-03, -1.8507e+00, -1.2007e-01,\n           -1.0269e+00,  1.5471e-01, -3.1892e-01,  1.9982e+00, -5.4312e-01,\n           -7.4442e-01, -1.0691e+00, -1.7898e-01,  6.1727e-01, -1.9505e-01,\n            1.4792e-01,  1.1284e-01,  1.4051e+00, -5.0986e-01,  5.5720e-01,\n            3.4000e-01,  8.6722e-01, -9.5265e-01,  3.7534e-01,  1.6330e-01,\n           -2.7295e-01, -8.9077e-01, -3.7945e-01,  3.8527e-01,  5.4993e-01],\n          [ 6.0501e-01,  2.5534e-01, -7.6585e-01, -1.7399e+00,  3.2535e-01,\n            4.0151e-02,  1.8454e-01, -4.5422e-01,  9.7895e-01, -2.4977e+00,\n           -6.7726e-02, -9.5478e-01,  2.4511e+00,  1.4961e+00,  1.8233e-01,\n            7.5620e-01,  7.2554e-01, -6.7508e-01,  7.8562e-01, -5.2428e-01,\n           -9.9797e-02, -2.5042e+00, -2.6180e-01,  1.1494e+00,  8.1026e-01,\n           -3.1307e-01, -1.7973e-01, -1.4453e-01,  7.0721e-01,  1.1390e+00],\n          [ 3.0783e-01, -1.1077e+00, -1.8942e+00,  1.7024e-01, -6.6949e-01,\n           -1.2251e+00,  3.5624e-02,  5.7778e-01, -1.4182e+00, -1.8350e+00,\n            5.6375e-01,  1.1890e-01,  8.4327e-01,  1.1984e+00,  4.2358e-01,\n           -6.2361e-01,  5.4366e-01,  9.0170e-01,  6.5137e-01, -1.3444e+00,\n           -1.1378e+00,  1.3036e+00, -1.5774e+00,  1.6714e+00,  1.7150e+00,\n           -1.3116e+00,  9.0845e-01,  9.2709e-01,  1.8551e+00,  7.8532e-01],\n          [ 7.4414e-01, -4.7755e-01, -1.4410e+00,  1.5448e-01,  6.8850e-02,\n            3.8076e-01,  1.3381e+00,  2.9372e-01, -1.6362e+00,  9.2048e-01,\n           -1.8455e+00,  2.3156e-02,  1.0486e+00,  8.2714e-01,  5.2609e-01,\n            1.3354e+00, -4.3915e-01, -1.6378e-01,  3.8430e-01, -3.6386e-01,\n            6.3985e-02,  1.5893e+00, -1.4244e+00, -5.7070e-02,  6.4350e-02,\n            9.4554e-01,  6.1518e-01, -6.3736e-01, -1.7041e-01, -1.0902e+00],\n          [-1.4705e-01,  6.7101e-01,  2.8321e-02, -2.1451e-01, -1.8414e+00,\n            3.6582e-01,  1.4234e-01, -8.9548e-01,  8.4184e-01,  1.0759e+00,\n           -3.9422e-01,  6.7072e-01, -1.2471e+00,  5.8311e-01, -7.5436e-01,\n           -7.1478e-01,  6.6875e-03, -1.1265e+00,  3.4903e-01,  6.6271e-01,\n           -2.9163e-01, -2.0758e+00, -1.4624e+00, -2.7344e-01, -2.1350e+00,\n           -1.2758e+00, -6.4881e-01, -1.4047e+00, -1.9267e+00,  1.3297e+00],\n          [ 3.1343e-01, -6.0071e-01, -3.0446e-01,  1.6178e+00,  4.0216e-01,\n            1.1217e+00, -5.2324e-01, -1.1308e+00, -1.8342e+00,  4.9819e-01,\n           -9.0766e-01,  5.7787e-01, -4.6272e-01, -1.8113e+00,  3.5980e-01,\n           -2.1265e+00,  1.6027e+00,  7.7926e-02, -9.9787e-01, -5.9818e-01,\n            3.2262e-01,  1.3585e+00,  7.7904e-01,  1.3439e+00,  1.7304e+00,\n           -1.0675e-01,  3.3802e-01, -1.3705e-01, -1.3241e+00,  1.1956e+00],\n          [-1.2377e-01,  1.0342e+00, -1.7632e+00,  7.5666e-01,  3.4254e-01,\n            1.2580e-01, -9.5098e-01, -7.7024e-01, -7.3431e-01,  8.7728e-01,\n           -3.0205e-01, -1.1579e+00,  1.2806e+00, -1.0540e+00,  8.9494e-01,\n            3.0510e-01, -6.6784e-01,  8.8002e-01, -3.6468e-02,  5.2952e-01,\n           -1.7799e+00, -1.9028e-01, -9.7610e-01, -5.7426e-02, -1.9799e-01,\n           -5.4154e-01, -8.4982e-01, -4.5247e-01, -1.2761e+00,  8.8394e-02],\n          [-9.3424e-01,  2.4683e-01,  3.5292e-01, -5.5899e-01,  1.4619e+00,\n            4.4610e-01,  3.1339e-01,  1.1767e+00, -1.0335e+00, -5.1410e-01,\n           -7.6170e-01, -5.9573e-01, -9.9548e-02,  2.7749e-01, -5.5345e-01,\n           -2.3353e-01, -8.5598e-01, -1.5673e-01, -8.0367e-01,  3.3628e-01,\n           -1.2531e+00,  1.1158e+00, -1.3542e+00,  1.2556e+00, -3.4276e-01,\n           -7.2869e-02,  4.3526e-01, -8.2815e-01, -2.7710e-01, -2.9537e-03],\n          [-1.5033e+00,  6.0559e-01, -3.0092e-01, -4.7651e-01,  1.4071e+00,\n            8.7028e-01, -5.2984e-01,  1.8092e-01, -2.8117e-01,  6.3277e-01,\n            1.4870e+00, -5.2491e-01,  6.9497e-01, -1.7316e+00,  4.3241e-01,\n            6.0026e-01,  1.6498e+00,  5.8737e-01, -3.6811e-01, -1.8356e-01,\n            4.8118e-01,  3.9321e-02, -1.3612e-01, -1.3162e+00,  4.4687e-02,\n            7.1617e-01,  1.2122e+00,  7.3782e-01, -7.3498e-01, -1.5080e+00],\n          [ 1.8114e-01, -1.3640e+00, -1.5445e+00, -3.5225e-02, -6.0833e-01,\n           -2.7582e-03, -1.1968e+00,  3.6385e-02, -1.2569e+00, -1.0819e+00,\n           -1.1079e-01,  3.4995e-01,  6.0189e-01, -1.5076e+00, -5.3596e-01,\n           -9.2470e-01, -3.7850e-01,  3.9207e-01,  3.4938e-01, -6.5219e-01,\n           -7.8765e-01, -1.2242e+00,  1.3162e+00,  2.2380e-01,  3.5881e-01,\n           -4.5225e-01,  1.4898e+00,  7.0685e-02, -8.4572e-01,  7.9501e-01],\n          [-1.9282e+00, -3.5938e-01,  9.5819e-01, -8.0434e-01, -5.2165e-01,\n            9.8847e-01,  4.7800e-02, -9.5599e-01, -1.0670e+00, -1.0393e+00,\n            1.6830e-01, -1.1973e+00, -3.5774e-01,  3.5958e-01,  8.8109e-02,\n           -5.6943e-01, -1.5741e+00, -7.1429e-01,  1.5222e+00, -2.5214e+00,\n            5.0864e-01, -1.3099e-01, -7.4801e-01, -7.5849e-01,  7.4846e-01,\n            4.6266e-02, -5.5792e-02, -1.8876e+00, -6.4001e-01, -9.6016e-01],\n          [ 8.6637e-01, -4.3006e-01, -2.0473e-02, -8.0557e-01,  1.9024e+00,\n            1.5728e+00,  1.9195e+00,  4.1261e-01,  1.0003e+00, -2.0697e+00,\n            1.9188e-01, -1.3386e+00,  5.7490e-01,  1.4146e+00,  1.0152e+00,\n           -1.0107e+00,  6.3837e-01, -4.2876e-01, -9.3162e-01, -7.5924e-01,\n           -6.2158e-01, -4.6977e-01, -1.0402e+00, -1.0903e+00,  2.0703e-01,\n            5.2227e-01, -1.0008e-01,  1.0941e+00, -1.0285e-01, -5.5771e-01],\n          [-6.9071e-01,  1.0253e+00,  1.2597e+00,  1.8047e+00,  8.8422e-01,\n            1.9786e-03, -2.5226e-01, -2.6519e-01,  1.0348e+00, -1.2693e+00,\n            5.7498e-01,  6.1501e-01,  4.6000e-01,  9.1103e-02, -1.7940e+00,\n            4.4524e-01, -4.5273e-01, -5.3288e-01, -5.6014e-01,  2.2539e-01,\n           -1.8770e-01,  3.7890e-01,  9.0170e-01, -6.9655e-01,  5.0532e-01,\n            5.5349e-01,  2.7635e-01,  1.0069e-01, -2.1066e+00,  3.2494e-01],\n          [ 1.4173e+00, -2.7029e-02, -1.8026e-01,  8.2028e-01,  2.7488e-01,\n            6.6546e-01, -2.8938e-01, -5.7687e-01, -7.1585e-01,  4.1879e-02,\n            9.5702e-01, -1.0773e+00, -2.0382e+00, -3.3324e-01,  1.2392e+00,\n           -5.9545e-01,  7.3650e-01,  4.0073e-01, -1.1140e-01,  2.7177e-01,\n            2.0131e+00, -2.5552e-01,  6.6196e-01, -2.9545e-02,  1.3205e+00,\n           -2.1027e-02,  1.0108e+00, -1.9630e-01,  1.3420e+00, -1.2159e+00],\n          [-1.3198e+00,  8.4190e-01, -1.4506e+00, -6.0960e-02, -3.6308e-01,\n            3.2975e-02,  1.5215e+00,  8.7675e-01,  1.6214e+00,  1.1585e+00,\n            5.5578e-01, -2.4720e+00, -1.3072e+00,  6.4752e-01,  1.5840e-01,\n            1.1498e+00, -3.9016e-03,  2.1338e-01,  8.2218e-01,  1.0692e+00,\n           -1.3653e+00,  7.7464e-02,  7.1464e-01, -3.1771e-01,  7.0452e-02,\n            2.3986e+00,  1.6808e-01, -2.0478e-01,  9.6582e-01, -1.4479e-01],\n          [ 1.7160e+00,  4.7124e-01, -1.0706e+00, -7.8765e-01,  8.6559e-01,\n            6.4096e-01,  7.3853e-01,  1.0671e+00, -8.5924e-01, -1.0711e+00,\n            1.9116e-01,  1.0437e+00,  5.8456e-01, -6.6964e-01,  8.6480e-01,\n           -2.0077e+00, -5.5895e-01, -2.0923e+00,  1.0274e+00,  9.2495e-01,\n           -3.5883e-01, -3.1422e-01,  7.9807e-01, -2.4332e+00, -1.5224e+00,\n            1.3294e+00, -8.3847e-02, -2.2235e-01, -1.3306e+00,  1.0363e+00],\n          [ 2.2554e+00,  5.0563e-01, -3.4591e-01, -4.6970e-01, -2.2675e-02,\n            1.0025e-01, -1.0533e+00,  3.8413e-02,  9.2414e-01,  7.2678e-01,\n           -1.9199e+00, -1.0062e+00, -4.4478e-01, -8.1293e-01,  5.8870e-01,\n           -6.4230e-01, -9.1458e-01,  4.9838e-01, -5.6151e-01, -2.9798e-01,\n            1.3290e+00,  1.1014e+00,  1.3343e+00,  1.1199e-01, -3.1129e-01,\n           -5.4131e-01,  1.8638e+00, -1.4065e+00, -1.4215e-01,  6.7685e-01],\n          [ 1.3253e+00,  2.0478e-01, -3.1078e-02, -3.9148e-01, -1.4760e+00,\n           -1.7693e-01,  9.8590e-01, -2.9813e-01,  2.0205e+00, -8.1840e-01,\n           -8.9215e-01,  9.6097e-01,  1.1250e+00, -1.0466e+00, -6.6874e-01,\n            1.2614e-01, -5.3869e-01, -3.3427e-01,  8.1850e-01,  3.3937e-01,\n           -4.5087e-01,  1.3787e-01, -6.0957e-01,  1.7273e-01, -1.8168e+00,\n           -3.2618e-01, -5.3163e-02,  1.0176e+00,  1.0967e+00, -2.7131e-01],\n          [-1.7094e+00, -1.5001e-01, -1.9314e+00, -1.1023e+00,  3.2587e-01,\n            2.6805e-01, -5.8648e-01,  1.4955e+00, -7.1699e-01,  1.8435e-01,\n           -7.4012e-02, -1.3484e-01,  2.8404e-02,  2.8994e-01,  5.1803e-01,\n           -4.9181e-01, -1.5606e+00,  4.1830e-01, -1.2294e+00, -1.5968e+00,\n           -5.9425e-01,  1.0331e-01, -3.8498e-01,  1.3881e+00, -2.4798e-01,\n           -1.9637e-01,  1.3620e+00,  1.0632e+00,  8.1194e-01,  1.5431e-01],\n          [ 5.6175e-01, -1.3558e-01,  2.3640e-01, -6.2095e-01, -1.1416e+00,\n           -3.2528e-01, -3.1791e-01, -5.9153e-01,  6.0538e-01,  1.1168e+00,\n            2.6976e+00, -4.4282e-01,  3.7031e-03,  4.6405e-02, -2.5509e-01,\n           -1.6209e-01, -2.1660e+00, -1.9956e-01, -1.3872e+00, -4.8086e-02,\n           -6.4662e-01, -7.8266e-01,  1.5485e-01, -2.0015e+00, -3.2210e-01,\n            4.0676e-01,  2.3165e+00, -2.9975e-01,  2.2438e-01,  5.9673e-01],\n          [ 9.0480e-01, -5.3629e-01, -1.9169e-01, -1.5025e+00, -1.8909e-01,\n            1.5905e+00, -1.5759e+00,  3.7532e-01,  4.7385e-01,  1.4174e+00,\n            2.7628e-01,  2.3792e-01, -1.4083e+00, -6.2267e-01,  1.9990e+00,\n           -4.1154e-01, -7.4802e-01,  1.1457e+00,  1.3292e+00,  9.4795e-01,\n            6.7495e-02, -7.1696e-01, -1.3995e+00, -1.6536e-01, -1.0504e+00,\n            2.1942e+00,  1.1264e+00,  2.2021e-01,  4.5790e-02,  1.1120e+00]]]]), tensor([[[[7, 5, 5, 8, 6, 0, 4, 3, 9, 4, 1, 6, 4, 6, 3, 5, 4, 7, 3, 0, 5, 5, 6,\n           1, 7, 1, 2, 6, 9, 6],\n          [1, 6, 3, 2, 0, 4, 6, 0, 8, 3, 2, 0, 2, 9, 4, 6, 8, 5, 6, 0, 9, 4, 9,\n           2, 2, 7, 2, 0, 7, 3],\n          [6, 5, 6, 5, 1, 5, 1, 8, 6, 7, 9, 8, 4, 7, 8, 6, 1, 8, 5, 0, 0, 8, 4,\n           7, 4, 1, 5, 1, 6, 7],\n          [5, 5, 1, 9, 6, 3, 5, 5, 5, 4, 5, 3, 7, 9, 6, 5, 1, 1, 9, 7, 3, 1, 1,\n           4, 2, 1, 4, 4, 4, 7],\n          [6, 8, 5, 7, 7, 6, 0, 7, 4, 6, 5, 8, 8, 6, 6, 3, 6, 5, 5, 1, 8, 3, 9,\n           2, 4, 9, 7, 7, 0, 6],\n          [8, 5, 4, 3, 9, 6, 3, 8, 3, 2, 8, 8, 6, 8, 6, 8, 3, 6, 1, 4, 3, 2, 6,\n           6, 5, 1, 4, 1, 0, 2],\n          [9, 6, 5, 9, 0, 7, 6, 9, 3, 0, 3, 0, 0, 0, 3, 3, 9, 8, 4, 5, 8, 2, 8,\n           2, 4, 5, 5, 4, 6, 1],\n          [7, 4, 9, 0, 2, 1, 6, 3, 7, 9, 0, 6, 3, 7, 5, 4, 3, 5, 8, 6, 6, 9, 6,\n           5, 6, 4, 1, 9, 7, 7],\n          [0, 3, 4, 3, 8, 0, 6, 6, 4, 5, 9, 3, 2, 5, 2, 2, 6, 1, 9, 8, 7, 4, 3,\n           1, 4, 5, 9, 4, 9, 8],\n          [3, 7, 8, 6, 9, 1, 7, 1, 2, 1, 4, 0, 9, 1, 6, 1, 7, 9, 4, 7, 1, 1, 9,\n           4, 3, 0, 4, 4, 0, 2],\n          [0, 2, 5, 0, 9, 1, 9, 6, 2, 0, 3, 4, 0, 5, 1, 9, 2, 3, 8, 0, 8, 3, 1,\n           8, 0, 8, 5, 0, 7, 2],\n          [8, 8, 4, 7, 6, 8, 5, 4, 8, 7, 8, 4, 8, 9, 6, 6, 7, 5, 6, 4, 0, 0, 3,\n           0, 8, 3, 7, 5, 3, 2],\n          [8, 0, 9, 0, 0, 2, 0, 7, 8, 2, 3, 6, 8, 2, 0, 7, 3, 6, 8, 5, 6, 2, 6,\n           3, 3, 3, 9, 9, 9, 1],\n          [2, 2, 4, 4, 3, 1, 8, 6, 9, 1, 0, 9, 9, 5, 5, 5, 1, 3, 5, 0, 3, 6, 5,\n           5, 4, 5, 7, 7, 0, 5],\n          [5, 0, 6, 5, 0, 0, 8, 9, 2, 5, 6, 0, 7, 3, 7, 0, 3, 2, 0, 9, 0, 4, 4,\n           2, 7, 6, 8, 3, 5, 6],\n          [0, 8, 4, 9, 0, 3, 8, 7, 3, 0, 6, 8, 7, 9, 9, 6, 1, 0, 0, 0, 8, 0, 8,\n           3, 0, 4, 3, 7, 2, 0],\n          [3, 6, 8, 4, 9, 1, 2, 3, 0, 3, 4, 7, 2, 9, 2, 8, 1, 4, 2, 8, 2, 4, 6,\n           3, 6, 0, 1, 8, 0, 7],\n          [8, 5, 4, 6, 5, 9, 6, 3, 7, 2, 2, 1, 0, 0, 6, 8, 4, 4, 1, 6, 0, 2, 7,\n           3, 7, 0, 5, 1, 6, 2],\n          [9, 6, 0, 3, 3, 4, 4, 4, 4, 3, 4, 6, 7, 6, 6, 6, 4, 2, 9, 6, 3, 2, 5,\n           4, 5, 5, 5, 0, 5, 9],\n          [6, 9, 9, 4, 9, 3, 1, 9, 1, 4, 8, 9, 2, 7, 1, 0, 2, 5, 9, 3, 4, 4, 5,\n           8, 8, 7, 7, 7, 1, 9],\n          [4, 8, 1, 1, 9, 2, 9, 1, 8, 0, 5, 3, 4, 1, 3, 1, 4, 5, 1, 9, 8, 3, 7,\n           5, 9, 6, 3, 7, 9, 5],\n          [9, 5, 0, 4, 1, 3, 0, 1, 3, 8, 1, 7, 9, 6, 4, 7, 7, 8, 3, 6, 1, 8, 2,\n           9, 1, 3, 5, 6, 1, 8],\n          [7, 9, 3, 3, 6, 0, 4, 4, 2, 2, 5, 8, 4, 6, 9, 2, 1, 3, 0, 8, 4, 5, 0,\n           2, 3, 6, 3, 4, 9, 1],\n          [8, 0, 4, 3, 0, 0, 3, 8, 7, 9, 6, 2, 2, 2, 7, 9, 9, 5, 3, 1, 7, 7, 9,\n           2, 3, 9, 9, 6, 3, 7],\n          [2, 5, 2, 8, 2, 5, 7, 4, 6, 8, 4, 9, 6, 1, 2, 2, 3, 7, 9, 9, 4, 7, 7,\n           7, 1, 4, 3, 2, 3, 1],\n          [8, 1, 3, 1, 3, 7, 9, 7, 7, 8, 9, 5, 2, 6, 4, 5, 7, 0, 2, 3, 2, 2, 7,\n           2, 6, 0, 7, 7, 7, 7],\n          [2, 8, 3, 2, 2, 3, 1, 7, 6, 3, 7, 9, 1, 5, 5, 6, 7, 7, 0, 6, 3, 3, 5,\n           3, 0, 8, 4, 7, 4, 6],\n          [0, 2, 9, 9, 8, 5, 9, 0, 8, 7, 8, 0, 9, 6, 4, 3, 9, 7, 6, 8, 3, 0, 7,\n           5, 1, 6, 2, 6, 6, 5],\n          [4, 2, 5, 6, 6, 9, 8, 6, 0, 7, 3, 0, 5, 4, 9, 3, 6, 9, 9, 8, 8, 7, 5,\n           5, 5, 7, 6, 7, 7, 9],\n          [4, 7, 4, 2, 1, 0, 2, 6, 6, 3, 5, 5, 1, 1, 6, 1, 9, 8, 5, 1, 4, 9, 0,\n           6, 1, 8, 4, 8, 1, 1]]]]), ['task_1']]\nINFO     benchmark:benchmark.py:149 Batch 1: CPU Usage: 0.0%, Memory Usage: 79.9%\nDEBUG    benchmark:benchmark.py:157 Invoking the model with inputs and attention_mask\nINFO     benchmark:benchmark.py:194 Total Time: 0.0002 seconds, Grids per Second: 6132.02\nINFO     benchmark:benchmark.py:226 Total Time: 0.0002 seconds, Grids per Second: 6132.02\nINFO     benchmark:benchmark.py:256 Improvement in average total time: -1.6389 seconds (99.99%)\nINFO     benchmark:benchmark.py:261 Improvement in average grids per second: +5932.75 (2977.24%)\nINFO     benchmark:benchmark.py:272 The improvement in average total time is practically significant.\nINFO     benchmark:benchmark.py:283 The improvement in average grids per second is practically significant.\nINFO     benchmark:benchmark.py:296 T-Test for total time: t-statistic = nan, p-value = nan\nINFO     benchmark:benchmark.py:297 T-Test for grids per second: t-statistic = nan, p-value = nan\nINFO     benchmark:benchmark.py:300 Run Summary:\nINFO     benchmark:benchmark.py:301  \u2022 Avg Total Time: 0.0002s (CI 95%: \u00b1nans)\nINFO     benchmark:benchmark.py:302  \u2022 Avg Grids per Second: 6132.02 (CI 95%: \u00b1nan)\nINFO     benchmark:benchmark.py:303  \u2022 Effect Size (Total Time): nan, Effect Size (Grids per Second): nan",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_benchmark.py"
        }
    ],
    "tests/test_end_to_end.py": [
        {
            "function": "test_end_to_end",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "128",
            "code_snippet": "    pl_trainer.fit(trainer)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538: in fit\n    call._call_and_handle_interrupt(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47: in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574: in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981: in _run\n    results = self._run_stage()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025: in _run_stage\n    self.fit_loop.run()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205: in run\n    self.advance()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363: in advance\n    self.epoch_loop.run(self._data_fetcher)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140: in run\n    self.advance(data_fetcher)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250: in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:190: in run\n    self._optimizer_step(batch_idx, closure)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:268: in _optimizer_step\n    call._call_lightning_module_hook(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:167: in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1306: in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:153: in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:238: in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:122: in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:130: in wrapper\n    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:484: in wrapper\n    out = func(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:89: in _use_grad\n    ret = func(self, *args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/adam.py:205: in step\n    loss = closure()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:108: in _wrap_closure\n    closure_result = closure()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:144: in __call__\n    self._result = self.closure(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116: in decorate_context\n    return func(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:129: in closure\n    step_output = self._step_fn()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:317: in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319: in _call_strategy_hook\n    output = fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:390: in training_step\n    return self.lightning_module.training_step(*args, **kwargs)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "DEBUG: Starting ARCDataset initialization\nDEBUG: data_source type: <class 'arckit.data.TaskSet'>\nDEBUG: data_source content: <TaskSet: 400 tasks>\nDEBUG: Processed data length: 400\nDEBUG: First item keys: dict_keys(['id', 'train', 'test'])\nDEBUG: First train item: {'input': array([[0, 7, 7],\n       [7, 7, 7],\n       [0, 7, 7]]), 'output': array([[0, 0, 0, 0, 7, 7, 0, 7, 7],\n       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n       [7, 7, 7, 7, 7, 7, 7, 7, 7],\n       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n       [0, 0, 0, 0, 7, 7, 0, 7, 7]])}\nNumber of train samples: 1302\nNumber of test samples: 416\nDEBUG: Initialized self.results['train'] as <class 'dict'>\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 1775\nCorrectly predicted different pixels: 21\nCalculated accuracy: 0.011830985915492958\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 639\nCorrectly predicted different pixels: 11\nCalculated accuracy: 0.017214397496087636\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 1009\nCorrectly predicted different pixels: 7\nCalculated accuracy: 0.006937561942517344\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 948\nCorrectly predicted different pixels: 0\nCalculated accuracy: 0.0\nDifferential pixel accuracy - Input shape: torch.Size([2, 1, 30, 30]), Target shape: torch.Size([2, 900]), Prediction shape: torch.Size([2, 900])\nReshaped - Input: torch.Size([2, 900]), Target: torch.Size([2, 900]), Prediction: torch.Size([2, 900])\nTotal different pixels: 16\nCorrectly predicted different pixels: 0\nCalculated accuracy: 0.0\nInitial validation results: [{'test_loss': 5.537572860717773, 'test_accuracy': 0.0013931624125689268, 'test_diff_accuracy': 0.00885733962059021}]\nInitial validation accuracy: 0.0013931624125689268, Initial loss: 5.537572860717773",
            "captured_log": "DEBUG    tests.test_end_to_end:test_end_to_end.py:28 Starting end-to-end test\nDEBUG    tests.test_end_to_end:test_end_to_end.py:32 Loading data using arckit\nDEBUG    tests.test_end_to_end:test_end_to_end.py:36 Creating train and validation datasets\nDEBUG    tests.test_end_to_end:test_end_to_end.py:42 Train dataset size: 130, Validation dataset size: 130\nDEBUG    tests.test_end_to_end:test_end_to_end.py:81 Initializing model\nDEBUG    src.models.gpt2:gpt2.py:23 Initialized Attention with n_embd=64, n_head=2\nDEBUG    src.models.gpt2:gpt2.py:51 Initialized FeedForward with n_embd=64\nDEBUG    src.models.gpt2:gpt2.py:69 Initialized TransformerBlock with n_embd=64, n_head=2\nDEBUG    tests.test_end_to_end:test_end_to_end.py:84 Model initialized with config: ModelConfig(n_embd=64, n_head=2, n_layer=1, dropout=0.1)\nDEBUG    tests.test_end_to_end:test_end_to_end.py:94 Initializing trainer\nDEBUG    tests.test_end_to_end:test_end_to_end.py:100 Trainer initialized with config: Config(model=ModelConfig(n_embd=64, n_head=2, n_layer=1, dropout=0.1), training=TrainingConfig(batch_size=32, learning_rate=0.0001, max_epochs=2, use_gpu=True, log_level='INFO'))\nDEBUG    tests.test_end_to_end:test_end_to_end.py:103 Creating PyTorch Lightning trainer\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 GPU available: True (mps), used: True\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 TPU available: False, using: 0 TPU cores\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 HPU available: False, using: 0 HPUs\nDEBUG    tests.test_end_to_end:test_end_to_end.py:113 PyTorch Lightning trainer created\nINFO     tests.test_end_to_end:test_end_to_end.py:116 Evaluating model before training\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:113 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:123 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:113 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:123 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:113 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:123 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:113 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:123 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:113 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:123 Task IDs in batch: ['task_0', 'task_1']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([2, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([2, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([2, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:118 Initial validation results: [{'test_loss': 5.537572860717773, 'test_accuracy': 0.0013931624125689268, 'test_diff_accuracy': 0.00885733962059021}]\nINFO     tests.test_end_to_end:test_end_to_end.py:125 Initial validation accuracy: 0.0013931624125689268, Initial loss: 5.537572860717773\nDEBUG    tests.test_end_to_end:test_end_to_end.py:127 Starting model training\nINFO     pytorch_lightning.callbacks.model_summary:model_summary.py:104 \n  | Name  | Type    | Params | Mode",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_end_to_end.py"
        }
    ],
    "tests/test_integration_experiment.py": [
        {
            "function": "test_full_experiment_run",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "62",
            "code_snippet": "    pl_trainer.fit(trainer)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538: in fit\n    call._call_and_handle_interrupt(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47: in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574: in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981: in _run\n    results = self._run_stage()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025: in _run_stage\n    self.fit_loop.run()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205: in run\n    self.advance()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363: in advance\n    self.epoch_loop.run(self._data_fetcher)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140: in run\n    self.advance(data_fetcher)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250: in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:190: in run\n    self._optimizer_step(batch_idx, closure)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:268: in _optimizer_step\n    call._call_lightning_module_hook(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:167: in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1306: in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:153: in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:238: in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:122: in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:130: in wrapper\n    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:484: in wrapper\n    out = func(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:89: in _use_grad\n    ret = func(self, *args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/adam.py:205: in step\n    loss = closure()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:108: in _wrap_closure\n    closure_result = closure()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:144: in __call__\n    self._result = self.closure(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116: in decorate_context\n    return func(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:129: in closure\n    step_output = self._step_fn()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:317: in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319: in _call_strategy_hook\n    output = fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:390: in training_step\n    return self.lightning_module.training_step(*args, **kwargs)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "",
            "captured_log": "INFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 GPU available: True (mps), used: True\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 TPU available: False, using: 0 TPU cores\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 HPU available: False, using: 0 HPUs\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\nINFO     pytorch_lightning.callbacks.model_summary:model_summary.py:104 \n  | Name  | Type    | Params | Mode",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_integration_experiment.py"
        },
        {
            "function": "test_model_convergence_issue",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "92",
            "code_snippet": "    pl_trainer.fit(trainer)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:538: in fit\n    call._call_and_handle_interrupt(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47: in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:574: in _fit_impl\n    self._run(model, ckpt_path=ckpt_path)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981: in _run\n    results = self._run_stage()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1025: in _run_stage\n    self.fit_loop.run()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:205: in run\n    self.advance()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:363: in advance\n    self.epoch_loop.run(self._data_fetcher)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:140: in run\n    self.advance(data_fetcher)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/training_epoch_loop.py:250: in advance\n    batch_output = self.automatic_optimization.run(trainer.optimizers[0], batch_idx, kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:190: in run\n    self._optimizer_step(batch_idx, closure)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:268: in _optimizer_step\n    call._call_lightning_module_hook(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:167: in _call_lightning_module_hook\n    output = fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/core/module.py:1306: in optimizer_step\n    optimizer.step(closure=optimizer_closure)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/core/optimizer.py:153: in step\n    step_output = self._strategy.optimizer_step(self._optimizer, closure, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:238: in optimizer_step\n    return self.precision_plugin.optimizer_step(optimizer, model=model, closure=closure, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:122: in optimizer_step\n    return optimizer.step(closure=closure, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:130: in wrapper\n    return func.__get__(opt, opt.__class__)(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:484: in wrapper\n    out = func(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/optimizer.py:89: in _use_grad\n    ret = func(self, *args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/optim/adam.py:205: in step\n    loss = closure()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/plugins/precision/precision.py:108: in _wrap_closure\n    closure_result = closure()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:144: in __call__\n    self._result = self.closure(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116: in decorate_context\n    return func(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:129: in closure\n    step_output = self._step_fn()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/optimization/automatic.py:317: in _training_step\n    training_step_output = call._call_strategy_hook(trainer, \"training_step\", *kwargs.values())\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:319: in _call_strategy_hook\n    output = fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py:390: in training_step\n    return self.lightning_module.training_step(*args, **kwargs)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "",
            "captured_log": "INFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 GPU available: True (mps), used: True\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 TPU available: False, using: 0 TPU cores\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 HPU available: False, using: 0 HPUs\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.\nINFO     pytorch_lightning.callbacks.model_summary:model_summary.py:104 \n  | Name  | Type    | Params | Mode",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_integration_experiment.py"
        }
    ],
    "tests/test_train.py": [
        {
            "function": "test_logging",
            "error_type": "RecursionError",
            "error_details": "RecursionError: maximum recursion depth exceeded",
            "line_number": "169",
            "code_snippet": "    main(mock_args)\nsrc/training/train.py:85: in main\n    train_loader = DataLoader(train_data, batch_size=args.batch_size, num_workers=7)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1141: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call\n    result = effect(*args, **kwargs)\ntests/test_train.py:150: in <lambda>\n    mock_dataloader.side_effect = lambda *args, **kwargs: torch.utils.data.DataLoader(*args, **{**kwargs, \"num_workers\": 0})\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1141: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call\n    result = effect(*args, **kwargs)\ntests/test_train.py:150: in <lambda>\n    mock_dataloader.side_effect = lambda *args, **kwargs: torch.utils.data.DataLoader(*args, **{**kwargs, \"num_workers\": 0})\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n!!! Recursion detected (same locals & position)",
            "captured_output": "Entering test_logging\nDEBUG: Initialized self.results['train'] as <class 'dict'>\nExperimentTracker initialized with config: {\n  \"model\": {\n    \"n_embd\": 96,\n    \"n_head\": 3,\n    \"n_layer\": 1,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 32,\n    \"learning_rate\": 0.0001,\n    \"max_epochs\": 10,\n    \"use_gpu\": true,\n    \"log_level\": \"INFO\"\n  }\n}\nProject: test_project, Entity: None\nuse_wandb: False",
            "captured_log": "INFO     gpt2_arc.src.training.train:train.py:42 Data loaded successfully using arckit\nINFO     gpt2_arc.src.training.train:train.py:44 Initializing model with new configuration\nINFO     gpt2_arc.src.training.train:train.py:48 Initializing trainer with new configuration",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_fit_call",
            "error_type": "RecursionError",
            "error_details": "RecursionError: maximum recursion depth exceeded",
            "line_number": "203",
            "code_snippet": "    main(mock_args)\nsrc/training/train.py:85: in main\n    train_loader = DataLoader(train_data, batch_size=args.batch_size, num_workers=7)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1141: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call\n    result = effect(*args, **kwargs)\ntests/test_train.py:185: in <lambda>\n    mock_dataloader.side_effect = lambda *args, **kwargs: torch.utils.data.DataLoader(*args, **{**kwargs, \"num_workers\": 0})\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1141: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call\n    result = effect(*args, **kwargs)\ntests/test_train.py:185: in <lambda>\n    mock_dataloader.side_effect = lambda *args, **kwargs: torch.utils.data.DataLoader(*args, **{**kwargs, \"num_workers\": 0})\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n!!! Recursion detected (same locals & position)",
            "captured_output": "Entering test_fit_call\nDEBUG: Initialized self.results['train'] as <class 'dict'>\nExperimentTracker initialized with config: {\n  \"model\": {\n    \"n_embd\": 96,\n    \"n_head\": 3,\n    \"n_layer\": 1,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 32,\n    \"learning_rate\": 0.0001,\n    \"max_epochs\": 10,\n    \"use_gpu\": true,\n    \"log_level\": \"INFO\"\n  }\n}\nProject: test_project, Entity: None\nuse_wandb: False",
            "captured_log": "INFO     gpt2_arc.src.training.train:train.py:42 Data loaded successfully using arckit\nINFO     gpt2_arc.src.training.train:train.py:44 Initializing model with new configuration\nINFO     gpt2_arc.src.training.train:train.py:48 Initializing trainer with new configuration",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_gpu_not_available",
            "error_type": "AttributeError",
            "error_details": "AttributeError: module 'gpt2_arc.src.training.train' has no attribute 'ARCTrainergpt2_arc'",
            "line_number": "310",
            "code_snippet": "    with patch(\"torch.cuda.is_available\", return_value=False), patch(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1445: in __enter__\n    self.target = self.getter()\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pkgutil.py:528: in resolve_name\n    result = getattr(result, p)",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_valid_learning_rates",
            "error_type": "NameError",
            "error_details": "NameError: name 'mock_ARCTrainer' is not defined\nFalsifying example: test_valid_learning_rates(\nmock_args=Namespace(train_data='mock_train_data.json', val_data='mock_val_data.json', batch_size=32, learning_rate=1e-05, max_epochs=10, use_gpu=False, no_logging=False, no_checkpointing=False, no_progress_bar=False, log_level='INFO', fast_dev_run=False, project='test_project'),\nlearning_rate=1.0,  # or any other generated value\n)",
            "line_number": "369",
            "code_snippet": "    @given(\ntests/test_train.py:389: in test_valid_learning_rates\n    mock_trainer_instance = mock_ARCTrainer.return_value",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_tensorboard_logging",
            "error_type": "RecursionError",
            "error_details": "RecursionError: maximum recursion depth exceeded",
            "line_number": "481",
            "code_snippet": "    main(mock_args)\nsrc/training/train.py:85: in main\n    train_loader = DataLoader(train_data, batch_size=args.batch_size, num_workers=7)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1141: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call\n    result = effect(*args, **kwargs)\ntests/test_train.py:480: in <lambda>\n    mock_dataloader.side_effect = lambda *args, **kwargs: torch.utils.data.DataLoader(*args, **{**kwargs, \"num_workers\": 0})\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1141: in _mock_call\n    return self._execute_mock_call(*args, **kwargs)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1202: in _execute_mock_call\n    result = effect(*args, **kwargs)\ntests/test_train.py:480: in <lambda>\n    mock_dataloader.side_effect = lambda *args, **kwargs: torch.utils.data.DataLoader(*args, **{**kwargs, \"num_workers\": 0})\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1137: in __call__\n    return self._mock_call(*args, **kwargs)\n!!! Recursion detected (same locals & position)",
            "captured_output": "DEBUG: Initialized self.results['train'] as <class 'dict'>\nExperimentTracker initialized with config: {\n  \"model\": {\n    \"n_embd\": 96,\n    \"n_head\": 3,\n    \"n_layer\": 1,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 32,\n    \"learning_rate\": 0.0001,\n    \"max_epochs\": 10,\n    \"use_gpu\": true,\n    \"log_level\": \"INFO\"\n  }\n}\nProject: test_project, Entity: None\nuse_wandb: False",
            "captured_log": "INFO     gpt2_arc.src.training.train:train.py:42 Data loaded successfully using arckit\nINFO     gpt2_arc.src.training.train:train.py:44 Initializing model with new configuration\nINFO     gpt2_arc.src.training.train:train.py:48 Initializing trainer with new configuration",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_arctrainer_training_step",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "510",
            "code_snippet": "    loss = trainer.training_step(batch, 0)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_validation_step_with_correct_batch_format",
            "error_type": "RuntimeError",
            "error_details": "RuntimeError: You provided multiple `val_dataloader`, but no `dataloader_idx` argument in `ARCTrainer.validation_step()`. Try adding `dataloader_idx=0` to its signature.",
            "line_number": "533",
            "code_snippet": "    pl_trainer.validate(trainer, dataloaders=[batch])\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:639: in validate\n    return call._call_and_handle_interrupt(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py:47: in _call_and_handle_interrupt\n    return trainer_fn(*args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:679: in _validate_impl\n    results = self._run(model, ckpt_path=ckpt_path)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:981: in _run\n    results = self._run_stage()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py:1018: in _run_stage\n    return self._evaluation_loop.run()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:178: in _decorator\n    return loop_run(self, *args, **kwargs)\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:114: in run\n    self.on_run_start()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:242: in on_run_start\n    self._verify_dataloader_idx_requirement()\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py:459: in _verify_dataloader_idx_requirement\n    _verify_dataloader_idx_requirement(\n../.venv/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py:200: in _verify_dataloader_idx_requirement\n    raise RuntimeError(",
            "captured_output": "",
            "captured_log": "INFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 GPU available: True (mps), used: True\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 TPU available: False, using: 0 TPU cores\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 HPU available: False, using: 0 HPUs\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 Running in `fast_dev_run` mode: will run the requested loop using 1 batch(es). Logging and checkpointing is suppressed.",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_arctrainer_batch_format",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "558",
            "code_snippet": "    loss = trainer.training_step(batch, 0)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_arctrainer_batch_format",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "558",
            "code_snippet": "    loss = trainer.training_step(batch, 0)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        }
    ],
    "/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py": [
        {
            "function": "assert_called_once",
            "error_type": "AssertionError",
            "error_details": "AssertionError: Expected 'fit' to have been called once. Called 0 times.\nAssertionError: Expected 'fit' to have been called once. Called 0 times.",
            "line_number": "926",
            "code_snippet": "    raise AssertionError(msg)\n\nDuring handling of the above exception, another exception occurred:\ntests/test_train.py:447: in test_end_to_end_training\n    mock_trainer.return_value.fit.assert_called_once()",
            "captured_output": "DEBUG: Initialized self.results['train'] as <class 'dict'>\nExperimentTracker initialized with config: {\n  \"model\": {\n    \"n_embd\": 96,\n    \"n_head\": 3,\n    \"n_layer\": 1,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 32,\n    \"learning_rate\": 0.0001,\n    \"max_epochs\": 10,\n    \"use_gpu\": true,\n    \"log_level\": \"INFO\"\n  }\n}\nProject: test_project, Entity: None\nuse_wandb: False\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13106005920'>, step=0",
            "captured_log": "INFO     gpt2_arc.src.training.train:train.py:42 Data loaded successfully using arckit\nINFO     gpt2_arc.src.training.train:train.py:44 Initializing model with new configuration\nINFO     gpt2_arc.src.training.train:train.py:48 Initializing trainer with new configuration",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        }
    ],
    "tests/test_trainer.py": [
        {
            "function": "test_training_step_with_list_input",
            "error_type": "NameError",
            "error_details": "NameError: name 'context' is not defined",
            "line_number": "110",
            "code_snippet": "    loss = trainer.training_step(batch, 0)\nsrc/training/trainer.py:58: in training_step\n    self.results_collector.update_train_metrics(self.current_epoch, {\"loss\": loss.item()})\nsrc/utils/results_collector.py:40: in update_train_metrics\n    print(f\"DEBUG: {context} - self.results['train'] is of type {type(self.results['train'])}\")",
            "captured_output": "DEBUG: Initialized self.results['train'] as <class 'dict'>",
            "captured_log": "DEBUG    src.models.gpt2:gpt2.py:23 Initialized Attention with n_embd=64, n_head=2\nDEBUG    src.models.gpt2:gpt2.py:51 Initialized FeedForward with n_embd=64\nDEBUG    src.models.gpt2:gpt2.py:69 Initialized TransformerBlock with n_embd=64, n_head=2\nDEBUG    src.training.trainer:trainer.py:37 Training step - Batch type: <class 'tuple'>, length: 3\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([2, 900]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([2, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([2, 900, 64])",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_trainer.py"
        }
    ]
}