{
    "tests/test_benchmark.py": [
        {
            "function": "test_benchmark_model_model_error",
            "error_type": "Failed",
            "error_details": "Failed: DID NOT RAISE <class 'RuntimeError'>",
            "line_number": "216",
            "code_snippet": "    with pytest.raises(RuntimeError, match=\"Model execution failed\"):",
            "captured_output": "DEBUG: Invoking benchmark_model\nStarting benchmark_model with parameters: batch_size=1, num_batches=1, device_type=cpu, precision=medium, model_checkpoint=None\nCollating batch of size: 1\nInput shapes: [torch.Size([1, 30, 30])]\nOutput shapes: [torch.Size([1, 30, 30])]\nMax dimensions: height=30, width=30\nPadded input shape: torch.Size([1, 1, 30, 30])\nPadded output shape: torch.Size([1, 1, 30, 30])\nProcessing batch 1/1\nInputs type: <class 'torch.Tensor'>\nInputs shape: torch.Size([1, 1, 30, 30])\nOutputs type: <class 'torch.Tensor'>, shape: torch.Size([1, 1, 30, 30])\nTask IDs: ['task_1']\nInputs shape: torch.Size([1, 1, 30, 30]), Outputs shape: torch.Size([1, 1, 30, 30]), Task IDs: ['task_1']\nBatch time: 0.0001621246337890625\nCollating batch of size: 1\nInput shapes: [torch.Size([1, 30, 30])]\nOutput shapes: [torch.Size([1, 30, 30])]\nMax dimensions: height=30, width=30\nPadded input shape: torch.Size([1, 1, 30, 30])\nPadded output shape: torch.Size([1, 1, 30, 30])\nBenchmark completed. Total time: 0.0001621246337890625, Total grids: 1\nBenchmark completed. Final results - avg_time: 0.0001621246337890625, avg_grids: 6168.094117647059",
            "captured_log": "DEBUG    benchmark:benchmark.py:106 Batch content before unpacking: [tensor([[[[-1.3831e+00, -5.4992e-01, -1.4592e+00, -1.5366e+00,  1.7143e+00,\n           -1.1907e-01, -1.2863e+00, -3.4837e-01, -1.0802e+00,  2.2947e-01,\n            6.2989e-01,  2.9892e-01, -2.7381e-01, -9.7792e-01, -1.2434e+00,\n            1.2732e-01,  1.7348e+00, -1.4213e-01,  1.9327e+00,  8.9349e-01,\n           -4.2993e-01, -1.9245e+00,  2.3543e-01, -6.2826e-01,  1.4804e+00,\n            2.5302e-01,  2.9593e-01,  4.9203e-01, -4.5769e-02, -3.9542e-01],\n          [-1.2734e-01, -2.7092e+00, -1.8087e+00, -1.8739e-01,  1.2189e+00,\n            4.4236e-01,  8.2667e-02, -1.3303e+00,  3.2711e-01, -1.6807e-01,\n           -1.6339e+00, -1.8599e+00,  6.3290e-01,  3.1893e-02, -3.8493e-01,\n            4.9047e-01,  9.2785e-01,  5.6860e-01,  6.4792e-01, -4.2344e-01,\n            8.6769e-01, -1.2916e-02, -1.6461e+00, -3.0097e-01, -1.2523e+00,\n           -2.9846e-02,  1.3554e+00, -7.7249e-01, -1.4582e-01,  3.3433e-01],\n          [ 2.7715e-01,  1.1665e-01,  2.0552e+00,  9.0498e-02, -5.5507e-01,\n           -9.1263e-01, -5.7194e-01, -1.9222e+00,  9.9805e-01, -6.9380e-01,\n           -1.3782e-01,  5.9406e-01,  6.2032e-01, -5.0776e-01,  5.8199e-02,\n           -8.8428e-01, -2.4954e-02,  5.6936e-01, -6.6317e-01,  8.3478e-01,\n           -1.1427e-01,  8.2811e-01, -4.5877e-02,  2.3542e-01,  2.9423e-01,\n           -6.0698e-01,  4.6834e-01, -1.1257e-01, -4.6230e-01,  3.3160e-01],\n          [ 4.0281e-01,  9.9303e-01, -2.1286e-01,  7.9770e-01, -8.1945e-01,\n            9.1413e-01, -6.7503e-01,  3.0823e-01,  1.7733e+00, -1.8314e-02,\n           -1.2373e-01,  7.2175e-01, -2.5809e-01,  3.7376e+00,  4.8301e-01,\n           -6.3854e-01,  3.1131e-01,  1.2117e+00, -2.7198e-01, -5.2744e-01,\n            1.1254e+00,  8.3226e-01, -1.4253e-01,  3.9498e-01,  1.2988e-01,\n            4.7025e-01, -1.0161e+00, -1.6208e+00,  5.5415e-01,  1.7845e+00],\n          [ 8.8959e-01,  1.4620e+00, -2.6293e-01,  2.4379e-01, -1.6803e+00,\n            1.5255e+00,  1.5921e+00,  1.3134e+00,  7.4986e-01, -1.7766e+00,\n            1.4211e+00,  1.5950e+00, -7.4086e-01,  5.0769e-01,  8.7882e-01,\n            1.5124e+00,  1.2490e+00, -2.3823e+00,  1.0841e-01, -2.1352e-01,\n           -1.6687e+00,  1.5613e+00, -2.3007e+00, -3.3308e-01,  5.3450e-01,\n            2.7675e-01, -7.9684e-01,  5.7422e-01, -1.4038e-01,  1.9037e+00],\n          [-3.8863e-01, -1.1239e+00, -1.8362e+00, -3.5695e-01, -9.4186e-02,\n           -8.6979e-01,  1.8900e-01, -4.1921e-01, -1.3318e+00,  1.5997e+00,\n           -8.4888e-01, -4.2991e-01, -2.5144e+00, -6.7305e-01,  8.4277e-01,\n            4.3730e-01,  2.9727e-01,  1.9463e+00, -3.6476e-01, -3.7499e-01,\n           -1.0417e+00, -6.6281e-01,  1.1876e+00,  4.4289e-01, -7.0435e-02,\n           -1.2523e+00,  1.6718e-01, -6.2288e-01, -1.4314e+00, -1.0432e+00],\n          [-9.8430e-01,  7.4699e-01,  1.1045e-01, -1.2370e+00,  1.0406e-01,\n           -1.3244e-01, -6.8538e-02, -1.8370e-01,  1.1296e-01,  1.1124e-01,\n            7.6516e-01,  1.8415e-01,  8.6758e-01, -5.0589e-01,  7.6072e-01,\n            1.4837e-01,  6.4835e-02,  1.0622e+00,  8.3198e-01,  7.2705e-01,\n            9.3812e-01,  1.3822e+00,  8.0809e-01, -8.5156e-01, -1.4970e+00,\n           -1.6693e+00, -5.6871e-01,  6.7045e-01,  1.5447e+00, -9.7695e-01],\n          [-3.9746e-01, -1.0452e+00,  2.7027e+00, -2.1090e-02,  7.7354e-01,\n           -8.4887e-01, -2.1361e-02, -9.5249e-01,  4.0800e-01,  5.8588e-01,\n            1.5086e+00,  6.7990e-01,  4.7739e-01,  1.2846e+00,  1.5038e-01,\n            1.3496e+00,  2.9076e+00, -9.7098e-01,  5.4480e-01,  1.2931e+00,\n           -5.7313e-01, -2.1554e+00, -8.6994e-02, -1.6727e+00,  1.1350e+00,\n           -1.5437e+00, -8.2777e-01, -3.1837e-01, -1.4688e+00,  3.4721e-01],\n          [-7.2425e-01, -1.4085e+00, -6.0569e-01,  9.0310e-02, -8.0076e-01,\n           -1.5402e+00, -8.2194e-01,  4.7330e-01, -3.1955e-01,  1.1873e+00,\n            1.2629e+00, -2.1855e-01, -2.5864e-01, -5.7587e-01, -4.1715e-01,\n           -1.2523e+00,  5.5766e-01,  3.8362e-01, -3.4063e-01, -3.0219e-02,\n           -3.3245e-02, -1.3523e-01, -6.8704e-01,  2.5682e-01, -5.2324e-01,\n           -1.5424e-01, -1.0690e+00,  2.2606e-01,  1.9516e+00,  1.1952e-01],\n          [ 6.4924e-01,  8.6809e-01,  4.6581e-01,  7.0350e-01,  1.4031e+00,\n           -1.0673e+00,  1.5397e-01, -9.5145e-02,  3.4360e-01, -6.4884e-01,\n            1.2515e+00, -4.4031e-01,  1.8393e+00,  6.2274e-01, -6.3850e-01,\n            3.6752e-01, -1.0417e+00, -5.7336e-01, -1.9272e-01, -1.1682e+00,\n            3.6046e-01,  2.1032e+00,  5.2994e-01, -1.2709e+00,  6.3533e-02,\n            1.9534e-01,  1.6564e+00, -1.0592e+00,  5.8566e-01,  8.6328e-01],\n          [-1.6582e+00,  3.3375e-01,  3.1719e-01, -6.0911e-01, -8.8026e-01,\n            1.8540e+00, -1.3280e+00,  1.3029e+00, -8.9974e-01,  9.7115e-01,\n            5.7557e-01, -2.5148e-02, -8.3612e-01, -1.0214e+00,  1.0856e+00,\n           -1.2615e+00, -5.2103e-01, -4.3434e-01,  2.0657e-02,  7.0810e-02,\n            1.5118e+00, -2.0848e+00,  4.6114e-01,  4.5286e-01, -2.8043e-01,\n           -1.9596e+00, -1.0705e+00,  1.7508e+00,  5.2751e-01,  6.1125e-01],\n          [ 1.5792e+00, -5.0014e-01,  3.7872e-01, -1.8020e+00,  1.2934e+00,\n           -5.0376e-01,  3.8758e-01, -1.7831e+00,  9.8858e-02, -5.7452e-01,\n            7.5723e-02,  2.1252e+00, -1.5270e-02, -7.9503e-01,  1.7338e+00,\n           -1.6586e+00, -1.2031e+00,  1.3254e+00,  1.7630e+00,  2.9984e-01,\n            2.4709e-01, -1.4774e+00, -1.3995e+00,  1.1465e+00, -3.3508e-01,\n            2.5798e+00, -8.1028e-01, -5.5743e-01, -7.7231e-02,  4.3672e-01],\n          [ 5.5158e-01, -3.2403e-01,  8.2373e-01,  1.9147e+00,  5.1972e-01,\n           -1.5002e+00,  5.5330e-01, -4.1245e-01,  9.0484e-01, -1.4315e+00,\n           -7.7734e-01,  3.5136e-01, -5.3797e-01, -1.2251e+00, -9.7408e-01,\n           -9.7752e-01,  1.5936e+00,  7.2139e-01,  3.5673e-01,  1.1388e+00,\n           -3.1165e-01, -6.8526e-01, -1.8632e-01,  4.5101e-01, -1.1436e+00,\n            1.3686e-01, -1.4222e+00,  1.6149e+00, -8.0900e-01,  7.0230e-01],\n          [-3.8629e-01, -2.4404e+00, -1.6056e+00, -1.7149e-01,  1.5019e-02,\n            1.7718e+00, -4.1197e-01,  3.2723e-01, -7.0214e-01,  7.4212e-01,\n            9.6873e-01, -3.4530e-01, -4.9707e-01,  6.6158e-01, -5.3344e-01,\n            1.3665e+00, -1.6220e+00,  8.8401e-03, -2.0159e+00, -9.4132e-03,\n           -1.6971e-01, -1.3562e+00,  8.6891e-01,  4.5812e-02, -1.2703e+00,\n            1.3511e+00,  6.9252e-01,  2.5943e-01, -3.7795e-01,  2.8492e-01],\n          [ 8.8597e-01,  1.5964e+00, -8.8734e-01,  1.3003e+00, -3.0512e-01,\n            3.1187e-01, -1.0500e+00, -1.3907e+00, -9.0398e-01, -6.4476e-01,\n           -1.9770e+00, -1.4107e+00, -3.1733e-01, -1.7166e+00, -1.5953e-01,\n            5.0322e-01, -3.4461e-01,  1.9074e+00, -2.0104e+00,  1.5512e+00,\n            6.7285e-01, -4.9539e-01,  5.8940e-01,  4.5301e-01, -3.4820e-01,\n           -4.2529e-01,  4.3628e-02,  1.1729e+00, -1.2280e+00,  1.1023e+00],\n          [-1.0558e+00, -1.0738e+00,  1.3054e+00,  1.4115e-01, -8.1034e-01,\n           -1.2218e+00,  5.8609e-01, -7.3626e-01,  1.8519e+00, -1.3020e+00,\n           -1.3659e+00, -4.0795e-01, -6.2651e-01, -8.1111e-01,  1.1516e+00,\n            1.1316e+00, -2.9024e-02, -1.0089e+00,  9.1232e-01,  1.0491e+00,\n            5.1206e-01, -1.3718e+00,  2.1436e-01,  2.9017e-01,  9.5840e-01,\n           -6.5594e-01, -1.9535e+00, -7.8490e-01,  8.3989e-01,  1.2884e-01],\n          [-1.6585e+00, -1.5455e+00, -9.9925e-01,  5.8399e-02, -3.9040e-02,\n            6.5892e-01,  5.9649e-01, -4.5398e-01, -1.3967e+00,  2.2714e-01,\n           -1.3343e-01,  9.6613e-01,  2.3873e+00,  4.9573e-01,  1.6093e+00,\n            4.3918e-01,  8.1521e-02,  9.5888e-01, -3.5687e-01,  8.0068e-01,\n            1.8197e+00, -1.0378e+00, -4.6767e-01,  2.4513e-01,  3.0829e-02,\n            2.0258e+00, -1.6950e-01,  1.2333e+00, -1.8466e-01, -8.4146e-01],\n          [ 1.0581e+00,  8.3303e-01, -2.0898e-01, -7.1238e-02, -1.7806e+00,\n           -1.3819e+00, -7.3922e-01, -1.2470e+00,  2.2704e+00,  1.0915e+00,\n            7.2466e-01, -7.3870e-01, -6.0574e-01, -9.8931e-01,  3.9956e-01,\n           -3.3178e-02,  6.9881e-01, -2.8874e-02,  9.4300e-01, -1.1329e+00,\n           -1.2066e-01, -7.9488e-01,  8.1168e-01, -3.6393e-01,  9.8861e-01,\n           -1.3018e+00, -7.9006e-01, -1.6267e+00, -1.6933e-01, -1.9161e+00],\n          [ 1.2976e+00, -3.0707e-01,  1.5206e-01, -1.2924e+00,  1.3695e+00,\n            4.8679e-01,  2.8015e-01, -1.0598e+00,  8.5453e-01,  1.6953e+00,\n           -7.3026e-02, -2.6229e-01,  3.7332e-01,  4.7219e-01,  1.2994e+00,\n            8.4231e-02, -3.7173e-01,  1.2844e+00, -3.0879e-01, -2.4056e-01,\n            3.5385e-01,  1.7881e+00, -5.6230e-02,  1.8647e+00, -2.9538e-01,\n           -1.5309e-01,  6.7980e-01, -1.1389e+00,  7.7063e-01,  4.0792e-01],\n          [ 1.6537e-01, -5.4268e-01,  1.4361e+00, -1.8514e+00, -9.9090e-01,\n            1.3681e+00, -1.1925e+00, -1.4675e+00, -1.7696e+00,  2.7473e-01,\n            4.5793e-01,  1.2002e+00, -1.0173e-02,  6.7341e-01, -6.9389e-01,\n           -3.0568e-01,  1.4743e+00, -4.5715e-01, -3.1678e-01, -8.0420e-01,\n            9.3507e-01, -2.5727e+00, -6.2301e-01,  5.2458e-01,  1.7876e+00,\n            1.9953e+00, -8.2280e-01, -9.3220e-01,  7.3520e-01, -1.9450e-02],\n          [ 3.2552e-01, -5.5726e-01, -1.5436e-01, -3.1833e-01, -7.3180e-01,\n            2.4985e+00,  1.5079e+00, -1.8147e+00, -1.5770e+00, -7.1686e-01,\n           -1.7014e-01,  5.5542e-01,  6.6698e-01,  4.9815e-01, -7.7865e-01,\n            1.0010e-01,  5.4801e-01, -1.4824e+00, -1.0891e+00, -5.2835e-01,\n           -1.8059e+00,  5.1310e-01, -6.0952e-02, -8.2190e-02, -7.6479e-01,\n            4.5870e-01,  2.5672e-01, -4.1584e-01, -1.3317e-01, -1.0558e+00],\n          [-1.9757e+00,  5.3538e-01,  9.1843e-01,  8.2624e-01, -7.9744e-01,\n            4.1966e-01, -2.0180e-01, -1.5019e+00,  8.5708e-01, -1.9820e+00,\n           -1.9907e+00,  2.7299e-01,  1.4611e+00,  1.4276e+00,  1.9917e-01,\n           -4.5425e-01,  1.1767e+00, -2.3913e-01, -1.1886e+00,  3.6520e-01,\n           -7.0246e-01,  4.0937e-01,  1.2188e+00,  7.0814e-01, -2.0703e-01,\n           -1.3377e+00,  1.3047e+00, -1.8002e+00, -6.5496e-01,  1.1960e+00],\n          [-5.5631e-01, -7.3120e-01,  5.9734e-01,  6.0001e-01,  5.8522e-01,\n            1.2644e+00, -1.9754e-01,  2.6395e-01, -2.6131e+00, -6.3757e-01,\n           -7.8628e-01,  9.0005e-02,  1.5931e-01,  1.1850e+00,  1.4888e+00,\n           -1.7166e+00, -7.7588e-01, -1.3362e+00,  6.1082e-01,  4.4597e-01,\n           -4.3007e-01, -9.9766e-01,  1.5629e+00, -1.0948e+00,  2.0367e-02,\n            4.9412e-01, -1.7368e+00,  1.8173e+00, -1.2984e-01,  1.4982e+00],\n          [-2.7025e+00, -6.8892e-01, -5.5800e-01,  6.9908e-01,  1.6942e+00,\n            7.1035e-01,  5.9015e-01, -9.1358e-01,  2.1294e+00, -2.1806e+00,\n            1.4493e+00, -8.8173e-02, -6.4871e-01, -5.5206e-01,  1.5681e+00,\n            1.6634e-01, -4.5019e-01,  1.2586e+00, -1.3081e+00,  1.7071e-01,\n            1.4588e+00,  1.1510e-01, -8.0856e-01,  1.1165e+00,  7.9243e-01,\n            2.7658e+00,  2.6158e-01,  7.3052e-01, -1.3441e+00,  2.6824e+00],\n          [ 2.7831e-01,  1.0835e+00, -1.3908e+00,  1.6786e+00,  3.8011e-01,\n           -9.7523e-01, -8.0604e-01,  4.8982e-01,  1.8820e+00,  9.9751e-01,\n            6.2731e-02, -1.8840e-01,  1.0522e-01,  1.9316e+00, -1.1759e+00,\n            1.0446e+00, -1.1021e+00, -7.7142e-02,  9.2619e-01, -2.7539e-01,\n            1.7208e+00,  1.5406e+00,  1.2859e+00, -6.5549e-01, -2.5944e-01,\n            1.4397e-01, -7.8790e-01, -1.5206e+00,  1.1951e-01, -1.8792e-01],\n          [ 3.5961e-01, -1.2273e+00,  2.0295e+00, -9.2731e-02,  1.2507e+00,\n            1.3585e+00, -5.2017e-01, -1.7450e+00,  1.2394e+00, -1.2314e+00,\n            7.7888e-01,  1.3682e+00,  4.5639e-01,  2.4454e-01, -6.1424e-02,\n            5.1610e-01, -1.9481e-01, -1.3715e+00,  8.2479e-01, -1.0218e+00,\n            9.8315e-01,  7.0651e-01,  3.5376e-01,  2.4865e-02,  2.3497e-01,\n            9.9230e-02,  1.2392e+00,  4.1548e-01,  2.9048e-01,  1.0178e-01],\n          [-6.9147e-01, -1.8126e+00, -1.2428e+00,  1.5220e-02,  7.5013e-01,\n           -1.5758e-02, -7.7818e-01, -1.5513e+00, -1.5681e+00, -2.4497e+00,\n            8.3530e-01, -9.1681e-01, -4.0946e-01, -3.2323e-01, -3.1722e-01,\n            3.8798e-01, -5.9631e-01,  1.0026e+00, -1.3512e+00,  1.3316e+00,\n            1.1616e+00,  4.8224e-01, -5.7140e-01, -2.3551e-01, -2.0667e+00,\n           -2.4607e-01, -1.2584e+00, -6.1600e-02, -5.5267e-01,  1.6676e-01],\n          [-2.2843e+00,  5.9522e-01, -5.0337e-01,  7.4827e-01, -8.0416e-01,\n            1.0631e+00,  2.0763e-02, -7.2866e-01,  7.9353e-02,  8.5070e-01,\n            1.0560e-01, -2.8168e-01,  7.8477e-02, -5.8358e-01, -9.6014e-01,\n            1.3017e+00, -1.8187e+00, -7.3342e-01,  1.4290e+00, -2.2675e+00,\n            9.5192e-01, -7.5336e-01,  2.1733e+00,  1.9608e+00,  3.0473e-01,\n            2.2348e-01, -7.4514e-01, -8.2606e-03, -6.7767e-01, -9.9884e-01],\n          [-2.3580e-01,  1.3883e+00, -1.1037e+00, -1.2326e+00, -9.6448e-02,\n           -1.6790e+00, -2.0778e+00,  3.1124e-01, -1.0351e+00,  1.1621e+00,\n           -3.2301e-01, -8.8291e-01,  4.7457e-01,  1.2212e+00, -3.4319e-01,\n           -1.7427e-01, -6.7032e-01,  1.6412e+00,  6.0941e-01,  1.1215e+00,\n            4.3652e-02, -1.3146e-01, -1.7225e+00,  7.8213e-01, -4.3919e-01,\n            9.6145e-02,  5.3442e-01,  1.3682e+00, -3.4351e-01,  8.0488e-01],\n          [ 1.1022e-03, -1.4277e+00, -7.0275e-01, -4.2363e-02,  5.7200e-01,\n            1.2777e+00,  6.6747e-01,  7.1228e-01,  1.5124e+00,  1.4452e+00,\n           -1.0309e+00,  1.2081e+00, -8.4914e-01, -5.7011e-01,  1.5523e-01,\n            1.6458e+00, -9.3879e-02, -1.3786e+00,  1.7234e+00, -2.1187e+00,\n            2.0375e-01, -1.7824e-01, -1.0413e+00, -4.5783e-01, -9.1670e-01,\n           -9.3181e-01,  2.9337e-01, -1.0589e+00, -1.7452e+00, -2.8967e-01]]]]), tensor([[[[5, 3, 6, 8, 8, 6, 2, 5, 6, 5, 5, 2, 8, 3, 2, 4, 4, 0, 6, 6, 2, 8, 6,\n           6, 0, 1, 0, 9, 1, 3],\n          [8, 9, 0, 6, 2, 4, 4, 4, 7, 2, 3, 5, 3, 0, 1, 4, 2, 1, 9, 1, 4, 8, 2,\n           1, 3, 3, 7, 4, 4, 2],\n          [7, 4, 2, 1, 0, 4, 4, 4, 7, 2, 5, 6, 5, 2, 9, 0, 5, 0, 5, 0, 1, 0, 2,\n           8, 7, 6, 7, 0, 2, 5],\n          [8, 1, 5, 9, 9, 4, 7, 8, 0, 4, 3, 1, 2, 2, 3, 8, 7, 6, 7, 9, 4, 1, 7,\n           5, 6, 6, 9, 5, 6, 1],\n          [6, 0, 3, 8, 7, 9, 1, 7, 1, 1, 5, 5, 3, 2, 0, 7, 3, 0, 0, 8, 9, 8, 5,\n           1, 1, 1, 1, 8, 4, 8],\n          [1, 6, 3, 4, 4, 2, 7, 1, 4, 5, 8, 1, 1, 6, 5, 9, 7, 0, 8, 9, 6, 3, 1,\n           1, 1, 6, 6, 2, 1, 5],\n          [9, 5, 6, 2, 2, 1, 5, 0, 4, 1, 7, 5, 8, 4, 3, 9, 0, 9, 9, 4, 6, 3, 1,\n           5, 9, 7, 4, 7, 6, 8],\n          [2, 6, 8, 0, 5, 3, 2, 7, 0, 3, 4, 6, 5, 6, 1, 4, 7, 6, 4, 1, 9, 8, 7,\n           5, 9, 8, 4, 4, 2, 1],\n          [6, 2, 1, 2, 8, 2, 9, 0, 7, 8, 7, 5, 7, 4, 6, 4, 8, 9, 4, 3, 3, 4, 5,\n           7, 0, 1, 6, 3, 4, 5],\n          [8, 4, 3, 0, 9, 4, 7, 8, 4, 9, 5, 4, 0, 6, 7, 2, 4, 3, 4, 4, 4, 3, 5,\n           2, 6, 0, 2, 4, 8, 5],\n          [7, 5, 9, 1, 8, 7, 2, 5, 7, 0, 6, 1, 6, 1, 6, 6, 2, 9, 5, 5, 8, 4, 2,\n           5, 9, 6, 0, 6, 2, 3],\n          [9, 5, 7, 4, 5, 5, 7, 0, 0, 9, 8, 1, 7, 4, 4, 4, 3, 1, 9, 7, 9, 7, 0,\n           9, 3, 5, 4, 2, 3, 2],\n          [0, 6, 3, 1, 7, 1, 1, 4, 5, 1, 8, 6, 0, 4, 9, 7, 2, 5, 0, 0, 4, 9, 1,\n           9, 6, 5, 4, 8, 8, 0],\n          [9, 6, 3, 1, 3, 1, 9, 6, 8, 8, 2, 7, 0, 3, 1, 1, 2, 5, 3, 6, 7, 5, 9,\n           1, 1, 6, 6, 8, 8, 3],\n          [6, 7, 0, 7, 1, 4, 2, 3, 3, 2, 4, 5, 2, 3, 0, 5, 0, 7, 3, 0, 3, 0, 7,\n           5, 7, 2, 0, 1, 5, 1],\n          [2, 3, 5, 8, 1, 4, 7, 5, 2, 7, 3, 2, 9, 9, 2, 7, 8, 2, 5, 6, 3, 5, 3,\n           0, 7, 6, 6, 1, 3, 9],\n          [7, 5, 1, 3, 4, 6, 9, 7, 7, 2, 2, 5, 3, 0, 1, 3, 6, 0, 7, 7, 5, 7, 2,\n           1, 9, 5, 1, 4, 2, 9],\n          [0, 1, 8, 5, 9, 9, 7, 7, 8, 2, 0, 8, 4, 5, 0, 1, 4, 1, 2, 1, 3, 1, 4,\n           4, 8, 2, 8, 8, 4, 4],\n          [0, 9, 6, 6, 6, 5, 7, 2, 8, 2, 8, 8, 3, 5, 2, 8, 3, 7, 3, 9, 8, 5, 0,\n           3, 4, 5, 5, 9, 9, 0],\n          [5, 3, 0, 1, 4, 9, 2, 6, 8, 4, 9, 2, 8, 5, 6, 4, 1, 7, 0, 2, 0, 5, 3,\n           1, 6, 3, 4, 3, 8, 4],\n          [6, 0, 6, 7, 1, 3, 3, 2, 7, 8, 2, 6, 6, 9, 4, 2, 6, 3, 6, 9, 3, 3, 6,\n           3, 1, 6, 6, 5, 5, 7],\n          [7, 5, 4, 6, 6, 7, 7, 4, 9, 8, 8, 5, 1, 5, 5, 5, 5, 7, 4, 8, 7, 8, 6,\n           4, 5, 9, 0, 4, 2, 0],\n          [4, 5, 6, 3, 5, 6, 1, 1, 3, 7, 2, 9, 9, 3, 9, 4, 5, 0, 1, 8, 8, 3, 6,\n           3, 7, 0, 5, 1, 9, 1],\n          [3, 8, 6, 8, 7, 5, 4, 6, 5, 4, 8, 7, 8, 0, 9, 6, 0, 0, 0, 5, 1, 6, 7,\n           1, 7, 4, 5, 5, 6, 4],\n          [2, 8, 2, 7, 3, 9, 0, 0, 2, 3, 9, 2, 8, 1, 9, 3, 7, 3, 6, 0, 8, 3, 0,\n           8, 0, 5, 7, 1, 1, 1],\n          [8, 2, 1, 0, 9, 4, 1, 1, 4, 0, 2, 1, 3, 3, 7, 6, 1, 0, 4, 1, 1, 3, 3,\n           9, 0, 4, 6, 3, 5, 8],\n          [5, 9, 5, 2, 1, 1, 8, 6, 1, 0, 3, 9, 3, 7, 3, 8, 7, 1, 5, 2, 5, 7, 7,\n           5, 4, 0, 5, 3, 1, 1],\n          [5, 3, 8, 7, 0, 8, 7, 8, 9, 9, 1, 5, 7, 1, 7, 5, 5, 8, 4, 5, 7, 0, 0,\n           4, 0, 6, 6, 0, 0, 0],\n          [7, 9, 8, 5, 4, 9, 9, 9, 2, 6, 1, 2, 1, 3, 1, 6, 7, 2, 1, 5, 7, 4, 0,\n           3, 6, 7, 8, 5, 3, 9],\n          [2, 9, 6, 5, 9, 8, 7, 5, 6, 3, 6, 3, 9, 0, 7, 9, 5, 5, 8, 4, 9, 5, 0,\n           8, 1, 8, 7, 2, 0, 2]]]]), ['task_1']]\nINFO     benchmark:benchmark.py:149 Batch 1: CPU Usage: 0.0%, Memory Usage: 79.9%\nDEBUG    benchmark:benchmark.py:157 Invoking the model with inputs and attention_mask\nINFO     benchmark:benchmark.py:194 Total Time: 0.0002 seconds, Grids per Second: 6168.09\nINFO     benchmark:benchmark.py:226 Total Time: 0.0002 seconds, Grids per Second: 6168.09\nINFO     benchmark:benchmark.py:256 Improvement in average total time: -1.6389 seconds (99.99%)\nINFO     benchmark:benchmark.py:261 Improvement in average grids per second: +5968.82 (2995.35%)\nINFO     benchmark:benchmark.py:272 The improvement in average total time is practically significant.\nINFO     benchmark:benchmark.py:283 The improvement in average grids per second is practically significant.\nINFO     benchmark:benchmark.py:296 T-Test for total time: t-statistic = nan, p-value = nan\nINFO     benchmark:benchmark.py:297 T-Test for grids per second: t-statistic = nan, p-value = nan\nINFO     benchmark:benchmark.py:300 Run Summary:\nINFO     benchmark:benchmark.py:301  \u2022 Avg Total Time: 0.0002s (CI 95%: \u00b1nans)\nINFO     benchmark:benchmark.py:302  \u2022 Avg Grids per Second: 6168.09 (CI 95%: \u00b1nan)\nINFO     benchmark:benchmark.py:303  \u2022 Effect Size (Total Time): nan, Effect Size (Grids per Second): nan",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_benchmark.py"
        }
    ],
    "tests/test_end_to_end.py": [
        {
            "function": "test_end_to_end",
            "error_type": "IndexError",
            "error_details": "IndexError: list index out of range",
            "line_number": "137",
            "code_snippet": "    assert train_losses[-1] < train_losses[0], f\"Training loss did not decrease. Initial loss: {train_losses[0]}, Final loss: {train_losses[-1]}\"",
            "captured_output": "DEBUG: Starting ARCDataset initialization\nDEBUG: data_source type: <class 'arckit.data.TaskSet'>\nDEBUG: data_source content: <TaskSet: 400 tasks>\nDEBUG: Processed data length: 400\nDEBUG: First item keys: dict_keys(['id', 'train', 'test'])\nDEBUG: First train item: {'input': array([[0, 7, 7],\n       [7, 7, 7],\n       [0, 7, 7]]), 'output': array([[0, 0, 0, 0, 7, 7, 0, 7, 7],\n       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n       [7, 7, 7, 7, 7, 7, 7, 7, 7],\n       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n       [0, 0, 0, 0, 7, 7, 0, 7, 7]])}\nNumber of train samples: 1302\nNumber of test samples: 416\nDEBUG: Initialized self.results['train'] as <class 'dict'>\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 1247\nCorrectly predicted different pixels: 40\nCalculated accuracy: 0.03207698476343224\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 1353\nCorrectly predicted different pixels: 18\nCalculated accuracy: 0.013303769401330377\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 1515\nCorrectly predicted different pixels: 13\nCalculated accuracy: 0.008580858085808581\nDifferential pixel accuracy - Input shape: torch.Size([32, 1, 30, 30]), Target shape: torch.Size([32, 900]), Prediction shape: torch.Size([32, 900])\nReshaped - Input: torch.Size([32, 900]), Target: torch.Size([32, 900]), Prediction: torch.Size([32, 900])\nTotal different pixels: 804\nCorrectly predicted different pixels: 1\nCalculated accuracy: 0.0012437810945273632\nDifferential pixel accuracy - Input shape: torch.Size([2, 1, 30, 30]), Target shape: torch.Size([2, 900]), Prediction shape: torch.Size([2, 900])\nReshaped - Input: torch.Size([2, 900]), Target: torch.Size([2, 900]), Prediction: torch.Size([2, 900])\nTotal different pixels: 32\nCorrectly predicted different pixels: 0\nCalculated accuracy: 0.0\nInitial validation results: [{'test_loss': 4.831074237823486, 'test_accuracy': 0.00886324793100357, 'test_diff_accuracy': 0.013589019887149334, 'task_0_test_loss': 4.831074237823486, 'task_0_test_accuracy': 0.00886324793100357, 'task_0_test_diff_accuracy': 0.013589019887149334, 'task_1_test_loss': 4.831074237823486, 'task_1_test_accuracy': 0.00886324793100357, 'task_1_test_diff_accuracy': 0.013589019887149334, 'task_2_test_loss': 4.831332206726074, 'task_2_test_accuracy': 0.008993055671453476, 'task_2_test_diff_accuracy': 0.013801348395645618, 'task_3_test_loss': 4.831332206726074, 'task_3_test_accuracy': 0.008993055671453476, 'task_3_test_diff_accuracy': 0.013801348395645618, 'task_4_test_loss': 4.831332206726074, 'task_4_test_accuracy': 0.008993055671453476, 'task_4_test_diff_accuracy': 0.013801348395645618, 'task_5_test_loss': 4.831332206726074, 'task_5_test_accuracy': 0.008993055671453476, 'task_5_test_diff_accuracy': 0.013801348395645618, 'task_6_test_loss': 4.831332206726074, 'task_6_test_accuracy': 0.008993055671453476, 'task_6_test_diff_accuracy': 0.013801348395645618, 'task_7_test_loss': 4.831332206726074, 'task_7_test_accuracy': 0.008993055671453476, 'task_7_test_diff_accuracy': 0.013801348395645618, 'task_8_test_loss': 4.831332206726074, 'task_8_test_accuracy': 0.008993055671453476, 'task_8_test_diff_accuracy': 0.013801348395645618, 'task_9_test_loss': 4.831332206726074, 'task_9_test_accuracy': 0.008993055671453476, 'task_9_test_diff_accuracy': 0.013801348395645618, 'task_10_test_loss': 4.831332206726074, 'task_10_test_accuracy': 0.008993055671453476, 'task_10_test_diff_accuracy': 0.013801348395645618, 'task_11_test_loss': 4.831332206726074, 'task_11_test_accuracy': 0.008993055671453476, 'task_11_test_diff_accuracy': 0.013801348395645618, 'task_12_test_loss': 4.831332206726074, 'task_12_test_accuracy': 0.008993055671453476, 'task_12_test_diff_accuracy': 0.013801348395645618, 'task_13_test_loss': 4.831332206726074, 'task_13_test_accuracy': 0.008993055671453476, 'task_13_test_diff_accuracy': 0.013801348395645618, 'task_14_test_loss': 4.831332206726074, 'task_14_test_accuracy': 0.008993055671453476, 'task_14_test_diff_accuracy': 0.013801348395645618, 'task_15_test_loss': 4.831332206726074, 'task_15_test_accuracy': 0.008993055671453476, 'task_15_test_diff_accuracy': 0.013801348395645618, 'task_16_test_loss': 4.831332206726074, 'task_16_test_accuracy': 0.008993055671453476, 'task_16_test_diff_accuracy': 0.013801348395645618, 'task_17_test_loss': 4.831332206726074, 'task_17_test_accuracy': 0.008993055671453476, 'task_17_test_diff_accuracy': 0.013801348395645618, 'task_18_test_loss': 4.831332206726074, 'task_18_test_accuracy': 0.008993055671453476, 'task_18_test_diff_accuracy': 0.013801348395645618, 'task_19_test_loss': 4.831332206726074, 'task_19_test_accuracy': 0.008993055671453476, 'task_19_test_diff_accuracy': 0.013801348395645618, 'task_20_test_loss': 4.831332206726074, 'task_20_test_accuracy': 0.008993055671453476, 'task_20_test_diff_accuracy': 0.013801348395645618, 'task_21_test_loss': 4.831332206726074, 'task_21_test_accuracy': 0.008993055671453476, 'task_21_test_diff_accuracy': 0.013801348395645618, 'task_22_test_loss': 4.831332206726074, 'task_22_test_accuracy': 0.008993055671453476, 'task_22_test_diff_accuracy': 0.013801348395645618, 'task_23_test_loss': 4.831332206726074, 'task_23_test_accuracy': 0.008993055671453476, 'task_23_test_diff_accuracy': 0.013801348395645618, 'task_24_test_loss': 4.831332206726074, 'task_24_test_accuracy': 0.008993055671453476, 'task_24_test_diff_accuracy': 0.013801348395645618, 'task_25_test_loss': 4.831332206726074, 'task_25_test_accuracy': 0.008993055671453476, 'task_25_test_diff_accuracy': 0.013801348395645618, 'task_26_test_loss': 4.831332206726074, 'task_26_test_accuracy': 0.008993055671453476, 'task_26_test_diff_accuracy': 0.013801348395645618, 'task_27_test_loss': 4.831332206726074, 'task_27_test_accuracy': 0.008993055671453476, 'task_27_test_diff_accuracy': 0.013801348395645618, 'task_28_test_loss': 4.831332206726074, 'task_28_test_accuracy': 0.008993055671453476, 'task_28_test_diff_accuracy': 0.013801348395645618, 'task_29_test_loss': 4.831332206726074, 'task_29_test_accuracy': 0.008993055671453476, 'task_29_test_diff_accuracy': 0.013801348395645618, 'task_30_test_loss': 4.831332206726074, 'task_30_test_accuracy': 0.008993055671453476, 'task_30_test_diff_accuracy': 0.013801348395645618, 'task_31_test_loss': 4.831332206726074, 'task_31_test_accuracy': 0.008993055671453476, 'task_31_test_diff_accuracy': 0.013801348395645618}]\nInitial validation accuracy: 0.00886324793100357, Initial loss: 4.831074237823486\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: self.results['train'] is of type <class 'dict'>\nDEBUG: Before setting default, self.results['train'] is of type <class 'dict'>\nDEBUG: After setting default, self.results['train'] is of type <class 'dict'>",
            "captured_log": "DEBUG    tests.test_end_to_end:test_end_to_end.py:28 Starting end-to-end test\nDEBUG    tests.test_end_to_end:test_end_to_end.py:32 Loading data using arckit\nDEBUG    tests.test_end_to_end:test_end_to_end.py:36 Creating train and validation datasets\nDEBUG    tests.test_end_to_end:test_end_to_end.py:42 Train dataset size: 130, Validation dataset size: 130\nDEBUG    tests.test_end_to_end:test_end_to_end.py:81 Initializing model\nDEBUG    src.models.gpt2:gpt2.py:23 Initialized Attention with n_embd=64, n_head=2\nDEBUG    src.models.gpt2:gpt2.py:51 Initialized FeedForward with n_embd=64\nDEBUG    src.models.gpt2:gpt2.py:69 Initialized TransformerBlock with n_embd=64, n_head=2\nDEBUG    tests.test_end_to_end:test_end_to_end.py:84 Model initialized with config: ModelConfig(n_embd=64, n_head=2, n_layer=1, dropout=0.1)\nDEBUG    tests.test_end_to_end:test_end_to_end.py:94 Initializing trainer\nDEBUG    tests.test_end_to_end:test_end_to_end.py:100 Trainer initialized with config: Config(model=ModelConfig(n_embd=64, n_head=2, n_layer=1, dropout=0.1), training=TrainingConfig(batch_size=32, learning_rate=0.0001, max_epochs=2, use_gpu=True, log_level='INFO'))\nDEBUG    tests.test_end_to_end:test_end_to_end.py:103 Creating PyTorch Lightning trainer\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 GPU available: True (mps), used: True\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 TPU available: False, using: 0 TPU cores\nINFO     pytorch_lightning.utilities.rank_zero:rank_zero.py:63 HPU available: False, using: 0 HPUs\nDEBUG    tests.test_end_to_end:test_end_to_end.py:113 PyTorch Lightning trainer created\nINFO     tests.test_end_to_end:test_end_to_end.py:116 Evaluating model before training\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:114 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:124 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:114 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:124 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:114 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:124 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:114 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:124 Task IDs in batch: ['task_0', 'task_1', 'task_2', 'task_3', 'task_4', 'task_5', 'task_6', 'task_7', 'task_8', 'task_9', 'task_10', 'task_11', 'task_12', 'task_13', 'task_14', 'task_15', 'task_16', 'task_17', 'task_18', 'task_19', 'task_20', 'task_21', 'task_22', 'task_23', 'task_24', 'task_25', 'task_26', 'task_27', 'task_28', 'task_29', 'task_30', 'task_31']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([32, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([32, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([32, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([32, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:48 Batch input dtypes before stack: [torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:49 Batch output dtypes before stack: [torch.float32, torch.float32]\nDEBUG    tests.test_end_to_end:test_end_to_end.py:56 Collate function input_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:57 Collate function output_stack dtype: torch.float32\nDEBUG    tests.test_end_to_end:test_end_to_end.py:62 Collate function attention_mask dtype: torch.float32\nDEBUG    src.training.trainer:trainer.py:114 Test step - Batch type: <class 'tuple'>, length: 4\nDEBUG    src.training.trainer:trainer.py:124 Task IDs in batch: ['task_0', 'task_1']\nDEBUG    src.models.gpt2:gpt2.py:129 GPT2ARC input shape: torch.Size([2, 1, 30, 30]), dtype: torch.float32\nDEBUG    src.models.gpt2:gpt2.py:142 After conv1 shape: torch.Size([2, 64, 30, 30])\nDEBUG    src.models.gpt2:gpt2.py:146 Reshaped for transformer blocks: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:75 TransformerBlock input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:28 Attention input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:41 Attention output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:55 FeedForward input shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:58 FeedForward output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:79 TransformerBlock output shape: torch.Size([2, 900, 64])\nDEBUG    src.models.gpt2:gpt2.py:150 After block 1 shape: torch.Size([2, 900, 64])\nDEBUG    tests.test_end_to_end:test_end_to_end.py:118 Initial validation results: [{'test_loss': 4.831074237823486, 'test_accuracy': 0.00886324793100357, 'test_diff_accuracy': 0.013589019887149334, 'task_0_test_loss': 4.831074237823486, 'task_0_test_accuracy': 0.00886324793100357, 'task_0_test_diff_accuracy': 0.013589019887149334, 'task_1_test_loss': 4.831074237823486, 'task_1_test_accuracy': 0.00886324793100357, 'task_1_test_diff_accuracy': 0.013589019887149334, 'task_2_test_loss': 4.831332206726074, 'task_2_test_accuracy': 0.008993055671453476, 'task_2_test_diff_accuracy': 0.013801348395645618, 'task_3_test_loss': 4.831332206726074, 'task_3_test_accuracy': 0.008993055671453476, 'task_3_test_diff_accuracy': 0.013801348395645618, 'task_4_test_loss': 4.831332206726074, 'task_4_test_accuracy': 0.008993055671453476, 'task_4_test_diff_accuracy': 0.013801348395645618, 'task_5_test_loss': 4.831332206726074, 'task_5_test_accuracy': 0.008993055671453476, 'task_5_test_diff_accuracy': 0.013801348395645618, 'task_6_test_loss': 4.831332206726074, 'task_6_test_accuracy': 0.008993055671453476, 'task_6_test_diff_accuracy': 0.013801348395645618, 'task_7_test_loss': 4.831332206726074, 'task_7_test_accuracy': 0.008993055671453476, 'task_7_test_diff_accuracy': 0.013801348395645618, 'task_8_test_loss': 4.831332206726074, 'task_8_test_accuracy': 0.008993055671453476, 'task_8_test_diff_accuracy': 0.013801348395645618, 'task_9_test_loss': 4.831332206726074, 'task_9_test_accuracy': 0.008993055671453476, 'task_9_test_diff_accuracy': 0.013801348395645618, 'task_10_test_loss': 4.831332206726074, 'task_10_test_accuracy': 0.008993055671453476, 'task_10_test_diff_accuracy': 0.013801348395645618, 'task_11_test_loss': 4.831332206726074, 'task_11_test_accuracy': 0.008993055671453476, 'task_11_test_diff_accuracy': 0.013801348395645618, 'task_12_test_loss': 4.831332206726074, 'task_12_test_accuracy': 0.008993055671453476, 'task_12_test_diff_accuracy': 0.013801348395645618, 'task_13_test_loss': 4.831332206726074, 'task_13_test_accuracy': 0.008993055671453476, 'task_13_test_diff_accuracy': 0.013801348395645618, 'task_14_test_loss': 4.831332206726074, 'task_14_test_accuracy': 0.008993055671453476, 'task_14_test_diff_accuracy': 0.013801348395645618, 'task_15_test_loss': 4.831332206726074, 'task_15_test_accuracy': 0.008993055671453476, 'task_15_test_diff_accuracy': 0.013801348395645618, 'task_16_test_loss': 4.831332206726074, 'task_16_test_accuracy': 0.008993055671453476, 'task_16_test_diff_accuracy': 0.013801348395645618, 'task_17_test_loss': 4.831332206726074, 'task_17_test_accuracy': 0.008993055671453476, 'task_17_test_diff_accuracy': 0.013801348395645618, 'task_18_test_loss': 4.831332206726074, 'task_18_test_accuracy': 0.008993055671453476, 'task_18_test_diff_accuracy': 0.013801348395645618, 'task_19_test_loss': 4.831332206726074, 'task_19_test_accuracy': 0.008993055671453476, 'task_19_test_diff_accuracy': 0.013801348395645618, 'task_20_test_loss': 4.831332206726074, 'task_20_test_accuracy': 0.008993055671453476, 'task_20_test_diff_accuracy': 0.013801348395645618, 'task_21_test_loss': 4.831332206726074, 'task_21_test_accuracy': 0.008993055671453476, 'task_21_test_diff_accuracy': 0.013801348395645618, 'task_22_test_loss': 4.831332206726074, 'task_22_test_accuracy': 0.008993055671453476, 'task_22_test_diff_accuracy': 0.013801348395645618, 'task_23_test_loss': 4.831332206726074, 'task_23_test_accuracy': 0.008993055671453476, 'task_23_test_diff_accuracy': 0.013801348395645618, 'task_24_test_loss': 4.831332206726074, 'task_24_test_accuracy': 0.008993055671453476, 'task_24_test_diff_accuracy': 0.013801348395645618, 'task_25_test_loss': 4.831332206726074, 'task_25_test_accuracy': 0.008993055671453476, 'task_25_test_diff_accuracy': 0.013801348395645618, 'task_26_test_loss': 4.831332206726074, 'task_26_test_accuracy': 0.008993055671453476, 'task_26_test_diff_accuracy': 0.013801348395645618, 'task_27_test_loss': 4.831332206726074, 'task_27_test_accuracy': 0.008993055671453476, 'task_27_test_diff_accuracy': 0.013801348395645618, 'task_28_test_loss': 4.831332206726074, 'task_28_test_accuracy': 0.008993055671453476, 'task_28_test_diff_accuracy': 0.013801348395645618, 'task_29_test_loss': 4.831332206726074, 'task_29_test_accuracy': 0.008993055671453476, 'task_29_test_diff_accuracy': 0.013801348395645618, 'task_30_test_loss': 4.831332206726074, 'task_30_test_accuracy': 0.008993055671453476, 'task_30_test_diff_accuracy': 0.013801348395645618, 'task_31_test_loss': 4.831332206726074, 'task_31_test_accuracy': 0.008993055671453476, 'task_31_test_diff_accuracy': 0.013801348395645618}]\nINFO     tests.test_end_to_end:test_end_to_end.py:125 Initial validation accuracy: 0.00886324793100357, Initial loss: 4.831074237823486\nDEBUG    tests.test_end_to_end:test_end_to_end.py:127 Starting model training\nINFO     pytorch_lightning.callbacks.model_summary:model_summary.py:104 \n  | Name  | Type    | Params | Mode",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_end_to_end.py"
        }
    ],
    "tests/test_train.py": [
        {
            "function": "test_logging",
            "error_type": "TypeError",
            "error_details": "TypeError: patch() missing 1 required positional argument: 'target'",
            "line_number": "145",
            "code_snippet": "    ), patch(",
            "captured_output": "Entering test_logging",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_batch_size_extremes",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='13254162864'>]",
            "line_number": "261",
            "code_snippet": "    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_batch_size_extremes",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='13254141248'>]",
            "line_number": "261",
            "code_snippet": "    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_learning_rate_extremes",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='13254129952'>]",
            "line_number": "288",
            "code_snippet": "    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_learning_rate_extremes",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='13256093488'>]",
            "line_number": "288",
            "code_snippet": "    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_gpu_not_available",
            "error_type": "AttributeError",
            "error_details": "AttributeError: module 'gpt2_arc.src.training.train' has no attribute 'ARCTrainergpt2_arc'",
            "line_number": "319",
            "code_snippet": "    with patch(\"torch.cuda.is_available\", return_value=False), patch(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1445: in __enter__\n    self.target = self.getter()\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pkgutil.py:528: in resolve_name\n    result = getattr(result, p)",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_valid_batch_sizes",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='13252595056'>]\nFalsifying example: test_valid_batch_sizes(\nmock_args=Namespace(train_data='mock_train_data.json', val_data='mock_val_data.json', batch_size=560, learning_rate=0.0001, max_epochs=10, use_gpu=False, no_logging=False, no_checkpointing=False, no_progress_bar=False, log_level='INFO', fast_dev_run=False, project='test_project'),\nbatch_size=1,  # or any other generated value\n)",
            "line_number": "358",
            "code_snippet": "    @given(batch_size=st.integers(min_value=1, max_value=1024))\ntests/test_train.py:374: in test_valid_batch_sizes\n    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_valid_learning_rates",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='6172415504'>]\nFalsifying example: test_valid_learning_rates(\nmock_args=Namespace(train_data='mock_train_data.json', val_data='mock_val_data.json', batch_size=32, learning_rate=0.5, max_epochs=10, use_gpu=False, no_logging=False, no_checkpointing=False, no_progress_bar=False, log_level='INFO', fast_dev_run=False, project='test_project'),\nlearning_rate=1.0,  # or any other generated value\n)",
            "line_number": "380",
            "code_snippet": "    @given(\ntests/test_train.py:398: in test_valid_learning_rates\n    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_end_to_end_training",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='6172408432'>]",
            "line_number": "440",
            "code_snippet": "    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        },
        {
            "function": "test_tensorboard_logging",
            "error_type": "unittest.mock.InvalidSpecError",
            "error_details": "unittest.mock.InvalidSpecError: Cannot spec a Mock object. [object=<MagicMock name='DataLoader' id='6171947520'>]",
            "line_number": "476",
            "code_snippet": "    mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:2128: in __init__\n    _safe_super(MagicMixin, self).__init__(*args, **kw)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:1119: in __init__\n    _safe_super(CallableMixin, self).__init__(\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:460: in __init__\n    self._mock_add_spec(spec, spec_set, _spec_as_instance, _eat_self)\n/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py:511: in _mock_add_spec\n    raise InvalidSpecError(f'Cannot spec a Mock object. [object={spec!r}]')",
            "captured_output": "",
            "captured_log": "",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        }
    ],
    "/opt/homebrew/Cellar/python@3.12/3.12.5/Frameworks/Python.framework/Versions/3.12/lib/python3.12/unittest/mock.py": [
        {
            "function": "assert_called_once_with",
            "error_type": "AssertionError",
            "error_details": "AssertionError: Expected 'fit' to be called once. Called 0 times.\nAssertionError: Expected 'fit' to be called once. Called 0 times.",
            "line_number": "958",
            "code_snippet": "    raise AssertionError(msg)\n\nDuring handling of the above exception, another exception occurred:\ntests/test_train.py:210: in test_fit_call\n    mock_pl_trainer.fit.assert_called_once_with(mock_trainer_instance)",
            "captured_output": "Entering test_fit_call\nDEBUG: Initialized self.results['train'] as <class 'dict'>\nExperimentTracker initialized with config: {\n  \"model\": {\n    \"n_embd\": 96,\n    \"n_head\": 3,\n    \"n_layer\": 1,\n    \"dropout\": 0.1\n  },\n  \"training\": {\n    \"batch_size\": 32,\n    \"learning_rate\": 0.0001,\n    \"max_epochs\": 10,\n    \"use_gpu\": true,\n    \"log_level\": \"INFO\"\n  }\n}\nProject: test_project, Entity: None\nuse_wandb: False\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0\nLogged metric locally: val_loss=<MagicMock name='ARCTrainer().validation_step()' id='13254331856'>, step=0",
            "captured_log": "INFO     gpt2_arc.src.training.train:train.py:41 Data loaded successfully using arckit\nINFO     gpt2_arc.src.training.train:train.py:43 Initializing model with new configuration\nINFO     gpt2_arc.src.training.train:train.py:47 Initializing trainer with new configuration",
            "test_file": "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/gpt2_arc/tests/test_train.py"
        }
    ]
}