<?xml version='1.0' encoding='utf-8'?>
<source type="local_directory" path="/workspaces/arc-neural-reasoning-model/gpt2_arc"><file name="benchmark.py"># gp2_arc/benchmark.py import sys import os # add project root directory python path sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) import torch._dynamo import csv import uuid datetime import datetime import os import torch torch.utils.data import dataloader import arckit gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import modelconfig import time torch.amp import autocast import psutil import logging import argparse import statistics import numpy np scipy import stats # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) # dynamically adjustable baseline values cpu, gpu, mps baselines = { 'cpu': {'total_time': 1.6391, 'grids_per_second': 199.27}, 'cuda': {'total_time': 0.0481, 'grids_per_second': 13774.98}, 'mps': {'total_time': 0.0481, 'grids_per_second': 13774.98} # updated baselines mps } def benchmark_model(model, dataset, batch_size=1, num_batches=1, num_runs=1, device_type='cpu', precision='medium', model_checkpoint=none): print(f"starting benchmark_model parameters: batch_size={batch_size}, num_batches={num_batches}, device_type={device_type}, precision={precision}, model_checkpoint={model_checkpoint}") device_type ['cpu', 'cuda', 'mps']: raise valueerror("invalid device type") len(dataset) == 0: raise valueerror("dataset empty") checkpoint_used = false checkpoint_info = {} model_checkpoint: checkpoint = torch.load(model_checkpoint) model_config = modelconfig(**checkpoint['config']) model = gpt2arc(model_config) state_dict = {k.replace("model.", ""): v k, v checkpoint['state_dict'].items()} model.load_state_dict(state_dict) model.to(device_type) model.eval() checkpoint_used = true checkpoint_info = { 'checkpoint_path': model_checkpoint, 'config': checkpoint['config'], 'state_dict_keys': list(checkpoint['state_dict'].keys()) } run_id = str(uuid.uuid4()) current_time = datetime.now().strftime("%y-%m-%d %h:%m:%s") practical_threshold = 20.0 # define threshold practical significance total_time_runs = [] grids_per_second_runs = [] cpu_usages = [] memory_usages = [] run_results = [] # initialize run_results store run's data gpu_usages = [] # initialize gpu_usages store gpu utilization data # set float32 matmul precision torch.set_float32_matmul_precision(precision) # select device based argument (including support mps) device = torch.device("cuda" device_type == "cuda" torch.cuda.is_available() else "mps" device_type == "mps" torch.backends.mps.is_available() else "cpu") model = model.to(device) torch._dynamo.config.suppress_errors = true device.type == "cpu": compiled_model = model # use model directly cpu else: try: device.type != "mps": compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=true) else: compiled_model = model # use model directly mps except importerror e: logger.warning(f"compilation failed error: {e}. falling back eager execution.") compiled_model = model try: dataloader = dataloader(dataset, batch_size=batch_size, collate_fn=arcdataset.collate_fn) total_time = 0.0 total_grids = 0 i, batch enumerate(dataloader): &gt;= num_batches: break print(f"processing batch {i+1}/{num_batches}") logger.debug(f"batch content unpacking: {batch}") len(batch) != 3: raise valueerror(f"unexpected batch format. expected 3 items, got {len(batch)}") inputs, outputs, task_ids = batch print(f"inputs type: {type(inputs)}") hasattr(inputs, 'shape'): print(f"inputs shape: {inputs.shape}") else: print("inputs shape: n/a") print(f"outputs type: {type(outputs)}, shape: {outputs.shape torch.is_tensor(outputs) else 'n/a'}") print(f"task ids: {task_ids}") inputs none isinstance(inputs, torch.tensor): raise valueerror(f"expected inputs torch.tensor, got {type(inputs)}") inputs.numel() == 0: raise valueerror("inputs tensor empty") print(f"inputs shape: {inputs.shape}, outputs shape: {outputs.shape}, task ids: {task_ids}") inputs.dim() == 2: # inputs 2d (batch_size, sequence_length), reshape 4d height = width = int(inputs.size(1)**0.5) inputs = inputs.view(inputs.size(0), 1, height, width) elif inputs.dim() == 3: # inputs 3d (batch_size, height, width), add channel dimension inputs = inputs.unsqueeze(1) elif inputs.dim() != 4: raise valueerror(f"unexpected input dimensions: {inputs.dim()}. expected 2, 3, 4 dimensions.") attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32) inputs, attention_mask = inputs.to(device), attention_mask.to(device) # log system load system state processing batch cpu_percent = psutil.cpu_percent(interval=none) memory_info = psutil.virtual_memory() cpu_usages.append(cpu_percent) memory_usages.append(memory_info.percent) device.type == 'cuda': gpu_utilization = torch.cuda.utilization(device.index) gpu_usages.append(gpu_utilization) logger.info(f"batch {i+1}: cpu usage: {cpu_percent}%, memory usage: {memory_info.percent}%, gpu utilization: {gpu_utilization}%") else: logger.info(f"batch {i+1}: cpu usage: {cpu_percent}%, memory usage: {memory_info.percent}%") # measure time taken process batch start_time = time.time() torch.cuda.is_available(): torch.cuda.synchronize() logger.debug("invoking model inputs attention_mask") torch.no_grad(): device.type == 'cuda': autocast(device_type=device.type, dtype=torch.float16): compiled_model(inputs, attention_mask) else: compiled_model(inputs, attention_mask) torch.cuda.is_available(): torch.cuda.synchronize() end_time = time.time() batch_time = end_time - start_time print(f"batch time: {batch_time}") batch_time &lt;= 0: print(f"warning: invalid batch time: {batch_time}. skipping batch.") continue total_time += batch_time total_grids += len(inputs) except exception e: logger.error(f"an error occurred benchmarking: {e}") raise print(f"benchmark completed. total time: {total_time}, total grids: {total_grids}") # calculate average standard deviation runs num_runs = len(total_time_runs) avg_total_time = np.mean(total_time_runs) std_total_time = np.std(total_time_runs) avg_grids_per_second = np.mean(grids_per_second_runs) std_grids_per_second = np.std(grids_per_second_runs) total_time &gt; 0: grids_per_second = total_grids / total_time else: grids_per_second = 0.0 # avoid division zero logger.warning("total time zero. setting grids_per_second 0.0 avoid division zero.") logger.info(f"total time: {total_time:.4f} seconds, grids per second: {grids_per_second:.2f}") # store results run run_results.append({ 'run_id': run_id, 'datetime': current_time, 'total_time': total_time, 'grids_per_second': grids_per_second, 'cpu_usage': np.mean(cpu_usages), 'memory_usage': np.mean(memory_usages), 'gpu_usage': np.mean(gpu_usages) gpu_usages else none, 'batch_size': batch_size, 'num_batches': num_batches, 'device': device.type, 'n_embd': model.config.n_embd, 'n_head': model.config.n_head, 'n_layer': model.config.n_layer, 'precision': precision, # add precision 'checkpoint_used': checkpoint_used, 'checkpoint_info': checkpoint_info }) total_time_runs.append(total_time) grids_per_second_runs.append(grids_per_second) total_time &lt;= 0 total_grids &lt;= 0: logger.warning(f"error: invalid total time ({total_time}) total grids ({total_grids}). check benchmark implementation.") return 0.0, 0.0 # return sensible defaults instead infinity avg_total_time = total_time avg_grids_per_second = total_grids / total_time total_time &gt; 0 else 0.0 logger.info(f"total time: {avg_total_time:.4f} seconds, grids per second: {avg_grids_per_second:.2f}") # perform statistical analysis (confidence intervals, effect size, etc.) confidence_level = 0.95 z_score = stats.norm.ppf((1 + confidence_level) / 2) ci_total_time = z_score * (std_total_time / np.sqrt(num_runs)) ci_grids_per_second = z_score * (std_grids_per_second / np.sqrt(num_runs)) effect_size_time = (avg_total_time - baselines[device.type]['total_time']) / std_total_time effect_size_grids = (avg_grids_per_second - baselines[device.type]['grids_per_second']) / std_grids_per_second # calculate improvements regressions based averages time_improvement = baselines[device.type]['total_time'] - avg_total_time time_improvement_percent = (time_improvement / baselines[device.type]['total_time']) * 100 time_regression = avg_total_time - baselines[device.type]['total_time'] time_regression_percent = (time_regression / baselines[device.type]['total_time']) * 100 grids_per_second_improvement = avg_grids_per_second - baselines[device.type]['grids_per_second'] grids_per_second_improvement_percent = (grids_per_second_improvement / baselines[device.type]['grids_per_second']) * 100 grids_per_second_regression = baselines[device.type]['grids_per_second'] - avg_grids_per_second grids_per_second_regression_percent = (grids_per_second_regression / baselines[device.type]['grids_per_second']) * 100 # determine improvement improvement_time = avg_total_time &lt; baselines[device.type]['total_time'] improvement_grids = avg_grids_per_second &gt; baselines[device.type]['grids_per_second'] # log improvements regressions based averages avg_total_time &lt; baselines[device.type]['total_time']: logger.info(f"improvement average total time: -{time_improvement:.4f} seconds ({time_improvement_percent:.2f}%)") else: logger.info(f"regression average total time: +{time_regression:.4f} seconds ({time_regression_percent:.2f}%)") avg_grids_per_second &gt; baselines[device.type]['grids_per_second']: logger.info(f"improvement average grids per second: +{grids_per_second_improvement:.2f} ({grids_per_second_improvement_percent:.2f}%)") else: logger.info(f"regression average grids per second: -{grids_per_second_regression:.2f} ({grids_per_second_regression_percent:.2f}%)") # update practical significance checks practical_significance_time = time_improvement_percent &gt;= practical_threshold practical_significance_grids = grids_per_second_improvement_percent &gt;= practical_threshold # log practical significance improvement_time: practical_significance_time: logger.info("the improvement average total time practically significant.") else: logger.info("the improvement average total time practically significant.") else: practical_significance_time: logger.info("the regression average total time practically significant.") else: logger.info("the regression average total time practically significant.") improvement_grids: practical_significance_grids: logger.info("the improvement average grids per second practically significant.") else: logger.info("the improvement average grids per second practically significant.") else: practical_significance_grids: logger.info("the regression average grids per second practically significant.") else: logger.info("the regression average grids per second practically significant.") # perform one-sample t-test t_stat_time, p_value_time = stats.ttest_1samp(total_time_runs, baselines[device.type]['total_time']) t_stat_grids, p_value_grids = stats.ttest_1samp(grids_per_second_runs, baselines[device.type]['grids_per_second']) logger.info(f"t-test total time: t-statistic = {t_stat_time:.4f}, p-value = {p_value_time:.4f}") logger.info(f"t-test grids per second: t-statistic = {t_stat_grids:.4f}, p-value = {p_value_grids:.4f}") # log results including confidence intervals logger.info(f"run summary:") logger.info(f" avg total time: {avg_total_time:.4f}s (ci 95%: {ci_total_time:.4f}s)") logger.info(f" avg grids per second: {avg_grids_per_second:.2f} (ci 95%: {ci_grids_per_second:.2f})") logger.info(f" effect size (total time): {effect_size_time:.4f}, effect size (grids per second): {effect_size_grids:.4f}") # determine improvement improvement_time = avg_total_time &lt; baselines[device.type]['total_time'] improvement_grids = avg_grids_per_second &gt; baselines[device.type]['grids_per_second'] csv_file_path = 'benchmark_results.csv' file_exists = os.path.isfile(csv_file_path) open(csv_file_path, 'a', newline='') csvfile: fieldnames = [ 'run_id', 'datetime', 'run', 'total_time', 'grids_per_second', 'cpu_usage', 'memory_usage', 'batch_size', 'num_batches', 'device', 'n_embd', 'n_head', 'n_layer', 'gpu_usage', 'precision', 'checkpoint_used', 'checkpoint_info' ] writer = csv.dictwriter(csvfile, fieldnames=fieldnames) file_exists: writer.writeheader() result run_results: writer.writerow(result) # write statistical summary csv stats_csv_file_path = 'benchmark_statistics.csv' stats_file_exists = os.path.isfile(stats_csv_file_path) open(stats_csv_file_path, 'a', newline='') csvfile: fieldnames = [ 'run_id', 'datetime', 'avg_total_time', 'std_total_time', 'ci_total_time', 'avg_grids_per_second', 'std_grids_per_second', 'ci_grids_per_second', 'effect_size_time', 'effect_size_grids', 'percent_change_time', 'percent_change_grids', 't_stat_time', 'p_value_time', 't_stat_grids', 'p_value_grids', 'improvement_time', 'improvement_grids', 'practical_significance_time', 'practical_significance_grids', 'precision', 'checkpoint_used', 'checkpoint_info' ] writer = csv.dictwriter(csvfile, fieldnames=fieldnames) stats_file_exists: writer.writeheader() writer.writerow({ 'run_id': run_id, 'datetime': current_time, 'avg_total_time': avg_total_time, 'std_total_time': std_total_time, 'ci_total_time': ci_total_time, 'avg_grids_per_second': avg_grids_per_second, 'std_grids_per_second': std_grids_per_second, 'ci_grids_per_second': ci_grids_per_second, 'effect_size_time': effect_size_time, 'effect_size_grids': effect_size_grids, 'percent_change_time': time_improvement_percent improvement_time else time_regression_percent, 'percent_change_grids': grids_per_second_improvement_percent improvement_grids else grids_per_second_regression_percent, 't_stat_time': t_stat_time, 'p_value_time': p_value_time, 't_stat_grids': t_stat_grids, 'p_value_grids': p_value_grids, 'improvement_time': improvement_time, 'improvement_grids': improvement_grids, 'practical_significance_time': practical_significance_time, 'practical_significance_grids': practical_significance_grids, 'precision': precision, # add precision 'checkpoint_used': checkpoint_used, 'checkpoint_info': checkpoint_info }) print(f"benchmark completed. final results - avg_time: {avg_total_time}, avg_grids: {avg_grids_per_second}") return avg_total_time, avg_grids_per_second def main(args): print(f"starting main function args: {args}") # set float32 matmul precision torch.set_float32_matmul_precision(args.precision) train_set, _ = arckit.load_data() full_dataset = arcdataset(train_set, is_test=false) # create model configuration model_config = modelconfig(n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer) model = gpt2arc(model_config) # run benchmark different configurations run_num range(args.num_full_runs): logger.info(f"starting full benchmark run {run_num + 1}/{args.num_full_runs}") avg_time, avg_grids = benchmark_model( model, full_dataset, batch_size=args.batch_size, num_batches=args.num_batches, num_runs=args.num_runs, device_type=args.device, precision=args.precision, model_checkpoint=args.model_checkpoint ) logger.info(f"full run {run_num + 1} - avg time: {avg_time:.4f}s, avg grids per second: {avg_grids:.2f}") __name__ == "__main__": parser = argparse.argumentparser(description="benchmark gpt2arc model.") parser.add_argument('--model_checkpoint', type=str, help='path model checkpoint') parser.add_argument('--num-runs', type=int, default=20, help='number runs configuration') parser.add_argument('--num-full-runs', type=int, default=1, help='number full configurations run') parser.add_argument('--batch-size', type=int, default=32, help='batch size run') parser.add_argument('--num-batches', type=int, default=10, help='number batches per run') parser.add_argument('--n-embd', type=int, default=64, help='number embeddings model') parser.add_argument('--n-head', type=int, default=2, help='number attention heads') parser.add_argument('--n-layer', type=int, default=1, help='number layers') parser.add_argument('--device', choices=['cpu', 'cuda', 'mps'], default='cpu', help='device run benchmark (cpu, cuda, mps)') parser.add_argument('--precision', choices=['highest', 'high', 'medium'], default='highest', help='precision level float32 matrix multiplications') args = parser.parse_args() main(args)</file><file name="setup.py">setuptools import setup setup()</file><file name="requirements.txt">aider-chat torch&gt;=2.0.0 transformers&gt;=4.0.0 pytorch-lightning&gt;=2.0.0 numpy&gt;=1.20.0 pytest&gt;=6.0 pytest-cov&gt;=2.0 black&gt;=20.8b1 isort&gt;=5.0 flake8&gt;=3.9 ruff scipy psutil ultralytics-thop mypy pynvml tox tensorboard arckit urllib3&gt;=1.26.0 chardet&gt;=5.0.0 wandb</file><file name="README.md"># gpt-2 arc neural reasoning model project implements neural reasoning model based gpt-2 architecture solve tasks abstraction reasoning corpus (arc) challenge. ## features - **data handling**: utilizes custom `arcdataset` class handling preprocessing arc data. - **model architecture**: implements `gpt2arc` model leveraging pre-trained gpt-2 architecture. - **training**: includes `train.py` script training model using pytorch lightning, support logging checkpointing. - **testing**: comprehensive test suite using `pytest` ensure model data integrity. ## installation clone repository install required packages: ```bash git clone https://github.com/yourusername/arc-neural-reasoning-model.git cd arc-neural-reasoning-model pip install -e . ``` development, install extra dependencies: ```bash pip install -e ".[dev]" ``` ## usage ### training model train model, use following command: ``` python src/train.py --train_data path/to/train_data --val_data path/to/val_data --batch_size 32 --learning_rate 1e-4 --max_epochs 10 --use_gpu ``` adjust parameters needed. trained model checkpoints saved `checkpoints` directory. ### evaluating model evaluate trained model test set, use following command: ``` python src/evaluate.py --test_data path/to/test_data --model_checkpoint path/to/model_checkpoint.ckpt --batch_size 32 ``` output evaluation metrics model test dataset. ## running tests run tests, use following command: ``` pytest -v ``` run tests display results, including test coverage. ## contributing [add contribution guidelines here] ## license project licensed mit license - see [license](license) file details.</file><file name="tests/test_model_evaluation.py"># gpt2_arc/tests/test_model_evaluation.py import sys import os # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")) sys.path.insert(0, project_root) print("current pythonpath:", sys.path) import sys import os import json import pytest import torch pytest_mock import mocker src.models.gpt2 import gpt2arc src.config import config, modelconfig, trainingconfig torch.utils.data import dataloader src.utils.helpers import differential_pixel_accuracy src.training.trainer import arctrainer # add project root python path project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')) sys.path.insert(0, project_root) print(f"updated python path: {sys.path}") @pytest.fixture def trainer(): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) return arctrainer(model, none, none, config) import logging unittest.mock import mock # set logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def model(mocker): mock_model = mocker.mock() mock_model.eval = mocker.mock() mock_model.side_effect = lambda inputs, attention_mask=none: torch.randn(1, 4, 2, 2) logger.debug(f"created mock model") return mock_model @pytest.fixture def inputs(): # use predetermined input inputs = torch.tensor([[[[1.0, 0.0], [0.0, 1.0]]]]) logger.debug(f"input shape: {inputs.shape}, dtype: {inputs.dtype}") return inputs @pytest.fixture def targets(): # use predetermined target targets = torch.tensor([[[1, 0], [0, 1]]]) logger.debug(f"targets shape: {targets.shape}, dtype: {targets.dtype}") return targets @pytest.fixture def attention_mask(): mask = torch.ones(1, 4) logger.debug(f"attention mask shape: {mask.shape}, dtype: {mask.dtype}") return mask @pytest.fixture def dataloader(inputs, targets, attention_mask): dataset = list(zip(inputs, targets, attention_mask)) loader = dataloader(dataset, batch_size=1) logger.debug(f"dataloader created {len(loader)} batches") return loader def test_no_grad_calculation(model, inputs, attention_mask): logger.debug("starting test_no_grad_calculation") torch.no_grad(): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"output shape: {outputs.shape}, requires_grad: {outputs.requires_grad}") assert outputs.requires_grad, "gradients tracked evaluation mode." def test_data_loop_for_evaluation(model, dataloader): logger.debug("starting test_data_loop_for_evaluation") model.eval() batch_idx, (inputs, targets, attention_mask) enumerate(dataloader): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"batch {batch_idx}: input shape: {inputs.shape}, output shape: {outputs.shape}") assert outputs none, f"model returned none batch {batch_idx}" assert outputs.shape == (1, 4, 2, 2), f"expected output shape (1, 4, 2, 2), got {outputs.shape}" def test_model_predictions(model, inputs, attention_mask): logger.debug("starting test_model_predictions") outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"model output shape: {outputs.shape}, dtype: {outputs.dtype}") initial_output = model(inputs, attention_mask=attention_mask) logger.debug(f"initial output shape: {initial_output.shape}") # change input check output changes modified_inputs = inputs + 1 modified_output = model(modified_inputs, attention_mask=attention_mask) logger.debug(f"modified output shape: {modified_output.shape}") assert torch.allclose(initial_output, modified_output), "output change input changes" @pytest.mark.skip(reason="needs checked known value arc data") def test_standard_pixel_accuracy(model, inputs, targets): logger.debug("starting test_standard_pixel_accuracy") outputs = model(inputs) logger.debug(f"outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) predicted = outputs.argmax(dim=1) accuracy = (predicted == targets).float().mean().item() logger.debug(f"calculated accuracy: {accuracy}") assert 0.0 &lt;= accuracy &lt;= 1.0, f"accuracy 0 1, got {accuracy}" # test known values known_outputs = torch.floattensor([[[[0.9, 0.1], [0.1, 0.9]]]]) known_targets = torch.tensor([[[0, 1], [1, 0]]]) known_accuracy = (known_outputs.argmax(dim=1) == known_targets).float().mean().item() logger.debug(f"known accuracy: {known_accuracy}") assert known_accuracy == 1.0, f"expected known accuracy 1.0, got {known_accuracy}" def test_differential_pixel_accuracy(model, inputs, targets): logger.debug("starting test_differential_pixel_accuracy") outputs = model(inputs) logger.debug(f"outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) predicted = outputs.argmax(dim=1) diff_accuracy, _, _ = differential_pixel_accuracy(inputs, targets, predicted) logger.debug(f"calculated differential accuracy: {diff_accuracy}") assert 0.0 &lt;= diff_accuracy &lt;= 1.0, f"differential pixel accuracy 0 1, got {diff_accuracy}" # test known values known_inputs = torch.tensor([[[[1, 0], [0, 1]]]]) known_targets = torch.tensor([[[0, 1], [1, 0]]]) known_predicted = torch.tensor([[[0, 1], [1, 0]]]) known_diff_accuracy, known_total_diff, known_correct_diff = differential_pixel_accuracy(known_inputs, known_targets, known_predicted) logger.debug(f"known differential accuracy: {known_diff_accuracy}, total diff: {known_total_diff}, correct diff: {known_correct_diff}") assert known_diff_accuracy == 1.0, f"expected known differential accuracy 1.0, got {known_diff_accuracy}" def test_task_accuracies_tracking(model, dataloader, is_training=false): logger.debug("starting test_task_accuracies_tracking") task_accuracies = {} model.eval() batch_idx, (inputs, targets, attention_mask) enumerate(dataloader): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"batch {batch_idx}: outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) accuracy = (outputs.argmax(dim=1) == targets).float().mean().item() task_id = getattr(dataloader, 'task_id', 'default_task') task_id task_accuracies: task_accuracies[task_id] = {'train': [], 'test': []} task_accuracies[task_id]['train' is_training else 'test'].append(accuracy) logger.debug(f"task accuracies batch {batch_idx}: {task_accuracies}") assert task_accuracies, "task accuracies dictionary empty" assert 'default_task' task_accuracies, "default task logged task accuracies" assert 'test' task_accuracies['default_task'], "test accuracies logged default task" def test_final_metric_calculation(model, dataloader, attention_mask): logger.debug("starting test_final_metric_calculation") model.eval() total_loss, total_accuracy = 0, 0 num_batches = 0 batch_idx, (inputs, targets, attention_mask) enumerate(dataloader): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"batch {batch_idx}: outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(1)), targets.view(-1)) total_loss += loss.item() accuracy = (outputs.argmax(dim=1) == targets).float().mean().item() logger.debug(f"batch {batch_idx}: loss: {loss.item()}, accuracy: {accuracy}") total_accuracy += accuracy num_batches += 1 avg_loss = total_loss / num_batches avg_accuracy = total_accuracy / num_batches logger.debug(f"final metrics - average loss: {avg_loss}, average accuracy: {avg_accuracy}") assert avg_loss &gt;= 0, f"average loss non-negative, got {avg_loss}" assert 0.0 &lt;= avg_accuracy &lt;= 1.0, f"average accuracy 0 1, got {avg_accuracy}" def test_return_of_evaluation_results(model, dataloader, mocker): logger.debug("starting test_return_of_evaluation_results") # simulate simple evaluation result model.evaluate = lambda dataloader: {'loss': 0.5, 'accuracy': 0.75} results = model.evaluate(dataloader) logger.debug(f"evaluation results: {results}") assert "loss" results "accuracy" results, "evaluation results return loss accuracy." assert isinstance(results["loss"], float), f"loss float, got {type(results['loss'])}" assert 0.0 &lt;= results["accuracy"] &lt;= 1.0, f"accuracy 0 1, got {results['accuracy']}" def test_validation_step_with_incorrect_batch_format(trainer): """test validation_step raises valueerror incorrect batch format.""" # create batch incorrect format (e.g., list) incorrect_batch = [ torch.randint(0, 10, (2, 900)), # random input data # labels missing ] logger.debug(f"testing incorrect batch format: {type(incorrect_batch)}") pytest.raises(valueerror, match="batch must contain inputs labels."): trainer.validation_step(incorrect_batch, 0) def test_model_loading_from_checkpoint(mocker): logger.debug("starting test_model_loading_from_checkpoint") # load model checkpoint checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt" logger.debug(f"attempting load checkpoint from: {checkpoint_path}") # check checkpoint file exists os.path.isfile(checkpoint_path): pytest.skip(f"checkpoint file found: {checkpoint_path}") try: checkpoint = torch.load(checkpoint_path) logger.debug(f"checkpoint loaded successfully. keys: {checkpoint.keys()}") except filenotfounderror e: logger.error(f"failed load checkpoint: {str(e)}") pytest.fail(f"failed load checkpoint: {str(e)}") except exception e: logger.error(f"unexpected error: {str(e)}") pytest.fail(f"unexpected error: {str(e)}") # extract print config checkpoint 'config' checkpoint: config_dict = checkpoint['config'] logger.debug(f"config found checkpoint: {json.dumps(config_dict, indent=2)}") else: logger.error("config found checkpoint") pytest.fail("config found checkpoint") # reconstruct modelconfig try: model_config = modelconfig( n_embd=config_dict['n_embd'], n_head=config_dict['n_head'], n_layer=config_dict['n_layer'], dropout=config_dict['dropout'] ) logger.debug(f"modelconfig reconstructed: {model_config}") except keyerror e: logger.error(f"missing key config_dict: {str(e)}") pytest.fail(f"failed reconstruct modelconfig: {str(e)}") # initialize model try: model = gpt2arc(model_config) logger.debug("model initialized successfully") except exception e: logger.error(f"failed initialize model: {str(e)}") pytest.fail(f"failed initialize model: {str(e)}") # load state dict try: state_dict = {k.replace("model.", ""): v k, v checkpoint['state_dict'].items()} model.load_state_dict(state_dict) logger.debug("state dict loaded successfully") except exception e: logger.error(f"failed load state dict: {str(e)}") pytest.fail(f"failed load state dict: {str(e)}") # ensure model evaluation mode model.eval() assert model.training, "model evaluation mode calling eval()" logger.debug("completed test_model_loading_from_checkpoint") def test_checkpoint_contains_model_config(): checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt" logger.debug(f"checking checkpoint file at: {checkpoint_path}") os.path.isfile(checkpoint_path): logger.warning(f"checkpoint file found: {checkpoint_path}") pytest.skip(f"checkpoint file found: {checkpoint_path}") try: checkpoint = torch.load(checkpoint_path) # log keys checkpoint logger.debug(f"checkpoint keys: {checkpoint.keys()}") except filenotfounderror e: logger.error(f"filenotfounderror: {str(e)}") pytest.fail(f"filenotfounderror: {str(e)}") except exception e: logger.error(f"unexpected error: {str(e)}") pytest.fail(f"unexpected error: {str(e)}") # check model configuration assert 'config' checkpoint, "model configuration found checkpoint." model_config = checkpoint['config'] logger.debug(f"model configuration found checkpoint: {model_config}") print("model configuration found checkpoint:", model_config)</file><file name="tests/test_checkpoint_loading.py">import torch import pytest def test_actual_checkpoint_loading(): # path actual checkpoint file checkpoint_path = 'final_model_4fe9801e-c839-454f-a46c-6e94e3c04e81.pth' # load checkpoint checkpoint = torch.load(checkpoint_path) # print keys debugging purposes print("checkpoint keys:", checkpoint.keys()) # check checkpoint contains expected keys expected_keys = ['conv1.weight', 'conv1.bias', 'blocks.0.attention.key.weight'] key expected_keys: assert key checkpoint, f"checkpoint contain expected key: {key}" __name__ == "__main__": pytest.main([__file__])</file><file name="tests/test_integration_experiment.py"># gpt2_arc/tests/test_integration_experiment.py import sys import os # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) sys.path.insert(0, project_root) import pytest import torch gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.utils.results_collector import resultscollector pytorch_lightning import trainer pytorch_lightning.callbacks import modelcheckpoint pytorch_lightning.loggers import tensorboardlogger import arckit @pytest.fixture def setup_experiment(): # load sample task arckit task_id = "007bbfb7" # example task id task_data = arckit.load_single(task_id) train_data = task_data.train val_data = task_data.test print(f"debug: task_data type: {type(task_data)}") print(f"debug: task_data attributes: {dir(task_data)}") print(f"debug: train_data type: {type(train_data)}") print(f"debug: train_data content: {train_data}") print(f"debug: val_data type: {type(val_data)}") print(f"debug: val_data content: {val_data}") train_dataset = arcdataset([{"train": train_data, "test": val_data}]) val_dataset = arcdataset([{"train": train_data, "test": val_data}]) # model config setup model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) training_config = trainingconfig(batch_size=1, learning_rate=1e-4, max_epochs=1) config = config(model=model_config, training=training_config) model = gpt2arc(config=model_config) # trainer setup trainer = arctrainer(model=model, train_dataset=train_dataset, val_dataset=val_dataset, config=config) return trainer, config def test_full_experiment_run(setup_experiment): trainer, config = setup_experiment # pytorch lightning trainer pl_trainer = trainer( max_epochs=config.training.max_epochs, logger=tensorboardlogger("tb_logs", name="arc_model_test"), callbacks=[modelcheckpoint(dirpath="checkpoints", save_top_k=1, monitor="val_loss")], enable_checkpointing=true, enable_progress_bar=false, fast_dev_run=true ) # run training pl_trainer.fit(trainer) # verify results results_summary = trainer.results_collector.get_summary() assert results_summary["experiment_id"] none assert "final_train_loss" results_summary assert "final_val_loss" results_summary @pytest.mark.parametrize("invalid_data", [ ({"input": [[0] * 30 _ range(30)]}), # missing output ({"output": [[0] * 30 _ range(30)]}), # missing input ]) def test_invalid_data_handling(invalid_data): pytest.raises(valueerror): arcdataset([invalid_data]) def test_model_convergence_issue(setup_experiment): trainer, config = setup_experiment trainer.config.training.learning_rate = 1e-10 # set inappropriate learning rate # pytorch lightning trainer pl_trainer = trainer( max_epochs=config.training.max_epochs, logger=false, enable_checkpointing=false, enable_progress_bar=false, fast_dev_run=true ) # run training pl_trainer.fit(trainer) # verify model converge results_summary = trainer.results_collector.get_summary() assert results_summary["final_train_loss"] none assert results_summary["final_train_loss"] &gt; 1.0 # assuming high loss indicates non-convergence</file><file name="tests/test_results_collector.py"># gpt2_arc/tests/test_results_collector.py import unittest gpt2_arc.src.utils.results_collector import resultscollector gpt2_arc.src.config import config, modelconfig, trainingconfig class testresultscollector(unittest.testcase): def setup(self): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) training_config = trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=10) config = config(model=model_config, training=training_config) self.results_collector = resultscollector(config) def test_initialization(self): self.assertisnotnone(self.results_collector.experiment_id) self.assertisnotnone(self.results_collector.timestamp) self.assertequal(self.results_collector.config['model']['n_embd'], 96) def test_update_train_metrics(self): self.results_collector.update_train_metrics(1, {"loss": 0.5}) self.assertin(1, self.results_collector.results["train"]) self.assertequal(self.results_collector.results["train"][1]["loss"], 0.5) def test_update_val_metrics(self): self.results_collector.update_val_metrics(1, {"loss": 0.3}) self.assertin(1, self.results_collector.results["validation"]) self.assertequal(self.results_collector.results["validation"][1]["loss"], 0.3) def test_set_test_results(self): self.results_collector.set_test_results({"accuracy": 0.8}) self.assertequal(self.results_collector.results["test"]["accuracy"], 0.8) def test_add_task_specific_result(self): self.results_collector.add_task_specific_result("task_1", {"accuracy": 0.9}) self.assertin("task_1", self.results_collector.task_specific_results) self.assertequal(self.results_collector.task_specific_results["task_1"]["accuracy"], 0.9) def test_get_summary(self): summary = self.results_collector.get_summary() self.assertequal(summary["experiment_id"], self.results_collector.experiment_id) __name__ == '__main__': unittest.main()</file><file name="tests/test_differential_pixel_accuracy.py"># gpt2_arc/tests/test_differential_pixel_accuracy.py import sys import os # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) sys.path.insert(0, project_root) import torch gpt2_arc.src.utils.helpers import differential_pixel_accuracy gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import modelconfig gpt2_arc.src.data.arc_dataset import arcdataset import arckit def test_identical_inputs_and_targets(): input_tensor = torch.tensor([[1, 2], [3, 4]]) target_tensor = torch.tensor([[1, 2], [3, 4]]) prediction_tensor = torch.tensor([[1, 2], [3, 4]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 identical input target" def test_completely_different_inputs_and_targets(): input_tensor = torch.tensor([[1, 1], [1, 1]]) target_tensor = torch.tensor([[0, 0], [0, 0]]) prediction_tensor = torch.tensor([[0, 0], [0, 0]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 correct prediction differing pixels" def test_partial_differences(): input_tensor = torch.tensor([[1, 2], [3, 4]]) target_tensor = torch.tensor([[1, 0], [3, 0]]) prediction_tensor = torch.tensor([[1, 0], [3, 4]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 0.5, "expected accuracy 0.5 partial correct predictions" def test_empty_tensors(): input_tensor = torch.tensor([]) target_tensor = torch.tensor([]) prediction_tensor = torch.tensor([]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 empty tensors" def test_single_pixel_difference(): input_tensor = torch.tensor([[1]]) target_tensor = torch.tensor([[0]]) prediction_tensor = torch.tensor([[0]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 single pixel difference" def test_differential_pixel_accuracy_with_arckit_data(): print("starting test_differential_pixel_accuracy_with_arckit_data") task_id = "007bbfb7" task_data = arckit.load_single(task_id) print(f"loaded task data: {task_data}") print(f"debug: task_data type: {type(task_data)}") print(f"debug: task_data attributes: {dir(task_data)}") dataset = arcdataset([task_data]) # wrap list simulate multiple tasks input_tensor, target_tensor, _ = dataset[0] print(f"dataset input tensor shape: {input_tensor.shape}") print(f"dataset target tensor shape: {target_tensor.shape}") model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) model = gpt2arc(model_config) model.eval() print("model initialized set eval mode") torch.no_grad(): prediction_tensor = model(input_tensor.unsqueeze(0)) print(f"model prediction tensor shape: {prediction_tensor.shape}") # reverse scaling evaluation original_input = task_data.train[0][0] original_target = task_data.train[0][1] print(f"original input shape: {original_input.shape}") print(f"original target shape: {original_target.shape}") prediction_np = prediction_tensor.squeeze().argmax(dim=0).numpy() print(f"prediction numpy array shape: {prediction_np.shape}") reversed_prediction = dataset.reverse_scaling(original_input, prediction_np) print(f"reversed prediction shape: {reversed_prediction.shape}") # convert back tensors differential_pixel_accuracy # ensure tensors shape input_tensor = torch.tensor(original_input, dtype=torch.float32).resize_(original_target.shape) target_tensor = torch.tensor(original_target, dtype=torch.float32) prediction_tensor = torch.tensor(reversed_prediction, dtype=torch.float32).resize_(original_target.shape) print(f"final input tensor shape: {input_tensor.shape}") print(f"final target tensor shape: {target_tensor.shape}") print(f"final prediction tensor shape: {prediction_tensor.shape}") accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) print(f"differential pixel accuracy task {task_id}: {accuracy}") assert 0 &lt;= accuracy &lt;= 1, f"accuracy 0 1, got {accuracy}" # run tests __name__ == "__main__": test_identical_inputs_and_targets() test_completely_different_inputs_and_targets() test_partial_differences() test_empty_tensors() test_single_pixel_difference() print("all tests passed!")</file><file name="tests/__init__.py" /><file name="tests/test_arc_dataset.py"># gpt2_arc/tests/test_arc_dataset.py import os import numpy np import pytest import torch import random import logging import arckit torch.utils.data import dataloader src.data.arc_dataset import arcdataset, set_debug_mode # set logging tests logger = logging.getlogger(__name__) logger.setlevel(logging.error) @pytest.fixture(scope="module") def debug_mode(): set_debug_mode(true) yield set_debug_mode(false) unittest.mock import mock arckit.data import taskset @pytest.fixture def sample_data(): return [ {'input': [[1, 0], [0, 1]], 'output': [[0, 1], [1, 0]]}, {'input': [[0, 1], [1, 0]], 'output': [[1, 0], [0, 1]]} ] @pytest.fixture def mock_taskset(): mock_task = mock() mock_task.id = "mock_task_1" mock_task.train = [ (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])), (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]])) ] mock_task.test = [ (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]])) ] mock_taskset = mock(spec=taskset) mock_taskset.tasks = [mock_task] return mock_taskset def test_arc_dataset_initialization(sample_data, debug_mode): dataset = arcdataset(sample_data, debug=true) logger.debug(f"dataset length: {len(dataset)}, expected: {len(sample_data)}") assert len(dataset) == len(sample_data), "dataset length mismatch" input_grid, output_grid, *_ = dataset[0] logger.debug(f"input grid shape: {input_grid.shape}, expected: (1, 30, 30)") logger.debug(f"output grid shape: {output_grid.shape}, expected: (1, 30, 30)") assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" # update shape check match new preprocessing logic assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # verify original data preserved center padded grid center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] logger.debug(f"center input:\n{center_input}") logger.debug(f"center output:\n{center_output}") assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" dataset = arcdataset(sample_data) assert len(dataset) == 2, "dataset 2 samples" input_grid, output_grid, *_ = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" # update shape check match new preprocessing logic assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # verify original data preserved center padded grid center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" #skip @pytest.mark.skip(reason="skipping test synthetic data test problematic") def test_arc_dataset_synthetic_data(debug_mode): synthetic_data_path = "/volumes/totallynotaharddrive/arc-neural-reasoning-model/syntheticarc/tasks" assert os.path.isdir(synthetic_data_path), f"directory exist: {synthetic_data_path}" train_dataset = arcdataset(synthetic_data_path, is_test=false, debug=true) test_dataset = arcdataset(synthetic_data_path, is_test=true, debug=true) assert len(train_dataset) &gt; 0, "synthetic train dataset empty" assert len(test_dataset) &gt; 0, "synthetic test dataset empty" logger.debug(f"loaded {len(train_dataset.data)} synthetic tasks") logger.debug(f"total train dataset length: {len(train_dataset)}") logger.debug(f"total test dataset length: {len(test_dataset)}") total_train = sum(len(task['train']) task train_dataset.data) total_test = sum(len(task['test']) task test_dataset.data) logger.debug(f"total train samples: {total_train}") logger.debug(f"total test samples: {total_test}") i, task enumerate(train_dataset.data): print(f"task {i} - train samples: {len(task['train'])}, test samples: {len(task['test'])}") assert len(train_dataset) == total_train, f"train dataset length ({len(train_dataset)}) match total train samples ({total_train})" assert len(test_dataset) == total_test, f"test dataset length ({len(test_dataset)}) match total test samples ({total_test})" len(train_dataset) == 0: pytest.skip("train dataset empty; skipping random sample tests.") print(f"train dataset size: {len(train_dataset)}") print(f"test dataset size: {len(test_dataset)}") len(train_dataset) &lt; 3: pytest.skip("not enough data train dataset random sampling tests.") # test random samples train dataset range(3): idx = random.choice(range(len(train_dataset))) try: print(f"\ntrain sample {i + 1}:") print(f"generated index: {idx}") input_grid, output_grid = train_dataset[idx] print(f"input grid shape: {input_grid.shape}") print(f"output grid shape: {output_grid.shape}") except indexerror e: print(f"error: attempted access index {idx} range. train dataset size {len(train_dataset)}.") pytest.fail(f"generated index {idx} range train dataset size {len(train_dataset)}: {str(e)}") # verify grid sizes max_h, max_w = train_dataset.max_grid_size assert max_h &gt; 0 max_w &gt; 0, "grid size positive" print(f"maximum grid size: {train_dataset.max_grid_size}") # verify access train test splits assert len(train_dataset.data) &gt; 0, "dataset contain least one task" assert 'train' train_dataset.data[0], "each task 'train' split" assert 'test' train_dataset.data[0], "each task 'test' split" print(f"train dataset length: {len(train_dataset)}") print(f"test dataset length: {len(test_dataset)}") def test_arc_dataset_getitem(sample_data): dataset = arcdataset(sample_data) input_grid, output_grid, *_ = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # check original data preserved center center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" def test_arc_dataset_len(sample_data): print("debugging: entering test_arc_dataset_len") print(f"debugging: sample_data = {sample_data}") dataset = arcdataset(sample_data) print(f"debugging: len(dataset) = {len(dataset)}, len(sample_data) = {len(sample_data)}") assert len(dataset) == len(sample_data), "dataset length match input data length" print("debugging: exiting test_arc_dataset_len") def test_arc_dataset_invalid_data(sample_data): invalid_data = [{"input": [1, 0], "output": [[0, 1], [1, 0]]}] pytest.raises(valueerror): arcdataset(invalid_data) invalid_data = [{"input": [[1, 0], [0, 1]], "output": "not list"}] pytest.raises(valueerror): arcdataset(invalid_data) def test_arc_dataset_preprocess_grid(sample_data): dataset = arcdataset(sample_data, num_symbols=10) input_grid, output_grid, *_ = dataset[0] print(f"input grid shape: {input_grid.shape}") print(f"output grid shape: {output_grid.shape}") print(f"input grid content:\n{input_grid}") print(f"output grid content:\n{output_grid}") # check grids indeed 3d assert input_grid.ndim == 3, f"expected 3d input grid, got {input_grid.ndim}d" assert output_grid.ndim == 3, f"expected 3d output grid, got {output_grid.ndim}d" # check shape (1, 30, 30) assert input_grid.shape == (1, 30, 30), f"preprocessed grid shape (1, 30, 30), got {input_grid.shape}" assert output_grid.shape == (1, 30, 30), f"preprocessed grid shape (1, 30, 30), got {output_grid.shape}" # check original data preserved center expected_input = torch.zeros((1, 30, 30)) expected_input[0, 14:16, 14:16] = torch.tensor([[1., 0.], [0., 1.]]) expected_output = torch.zeros((1, 30, 30)) expected_output[0, 14:16, 14:16] = torch.tensor([[0., 1.], [1., 0.]]) print(f"expected input:\n{expected_input}") print(f"expected output:\n{expected_output}") assert torch.allclose(input_grid, expected_input), "input grid data mismatch" assert torch.allclose(output_grid, expected_output), "output grid data mismatch" @pytest.fixture def mock_taskset(): mock_task = mock() mock_task.id = "mock_task_1" mock_task.train = [ (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])), (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]])) ] mock_task.test = [ (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]])) ] mock_taskset = mock(spec=taskset) mock_taskset.tasks = [mock_task] return mock_taskset def test_collate_fn_output(): sample_data = [ {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}, {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]}, ] dataset = arcdataset(sample_data) dataloader = dataloader(dataset, batch_size=2, collate_fn=arcdataset.collate_fn) batch = next(iter(dataloader)) assert isinstance(batch, list), "collate function return list" assert len(batch) == 3, "collate function return list 3 elements" assert isinstance(batch[0], torch.tensor), "first element tensor (inputs)" assert isinstance(batch[1], torch.tensor), "second element tensor (outputs)" assert batch[0].shape == (2, 1, 30, 30), "input tensor shape (batch_size, 1, 30, 30)" assert batch[1].shape == (2, 1, 30, 30), "output tensor shape (batch_size, 1, 30, 30)" assert batch[0].dtype == torch.float32, "input tensor type float32" assert batch[1].dtype == torch.float32, "output tensor type float32" def test_getitem_output(): sample_data = [ {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}, ] dataset = arcdataset(sample_data) input_grid, output_grid, *_ = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" assert input_grid.dtype == torch.float32, "input grid float32" assert output_grid.dtype == torch.float32, "output grid float32" #skip @pytest.mark.skip(reason="skipping test problematic") def test_arc_dataset_taskset_initialization(mock_taskset): import logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) logger.debug(f"mock taskset: {mock_taskset}") logger.debug(f"mock taskset attributes: {dir(mock_taskset)}") print(f"mock task train data: {mock_taskset.tasks[0].train}") print(f"mock task test data: {mock_taskset.tasks[0].test}") dataset = arcdataset(mock_taskset) logger.debug(f"dataset length: {len(dataset)}") print(f"dataset length: {len(dataset)}, expected: 3") assert len(dataset) == 3, "dataset 3 samples (2 train + 1 test)" input_grid, output_grid, *_ = dataset[0] print(f"input grid shape: {input_grid.shape}, expected: (1, 30, 30)") print(f"output grid shape: {output_grid.shape}, expected: (1, 30, 30)") assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # check original data preserved center center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] print(f"center input: {center_input}") print(f"center output: {center_output}") assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" import logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) logger.debug(f"mock taskset: {mock_taskset}") logger.debug(f"mock taskset attributes: {dir(mock_taskset)}") dataset = arcdataset(mock_taskset) logger.debug(f"dataset length: {len(dataset)}") assert len(dataset) == 3, "dataset 3 samples (2 train + 1 test)" input_grid, output_grid = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # check original data preserved center center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" torch.utils.data import dataloader def test_arc_dataset_collate_fn(sample_data): logger.debug("starting test_arc_dataset_collate_fn") dataset = arcdataset(sample_data) dataloader = dataloader(dataset, batch_size=2, collate_fn=arcdataset.collate_fn) batch = next(iter(dataloader)) input_batch, output_batch, *_ = batch logger.debug(f"collated batch shapes - inputs: {input_batch.shape}, outputs: {output_batch.shape}") assert input_batch.shape == (2, 1, 30, 30), "batched input shape (2, 1, 30, 30)" assert output_batch.shape == (2, 1, 30, 30), "batched output shape (2, 1, 30, 30)" logger.debug("completed test_arc_dataset_collate_fn") def test_arc_dataset_variable_size_grids(sample_data): logger.debug("starting test_arc_dataset_variable_size_grids") variable_data = sample_data + [{"input": [[1, 0, 2], [0, 2, 1], [2, 1, 0]], "output": [[2, 1, 0], [1, 0, 2], [0, 2, 1]]}] dataset = arcdataset(variable_data) # check first sample (2x2) input_grid_1, output_grid_1, *_ = dataset[0] assert input_grid_1.shape == (1, 30, 30), "first sample shape (1, 30, 30)" assert output_grid_1.shape == (1, 30, 30), "first sample shape (1, 30, 30)" # check center first sample (2x2) center_input_1 = input_grid_1[0, 14:16, 14:16] center_output_1 = output_grid_1[0, 14:16, 14:16] assert torch.allclose(center_input_1, torch.tensor([[1., 0.], [0., 1.]])), "first sample input data preserved correctly" assert torch.allclose(center_output_1, torch.tensor([[0., 1.], [1., 0.]])), "first sample output data preserved correctly" # check third sample (3x3) input_grid_2, output_grid_2, *_ = dataset[2] assert input_grid_2.shape == (1, 30, 30), "third sample shape (1, 30, 30)" assert output_grid_2.shape == (1, 30, 30), "third sample shape (1, 30, 30)" # check center third sample (3x3) center_input_2 = input_grid_2[0, 13:16, 13:16] center_output_2 = output_grid_2[0, 13:16, 13:16] assert torch.allclose(center_input_2, torch.tensor([[1., 0., 2.], [0., 2., 1.], [2., 1., 0.]])), f"third sample input data preserved correctly. got:\n{center_input_2}" assert torch.allclose(center_output_2, torch.tensor([[2., 1., 0.], [1., 0., 2.], [0., 2., 1.]])), f"third sample output data preserved correctly. got:\n{center_output_2}" logger.debug("completed test_arc_dataset_variable_size_grids") def test_arc_dataset_with_arckit_data_get_task_id(): # load data using arckit train_set, _ = arckit.load_data() # initialize dataset dataset = arcdataset(train_set, is_test=false) # check __getitem__ returns correct structure input_grid, output_grid, task_id = dataset[0] assert isinstance(input_grid, torch.tensor), "input grid torch.tensor" assert isinstance(output_grid, torch.tensor), "output grid torch.tensor" assert isinstance(task_id, str), "task id string" # test collate_fn batch = [dataset[i] range(2)] # create batch two samples collated_inputs, collated_outputs, collated_task_ids = arcdataset.collate_fn(batch) assert len(collated_task_ids) == 2, "batch size 2" assert collated_inputs.shape[0] == 2, "batch size 2" assert collated_outputs.shape[0] == 2, "batch size 2"</file><file name="tests/test_gpt2.py"># gpt2_arc/tests/test_gpt2.py import logging import pytest import torch src.config import modelconfig src.models.gpt2 import gpt2arc, attention, feedforward, transformerblock logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def model(): config = modelconfig() return gpt2arc(config) def test_gpt2arc_initialization(model): assert isinstance(model, gpt2arc) assert hasattr(model, "conv1") # check conv1 instead token_embedding assert hasattr(model, "blocks") assert hasattr(model, "ln_f") assert hasattr(model, "config") def test_gpt2arc_forward_pass(model): batch_size = 2 height = 30 width = 30 input_ids = torch.randn(batch_size, 1, height, width) # simulate image-like input attention_mask = torch.ones((batch_size, height * width)) output = model(input_ids, attention_mask) assert isinstance(output, torch.tensor) assert output.shape == (batch_size, height * width, model.config.n_embd) logger.debug(f"output shape: {output.shape}") def test_gpt2arc_output_values(model): logger.debug("testing gpt2arc output values") batch_size = 1 channels = 1 height = 30 width = 30 input_ids = torch.randn(batch_size, channels, height, width) # simulate image-like input attention_mask = torch.ones((batch_size, height * width)) output = model(input_ids, attention_mask) assert torch.isnan(output).any(), "output contains nan values" def test_gpt2arc_forward_pass(model): batch_size = 2 channels = 1 height = 30 width = 30 input_ids = torch.randn(batch_size, channels, height, width) # simulate image-like input attention_mask = torch.ones((batch_size, height * width)) output_with_mask = model(input_ids, attention_mask) output_without_mask = model(input_ids) logger.debug( f"difference outputs: {(output_with_mask - output_without_mask).abs().mean()}" ) def test_attention_module(): logger.debug("testing attention module") attention = attention(n_embd=768, n_head=12) x = torch.randn(2, 10, 768) output = attention(x) assert output.shape == x.shape logger.debug(f"attention input shape: {x.shape}, output shape: {output.shape}") def test_feedforward_module(): logger.debug("testing feedforward module") ff = feedforward(n_embd=768) x = torch.randn(2, 10, 768) output = ff(x) assert output.shape == x.shape logger.debug(f"feedforward input shape: {x.shape}, output shape: {output.shape}") def test_transformer_block(): logger.debug("testing transformerblock") block = transformerblock(n_embd=768, n_head=12) x = torch.randn(2, 10, 768) output = block(x) assert output.shape == x.shape logger.debug( f"transformerblock input shape: {x.shape}, output shape: {output.shape}" )</file><file name="tests/test_train.py"># gpt2_arc/tests/test_train.py import os import sys sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))) import os import sys import pytest import logging logger = logging.getlogger(__name__) def set_logging_level(level=logging.error): logger = logging.getlogger() logger.setlevel(level) # add project root pythonpath sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))) import argparse unittest.mock import any, magicmock, patch import pytorch_lightning pl import torch gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.train import main gpt2_arc.src.training.trainer import arctrainer @pytest.fixture def mock_args(): args = argparse.namespace() args.train_data = "mock_train_data.json" args.val_data = "mock_val_data.json" args.batch_size = 32 args.learning_rate = 1e-4 args.max_epochs = 10 args.use_gpu = false args.no_logging = false args.no_checkpointing = false args.no_progress_bar = false args.log_level = "info" # add log_level attribute args.fast_dev_run = false # add fast_dev_run attribute args.project = "test_project" # add project attribute mock_args return args @pytest.fixture def mock_dataset(): dataset = magicmock(spec=arcdataset) dataset.data = [{"input": "mock input", "output": "mock output"}] dataset.__len__.return_value = 100 return dataset src.config import config, modelconfig, trainingconfig @pytest.fixture def model(): config = config(model=modelconfig(), training=trainingconfig()) return gpt2arc(config.model) @pytest.fixture def trainer(): model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) return arctrainer(model, none, none, config) @pytest.fixture def mock_pl_trainer(): return magicmock(spec=pl.trainer) # existing gpt2arc model tests def test_gpt2arc_initialization(model): assert isinstance(model, gpt2arc) assert hasattr(model, "conv1") # check conv1 instead token_embedding assert hasattr(model, "blocks") assert hasattr(model, "ln_f") assert hasattr(model, "config") def test_gpt2arc_forward_pass(model): batch_size = 2 height = width = 30 seq_length = height * width input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output_with_mask = model(input_ids, attention_mask) output_without_mask = model(input_ids) assert isinstance(output_with_mask, torch.tensor) assert output_with_mask.shape == (batch_size, seq_length, model.config.n_embd) assert isinstance(output_without_mask, torch.tensor) assert output_without_mask.shape == (batch_size, seq_length, model.config.n_embd) logger.debug(f"difference outputs: {(output_with_mask - output_without_mask).abs().mean()}") def test_gpt2arc_output_values(model): logger.debug("testing gpt2arc output values") batch_size = 1 height = width = 30 seq_length = height * width input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output = model(input_ids, attention_mask) assert torch.isnan(output).any(), "output contains nan values" assert torch.isinf(output).any(), "output contains infinity values" def test_gpt2arc_attention_mask(model): batch_size = 2 channels = 1 height = 30 width = 30 input_ids = torch.randint(0, 2, (batch_size, channels, height, width)) attention_mask = torch.zeros((batch_size, height * width)) attention_mask[:, :450] = 1 # attend first half pixels output_with_mask = model(input_ids, attention_mask) output_without_mask = model(input_ids) assert torch.allclose(output_with_mask, output_without_mask), "attention mask affect output" # new tests train.py def test_logging(mock_args, mock_dataset, model, mock_pl_trainer): print("entering test_logging") patch( "gpt2_arc.src.training.train.arcdataset", return_value=mock_dataset ), patch("gpt2_arc.src.training.train.gpt2arc", return_value=model), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( ) mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer", return_value=mock_pl_trainer ), patch("gpt2_arc.src.training.train.tensorboardlogger") mock_logger, patch( "gpt2_arc.src.training.train.modelcheckpoint" ), patch("torch.utils.data.dataloader") mock_dataloader: mock_dataloader.return_value = magicmock() # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "1234", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } mock_trainer_instance.results_collector = mock_results_collector # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_logger.assert_called_once_with("tb_logs", name="arc_model") def test_fit_call(mock_args, mock_dataset, model): mock_pl_trainer = magicmock() mock_pl_trainer.fit = magicmock() print("entering test_fit_call") patch( "gpt2_arc.src.training.train.arcdataset", return_value=mock_dataset ), patch("gpt2_arc.src.training.train.gpt2arc", return_value=model), patch( "gpt2_arc.src.training.train.arctrainer" ) mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer", return_value=mock_pl_trainer ), patch("gpt2_arc.src.training.train.tensorboardlogger"), patch( "gpt2_arc.src.training.train.modelcheckpoint" ), patch("torch.utils.data.dataloader", new_callable=magicmock) mock_dataloader: mock_dataloader.return_value = magicmock() # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_pl_trainer.fit.assert_called_once_with(mock_trainer_instance) def test_data_loading(mock_args): patch( "gpt2_arc.src.data.arc_dataset.arcdataset.__init__", return_value=none ) mock_init: arcdataset(mock_args.train_data) mock_init.assert_called_once_with(mock_args.train_data) def test_trainer_initialization(model, mock_dataset): config = config(model=modelconfig(), training=trainingconfig()) trainer = arctrainer( model=model, train_dataset=mock_dataset, val_dataset=mock_dataset, config=config ) assert isinstance(trainer, arctrainer) assert trainer.model == model assert trainer.train_dataset == mock_dataset assert trainer.val_dataset == mock_dataset assert trainer.batch_size == 32 assert trainer.lr == 1e-4 @pytest.mark.parametrize("batch_size", [1, 1000000]) def test_batch_size_extremes(mock_args, batch_size): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=batch_size, learning_rate=5e-4, max_epochs=10)) mock_args.batch_size = batch_size mock_args.no_logging = true mock_args.no_checkpointing = true mock_args.no_progress_bar = true mock_args.use_gpu = false patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch("gpt2_arc.src.training.trainer.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ) mock_trainer, patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) main(mock_args) mock_trainer.assert_called_with( max_epochs=config.training.max_epochs, logger=false, callbacks=none, enable_checkpointing=false, enable_progress_bar=false, fast_dev_run=false, # include fast_dev_run expected call gradient_clip_val=1.0, accelerator='cpu' ) @pytest.mark.parametrize("learning_rate", [1e-10, 1000]) def test_learning_rate_extremes(mock_args, learning_rate): set_logging_level(logging.warning) # suppress info debug messages mock_args.learning_rate = learning_rate logger.debug(f"testing learning_rate: {learning_rate}") patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ), patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "1234", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } main(mock_args) # raise exception def test_non_existent_train_data(mock_args): mock_args.train_data = "non_existent_path.json" pytest.raises(filenotfounderror): os.path.exists(mock_args.train_data): raise filenotfounderror(f"file found: {mock_args.train_data}") main(mock_args) def test_gpu_not_available(mock_args): mock_args.use_gpu = true mock_args.no_logging = false mock_args.no_checkpointing = false mock_args.no_progress_bar = false patch("torch.cuda.is_available", return_value=false), patch( "gpt2_arc.src.training.train.arcdataset" ), patch("gpt2_arc.src.training.train.gpt2arc"), patch( "gpt2_arc.src.training.train.arctrainer" "gpt2_arc.src.training.train.arctrainer" ), patch("gpt2_arc.src.training.train.pl.trainer") mock_trainer, \ patch("gpt2_arc.src.utils.results_collector.resultscollector.get_summary") mock_get_summary: # mock get_summary method return serializable dictionary mock_get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # use simple function instead magicmock main def simple_main(args): pass simple_main(mock_args) mock_trainer.assert_called_with( max_epochs=mock_args.max_epochs, logger=any, callbacks=any, enable_checkpointing=true, enable_progress_bar=true, fast_dev_run=false, gradient_clip_val=1.0, accelerator='cpu' ) hypothesis import healthcheck, given, settings hypothesis import strategies st @settings(suppress_health_check=[healthcheck.function_scoped_fixture], deadline=none) @given(batch_size=st.integers(min_value=1, max_value=1024)) def test_valid_batch_sizes(mock_args, batch_size): mock_args.batch_size = batch_size patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ), patch("gpt2_arc.src.training.train.resultscollector.get_summary", return_value={ "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} }), patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) main(mock_args) # raise exception @settings(suppress_health_check=[healthcheck.function_scoped_fixture], deadline=none) @given( learning_rate=st.floats( min_value=1e-6, max_value=1.0, allow_nan=false, allow_infinity=false ) ) def test_valid_learning_rates(mock_args, learning_rate): mock_args.learning_rate = learning_rate import glob import os patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ) mock_trainer, patch( "torch.utils.data.dataloader" ) mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) try: # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } mock_results_collector.config = {"model": {}, "training": {}} # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) # raise exception finally: # ensure cleanup generated files file glob.glob("results/summary_*.json"): os.remove(file) def test_end_to_end_training(mock_args, tmp_path): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=5e-4, max_epochs=2)) checkpoint_dir = tmp_path / "checkpoints" checkpoint_dir.mkdir() mock_args.checkpoint_dir = str(checkpoint_dir) patch("gpt2_arc.src.training.train.arcdataset"), \ patch("gpt2_arc.src.training.train.gpt2arc"), \ patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, \ patch("gpt2_arc.src.training.train.pl.trainer") mock_trainer, \ patch("gpt2_arc.src.training.train.modelcheckpoint") mock_checkpoint, \ patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_trainer.return_value.fit.assert_called_once() mock_checkpoint.assert_called_once() def test_tensorboard_logging(mock_args, tmp_path): log_dir = tmp_path / "tb_logs" log_dir.mkdir() patch("gpt2_arc.src.training.train.arcdataset"), \ patch("gpt2_arc.src.training.train.gpt2arc"), \ patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, \ patch("gpt2_arc.src.training.train.pl.trainer"), \ patch("gpt2_arc.src.training.train.tensorboardlogger") mock_logger, \ patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_logger.assert_called_once_with("tb_logs", name="arc_model") # additional test gpt2arc model training context def test_arctrainer_forward_pass(trainer): batch_size = 2 seq_length = 900 # 30x30 grid input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output = trainer(input_ids, attention_mask) assert isinstance(output, torch.tensor) assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd) def test_arctrainer_training_step(trainer): batch_size = 2 height = width = 30 # 30x30 grid seq_length = height * width vocab_size = 10 # use small vocab size testing batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), # inputs torch.ones((batch_size, seq_length)).float(), # labels torch.randint(0, vocab_size, (batch_size, seq_length)).long() # task_ids ) pl_trainer = magicmock() pl_trainer.validate = magicmock() pl_trainer.validate(trainer, dataloaders=[batch]) @pytest.mark.parametrize("batch_format", ["tuple", "dict"]) def test_arctrainer_batch_format(trainer, batch_format): batch_size = 2 height = width = 30 # 30x30 grid seq_length = height * width vocab_size = 10 # use small vocab size testing batch_format == "tuple": batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), torch.ones((batch_size, seq_length)).float(), torch.randint(0, vocab_size, (batch_size, seq_length)).long(), ) else: batch = { "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), "attention_mask": torch.ones((batch_size, seq_length)).float(), "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), } loss = trainer.training_step(batch, 0) assert isinstance(loss, torch.tensor) assert loss.shape == torch.size([]) # loss scalar assert torch.isnan(loss).any(), "loss contains nan values" assert torch.isinf(loss).any(), "loss contains infinity values"</file><file name="tests/test_benchmark.py"># gpt2_arc/tests/test_benchmark.py import pytest import torch import numpy np unittest.mock import magicmock, patch benchmark import benchmark_model, main, baselines src.config import modelconfig src.models.gpt2 import gpt2arc # mock classes fixtures @pytest.fixture def mock_model(): model = magicmock(spec=gpt2arc) model.forward = magicmock() # ensure forward mock return model @pytest.fixture def mock_dataset(): dataset = magicmock() dataset.__getitem__.return_value = ( torch.randn(1, 30, 30), # inputs torch.randint(0, 10, (1, 30, 30)), # outputs "task_1" # task_id ) dataset.__len__.return_value = 100 return dataset @pytest.fixture def mock_dataloader(): dataloader = magicmock() dataloader.__iter__.return_value = iter([ ( torch.randn(32, 1, 30, 30), # inputs torch.randn(32, 1, 30, 30), # outputs f"task_{i}" # task_ids ) range(10) ]) return dataloader # tests benchmark_model function def test_benchmark_model_basic(mock_model, mock_dataset, mock_dataloader): patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader), \ patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=false), \ patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=false): avg_time, avg_grids = benchmark_model(mock_model, mock_dataset) assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) assert avg_time &gt; 0 assert avg_grids &gt; 0 @pytest.mark.parametrize("batch_size,num_batches,num_runs", [ (16, 5, 10), (64, 20, 5), (128, 2, 3) ]) def test_benchmark_model_parameters(mock_model, mock_dataset, mock_dataloader, batch_size, num_batches, num_runs): patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader), \ patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=false), \ patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=false): avg_time, avg_grids = benchmark_model( mock_model, mock_dataset, batch_size=batch_size, num_batches=num_batches, num_runs=num_runs ) assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) def test_benchmark_model_cuda(mock_model, mock_dataset, mock_dataloader): patch('benchmark.torch.cuda.is_available', return_value=true), \ patch('benchmark.torch.cuda.synchronize'), \ patch('benchmark.dataloader', return_value=mock_dataloader), \ patch('benchmark.torch.compile', return_value=mock_model): torch.cuda.is_available(): pytest.skip("cuda available system") try: avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='cuda') except assertionerror e: "torch compiled cuda enabled" str(e): pytest.skip("pytorch compiled cuda support") else: raise assert isinstance(avg_time, float) assert isinstance(avg_grids, float) assert avg_time &gt;= 0 assert avg_grids &gt;= 0 def test_benchmark_model_mps(mock_model, mock_dataset, mock_dataloader): patch('benchmark.torch.backends.mps.is_available', return_value=true), \ patch('benchmark.dataloader', return_value=mock_dataloader): avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='mps') assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) def test_benchmark_model_error_handling(mock_model, mock_dataset): pytest.raises(valueerror, match="invalid device type"): benchmark_model(mock_model, mock_dataset, device_type='invalid_device') # tests main function @pytest.fixture def mock_argparse(): patch('benchmark.argparse.argumentparser') mock_argparse: mock_args = magicmock() mock_args.num_runs = 5 mock_args.num_full_runs = 1 mock_args.batch_size = 32 mock_args.num_batches = 10 mock_args.n_embd = 64 mock_args.n_head = 2 mock_args.n_layer = 1 mock_args.device = 'cpu' mock_args.precision = 'highest' mock_argparse.return_value.parse_args.return_value = mock_args yield mock_argparse def test_main_function(mock_argparse, mock_dataset, mock_model): patch('benchmark.arckit.load_data', return_value=(mock_dataset, none)), \ patch('benchmark.arcdataset', return_value=mock_dataset), \ patch('benchmark.gpt2arc', return_value=mock_model), \ patch('benchmark.benchmark_model', return_value=(1.0, 100.0)): main(mock_argparse.return_value.parse_args()) # performance tests @pytest.mark.benchmark(group="benchmark_model") def test_benchmark_model_performance(benchmark, mock_model): # create mock dataset one item mock_dataset = magicmock() mock_dataset.__len__.return_value = 1 mock_dataset.__getitem__.return_value = ( torch.randn(1, 30, 30), # input torch.randint(0, 10, (1, 30, 30)), # output "task_1" # task_id ) # create mock dataloader returns mock dataset item mock_dataloader = magicmock() mock_dataloader.__iter__.return_value = iter([mock_dataset.__getitem__()]) patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader), \ patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=false), \ patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=false): result = benchmark( benchmark_model, mock_model, mock_dataset, batch_size=1, num_batches=1, device_type='cpu', precision='medium', model_checkpoint=none ) assert isinstance(result, tuple), f"expected tuple, got {type(result)}" assert len(result) == 2, f"expected tuple length 2, got length {len(result)}" avg_time, grids_per_second = result print(f"benchmark result - average time: {avg_time}, grids per second: {grids_per_second}") assert isinstance(avg_time, float), f"expected float avg_time, got {type(avg_time)}" assert isinstance(grids_per_second, float), f"expected float grids_per_second, got {type(grids_per_second)}" assert avg_time &gt;= 0, f"average time non-negative, got {avg_time}" assert grids_per_second &gt;= 0, f"grids per second non-negative, got {grids_per_second}" avg_time &gt; 0: assert grids_per_second &gt; 0, f"grids per second positive avg_time &gt; 0, got {grids_per_second}" # edge case tests def test_benchmark_model_empty_dataset(mock_model): empty_dataset = magicmock() empty_dataset.__len__.return_value = 0 pytest.raises(valueerror, match="dataset empty"): benchmark_model(mock_model, empty_dataset) def test_benchmark_model_single_item_dataset(mock_model): single_item_dataset = magicmock() single_item_dataset.__len__.return_value = 1 mock_dataloader = magicmock() mock_dataloader.__iter__.return_value = iter([ (torch.randn(1, 1, 30, 30), torch.randn(1, 1, 30, 30), "task_1") ]) patch('benchmark.dataloader', return_value=mock_dataloader): avg_time, avg_grids = benchmark_model(mock_model, single_item_dataset, batch_size=1, num_batches=1) assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) # error handling tests def test_benchmark_model_with_correct_data(mock_model, mock_dataset, mock_dataloader): patch('benchmark.dataloader', return_value=mock_dataloader): avg_time, avg_grids = benchmark_model(mock_model, mock_dataset) assert isinstance(avg_time, float), "avg_time float" assert isinstance(avg_grids, float), "avg_grids float" assert avg_time &gt; 0, "avg_time positive" assert avg_grids &gt; 0, "avg_grids positive" def test_benchmark_model_model_error(mock_model, mock_dataset, mock_dataloader): # mock model's forward method raise runtimeerror execution mock_model.forward = magicmock(side_effect=runtimeerror("model execution failed")) patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader): pytest.raises(runtimeerror, match="model execution failed"): print("debug: invoking benchmark_model") benchmark_model(mock_model, mock_dataset, device_type='cpu') # ensure forward method called assert mock_model.forward.call_count &gt; 0, "debug: forward method called" print(f"debug: forward method call count: {mock_model.forward.call_count}") #skip @pytest.mark.skip(reason="i dont want crash computer") def test_benchmark_model_out_of_memory(mock_model, mock_dataset, mock_dataloader): mock_model.side_effect = torch.cuda.outofmemoryerror("cuda memory") patch('benchmark.dataloader', return_value=mock_dataloader), \ patch('benchmark.torch.cuda.is_available', return_value=true), \ pytest.raises(torch.cuda.outofmemoryerror, match="cuda memory"): benchmark_model(mock_model, mock_dataset, device_type='cuda') # precision tests @pytest.fixture def mock_torch(): return magicmock() @pytest.mark.parametrize("precision", ['highest', 'high', 'medium']) def test_benchmark_model_precision(mock_model, mock_dataset, mock_torch, precision): patch('gpt2_arc.benchmark.dataloader') mock_dataloader_class: mock_dataloader = magicmock() mock_dataloader.__iter__.return_value = iter([ ( torch.randn(1, 1, 30, 30), # inputs torch.randint(0, 10, (1, 30, 30)), # outputs "task_1" # task_id ) ]) mock_dataloader_class.return_value = mock_dataloader patch('gpt2_arc.benchmark.torch.set_float32_matmul_precision') mock_set_precision: benchmark_model(mock_model, mock_dataset, precision=precision) mock_set_precision.assert_called_once_with(precision) # csv output tests def test_csv_output(mock_model, mock_dataset, mock_dataloader, tmp_path): csv_file = tmp_path / "benchmark_results.csv" stats_csv_file = tmp_path / "benchmark_statistics.csv" patch('benchmark.dataloader', return_value=mock_dataloader), \ patch('benchmark.csv.writer') mock_csv_writer: benchmark_model(mock_model, mock_dataset) assert mock_csv_writer.call_count == 2 # one results, one statistics # test suite execution __name__ == '__main__': pytest.main(['-v', '--cov=benchmark', '--cov-report=term-missing'])</file><file name="tests/test_end_to_end.py"># gpt2_arc/tests/test_end_to_end.py import pytest import torch import numpy np src.data.arc_dataset import arcdataset src.models.gpt2 import gpt2arc src.training.trainer import arctrainer src.config import config, modelconfig, trainingconfig import pytorch_lightning pl import time import logging import os thop import profile, clever_format # import thop pytest import approx # set logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def arc_data_path(): # adjust path location arc dataset json file return "/volumes/totallynotaharddrive/arc-neural-reasoning-model/syntheticarc/tasks/1c786137.json" import arckit def test_end_to_end(): logger.debug("starting end-to-end test") try: # load data using arckit logger.debug("loading data using arckit") train_set, eval_set = arckit.load_data() # create datasets using arcdataset logger.debug("creating train validation datasets") full_dataset = arcdataset(train_set, is_test=false) # use smaller subset dataset subset_size = int(0.1 * len(full_dataset)) # use 10% dataset train_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size]) val_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size]) logger.debug(f"train dataset size: {len(train_dataset)}, validation dataset size: {len(val_dataset)}") # create custom collate function handle data format def collate_fn(batch): inputs = [item[0].to(torch.float32) item batch] # convert float32 outputs = [item[1].to(torch.float32) item batch] # convert float32 logger.debug(f"batch input dtypes stack: {[item[0].dtype item batch]}") logger.debug(f"batch output dtypes stack: {[item[1].dtype item batch]}") # inputs outputs already tensors, need stack input_stack = torch.stack(inputs) output_stack = torch.stack(outputs) # log data types stacking logger.debug(f"collate function input_stack dtype: {input_stack.dtype}") logger.debug(f"collate function output_stack dtype: {output_stack.dtype}") # create dummy attention mask (all ones) attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32) logger.debug(f"collate function attention_mask dtype: {attention_mask.dtype}") # generate dummy task_ids item batch task_ids = [f"task_{i}" range(len(batch))] return input_stack, attention_mask, output_stack, task_ids logger.debug(f"batch output dtypes stack: {[item[1].dtype item batch]}") # inputs outputs already tensors, need stack input_stack = torch.stack(inputs) output_stack = torch.stack(outputs) # create dummy attention mask (all ones) attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32) logger.debug(f"collate function input dtype: {input_stack.dtype}") return input_stack, attention_mask, output_stack # initialize model logger.debug("initializing model") model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) # use smaller model configuration model = gpt2arc(model_config).to(torch.float32) logger.debug(f"model initialized config: {model_config}") # # thop profiling - commented due typeerror mps tensors # logger.debug("profiling model thop") # dummy_input = torch.randn(1, 1, 28, 28, dtype=torch.float32) # example input shape # macs, params = profile(model, inputs=(dummy_input,)) # macs, params = clever_format([macs, params], "%.3f") # logger.info(f"macs: {macs}, parameters: {params}") # initialize trainer logger.debug("initializing trainer") config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) # reduce epochs 2 trainer = arctrainer(model, train_dataset, val_dataset, config) trainer.train_dataloader = lambda: torch.utils.data.dataloader(train_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0) trainer.val_dataloader = lambda: torch.utils.data.dataloader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0) trainer.test_dataloader = lambda: torch.utils.data.dataloader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0) logger.debug(f"trainer initialized config: {config}") # create pytorch lightning trainer logger.debug("creating pytorch lightning trainer") # measure training time start_time = time.time() pl_trainer = pl.trainer( max_epochs=config.training.max_epochs, logger=false, enable_checkpointing=false, enable_progress_bar=false ) logger.debug("pytorch lightning trainer created") # evaluate model training get initial accuracy logger.info("evaluating model training") initial_val_results = pl_trainer.test(trainer, verbose=false) logger.debug(f"initial validation results: {initial_val_results}") initial_accuracy = initial_val_results[0].get('test_accuracy') initial_loss = initial_val_results[0].get('test_loss') print(f"initial validation results: {initial_val_results}") assert initial_accuracy none, "initial validation results missing 'test_accuracy'" assert initial_loss none, "initial validation results missing 'test_loss'" logger.info(f"initial validation accuracy: {initial_accuracy}, initial loss: {initial_loss}") print(f"initial validation accuracy: {initial_accuracy}, initial loss: {initial_loss}") logger.debug("starting model training") pl_trainer.fit(trainer) end_time = time.time() training_time = end_time - start_time logger.info(f"total training time: {training_time:.2f} seconds") logger.debug("model training completed") # check loss decreased train_losses = trainer.train_losses logger.info(f"training losses: {train_losses}") assert train_losses[-1] &lt; train_losses[0], f"training loss decrease. initial loss: {train_losses[0]}, final loss: {train_losses[-1]}" # check final loss lower initial loss assert train_losses[-1] &lt; train_losses[0], "final training loss lower initial loss" # check average loss per epoch decreases epoch_losses = [sum(train_losses[i:i+33])/33 range(0, len(train_losses), 33)] assert all(epoch_losses[i] &gt; epoch_losses[i+1] range(len(epoch_losses)-1)), "average training loss per epoch consistently decrease" # evaluate model training logger.debug("evaluating model training") final_val_results = pl_trainer.test(trainer, verbose=false) final_accuracy = final_val_results[0]['test_accuracy'] final_loss = final_val_results[0]['test_loss'] logger.info(f"final validation accuracy: {final_accuracy}, final loss: {final_loss}") print(f"final validation accuracy: {final_accuracy}, final loss: {final_loss}") # check validation accuracy improved assert final_accuracy &gt; initial_accuracy, f"validation accuracy improve. initial accuracy: {initial_accuracy}, final accuracy: {final_accuracy}" logger.info(f"final training loss: {train_losses[-1]:.4f}") logger.info(f"validation accuracy: {final_accuracy:.4f}") # check model parameters total_params = sum(p.numel() p model.parameters()) trainable_params = sum(p.numel() p model.parameters() p.requires_grad) assert total_params &gt; 0, "model parameters" assert trainable_params &gt; 0, "model trainable parameters" assert trainable_params == total_params, "not parameters trainable" logger.debug(f"total parameters: {total_params}") logger.debug(f"trainable parameters: {trainable_params}") logger.debug("end-to-end test completed successfully") except exception e: logger.error(f"end-to-end test failed error: {str(e)}") raise def test_evaluation_process_with_arckit_data(): logger.debug("starting evaluation process test arckit data") # load data using arckit _ , evaluation_data = arckit.load_data() # log structure evaluation data logger.debug(f"evaluation data structure: {evaluation_data}") logger.debug(f"evaluation data structure: {evaluation_data}") test_dataset = arcdataset(evaluation_data, is_test=true) # initialize model trainer model_configuration = modelconfig(n_embd=96, n_head=3, n_layer=1) model = gpt2arc(model_configuration) training_configuration = config(model=model_configuration, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) trainer = arctrainer(model, none, test_dataset, training_configuration) # run evaluation lightning_trainer = pl.trainer(logger=false, enable_checkpointing=false, enable_progress_bar=false) evaluation_results = lightning_trainer.test(trainer) # access test results trainer evaluation_results = trainer.test_results # log evaluation results logger.debug(f"evaluation results: {evaluation_results}") result evaluation_results: task_ids = result.get('task_ids', []) task_ids: logger.error(f"missing task_ids result: {result}") else: task_id task_ids: logger.info(f"task {task_id}: loss={result['test_loss']}, accuracy={result['test_accuracy']}") # check duplicate metrics unique_task_ids = set(task_id result evaluation_results task_id result.get('task_ids', [])) print("all task ids:", [task_id result evaluation_results task_id result.get('task_ids', [])]) print("unique task ids:", unique_task_ids) print(f"number evaluation results: {len(evaluation_results)}") print(f"number unique task ids: {len(unique_task_ids)}") len(unique_task_ids) != len(evaluation_results): print("warning: number unique task ids match number evaluation results") duplicate_tasks = [task_id task_id unique_task_ids sum(task_id result.get('task_ids', []) result evaluation_results) &gt; 1] print(f"duplicate task ids: {duplicate_tasks}") task_id duplicate_tasks: print(f"results task {task_id}:") result evaluation_results: task_id result.get('task_ids', []): print(result) print(f"unique task ids: {unique_task_ids}") print(f"evaluation results: {evaluation_results}") assert len(unique_task_ids) &gt; 0, "no tasks evaluated" logger.debug("completed evaluation process test arckit data")</file><file name="tests/test_pytest_error_fixer.py"># gpt2_arc/tests/test_pytest_error_fixer.py import os import json import pytest unittest.mock import patch, magicmock pytest_error_fixer import pytesterrorfixer # reusable fixtures test setup @pytest.fixture def error_fixer(tmp_path): # initialize pytesterrorfixer temporary directory progress log fixer = pytesterrorfixer("test_project_dir") fixer.progress_log = tmp_path / "test_progress_log.json" fixer.error_log = tmp_path / "test_error_log.json" return fixer @pytest.fixture def sample_errors(): # sample errors testing return { "gpt2_arc/test_file.py": [ "test_function assertionerror: assert 1 == 2", "test_another_function typeerror: unsupported operand type(s) +: 'int' 'str'" ] } # 1. test progress log initialization def test_init_progress_log(error_fixer): error_fixer.init_progress_log() assert os.path.exists(error_fixer.progress_log) open(error_fixer.progress_log, 'r') f: assert json.load(f) == [] # ensure log empty upon initialization # 2. test logging progress progress log def test_log_progress(error_fixer): error_fixer.init_progress_log() error_fixer.log_progress("fixed", "test error", "test_file.py") open(error_fixer.progress_log, 'r') f: log = json.load(f) assert len(log) == 1 assert log[0] == {"error": "test error", "file": "test_file.py", "status": "fixed"} # 3. test running full test suite capturing output @patch('subprocess.run') def test_run_full_test(mock_run, error_fixer): # mock output subprocess.run simulate pytest execution mock_run.return_value = magicmock(stdout="test output", stderr="test error") stdout, stderr = error_fixer.run_full_test() # assert stdout stderr captured correctly assert stdout == "test output" assert stderr == "test error" mock_run.assert_called_once() # 4. test parsing errors pytest output def test_parse_errors(error_fixer): # simulate pytest output multiple errors sample_output = """ gpt2_arc/test_file.py::test_function failed gpt2_arc/another_file.py::test_another_function failed """ errors = error_fixer.parse_errors(sample_output) # verify errors correctly parsed associated right test files assert "gpt2_arc/test_file.py" errors assert "gpt2_arc/another_file.py" errors assert "test_function failed" errors["gpt2_arc/test_file.py"] assert "test_another_function failed" errors["gpt2_arc/another_file.py"] # 5. test saving loading errors to/from json file def test_save_and_load_errors(error_fixer, sample_errors): # save errors file error_fixer.save_errors(sample_errors) # load errors back verify match original data loaded_errors = error_fixer.load_errors() assert loaded_errors == sample_errors # 6. test predicting relevant files using aider's output @patch.object(pytesterrorfixer, 'coder') def test_predict_relevant_files(mock_coder, error_fixer): # mock aider's file prediction output mock_coder.run.return_value = "the files likely involved gpt2_arc/file1.py gpt2_arc/file2.py" # predict files test error files = error_fixer.predict_relevant_files("test error") # assert correct files predicted assert files == ["gpt2_arc/file1.py", "gpt2_arc/file2.py"] mock_coder.run.assert_called_once() # 7. test fixing errors retrying needed @patch('subprocess.run') @patch.object(pytesterrorfixer, 'coder') def test_fix_error(mock_coder, mock_run, error_fixer): # simulate failed successful pytest runs mock_run.side_effect = [ magicmock(stdout="test failed", stderr="error occurred"), magicmock(stdout="test passed", stderr="") ] # simulate aider suggesting fixes mock_coder.run.return_value = "suggested fix" # run fix_error method verify retries eventually succeeds result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function") # assert error eventually fixed assert result == true assert mock_run.call_count == 2 mock_coder.run.assert_called_once() # 8. edge case: test handling invalid error output (additional coverage) def test_parse_errors_invalid_format(error_fixer): invalid_output = "this valid pytest output" errors = error_fixer.parse_errors(invalid_output) assert errors == {} # 9. edge case: test retry exhaustion errors remain unfixed @patch('subprocess.run') @patch.object(pytesterrorfixer, 'coder') def test_retry_exhaustion(mock_coder, mock_run, error_fixer): # simulate constant failure pytest runs mock_run.side_effect = [ magicmock(stdout="test failed", stderr="error occurred") ] * 3 # retry maximum number times mock_coder.run.return_value = "suggested fix" # run fix_error method ensure retries max limit result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function") # assert retries exhausted assert result == false assert mock_run.call_count == 3 # ensure retry mechanism works mock_coder.run.assert_called_once()</file><file name="tests/test_trainer.py"># gpt2_arc/tests/test_trainer.py import pytest import torch src.config import config, modelconfig, trainingconfig src.data.arc_dataset import arcdataset src.models.gpt2 import gpt2arc src.training.trainer import arctrainer @pytest.fixture def sample_data(): return [ { "train": [ {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]} ], "test": [ {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]} ] } ] @pytest.fixture def model(): config = modelconfig() return gpt2arc(config) @pytest.fixture def trainer(model, sample_data): config = config(model=modelconfig(), training=trainingconfig()) train_dataset = arcdataset(sample_data) val_dataset = arcdataset(sample_data) trainer = arctrainer(model, train_dataset, val_dataset, config) trainer.logged_metrics = {} trainer.config.training.log_level = "info" # add line trainer.log = lambda name, value, on_step=none, on_epoch=none, prog_bar=none, logger=none: trainer.logged_metrics.update({name: value}) return trainer def test_arctrainer_initialization(trainer): assert isinstance(trainer, arctrainer) assert hasattr(trainer, "model") assert hasattr(trainer, "train_dataset") assert hasattr(trainer, "val_dataset") def test_arctrainer_forward_pass(trainer): batch_size = 2 seq_length = 900 # 30x30 grid input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output = trainer(input_ids, attention_mask) assert isinstance(output, torch.tensor) assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd) @pytest.mark.parametrize("batch_format", ["tuple", "dict"]) def test_arctrainer_training_step(trainer, batch_format): batch_size = 2 seq_length = 900 # 30x30 grid vocab_size = 10 # use small vocab size testing batch_format == "tuple": batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), torch.ones((batch_size, seq_length)).float(), torch.randint(0, vocab_size, (batch_size, seq_length)).long(), ) else: batch = { "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), "attention_mask": torch.ones((batch_size, seq_length)).float(), "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), } loss = trainer.training_step(batch, 0) assert isinstance(loss, torch.tensor) assert loss.shape == torch.size([]) assert torch.isnan(loss).any(), "loss contains nan values" assert torch.isinf(loss).any(), "loss contains infinity values" def test_training_step_with_list_input(): model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=2, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) trainer = arctrainer(model, none, none, config) batch_size = 2 vocab_size = 10 # create batch tuple length 3 (input_ids, attention_mask, labels) inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float() inputs_flat = inputs.view(batch_size, -1) attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float() labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long() labels_flat = labels.view(batch_size, -1) batch = ( inputs_flat, # input_ids attention_mask, # attention_mask labels_flat # labels ) loss = trainer.training_step(batch, 0) assert isinstance(loss, torch.tensor), "loss torch.tensor" assert loss.shape == torch.size([]), "loss scalar" assert torch.isnan(loss).any(), "loss nan" assert torch.isinf(loss).any(), "loss infinity" def test_validation_step_with_list_input(): model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=2, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) trainer = arctrainer(model, none, none, config) batch_size = 2 vocab_size = 10 # create inputs labels inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float() inputs_flat = inputs.view(batch_size, -1) # flatten inputs labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long() labels_flat = labels.view(batch_size, -1) # flatten labels # create attention mask attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float() # create batch tuple length 3 batch = ( inputs_flat, # input_ids attention_mask, # attention_mask labels_flat # labels ) trainer.validation_step(batch, 0) assert "val_loss" trainer.logged_metrics, "validation loss logged" assert isinstance(trainer.logged_metrics["val_loss"], float), "logged validation loss float" @pytest.mark.parametrize("batch_format", ["tuple", "dict"]) def test_arctrainer_validation_step(trainer, batch_format): batch_size = 2 seq_length = 900 # 30x30 grid vocab_size = 10 # use small vocab size testing batch_format == "tuple": batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), torch.ones((batch_size, seq_length)).float(), torch.randint(0, vocab_size, (batch_size, seq_length)).long(), ) else: batch = { "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), "attention_mask": torch.ones((batch_size, seq_length)).float(), "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), } trainer.validation_step(batch, 0) # check val_loss logged assert "val_loss" trainer.logged_metrics def test_arctrainer_configure_optimizers(trainer): optimizers, schedulers = trainer.configure_optimizers() assert any(isinstance(opt, torch.optim.adam) opt optimizers), "expected adam optimizer" assert any(sch['scheduler'].__class__.__name__ == 'steplr' sch schedulers), "expected steplr scheduler" def test_arctrainer_train_dataloader(trainer): dataloader = trainer.train_dataloader() assert isinstance(dataloader, torch.utils.data.dataloader) assert len(dataloader.dataset) == len(trainer.train_dataset) def test_arctrainer_val_dataloader(trainer): dataloader = trainer.val_dataloader() assert isinstance(dataloader, torch.utils.data.dataloader) assert len(dataloader.dataset) == len(trainer.val_dataset) def test_arctrainer_test_step_with_task_ids(trainer): batch_size = 2 height = width = 30 num_symbols = 10 # create mock batch inputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).float() outputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).long() task_ids = ['task1', 'task2'] batch = (inputs, outputs, task_ids) # run test step result = trainer.test_step(batch, 0) # check result contains expected keys assert 'test_loss' result assert 'test_accuracy' result assert 'task_ids' result # check 'test_loss', 'test_accuracy', 'test_diff_accuracy' logged assert 'test_loss' trainer.logged_metrics assert 'test_accuracy' trainer.logged_metrics assert 'test_diff_accuracy' trainer.logged_metrics # check task-specific metrics logged task_id task_ids: assert f'{task_id}_test_loss' trainer.logged_metrics assert f'{task_id}_test_accuracy' trainer.logged_metrics assert f'{task_id}_test_diff_accuracy' trainer.logged_metrics</file><file name="src/evaluate.py"># gpt2_arc/src/evaluate.py import sys import os import json import argparse import pytorch_lightning pl import torch import wandb datetime import datetime # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) sys.path.insert(0, project_root) gpt2_arc.src.config import config, modelconfig import arckit import logging gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.utils.helpers import differential_pixel_accuracy # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) def evaluate(model, test_dataset, batch_size=32): trainer = arctrainer(model, none, test_dataset, config=config()) pl_trainer = pl.trainer(accelerator='gpu' torch.cuda.is_available() else 'cpu') results = pl_trainer.test(trainer) # collect individual task metrics individual_metrics = [] result results: task_id = result.get('task_id', 'unknown') individual_metrics.append((task_id, result)) # log individual task metrics logger.info("individual task metrics:") task_id, metrics individual_metrics: logger.info(f"task {task_id}: {metrics}") return results[0], individual_metrics def load_config_from_json(json_path): open(json_path, 'r') f: data = json.load(f) return data['config'] def save_results(results, individual_metrics, output_dir, model_name): timestamp = datetime.now().strftime("%y%m%d_%h%m%s") filename = f"{model_name}_eval_results_{timestamp}.json" output_path = os.path.join(output_dir, filename) open(output_path, 'w') f: json.dump({ "aggregate_results": results, "individual_metrics": individual_metrics }, f, indent=2) logger.info(f"results saved {output_path}") return output_path def main(args): # initialize wandb wandb.init(project=args.wandb_project, name=args.wandb_run_name) # load test data using arckit _, test_set = arckit.load_data() test_data = arcdataset(test_set) # load checkpoint checkpoint = torch.load(args.model_checkpoint) # load configuration json file config_path = f"results/experiment_{args.model_checkpoint.split('_')[-1].replace('.pth', '.json')}" config_dict = load_config_from_json(config_path) model_config = modelconfig( n_embd=config_dict['model']['n_embd'], n_head=config_dict['model']['n_head'], n_layer=config_dict['model']['n_layer'], dropout=config_dict['model']['dropout'] ) # initialize model checkpoint configuration model = gpt2arc(model_config) model.load_state_dict(checkpoint) model.eval() logger.info("model set evaluation mode") # evaluate model results, individual_metrics = evaluate(model, test_data, args.batch_size) logger.info("evaluation results:") metric, value results.items(): print(f"{metric}: {value}") wandb.log({f"eval/{metric}": value}) # save results locally model_name = os.path.basename(args.model_checkpoint).split('.')[0] results_path = save_results(results, individual_metrics, args.output_dir, model_name) # log results file wandb wandb.save(results_path) wandb.finish() __name__ == "__main__": parser = argparse.argumentparser(description="evaluate arc neural reasoning model") parser.add_argument("--model_checkpoint", type=str, required=true, help="path model checkpoint") parser.add_argument("--batch_size", type=int, default=32, help="batch size evaluation") parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="directory save evaluation results") parser.add_argument("--wandb_project", type=str, default="arc-evaluation", help="weights &amp; biases project name") parser.add_argument("--wandb_run_name", type=str, default=none, help="weights &amp; biases run name") args = parser.parse_args() # create output directory exist os.makedirs(args.output_dir, exist_ok=true) main(args)</file><file name="src/__init__.py"># file allows src directory recognized package.</file><file name="src/optimize_hyperparameters.py"># gpt2_arc/src/optimize_hyperparameters.py import optuna import logging import sys import os import torch import pytorch_lightning pl import psutil import gc # add project root python path project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) sys.path.insert(0, project_root) gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.data.arc_dataset import arcdataset import arckit # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) def log_memory_usage(): process = psutil.process(os.getpid()) memory_info = process.memory_info() logger.info(f"memory usage: {memory_info.rss / 1024 / 1024:.2f} mb") def objective(trial): logger.info(f"starting trial {trial.number}") log_memory_usage() # suggest hyperparameters model_config = modelconfig( n_embd=trial.suggest_int("n_embd", 32, 128), n_head=trial.suggest_int("n_head", 2, 4), n_layer=trial.suggest_int("n_layer", 1, 3) ) training_config = trainingconfig( batch_size=trial.suggest_int("batch_size", 16, 64), learning_rate=trial.suggest_float("learning_rate", 1e-5, 1e-2, log=true), max_epochs=trial.suggest_int("max_epochs", 5, 20) ) config = config(model=model_config, training=training_config) logger.info(f"trial {trial.number} config: {config}") try: # load data train_set, eval_set = arckit.load_data() train_data = arcdataset(train_set) val_data = arcdataset(eval_set) logger.info(f"loaded {len(train_data)} training samples {len(val_data)} validation samples") # create model trainer model = gpt2arc(config.model) arc_trainer = arctrainer(model, train_data, val_data, config) # set pytorch lightning trainer trainer = pl.trainer( max_epochs=config.training.max_epochs, accelerator='gpu' torch.cuda.is_available() else 'cpu', devices=1, logger=false, # disable logging keep things simple enable_checkpointing=false, # disable checkpointing simplicity accumulate_grad_batches=4, # add gradient accumulation ) # train evaluate logger.info(f"starting training trial {trial.number}") log_memory_usage() trainer.fit(arc_trainer) # test model log_memory_usage() test_results = trainer.test(arc_trainer) val_accuracy = test_results[0]['test_accuracy'] # assuming test_accuracy logged logger.info(f"trial {trial.number} completed validation accuracy: {val_accuracy}") log_memory_usage() # clean free memory del trainer, arc_trainer, model torch.cuda.empty_cache() gc.collect() log_memory_usage() return val_accuracy except exception e: logger.error(f"error trial {trial.number}: {str(e)}") log_memory_usage() raise optuna.exceptions.trialpruned() def run_optimization(n_trials=100): study_name = "gpt2_arc_optimization" storage_name = "sqlite:///optuna_results.db" study = optuna.create_study( study_name=study_name, storage=storage_name, load_if_exists=true, direction="maximize" ) logger.info(f"starting optimization {n_trials} trials") study.optimize(objective, n_trials=n_trials) logger.info("optimization completed") study.best_trial: logger.info(f"best trial: {study.best_trial.number}") logger.info(f"best value: {study.best_value}") logger.info("best hyperparameters:") key, value study.best_params.items(): logger.info(f" {key}: {value}") else: logger.warning("no successful trials found.") __name__ == "__main__": run_optimization(n_trials=10) # start small number trials testing</file><file name="src/config.py"># gpt2_arc/src/config.py dataclasses import dataclass, asdict @dataclass class modelconfig: n_embd: int = 768 n_head: int = 12 n_layer: int = 12 dropout: float = 0.1 @dataclass class trainingconfig: batch_size: int = 32 learning_rate: float = 1e-4 max_epochs: int = 10 use_gpu: bool = true log_level: str = "info" # add log_level attribute default value dataclasses import field @dataclass class config: def to_dict(self): return asdict(self) model: modelconfig = field(default_factory=modelconfig) training: trainingconfig = field(default_factory=trainingconfig)</file><file name="src/utils/experiment_tracker.py"># gpt2_arc/src/utils/experiment_tracker.py import wandb import json import time import uuid import torch import platform import os dataclasses import asdict typing import dict, any, optional class experimenttracker: def __init__(self, config: dict[str, any], project: str, entity: optional[str] = none, use_wandb: bool = false): self.experiment_id = str(uuid.uuid4()) self.timestamp = time.strftime("%y-%m-%d %h:%m:%s") self.config = config.to_dict() hasattr(config, 'to_dict') else self._config_to_dict(config) self.project = project self.entity = entity self.run = none self.use_wandb = use_wandb self.use_wandb: try: self.run = wandb.init(project=self.project, entity=self.entity, config=self.config) print(f"wandb run initialized: {self.run.id}") except exception e: print(f"error initializing wandb: {str(e)}") self.use_wandb = false self.results = { "train": [], "validation": [], "test": {} } self.metrics = {} self.task_specific_results = {} self.environment = self._get_environment_info() self.checkpoint_path = none # add debug logging print(f"experimenttracker initialized config: {json.dumps(self.config, indent=2)}") print(f"project: {project}, entity: {entity}") print(f"use_wandb: {self.use_wandb}") def _get_environment_info(self) -&gt; dict[str, str]: return { "python_version": platform.python_version(), "torch_version": torch.__version__, "gpu_info": torch.cuda.get_device_name(0) torch.cuda.is_available() else "cpu" } def _config_to_dict(self, config): isinstance(config, dict): return {k: self._config_to_dict(v) k, v config.items()} elif hasattr(config, '__dict__'): return {k: self._config_to_dict(v) k, v config.__dict__.items() k.startswith('_')} else: return config self.use_wandb: try: self.run = wandb.init(project=self.project, entity=self.entity, config=self.config) print(f"wandb run initialized: {self.run.id}") except exception e: print(f"error initializing wandb: {str(e)}") self.use_wandb = false self.use_wandb: print("using local logging only") def finish(self): self.use_wandb self.run: try: wandb.finish() print("wandb run finished") except exception e: print(f"error finishing wandb run: {str(e)}") def log_metric(self, name: str, value: float, step: optional[int] = none): self.use_wandb: try: wandb.log({name: value}, step=step) print(f"logged metric wandb: {name}={value}, step={step}") except exception e: print(f"error logging metric wandb: {str(e)}") # always log locally fallback print(f"logged metric locally: {name}={value}, step={step}") def update_train_metrics(self, epoch: int, metrics: dict[str, float]): "train" self.results: self.results["train"] = [] len(self.results["train"]) &lt;= epoch: self.results["train"].append({}) self.results["train"][epoch] = metrics self.use_wandb: wandb.log({"train": metrics}, step=epoch) def update_val_metrics(self, epoch: int, metrics: dict[str, float]): "validation" self.results: self.results["validation"] = [] len(self.results["validation"]) &lt;= epoch: self.results["validation"].append({}) self.results["validation"][epoch] = metrics self.use_wandb: wandb.log({"validation": metrics}, step=epoch) def set_test_results(self, metrics: dict[str, float]): self.results["test"] = metrics self.use_wandb: wandb.log({"test": metrics}) def add_task_specific_result(self, task_id: str, metrics: dict[str, float]): task_id self.task_specific_results: self.task_specific_results[task_id] = {} self.task_specific_results[task_id].update(metrics) self.use_wandb: wandb.log({f"task_{task_id}": metrics}) def set_final_metrics(self, metrics: dict[str, float]): self.metrics = metrics self.use_wandb: wandb.log(metrics) def set_checkpoint_path(self, path: str): self.checkpoint_path = path self.use_wandb: wandb.save(path) def save_to_json(self, filepath: str): try: directory = os.path.dirname(filepath) directory os.path.exists(directory): os.makedirs(directory) data = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "config": self.config, "results": self.results, "metrics": self.metrics, "task_specific_results": self.task_specific_results, "environment": self.environment, "checkpoint_path": self.checkpoint_path } open(filepath, 'w') f: json.dump(data, f, indent=2) print(f"results saved {filepath}") except ioerror e: print(f"error saving results {filepath}: {e}") def _ensure_directory_exists(self, directory: str): os.path.exists(directory): os.makedirs(directory) def get_summary(self) -&gt; dict[str, any]: summary = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "final_train_loss": self.results["train"][-1]["loss"] self.results["train"] else none, "final_val_loss": self.results["validation"][-1]["loss"] self.results["validation"] else none, "test_accuracy": self.results["test"].get("accuracy"), "config": self._serialize_config(self.config) } return {k: self._make_serializable(v) k, v summary.items()} def _make_serializable(self, obj): isinstance(obj, (int, float, str, bool, type(none))): return obj elif isinstance(obj, (list, tuple)): return [self._make_serializable(item) item obj] elif isinstance(obj, dict): return {k: self._make_serializable(v) k, v obj.items()} else: return str(obj) def _serialize_config(self, config): return {k: self._make_serializable(v) k, v config.items()} # add simple test __name__ == "__main__": config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": true} tracker = experimenttracker(config, project="test-project") tracker.start() tracker.log_metric("accuracy", 0.85, step=1) tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8}) tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75}) tracker.set_test_results({"loss": 0.55, "accuracy": 0.82}) tracker.add_task_specific_result("task_1", {"accuracy": 0.9}) tracker.set_final_metrics({"best_accuracy": 0.85}) tracker.set_checkpoint_path("model_checkpoint.pth") tracker.save_to_json("results.json") tracker.finish()</file><file name="src/utils/__init__.py" /><file name="src/utils/results_collector.py"># gpt2_arc/src/utils/results_collector.py import json import time import uuid import torch import platform import os dataclasses import asdict typing import dict, class resultscollector: def __init__(self, config): """initialize resultscollector given configuration.""" self.experiment_id = str(uuid.uuid4()) self.timestamp = time.strftime("%y-%m-%d %h:%m:%s") self.config = asdict(config) self.results = { "train": {}, "validation": {}, "test": {} } print(f"debug: initialized self.results['train'] {type(self.results['train'])}") self._log_results_type("after initialization") self.metrics = {} self.task_specific_results = {} self.environment = self._get_environment_info() self.checkpoint_path = none def _get_environment_info(self) -&gt; dict[str, str]: """retrieve environment information python pytorch versions.""" return { "python_version": platform.python_version(), "torch_version": torch.__version__, "gpu_info": torch.cuda.get_device_name(0) torch.cuda.is_available() else "cpu" } def _log_results_type(self, context: str): """log type self.results['train'] debugging.""" def update_train_metrics(self, epoch: int, metrics: dict[str, float]): print(f"debug: self.results['train'] type {type(self.results['train'])}") """update training metrics specific epoch.""" self._log_results_type("before checking 'train' results") "train" self.results: self.results["train"] = {} self._log_results_type("before type check") isinstance(self.results["train"], dict): raise typeerror(f"expected self.results['train'] dict, got {type(self.results['train'])}") self._log_results_type("before setting default") print(f"debug: setting default, self.results['train'] type {type(self.results['train'])}") self.results["train"].setdefault(epoch, {}) self._log_results_type("after setting default") print(f"debug: setting default, self.results['train'] type {type(self.results['train'])}") self.results["train"][epoch].update(metrics) def update_val_metrics(self, epoch: int, metrics: dict[str, float]): """update validation metrics specific epoch.""" "validation" self.results: self.results["validation"] = {} self.results["validation"][epoch] = metrics def set_test_results(self, metrics: dict[str, float]): """set test results metrics.""" self.results["test"] = metrics def add_task_specific_result(self, task_id: str, metrics: dict[str, float]): """add task-specific results given task id.""" task_id self.task_specific_results: self.task_specific_results[task_id] = {} self.task_specific_results[task_id].update(metrics) def set_final_metrics(self, metrics: dict[str, float]): """set final metrics training.""" self.metrics = metrics def set_checkpoint_path(self, path: str): """set path model checkpoint.""" self.checkpoint_path = path def save_to_json(self, filepath: str): """save collected results json file.""" try: self._ensure_directory_exists(os.path.dirname(filepath)) data = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "config": self.config, "results": self.results, "metrics": self.metrics, "task_specific_results": self.task_specific_results, "environment": self.environment, "checkpoint_path": self.checkpoint_path } open(filepath, 'w') f: json.dump(data, f, indent=2) except ioerror e: print(f"error saving results {filepath}: {e}") def _ensure_directory_exists(self, directory: str): """ensure directory exists; create not.""" os.path.exists(directory): os.makedirs(directory) def get_summary(self) -&gt; dict[str, any]: """get summary results.""" summary = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "final_train_loss": self.results["train"][-1]["loss"] self.results["train"] else none, "final_val_loss": self.results["validation"][-1]["loss"] self.results["validation"] else none, "test_accuracy": self.results["test"].get("accuracy"), "config": self._serialize_config(self.config) } return {k: self._make_serializable(v) k, v summary.items()} def _make_serializable(self, obj): isinstance(obj, (int, float, str, bool, type(none))): return obj elif isinstance(obj, (list, tuple)): return [self._make_serializable(item) item obj] elif isinstance(obj, dict): return {k: self._make_serializable(v) k, v obj.items()} else: return str(obj) def _serialize_config(self, config): return {k: self._make_serializable(v) k, v config.items()}</file><file name="src/utils/helpers.py"># gpt2_arc/src/utils/helpers.py import torch def differential_pixel_accuracy(input, target, prediction): print(f"differential pixel accuracy - input shape: {input.shape}, target shape: {target.shape}, prediction shape: {prediction.shape}") assert isinstance(input, torch.tensor) isinstance(target, torch.tensor) isinstance(prediction, torch.tensor), "all inputs must torch.tensor" assert input.numel() == target.numel() == prediction.numel(), "input, target, prediction must number elements" input = input.view_as(target) prediction = prediction.view_as(target) print(f"reshaped - input: {input.shape}, target: {target.shape}, prediction: {prediction.shape}") input_target_diff = input != target correct_diff_predictions = (prediction == target) &amp; input_target_diff total_diff_pixels = input_target_diff.sum().item() correct_diff_pixels = correct_diff_predictions.sum().item() print(f"total different pixels: {total_diff_pixels}") print(f"correctly predicted different pixels: {correct_diff_pixels}") total_diff_pixels &gt; 0: accuracy = correct_diff_pixels / total_diff_pixels else: accuracy = 1.0 # pixels differ, consider 100% accurate print(f"calculated accuracy: {accuracy}") return accuracy, input_target_diff, correct_diff_predictions</file><file name="src/models/gpt2.py"># gpt2_arc/src/models/gpt2.py import logging import torch import torch.nn.functional f torch import nn import torch.nn.init init logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) class attention(nn.module): def __init__(self, n_embd, n_head): super().__init__() self.n_head = n_head self.n_embd = n_embd self.key = nn.linear(n_embd, n_embd) self.query = nn.linear(n_embd, n_embd) self.value = nn.linear(n_embd, n_embd) self.proj = nn.linear(n_embd, n_embd) logger.debug(f"initialized attention n_embd={n_embd}, n_head={n_head}") def forward(self, x, mask=none): b, t, c = x.size() torch._dynamo.is_compiling(): logger.debug(f"attention input shape: {x.shape}") k = self.key(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) q = self.query(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) v = self.value(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))) mask none: att = att.masked_fill(mask[:, none, none, :] == 0, float("-inf")) att = f.softmax(att, dim=-1) = att @ v = y.transpose(1, 2).contiguous().view(b, t, c) output = self.proj(y) torch._dynamo.is_compiling(): logger.debug(f"attention output shape: {output.shape}") return output class feedforward(nn.module): def __init__(self, n_embd): super().__init__() self.net = nn.sequential( nn.linear(n_embd, 4 * n_embd), nn.relu(), nn.linear(4 * n_embd, n_embd) ) logger.debug(f"initialized feedforward n_embd={n_embd}") def forward(self, x): torch._dynamo.is_compiling(): logger.debug(f"feedforward input shape: {x.shape}") output = self.net(x) torch._dynamo.is_compiling(): logger.debug(f"feedforward output shape: {output.shape}") return output class transformerblock(nn.module): def __init__(self, n_embd, n_head): super().__init__() self.attention = attention(n_embd, n_head) self.feed_forward = feedforward(n_embd) self.ln1 = nn.layernorm(n_embd) self.ln2 = nn.layernorm(n_embd) logger.debug( f"initialized transformerblock n_embd={n_embd}, n_head={n_head}" ) def forward(self, x, mask=none): torch._dynamo.is_compiling(): logger.debug(f"transformerblock input shape: {x.shape}") x = x + self.attention(self.ln1(x), mask) x = x + self.feed_forward(self.ln2(x)) torch._dynamo.is_compiling(): logger.debug(f"transformerblock output shape: {x.shape}") return x dataclasses import dataclass ..config import modelconfig @dataclass class modelconfig: n_embd: int = 768 n_head: int = 12 n_layer: int = 12 dropout: float = 0.1 class gpt2arc(nn.module): def __init__(self, config: modelconfig): super().__init__() self.config = config # replace token embedding convolutional layer self.conv1 = nn.conv2d(in_channels=1, out_channels=self.config.n_embd, kernel_size=3, padding=1).to(torch.float32) self.blocks = nn.modulelist( [ transformerblock(self.config.n_embd, self.config.n_head) _ range(self.config.n_layer) ] ) self.ln_f = nn.layernorm(self.config.n_embd) # initialize weights self.apply(self._init_weights) def _init_weights(self, module): isinstance(module, nn.conv2d): # calculate fan_in conv2d fan_in = module.in_channels * module.kernel_size[0] * module.kernel_size[1] std = 1.0 / fan_in**0.5 init.normal_(module.weight, mean=0.0, std=std) module.bias none: init.zeros_(module.bias) elif isinstance(module, nn.linear): fan_in = module.in_features std = 1.0 / fan_in**0.5 init.normal_(module.weight, mean=0.0, std=std) module.bias none: init.zeros_(module.bias) # initialization nn.layernorm, using default def forward(self, input_ids, attention_mask=none): torch._dynamo.is_compiling(): logger.debug(f"gpt2arc input shape: {input_ids.shape}, dtype: {input_ids.dtype}") # check input_ids already correct shape input_ids.dim() == 4: x = input_ids.float() else: # reshape input_ids [batch_size, 1, height, width] batch_size = input_ids.size(0) seq_length = input_ids.size(1) height = width = int(seq_length ** 0.5) x = input_ids.float().view(batch_size, 1, height, width) x = self.conv1(x) logger.debug(f"after conv1 shape: {x.shape}") b, c, h, w = x.size() x = x.view(b, c, h * w) # flatten spatial dimensions x = x.permute(0, 2, 1) # rearrange (batch_size, sequence_length, channels) logger.debug(f"reshaped transformer blocks: {x.shape}") i, block enumerate(self.blocks): x = block(x, attention_mask) logger.debug(f"after block {i+1} shape: {x.shape}") x = self.ln_f(x) return x</file><file name="src/models/__init__.py" /><file name="src/data/__init__.py" /><file name="src/data/arc_dataset.py"># gp2_arc/src/data/arc_dataset.py import os import json import random typing import union, list, dict, tuple import numpy np import torch import torch.nn.functional f torch.utils.data import dataset import logging try: arckit.data import taskset, task except importerror: taskset = none logger = logging.getlogger(__name__) logger.setlevel(logging.error) # set error default # create handler writes stderr handler = logging.streamhandler() handler.setlevel(logging.error) # create formatting logs formatter = logging.formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') handler.setformatter(formatter) # add handler logger logger.addhandler(handler) # function set debug mode def set_debug_mode(debug=false): debug: logger.setlevel(logging.debug) handler.setlevel(logging.debug) else: logger.setlevel(logging.error) handler.setlevel(logging.error) class arcdataset(dataset): def __init__( self, data_source: union[str, list[dict], 'taskset', tuple[union[list, 'taskset'], str]], is_test: bool = false, num_symbols: int = 10, test_split: float = 0.2, debug=false, ): print("debug: starting arcdataset initialization") print(f"debug: data_source type: {type(data_source)}") print(f"debug: data_source content: {data_source}") logger.debug(f"initializing arcdataset data_source: {data_source}") # add logging verify task_id presence isinstance(data_source, list): item data_source: isinstance(item, task): logger.debug(f"processing task object id: {item.id}") elif 'task_id' item: logger.warning(f"missing task_id data item: {item}") self.debug_attr = "test" # simple attribute testing set_debug_mode(debug) # set debug mode based parameter logger.debug(f"initializing arcdataset data_source type: {type(data_source)}") isinstance(data_source, str): logger.debug(f"data source path: {data_source}") os.path.isdir(data_source): logger.debug("processing synthetic data directory") self.data = self._process_synthetic_data(data_source) elif os.path.isfile(data_source): logger.debug("processing json data file") open(data_source, 'r') f: raw_data = json.load(f) self.data = self._process_json_data(raw_data) else: raise filenotfounderror(f"data source file directory found: {data_source}") elif isinstance(data_source, list): logger.debug("processing list data") self.data = self._process_list_data(data_source) elif isinstance(data_source, tuple): logger.debug("processing combined data") self.data = self._combine_data(*data_source) elif taskset none isinstance(data_source, taskset): logger.debug("processing arckit data") self.data = self._process_arckit_data(data_source) else: logger.error(f"invalid data_source type: {type(data_source)}") raise valueerror("data source must either file path, list tasks, taskset") print(f"debug: processed data length: {len(self.data)}") self.data: print(f"debug: first item keys: {self.data[0].keys()}") 'train' self.data[0]: print(f"debug: first train item: {self.data[0]['train'][0] self.data[0]['train'] else 'no train data'}") logger.debug(f"number tasks: {len(self.data)}") logger.debug(f"first task structure: {self.data[0].keys()}") self.data 'train' self.data[0] self.data[0]['train']: first_train_sample = self.data[0]['train'][0] isinstance(first_train_sample, dict): logger.debug(f"first train sample structure: {first_train_sample.keys()}") else: logger.debug(f"first train sample dict. type: {type(first_train_sample)}, content: {first_train_sample}") else: logger.debug("no train samples found first task") logger.debug(f"first train input shape: {np.array(self.data[0]['train'][0]['input']).shape}") self.is_test = is_test self.num_symbols = num_symbols self.test_split = test_split logger.debug(f"test_split set to: {self.test_split}") self.test_split = test_split self.samples = [] taskset none isinstance(data_source, taskset): task data_source.tasks: self.samples.extend(task.train) self.samples.extend(task.test) isinstance(data_source, str): os.path.isdir(data_source): self.data = self._process_synthetic_data(data_source) elif os.path.isfile(data_source): open(data_source, 'r') f: raw_data = json.load(f) self.data = self._process_json_data(raw_data) else: raise filenotfounderror(f"data source file directory found: {data_source}") elif isinstance(data_source, list): self.data = self._process_list_data(data_source) elif isinstance(data_source, tuple): self.data = self._combine_data(*data_source) elif taskset none isinstance(data_source, taskset): self.data = self._process_arckit_data(data_source) else: logger.error(f"invalid data_source type: {type(data_source)}") raise valueerror("data source must either file path, list tasks, taskset") print(f"number train samples: {sum(len(task['train']) task self.data)}") print(f"number test samples: {sum(len(task['test']) task self.data)}") self.max_grid_size = self._compute_max_grid_size() self._validate_data() def _process_json_data(self, raw_data: list[dict]) -&gt; list[dict]: processed_data = [] task raw_data: logger.debug(f"processing task: {task}") processed_task = { "train": [ {"input": np.array(example["input"]), "output": np.array(example["output"])} example task["train"] ], "test": [ {"input": np.array(example["input"]), "output": np.array(example["output"])} example task["test"] ] } processed_data.append(processed_task) # flatten data structure flattened_data = [] task processed_data: flattened_data.extend(task['train']) flattened_data.extend(task['test']) return flattened_data def _validate_data(self): task self.data: split ["train", "test"]: split task: sample task[split]: ("input" sample "output" sample): raise valueerror(f"each sample must contain 'input' 'output'. task: {task.get('id', 'unknown')}") print("data validation passed.") def _compute_max_grid_size(self): max_h, max_w = 0, 0 task self.data: split ['train', 'test']: sample task[split]: isinstance(sample['input'], torch.tensor): sample['input'].dim() == 3: h, w = sample['input'].shape[1], sample['input'].shape[2] elif sample['input'].dim() == 2: h, w = sample['input'].shape else: raise valueerror(f"unexpected tensor dimensions: {sample['input'].dim()}") elif isinstance(sample['input'], np.ndarray): sample['input'].ndim == 2: h, w = sample['input'].shape elif sample['input'].ndim == 3: h, w = sample['input'].shape[1], sample['input'].shape[2] else: raise valueerror(f"unexpected ndarray dimensions: {sample['input'].ndim}") elif isinstance(sample['input'], list): h, w = len(sample['input']), len(sample['input'][0]) else: raise typeerror(f"unexpected input type: {type(sample['input'])}") max_h = max(max_h, h) max_w = max(max_w, w) logger.debug(f"computed max grid size: ({max_h}, {max_w})") return (max_h, max_w) def _combine_data(self, official_data, synthetic_data_path): official_processed = self._process_arckit_data(official_data) taskset none isinstance(official_data, taskset) else official_data synthetic_processed = self._process_synthetic_data(synthetic_data_path) return official_processed + synthetic_processed def _process_synthetic_data(self, directory: str) -&gt; list[dict]: processed_data = [] filename os.listdir(directory): filename.endswith('.json'): open(os.path.join(directory, filename), 'r') f: task_data = json.load(f) processed_data.append(self._process_single_task(task_data)) return processed_data def _process_single_task(self, task_data: union[dict, list]) -&gt; dict: logger.debug(f"inside _process_single_task, test_split is: {self.test_split}") isinstance(task_data, dict): train_examples = task_data.get("train", []) test_examples = task_data.get("test", []) elif isinstance(task_data, list): split_idx = int(len(task_data) * (1 - self.test_split)) train_examples = task_data[:split_idx] test_examples = task_data[split_idx:] else: raise valueerror("task data must either dictionary list") return { "train": [self._preprocess_grid(example) example train_examples], "test": [self._preprocess_grid(example) example test_examples] } def _process_arckit_data(self, taskset: 'taskset') -&gt; list[dict]: processed_data = [] logger.debug(f"processing taskset {len(taskset.tasks)} tasks") task taskset.tasks: logger.debug(f"task id: {task.id}") task taskset.tasks: logger.debug(f"processing task: {task.id}") logger.debug(f"task id: {task.id}") logger.debug(f"train samples: {len(task.train)}, test samples: {len(task.test)}") processed_task = { "id": task.id, "train": [ {"input": np.array(ex[0]), "output": np.array(ex[1])} ex task.train ], "test": [ {"input": np.array(ex[0]), "output": np.array(ex[1])} ex task.test ] } processed_data.append(processed_task) logger.debug(f"processed task {task.id}: train samples: {len(processed_task['train'])}, test samples: {len(processed_task['test'])}") logger.debug(f"processed {len(processed_data)} tasks") return processed_data def __len__(self) -&gt; int: self.is_test: total_samples = sum(len(task['test']) task self.data) else: total_samples = sum(len(task['train']) task self.data) logger.debug(f"total samples dataset: {total_samples}") return total_samples def __getitem__(self, idx: int) -&gt; tuple[torch.tensor, torch.tensor]: logger.debug(f"arcdataset __getitem__ called idx {idx}") idx &lt; 0 idx &gt;= len(self): raise indexerror(f"index {idx} range (total samples: {len(self)})") current_idx = 0 task self.data: split = 'test' self.is_test else 'train' idx &lt; current_idx + len(task[split]): sample = task[split][idx - current_idx] input_grid = self._preprocess_grid(sample["input"]) output_grid = self._preprocess_grid(sample["output"]) logger.debug(f"returning input shape: {input_grid.shape}, output shape: {output_grid.shape}") logger.debug(f"__getitem__ input dtype: {input_grid.dtype}, output dtype: {output_grid.dtype}") task_id = task.get("id", -1) # default -1 id found return input_grid, output_grid, task_id current_idx += len(task[split]) raise runtimeerror("unexpected error __getitem__") def _validate_data(self): task self.data: split ["train", "test"]: split task: continue idx, sample enumerate(task[split]): "input" sample "output" sample: raise keyerror(f"sample {idx} task {split} set missing 'input' 'output' key") input_data = sample["input"] output_data = sample["output"] (isinstance(input_data, (list, np.ndarray)) isinstance(output_data, (list, np.ndarray))): logger.warning(f"sample {idx} task {split} set 'input' 'output' must list numpy array") continue isinstance(input_data, list): input_data = np.array(input_data) isinstance(output_data, list): output_data = np.array(output_data) input_data.ndim != 2 output_data.ndim != 2: raise valueerror(f"sample {idx} task {split} set 'input' 'output' must 2d lists") np.any(input_data &gt;= self.num_symbols) np.any(output_data &gt;= self.num_symbols): logger.warning(f"sample {idx} task {split} set contains invalid symbols (&gt;= {self.num_symbols})") def _compute_grid_size_stats(self): max_height, max_width = 0, 0 task self.data: split ["train", "test"]: sample task[split]: max_height = max(max_height, sample["input"].shape[0], sample["output"].shape[0]) max_width = max(max_width, sample["input"].shape[1], sample["output"].shape[1]) self.max_grid_size = (max_height, max_width) def _compute_symbol_frequencies(self): symbol_counts = np.zeros(self.num_symbols, dtype=int) task self.data: split ["train", "test"]: sample task[split]: symbol_counts += np.bincount(sample["input"].flatten(), minlength=self.num_symbols) symbol_counts += np.bincount(sample["output"].flatten(), minlength=self.num_symbols) return symbol_counts / symbol_counts.sum() def _preprocess_grid(self, grid: union[dict, np.ndarray]) -&gt; torch.tensor: isinstance(grid, dict): input_grid = np.array(grid['input']) logger.debug(f"original grid shape: {input_grid.shape}") logger.debug(f"original grid content:\n{input_grid}") elif isinstance(grid, np.ndarray): input_grid = grid logger.debug(f"original grid shape: {input_grid.shape}") logger.debug(f"original grid content:\n{input_grid}") else: raise valueerror(f"unexpected grid type: {type(grid)}") # pad grid 30x30 padded_grid = self._pad_grid(input_grid, height=30, width=30) # convert tensor add channel dimension grid_tensor = torch.tensor(padded_grid, dtype=torch.float32).unsqueeze(0) logger.debug(f"preprocessed grid shape: {grid_tensor.shape}") logger.debug(f"preprocessed grid content:\n{grid_tensor}") return grid_tensor def kronecker_scale(self, x, target_height=30, target_width=30): print(f"kronecker scaling input shape: {x.shape}") h, w = x.shape scale_h = target_height / h scale_w = target_width / w = int(np.floor(min(scale_h, scale_w))) x_scaled = np.kron(x, np.ones((d, d))) print(f"kronecker scaled output shape: {x_scaled.shape}") return x_scaled def pad_grid(self, x, target_height=30, target_width=30): print(f"padding input shape: {x.shape}") h, w = x.shape pad_h = (target_height - h) // 2 pad_w = (target_width - w) // 2 padded = np.pad(x, ((pad_h, target_height - h - pad_h), (pad_w, target_width - w - pad_w)), mode='constant') print(f"padded output shape: {padded.shape}") return padded def reverse_scaling(self, x_orig, x_pred): print(f"reverse scaling - original shape: {x_orig.shape}, prediction shape: {x_pred.shape}") h, w = x_orig.shape # reshape x_pred 2d 1d x_pred.ndim == 1: x_pred = x_pred.reshape((int(np.sqrt(x_pred.size)), -1)) x_pred_cropped = x_pred[:h, :w] # crop original size h == x_pred.shape[0] w == x_pred.shape[1]: print("no rescaling needed") return x_pred_cropped # calculate downscale factor d_h = x_pred_cropped.shape[0] // h d_w = x_pred_cropped.shape[1] // w # ensure dimensions compatible reshaping d_h &gt; 0 d_w &gt; 0: try: x_rev = x_pred_cropped.reshape(h, d_h, w, d_w).mean(axis=(1, 3)) except valueerror e: print(f"error reshaping: {e}") print(f"x_pred_cropped shape: {x_pred_cropped.shape}, h: {h}, w: {w}, d_h: {d_h}, d_w: {d_w}") raise else: print(f"invalid downscale factors: d_h={d_h}, d_w={d_w}") raise valueerror("invalid dimensions reverse scaling") # resize result match original target shape result = np.resize(x_rev.round().astype(int), x_orig.shape) print(f"reverse scaled output shape: {result.shape}") return result def _scale_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray: return grid # scaling, preserve original size def _pad_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray: h, w = grid.shape pad_h = (height - h) // 2 pad_w = (width - w) // 2 return np.pad(grid, ((pad_h, height - h - pad_h), (pad_w, width - w - pad_w)), mode='constant') def _process_list_data(self, data_source): print(f"debug: processing {len(data_source)} items") processed_data = [] idx, item enumerate(data_source): print(f"debug: processing item {idx}") print(f"debug: item type: {type(item)}") print(f"debug: item content: {item}") isinstance(item, task): processed_item = { "train": [{"input": np.array(ex[0]), "output": np.array(ex[1])} ex item.train], "test": [{"input": np.array(ex[0]), "output": np.array(ex[1])} ex item.test] } elif 'train' item 'test' item: processed_item = { "train": [{"input": np.array(sample["input"]), "output": np.array(sample["output"])} sample item['train']], "test": [{"input": np.array(sample["input"]), "output": np.array(sample["output"])} sample item['test']] } elif 'input' item 'output' item: processed_item = { "train": [{"input": np.array(item["input"]), "output": np.array(item["output"])}], "test": [] } else: raise valueerror("unexpected item format data_source.") processed_data.append(processed_item) return processed_data @staticmethod def collate_fn(batch): print(f"collating batch size: {len(batch)}") batch: print("warning: empty batch received") return torch.tensor([]), torch.tensor([]), [] try: inputs, outputs, task_ids = zip(*batch) except valueerror e: print(f"error unpacking batch: {e}") print(f"batch content: {batch}") # return empty tensors list unpacking fails return torch.tensor([]), torch.tensor([]), [] print(f"input shapes: {[i.shape inputs]}") print(f"output shapes: {[o.shape outputs]}") # find max dimensions batch max_h = max(i.size(1) inputs) max_w = max(i.size(2) inputs) print(f"max dimensions: height={max_h}, width={max_w}") # pad inputs outputs max size batch padded_inputs = torch.stack([f.pad(i, (0, max_w - i.size(2), 0, max_h - i.size(1))) inputs]) padded_outputs = torch.stack([f.pad(o, (0, max_w - o.size(2), 0, max_h - o.size(1))) outputs]) print(f"padded input shape: {padded_inputs.shape}") print(f"padded output shape: {padded_outputs.shape}") return [padded_inputs, padded_outputs, list(task_ids)]</file><file name="src/training/__init__.py" /><file name="src/training/trainer.py"># gpt2_arc/src/training/trainer.py import pytorch_lightning pl import torch import logging torch import nn, optim import time typing import any, dict, optional collections import deque torch.optim.lr_scheduler import lambdalr ..config import config ..utils.helpers import differential_pixel_accuracy ..utils.results_collector import resultscollector torch.utils.data import dataloader torch.utils.tensorboard import summarywriter import os logger = logging.getlogger(__name__) class arctrainer(pl.lightningmodule): def __init__(self, model, train_dataset, val_dataset, config: config): super().__init__() self.model = model self.train_dataset = train_dataset self.val_dataset = val_dataset self.config = config self.batch_size = config.training.batch_size self.lr = config.training.learning_rate self.train_losses = [] self.logged_metrics = {} self.test_outputs = [] # store test outputs aggregation self.test_results = [] # initialize test results storing test outcomes self.best_val_loss = float('inf') self.best_epoch = 0 self.results_collector = resultscollector(config) log_dir = f"runs/experiment_{self.results_collector.experiment_id}" os.makedirs(log_dir, exist_ok=true) # ensure directory exists self.writer = summarywriter(log_dir) print(f"debug: tensorboard writer initialized experiment {self.results_collector.experiment_id}") print(f"debug: logs written {log_dir}") def training_step(self, batch, batch_idx): logger.debug(f"training step - batch type: {type(batch)}, length: {len(batch)}") isinstance(batch, (list, tuple)) len(batch) &gt;= 2: inputs, labels = batch[:2] task_ids = batch[2] len(batch) &gt; 2 else none elif isinstance(batch, dict): inputs = batch.get("input_ids") labels = batch.get("labels") task_ids = batch.get("task_ids") else: raise valueerror(f"unexpected batch format: {type(batch)}. content: {batch}") # ensure inputs labels correct type inputs = inputs.float() labels = labels.long() outputs = self(inputs) loss = self.compute_loss(outputs, labels) hasattr(self, 'log'): self.log("train_loss", loss, on_step=true, on_epoch=true, prog_bar=true, logger=true) self.results_collector.update_train_metrics(self.current_epoch, {"loss": loss.item()}) try: self.writer.add_scalar('train/loss', loss.item(), self.current_epoch * len(self.train_dataloader()) + batch_idx) print(f"debug: logged training loss: {loss.item()} step {self.current_epoch * len(self.train_dataloader()) + batch_idx}") except exception e: print(f"debug: error logging training step: {str(e)}") return loss def validation_step(self, batch, batch_idx, dataloader_idx=0): logger.debug(f"validation step - batch type: {type(batch)}, length: {len(batch)}") isinstance(batch, (list, tuple)): len(batch) &lt; 2: logger.error(f"missing inputs labels batch. inputs: {batch[0] len(batch) &gt; 0 else none}, labels: {batch[1] len(batch) &gt; 1 else none}") raise valueerror("batch must contain inputs labels.") inputs, labels = batch[:2] task_ids = batch[2] len(batch) &gt; 2 else none elif isinstance(batch, dict): inputs = batch.get("input_ids") labels = batch.get("labels") task_ids = batch.get("task_ids") inputs none labels none: logger.error(f"missing inputs labels batch. inputs: {inputs}, labels: {labels}") raise valueerror("batch must contain inputs labels.") else: logger.error(f"unexpected batch format: {type(batch)}. content: {batch}") raise valueerror(f"unexpected batch format: {type(batch)}. content: {batch}") # ensure inputs labels correct type inputs = inputs.float() labels = labels.long() outputs = self(inputs) loss = self.compute_loss(outputs, labels) hasattr(self, 'log'): self.log("val_loss", loss, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.logged_metrics["val_loss"] = loss.item() self.results_collector.update_val_metrics(self.current_epoch, {"loss": loss.item()}) try: self.writer.add_scalar('val/loss', loss.item(), self.current_epoch) print(f"debug: logged validation loss: {loss.item()} epoch {self.current_epoch}") except exception e: print(f"debug: error logging validation step: {str(e)}") return loss def test_step(self, batch, batch_idx): """ `test_step` function processes batch data testing model, computes metrics loss accuracy, logs results, stores analysis. :param batch: `batch` parameter `test_step` function expected contain input data, attention masks (optional), target outputs, task ids. function processes batch based format performs inference using model. calculates loss, accuracy, differential pixel accuracy metrics evaluation :param batch_idx: batch index used keep track current batch processed testing. helps identifying logging information specific batch, loss accuracy values. batch index typically integer value increments batch processed testing :return: `test_step` method returns dictionary named `result` containing keys 'loss', 'accuracy', 'task_ids', 'test_loss'. additionally, logs various metrics test_loss, test_accuracy, test_diff_accuracy. method also appends result `self.test_outputs` calculates task success tsr, logging well. """ logger.debug(f"test step - batch type: {type(batch)}, length: {len(batch)}") isinstance(batch, list) len(batch) == 3: inputs, outputs, task_ids = batch attention_mask = none elif isinstance(batch, tuple) len(batch) == 3: inputs, outputs, task_ids = batch attention_mask = none elif isinstance(batch, tuple) len(batch) == 4: inputs, attention_mask, outputs, task_ids = batch logger.debug(f"task ids batch: {task_ids}") else: raise valueerror(f"unexpected batch format: {type(batch)}. content: {batch}") # ensure inputs outputs correct type inputs = inputs.float() outputs = outputs.long() # create dummy attention mask none attention_mask none: attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32, device=inputs.device) model_outputs = self(inputs, attention_mask) loss = self.compute_loss(model_outputs, outputs) b, t, c = model_outputs.size() model_outputs = model_outputs.view(b, -1, c) predictions = torch.argmax(model_outputs, dim=-1) outputs = outputs.view(b, -1) accuracy = (predictions == outputs).float().mean() diff_accuracy, _, _ = differential_pixel_accuracy(inputs, outputs, predictions) # collect metrics dictionary metrics = { 'test_loss': loss.item() isinstance(loss, torch.tensor) else loss, 'test_accuracy': accuracy.item() isinstance(accuracy, torch.tensor) else accuracy, 'test_diff_accuracy': diff_accuracy.item() isinstance(diff_accuracy, torch.tensor) else diff_accuracy } # return metrics hasattr(self, 'log'): self.log("test_loss", loss, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log("test_accuracy", accuracy, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log("test_diff_accuracy", diff_accuracy, on_step=false, on_epoch=true, prog_bar=true, logger=true) result = { 'test_loss': metrics['test_loss'], 'test_accuracy': metrics['test_accuracy'], 'test_diff_accuracy': metrics['test_diff_accuracy'], 'task_ids': task_ids, } # log task-specific metrics task_id task_ids: self.log(f"{task_id}_test_loss", metrics['test_loss'], on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log(f"{task_id}_test_accuracy", metrics['test_accuracy'], on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log(f"{task_id}_test_diff_accuracy", metrics['test_diff_accuracy'], on_step=false, on_epoch=true, prog_bar=true, logger=true) try: self.writer.add_scalar('test/loss', metrics['test_loss'], self.current_epoch) self.writer.add_scalar('test/accuracy', metrics['test_accuracy'], self.current_epoch) self.writer.add_scalar('test/diff_accuracy', metrics['test_diff_accuracy'], self.current_epoch) print(f"debug: logged test metrics epoch {self.current_epoch}: loss={metrics['test_loss']}, accuracy={metrics['test_accuracy']}, diff_accuracy={metrics['test_diff_accuracy']}") except exception e: print(f"debug: error logging test step: {str(e)}") self.test_results.append(result) return result def on_validation_epoch_end(self): # compute average validation loss val_loss = self.trainer.callback_metrics.get('val_loss') val_loss none: avg_val_loss = val_loss.item() else: avg_val_loss = float('inf') # default high value val_loss available # update best_val_loss best_epoch avg_val_loss &lt; self.best_val_loss: self.best_val_loss = avg_val_loss self.best_epoch = self.current_epoch # log validation metrics self.log('val_loss', avg_val_loss) self.log('best_val_loss', self.best_val_loss) self.log('best_epoch', self.best_epoch) # update results collector self.results_collector.update_val_metrics(self.current_epoch, { "avg_loss": avg_val_loss, "best_val_loss": self.best_val_loss, "best_epoch": self.best_epoch }) # log additional information self.log('epoch', self.current_epoch) def configure_optimizers(self): optimizer = optim.adam(self.model.parameters(), lr=self.lr) lr_scheduler = { 'scheduler': optim.lr_scheduler.steplr(optimizer, step_size=1, gamma=0.95), 'name': 'learning_rate', } return [optimizer], [lr_scheduler] def on_fit_end(self): self.results_collector.save_to_json(f"results/experiment_{self.results_collector.experiment_id}.json") try: self.writer.close() print("debug: tensorboard writer closed successfully.") except exception e: print(f"debug: error closing tensorboard writer: {str(e)}") print("debug: results saved tensorboard writer closed.") def train_dataloader(self): return dataloader(self.train_dataset, batch_size=self.batch_size, shuffle=true, num_workers=4) def val_dataloader(self): return dataloader(self.val_dataset, batch_size=self.batch_size, num_workers=4) def test_dataloader(self): return dataloader(self.val_dataset, batch_size=self.batch_size, num_workers=4) def compute_loss(self, outputs, labels): return nn.crossentropyloss()( outputs.view(-1, outputs.size(-1)), labels.view(-1) ) def forward(self, input_ids, attention_mask=none): return self.model(input_ids, attention_mask) def log_hyperparameters(self): hparams = { 'learning_rate': self.config.training.learning_rate, 'batch_size': self.config.training.batch_size, 'n_embd': self.config.model.n_embd, 'n_head': self.config.model.n_head, 'n_layer': self.config.model.n_layer, } metric_dict = { 'train_loss': 0, 'val_loss': 0, 'test_accuracy': 0, } try: self.writer.add_hparams(hparams, metric_dict) print(f"debug: successfully logged hyperparameters: {hparams}") except exception e: print(f"debug: error logging hyperparameters: {str(e)}")</file><file name="src/training/train.py"># gpt2_arc/src/training/train.py import argparse import sys import logging import os import json unittest.mock import magicmock import optuna import arckit # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")) sys.path.insert(0, project_root) #print("current pythonpath:", sys.path) import pytorch_lightning pl import torch pytorch_lightning.callbacks import modelcheckpoint pytorch_lightning.loggers import tensorboardlogger gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig import json gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.utils.experiment_tracker import experimenttracker gpt2_arc.src.utils.results_collector import resultscollector import optuna import os # set logging logger = logging.getlogger(__name__) def main(args): logger.info("starting main function") logger.debug(f"command line arguments: {args}") try: args.use_optuna: logger.info("loading best hyperparameters optuna study") study = optuna.load_study(study_name=args.optuna_study_name, storage=args.optuna_storage) best_params = study.best_params logger.debug(f"loaded best parameters: {best_params}") model_config = modelconfig( n_embd=best_params.get("n_embd", 768), n_head=best_params.get("n_head", 12), n_layer=best_params.get("n_layer", 12) ) training_config = trainingconfig( batch_size=best_params.get("batch_size", 32), learning_rate=best_params.get("learning_rate", 1e-4), max_epochs=args.max_epochs # always use user-provided max_epochs ) else: logger.info("using provided default hyperparameters") model_config = modelconfig(n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer) training_config = trainingconfig(batch_size=args.batch_size, learning_rate=args.learning_rate, max_epochs=args.max_epochs) config = config(model=model_config, training=training_config) logger.debug(f"configuration: {config}") # load data logger.info("loading data") train_set, eval_set = arckit.load_data() train_data = arcdataset(train_set) val_data = arcdataset(eval_set) logger.debug(f"train data size: {len(train_data)}, validation data size: {len(val_data)}") # load model configuration json file config_path = f"results/experiment_{args.model_checkpoint.split('_')[-1].replace('.pth', '.json')}" open(config_path, 'r') f: config_data = json.load(f) model_config = modelconfig( n_embd=config_data['config']['model']['n_embd'], n_head=config_data['config']['model']['n_head'], n_layer=config_data['config']['model']['n_layer'], dropout=config_data['config']['model']['dropout'] ) # initialize model logger.info("initializing model") model = gpt2arc(config=model_config) logger.debug(f"model structure: {model}") # load checkpoint specified args.model_checkpoint: logger.info(f"loading model checkpoint: {args.model_checkpoint}") checkpoint = torch.load(args.model_checkpoint) model.load_state_dict(checkpoint) # initialize results collector results_collector = resultscollector(config) # initialize experiment tracker tracker = experimenttracker(config, project=args.project) # initialize trainer logger.info("initializing trainer") trainer = arctrainer( model=model, train_dataset=train_data, val_dataset=val_data, config=config ) trainer.log_hyperparameters() # set pytorch lightning trainer logger.info("setting pytorch lightning trainer") callbacks = [] args.no_checkpointing: checkpoint_callback = modelcheckpoint( dirpath="checkpoints", filename="arc_model-{epoch:02d}-{val_loss:.2f}", save_top_k=3, monitor="val_loss", mode="min", ) callbacks.append(checkpoint_callback) tb_logger = false args.no_logging else tensorboardlogger("tb_logs", name="arc_model") pl_trainer = pl.trainer( max_epochs=config.training.max_epochs, logger=tb_logger, callbacks=callbacks callbacks else none, enable_checkpointing=not args.no_checkpointing, enable_progress_bar=not args.no_progress_bar, fast_dev_run=args.fast_dev_run, gradient_clip_val=1.0, accelerator='gpu' args.use_gpu torch.cuda.is_available() else 'cpu', devices=1 ) # train model logger.info("starting model training") pl_trainer.fit(trainer) # training, run test logger.info("running model evaluation") test_results = pl_trainer.test(trainer) test_results: avg_test_loss = test_results[0]['test_loss'] avg_test_accuracy = test_results[0]['test_accuracy'] logger.info(f"test results - loss: {avg_test_loss}, accuracy: {avg_test_accuracy}") trainer.results_collector.set_test_results({ "test_loss": avg_test_loss, "test_accuracy": avg_test_accuracy }) trainer.results_collector.set_final_metrics({ "best_val_loss": trainer.best_val_loss, "best_epoch": trainer.best_epoch, "final_test_loss": avg_test_loss, "final_test_accuracy": avg_test_accuracy }) # save final model logger.info("saving final model") model_path = f"final_model_{trainer.results_collector.experiment_id}.pth" torch.save(trainer.model.state_dict(), model_path) trainer.results_collector.set_checkpoint_path(model_path) logger.debug(f"model saved to: {model_path}") # save results logger.info("saving experiment results") results_path = f"results/experiment_{trainer.results_collector.experiment_id}.json" trainer.results_collector.save_to_json(results_path) logger.debug(f"results saved to: {results_path}") except exception e: logger.error(f"an error occurred: {str(e)}", exc_info=true) 'tracker' locals(): tracker.log_metric("training_interrupted", 1) tracker.log_metric("error_message", str(e)) raise finally: 'tracker' locals(): tracker.finish() __name__ == "__main__": parser = argparse.argumentparser(description="train arc neural reasoning model") parser.add_argument("--use-optuna", action="store_true", help="use best hyperparameters optuna study") parser.add_argument("--optuna-study-name", type=str, default="gpt2_arc_optimization", help="name optuna study load") parser.add_argument("--optuna-storage", type=str, default="sqlite:///optuna_results.db", help="storage url optuna study") parser.add_argument("--n-embd", type=int, default=768, help="embedding dimension") parser.add_argument("--n-head", type=int, default=12, help="number attention heads") parser.add_argument("--n-layer", type=int, default=12, help="number transformer layers") parser.add_argument("--batch-size", type=int, default=32, help="batch size training") parser.add_argument("--learning-rate", type=float, default=1e-4, help="learning rate") parser.add_argument("--max-epochs", type=int, required=true, help="maximum number epochs") parser.add_argument("--use-gpu", action="store_true", help="use gpu training available") parser.add_argument("--no-logging", action="store_true", help="disable logging") parser.add_argument("--no-checkpointing", action="store_true", help="disable checkpointing") parser.add_argument("--no-progress-bar", action="store_true", help="disable progress bar") parser.add_argument("--fast-dev-run", action="store_true", help="run fast development test") parser.add_argument("--model_checkpoint", type=str, help="path model checkpoint resume training") parser.add_argument("--project", type=str, default="gpt2-arc", help="w&amp;b project name") parser.add_argument("--results-dir", type=str, default="./results", help="directory save results") parser.add_argument("--run-name", type=str, default="default_run", help="name run saving results") args = parser.parse_args() main(args)</file><file name="debug tips/test_differential_pixel_accuracy.md">identify fix errors `test_differential_pixel_accuracy.py` test script, essential examine files modules test interacts with. based provided code summaries repository files, following files likely candidates investigate: 1. **`gpt2_arc/src/utils/helpers.py`** - **reason:** file contains `differential_pixel_accuracy` function, central test cases. there's error related accuracy computation, implementation first place check. - **action:** review implementation `differential_pixel_accuracy` potential bugs inconsistencies. ensure correctly handles different tensor shapes, data types, edge cases like empty tensors. 2. **`gpt2_arc/src/models/gpt2.py`** - **reason:** `gpt2arc` model instantiated used one tests (`test_differential_pixel_accuracy_with_arckit_data`). errors related model initialization, prediction generation, tensor shapes likely rooted here. - **action:** - verify `gpt2arc` model correctly defined, especially forward pass. - ensure model's output dimensions match expected shapes used test. - check potential issues model's layers (e.g., `attention`, `feedforward`, `transformerblock`) might affect predictions. 3. **`gpt2_arc/src/config.py`** - **reason:** `modelconfig` dataclass used configure `gpt2arc` model. misconfigurations lead unexpected behaviors mismatches model parameters. - **action:** - ensure necessary configuration parameters correctly defined passed. - check consistency configuration used tests model's requirements. 4. **`gpt2_arc/src/data/arc_dataset.py`** - **reason:** `arcdataset` class responsible data loading preprocessing, critical generating valid input target tensors tests. - **action:** - verify data preprocessing methods (e.g., `_process_arckit_data`, `_preprocess_grid`) correctly handle data. - ensure dataset returns tensors expected shapes types. - check `reverse_scaling` method confirm accurately reverses scaling applied preprocessing. 5. **external dependency: `arckit`** - **reason:** test `test_differential_pixel_accuracy_with_arckit_data` relies `arckit` library load task data. issues data loading compatibility stem here. - **action:** - ensure `arckit` library correctly installed compatible project. - verify `task_id` used (`"007bbfb7"`) exists `arckit.load_single(task_id)` returns expected data structure. - check updates changes `arckit` api might affect data loading. 6. **additional considerations:** - **environment dependencies:** - ensure dependencies (e.g., pytorch, `arckit`) up-to-date compatible other. - verify python environment necessary packages installed. - **test environment:** - confirm test run environment relative paths module imports correctly resolved. - check recent changes project structure might affect import statements. 7. **debugging tips:** - **verbose logging:** enhance test functions detailed logging pinpoint error occurs. example, print shapes data types tensors operation. - **isolate tests:** run individual test functions separately identify specific test failing. - **use assertions carefully:** ensure assertions accurately reflect expected outcomes. example, floating-point comparisons might require tolerance level instead exact equality. 8. **if error persists:** - **provide error messages:** sharing specific error messages stack traces help diagnosing issue effectively. - **check version control:** review recent commits identify changes might introduced error. - **consult documentation:** refer documentation external libraries like `arckit` breaking changes known issues. systematically examining files following debugging steps, able identify resolve error test code effectively. need assistance specific files error messages, feel free share relevant code snippets details.</file><file name="debug tips/test_model_evaluation.md">identify fix errors `test_model_evaluation.py` file, essential focus dependencies modules test script interacts with. here's breakdown relevant files repository likely provide useful information debugging: 1. **`src/models/gpt2.py`** - **why:** file defines `gpt2arc` class, core component tested. issues related model architecture, forward pass, specific layers (like `attention`, `feedforward`, `transformerblock`) originate here. - **what check:** - initialization `gpt2arc` model. - implementation `forward` method. - custom layers operations might affect model outputs. 2. **`src/config.py`** - **why:** file contains configuration classes (`config`, `modelconfig`, `trainingconfig`) used instantiate models training parameters. misconfigurations lead unexpected behaviors initialization errors tests. - **what check:** - correct definitions default values dataclasses. - dependencies validations within configuration classes. - ensure required fields correctly passed utilized. 3. **`src/training/trainer.py`** - **why:** `arctrainer` class imported used fixtures. issues related training loop, validation steps, trainer interacts model manifest tests. - **what check:** - initialization setup `arctrainer`. - implementation methods like `validation_step`, explicitly tested. - handling incorrect batch formats error raising mechanisms. 4. **`src/utils/helpers.py`** - **why:** file includes utility functions like `differential_pixel_accuracy`, directly used tests. bugs unexpected behaviors helper functions cause test failures. - **what check:** - correct implementation `differential_pixel_accuracy`. - edge case handling input validations within helper functions. 5. **`src/data/arc_dataset.py`** - **why:** although directly imported test script, `dataloader` relies `arcdataset` class defined here. issues data preprocessing, batching, dataset splitting indirectly affect tests. - **what check:** - data loading preprocessing logic. - handling different data sources formats. - transformations applied data fed model. 6. **checkpoint files (`checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt`)** - **why:** tests involve loading model checkpoints. problems checkpoint integrity, missing keys, incompatible configurations lead errors model loading evaluation. - **what check:** - ensure checkpoint file exists accessible. - verify checkpoint contains necessary keys (`config`, `state_dict`, etc.). - confirm `modelconfig` checkpoint matches expected configuration code. 7. **`src/utils/experiment_tracker.py` &amp; `src/utils/results_collector.py`** - **why:** utilities handle experiment tracking results collection, influence metrics configurations logged stored. issues affect integrity metrics tested. - **what check:** - correct logging metrics configurations. - proper serialization deserialization experiment data. - error handling edge case management tracking methods. 8. **logging configuration `test_model_evaluation.py`** - **why:** since test script sets logging, misconfigurations obscure error messages make debugging challenging. - **what check:** - ensure logging level appropriately set (`debug` case). - verify log messages correctly formatted informative. ### steps diagnose fix errors: 1. **identify error source:** - look error message traceback pinpoint error originates. guide relevant file(s). 2. **check dependencies:** - know part code failing, inspect corresponding file(s) mentioned potential issues. 3. **validate configurations:** - ensure configurations passed models trainers correct complete. 4. **verify data integrity:** - make sure data loaded processed matches expected format structure required models trainers. 5. **inspect checkpoints:** - confirm checkpoint files corrupted contain necessary components reconstruct model state. 6. **enhance logging:** - utilize debug logs set gain insights internal states data flow test execution. systematically reviewing files following diagnostic steps, able identify resolve errors `test_model_evaluation.py` script effectively.</file><file name="debug tips/test_end_to_end.md">troubleshooting errors `test_end_to_end.py` script, several files within repository likely provide relevant information help identify fix issue. here's breakdown key files examine based different parts test script: ### 1. **data handling preprocessing** - **`src/data/arc_dataset.py`** - **relevance:** file contains `arcdataset` class, crucial loading preprocessing arc dataset. errors related data loading, dataset splitting, preprocessing steps (like `_process_synthetic_data` `_preprocess_grid`) likely originate here. - **what check:** - ensure dataset paths correct. - verify data processing methods handling data expected. - check issues `collate_fn` used batching data. - **`arckit` module** - **relevance:** test script uses `arckit.load_data()` load dataset. issues data loading process structure loaded data would tied module. - **what check:** - ensure `arckit` correctly installed accessible. - verify `load_data` function returns data expected format. ### 2. **model definition** - **`src/models/gpt2.py`** - **relevance:** file defines `gpt2arc` model components (`attention`, `feedforward`, `transformerblock`). errors related model architecture, layer mismatches incorrect configurations, likely originate here. - **what check:** - ensure model configuration (`modelconfig`) matches expected architecture. - verify layers correctly defined initialized. - check type mismatches tensor dimension issues within model. ### 3. **training logic** - **`src/training/trainer.py`** - **relevance:** file contains `arctrainer` class, manages training loop, loss computation, metric tracking. errors training, issues optimizer, loss functions, training steps, likely stem here. - **what check:** - ensure training configurations (`trainingconfig`) correctly set. - verify implementation training validation steps. - check runtime errors forward backward passes. ### 4. **configuration management** - **`src/config.py`** - **relevance:** file defines configuration data classes (`config`, `modelconfig`, `trainingconfig`). misconfigurations, incorrect hyperparameters mismatched settings, lead errors model initialization training. - **what check:** - ensure configuration parameters correctly set passed components. - verify default values appropriate overrides correctly applied. ### 5. **utility functions experiment tracking** - **`src/utils/experiment_tracker.py`** - **relevance:** test script involves experiment tracking logging metrics, issues could affect logging tracking experiments. - **what check:** - ensure experiment tracker correctly initialized configured. - verify metrics logged saved expected. - **`src/utils/results_collector.py`** - **relevance:** file manages collection storage results training evaluation. errors related result aggregation storage likely originate here. - **what check:** - ensure results correctly collected serialized. - verify issues saving loading result data. ### 6. **evaluation process** - **`src/evaluate.py`** - **relevance:** although directly referenced test script, evaluation logic invoked shared scripts, issues could affect evaluation metrics. - **what check:** - ensure evaluation metrics correctly computed. - verify evaluation data correctly processed fed model. ### 7. **other potential sources** - **`benchmark.py` `train.py`** - **relevance:** likely related running benchmarks training outside tests, shared components configurations could indirectly affect tests. - **what check:** - ensure shared utilities configurations used scripts consistent error-free. ### **general debugging tips:** 1. **logging:** test script extensive logging enabled (`logging.basicconfig(level=logging.debug)`). review debug logs pinpoint error occurs. logs provide step-by-step insights test execution flow. 2. **assertions error messages:** pay close attention assertion statements error messages produce. guide exact point failure. 3. **dependencies:** ensure dependencies (like `arckit`, `torch`, `pytorch_lightning`, etc.) correctly installed compatible other. 4. **environment issues:** sometimes, errors arise environment (e.g., incorrect cuda setup, incompatible library versions). verify environment matches expected setup. 5. **isolate issue:** possible, try running individual components smaller tests isolate error occurring. help narrow problematic file section code. ### **next steps:** reviewing files still unable identify issue, consider following: - **provide specific error messages:** sharing exact error messages stack traces help diagnosing problem accurately. - **add relevant files:** issue seems originate specific file listed here, feel free add content chat in-depth analysis. systematically reviewing files following debugging tips, able identify resolve errors end-to-end test script effectively.</file><file name="debug tips/test_arc_dataset.md">effectively troubleshoot fix errors `gpt2_arc/tests/test_arc_dataset.py` test suite, focus following key files repository: 1. **`gpt2_arc/src/data/arc_dataset.py`** - **why:** primary file `arcdataset` class `set_debug_mode` function defined. since tests directly interacting components, issues related dataset initialization, data preprocessing, utility functions likely originate here. - **what look for:** - **initialization logic:** ensure `__init__` method correctly handles different types `data_source` inputs (`str`, `list[dict]`, `taskset`, etc.). - **data processing methods:** check methods like `_process_synthetic_data`, `_process_arckit_data`, `_preprocess_grid` logical errors incorrect handling data. - **debug mode handling:** verify `set_debug_mode` function correctly toggles debug state debug-related logging behavior `arcdataset` functioning expected. 2. **`gpt2_arc/src/utils/experiment_tracker.py`** - **why:** directly referenced test file, utility classes like `experimenttracker` influence behavior dataset, especially used logging tracking metrics dataset processing. - **what look for:** - **logging configuration:** ensure logging correctly set interfere dataset operations. - **serialization methods:** check methods like `_make_serializable` `_serialize_config` ensure configurations correctly handled, affect dataset initialization configurations passed around. 3. **`gpt2_arc/src/models/gpt2.py`** - **why:** although tests focus dataset, models often interact closely datasets training evaluation. issues model configurations data handling within model indirectly affect dataset behavior. - **what look for:** - **data expectations:** ensure model correctly expects data shapes types provided `arcdataset`. - **integration points:** verify integration points model dataset (if present) correctly implemented. 4. **dependencies external libraries (`arckit`)** - **why:** tests import `taskset` `arckit.data`, suggests `arckit` external dependency. issues within library propagate dataset tests. - **what look for:** - **compatibility:** ensure version `arckit` using compatible dataset known bugs affecting `taskset`. - **mock implementations:** since use `unittest.mock.mock` `taskset`, ensure mock accurately reflects structure behavior expected `arcdataset`. 5. **test file (`gpt2_arc/tests/test_arc_dataset.py`)** - **why:** sometimes, issue might reside within test logic rather implementation. reviewing test file help identify incorrect assumptions faulty test setups. - **what look for:** - **test fixtures:** ensure fixtures like `sample_data` `mock_taskset` provide correct data structures expected `arcdataset`. - **assertions:** verify assertions correctly reflect intended behavior overly restrictive incorrectly specified. - **skipped tests:** review certain tests skipped determine need updated fixed included test suite. 6. **additional configuration files (`gpt2_arc/src/config.py`)** - **why:** configuration files often dictate datasets models initialized interacted with. errors configurations lead unexpected behaviors testing. - **what look for:** - **model dataset configurations:** ensure necessary configurations dataset correctly defined accessible. - **defaults overrides:** check default configurations set overridden, ensuring consistency across different test scenarios. **summary:** - **primary focus:** `gpt2_arc/src/data/arc_dataset.py` - **secondary focus:** utility files like `experiment_tracker.py`, model definitions `gpt2.py`, configuration files `config.py` - **dependencies:** ensure external libraries like `arckit` functioning expected - **test integrity:** verify correctness test setups assertions within `test_arc_dataset.py` systematically reviewing areas, able identify resolve errors within test suite effectively.</file><file name="debug tips/test_gpt2.md">encountering error `gpt2_arc/tests/test_gpt2.py` test suite, relevant files examine troubleshooting define components tested. here's breakdown primary files investigate: 1. **`gpt2_arc/src/models/gpt2.py`** - **why:** file contains definitions `gpt2arc` class well constituent modules like `attention`, `feedforward`, `transformerblock`. since tests directly interacting classes (e.g., initializing `gpt2arc`, performing forward passes, etc.), issues related model architecture, initialization, forward computations would likely originate here. - **key sections check:** - `gpt2arc` class initialization attributes (`conv1`, `blocks`, `ln_f`, `config`). - implementation details `attention`, `feedforward`, `transformerblock` modules. - custom methods overrides might affect model's behavior testing. 2. **`gpt2_arc/src/config.py`** - **why:** `modelconfig` class file used configure `gpt2arc` model initialization tests. errors related configuration parameters, default values, structure configuration lead issues model instantiation behavior. - **key sections check:** - definition `modelconfig` fields. - methods default values set model's configuration. - interactions `modelconfig` parts model (e.g., ensuring necessary configuration parameters correctly passed utilized). 3. **additional files consider:** - **`gpt2_arc/src/utils/experiment_tracker.py` &amp; `gpt2_arc/src/utils/results_collector.py`:** - **why:** tests involve tracking experiments collecting results, issues utility classes might indirectly affect tests. instance, incorrect logging result serialization could lead unexpected behavior errors test execution. - **`gpt2_arc/src/data/arc_dataset.py`:** - **why:** tests rely specific data preprocessing dataset structures, bugs changes data handling could impact inputs tests use. ensuring data correctly processed fed model crucial accurate testing. 4. **test-specific considerations:** - **duplicate test function:** - **issue:** `test_gpt2.py`, two functions named `test_gpt2arc_forward_pass`. python support function overloading, second definition overwrite first. could lead unexpected test behaviors skipped tests. - **solution:** rename one test functions ensure test unique name, `test_gpt2arc_forward_pass_with_mask` `test_gpt2arc_forward_pass_without_mask`. 5. **logging output:** - **why:** since test file configured `logging.debug`, reviewing log outputs provide detailed insights error might occurring. ensure logs captured review identify anomalies error messages test execution. 6. **dependencies environment:** - **why:** ensure dependencies (like `torch`, `pytest`, etc.) correctly installed compatible codebase. sometimes, version mismatches lead unexpected errors. - **solution:** verify `requirements.txt` environment configuration ensure packages up-to-date compatible. 7. **version control recent changes:** - **why:** error new, reviewing recent commits changes related files help pinpoint source issue. - **solution:** use `git` commands like `git blame` `git diff` identify recent modifications might introduced error. **next steps:** 1. **identify specific error:** - encounter error message traceback, use determine part code failing. 2. **inspect relevant files:** - focus files highlighted above, especially `gpt2_arc/src/models/gpt2.py` `gpt2_arc/src/config.py`. 3. **request additional file contents needed:** - determine specific section file might causing issue need details, feel free ask include file's content analysis. systematically reviewing areas, able identify resolve error test suite effectively.</file><file name="debug tips/test_pytest_error_fixer.md">effectively diagnose fix errors `test_pytest_error_fixer.py` test script, want focus several key files within repository. here's breakdown relevant files provide necessary information: 1. **primary module test:** - **`pytest_error_fixer.py`**: main module tested `test_pytest_error_fixer.py` script. errors tests likely related implementation details within file. since list file summaries, ensure available consider sharing contents need detailed assistance. 2. **configuration files:** - **`gpt2_arc/src/config.py`**: file contains configuration classes (`modelconfig` `config`) might used `pytesterrorfixer`. misconfigurations lead issues initializing running fixer. 3. **utility modules:** - **`gpt2_arc/src/utils/experiment_tracker.py`** **`gpt2_arc/src/utils/results_collector.py`**: utility classes (`experimenttracker` `resultscollector`) might dependencies `pytesterrorfixer`. errors utilities propagate tests. 4. **data handling:** - **`gpt2_arc/src/data/arc_dataset.py`**: `pytesterrorfixer` interacts datasets relies data preprocessing, issues module affect tests. 5. **model training modules:** - **`gpt2_arc/src/models/gpt2.py`**: file defines `gpt2arc` model related classes. `pytesterrorfixer` interacts model components, ensure issues here. - **`gpt2_arc/src/training/train.py`** **`gpt2_arc/src/training/trainer.py`**: modules handle training process. integration `pytesterrorfixer` training pipeline verified. 6. **evaluation benchmarking:** - **`gpt2_arc/src/evaluate.py`** **`gpt2_arc/benchmark.py`**: scripts essential evaluating model performance. ensure `pytesterrorfixer` correctly interacts evaluation metrics applicable. 7. **experiment tracking:** - **`gpt2_arc/src/utils/experiment_tracker.py`**: utility crucial logging tracking experiments. issues affect errors progress logged `pytesterrorfixer`. 8. **results collection:** - **`gpt2_arc/src/utils/results_collector.py`**: similar experiment tracker, module handles collection storage results, might integral `pytesterrorfixer` operates. ### next steps: - **check `pytest_error_fixer.py`**: start reviewing implementation `pytesterrorfixer` class `pytest_error_fixer.py`. look obvious issues dependencies might properly handled. - **verify dependencies**: ensure dependencies (`experiment_tracker.py`, `results_collector.py`, etc.) correctly implemented free errors. - **review configuration**: double-check configurations `config.py` ensure align requirements `pytesterrorfixer`. - **mock external interactions**: tests, youre using mocks subprocess calls `coder` object. ensure mocks accurately represent behavior actual components. - **add missing files needed**: encounter issues trace back files listed (like `pytest_error_fixer.py`), please add chat provide targeted assistance. focusing files, able identify resolve errors test script effectively. need detailed help, feel free share contents `pytest_error_fixer.py` relevant files.</file><file name="debug tips/test_trainer.md">encountering error `gpt2_arc/tests/test_trainer.py` file, relevant files examine debugging fixing issue test file directly interacts depends upon. here's breakdown key files roles: 1. **`src/config.py`** - **classes check:** - `config` - `modelconfig` - `trainingconfig` - **relevance:** file defines configuration classes used initialize models trainers. errors related configuration parameters, default values, initialization logic likely originate here. 2. **`src/data/arc_dataset.py`** - **classes functions check:** - `arcdataset` - `set_debug_mode` - `_process_synthetic_data` - `_process_arckit_data` - `_preprocess_grid` - **relevance:** file handles data preprocessing dataset creation. issues related data loading, preprocessing steps, dataset structure (e.g., unexpected data formats) would found here. 3. **`src/models/gpt2.py`** - **classes check:** - `gpt2arc` - `attention` - `feedforward` - `transformerblock` - `modelconfig` - **relevance:** file contains gpt-2 model architecture components. errors model's forward pass, layer configurations, parameter settings likely rooted file. 4. **`src/training/trainer.py`** - **classes methods check:** - `arctrainer` - `training_step` - `validation_step` - `configure_optimizers` - `train_dataloader` - `val_dataloader` - `test_step` - **relevance:** core training module orchestrates training validation processes. issues related training loop, optimizer configuration, data loaders, logging mechanisms would addressed here. 5. **`src/utils/experiment_tracker.py`** - **classes methods check:** - `experimenttracker` - `log_metric` - `update_train_metrics` - `update_val_metrics` - `set_test_results` - `save_to_json` - **relevance:** tests involve tracking experiments logging metrics, errors related metric logging, experiment initialization, result serialization would involve file. 6. **`src/utils/results_collector.py`** - **classes methods check:** - `resultscollector` - `update_train_metrics` - `update_val_metrics` - `set_test_results` - `add_task_specific_result` - `save_to_json` - **relevance:** similar `experiment_tracker.py`, file manages collection storage results. errors aggregating storing test results would pertinent here. ### steps debug: 1. **identify error message:** - start looking exact error message stack trace. often point directly file line number issue originated. 2. **trace dependencies:** - understand `test_trainer.py` interacts modules. instance, there's issue model initialization, focus `src/config.py` `src/models/gpt2.py`. 3. **check configurations:** - ensure configuration objects (`config`, `modelconfig`, `trainingconfig`) correctly set required parameters provided. 4. **validate data handling:** - error related data loading preprocessing, review methods `arc_dataset.py` ensure data processed expected. 5. **inspect model architecture:** - issues forward pass model outputs, delve `gpt2.py` verify layer configurations data flow within model. 6. **examine training logic:** - error occurs training validation steps, scrutinize `arctrainer` class `trainer.py`, focusing methods like `training_step` `validation_step`. 7. **review utility functions:** - issues related logging result collection, check utility files ensure metrics recorded stored correctly. ### additional tips: - **use debugging tools:** - incorporate debugging statements use tools like `pdb` step code inspect variable states different execution points. - **isolate issue:** - temporarily simplify tests mock certain components isolate error occurring. - **check dependencies versions:** - ensure dependencies (e.g., pytorch, pytest) date compatible codebase. - **consult documentation:** - review documentation third-party libraries frameworks youre using ensure adhering best practices usage patterns. systematically examining files following structured debugging approach, able identify resolve error `test_trainer.py` code.</file><file name="debug tips/test_train.md">encountering error `gpt2_arc/tests/test_train.py` test suite, relevant files examine debugging resolving issue directly imported utilized within test cases. here's breakdown key files focus on: 1. **source files test:** - **`gpt2_arc/src/training/train.py`** - **role:** contains `main` function orchestrates training process. - **why check:** since tests invoking `main(args)`, issues training initiated handled would likely originate here. - **`gpt2_arc/src/training/trainer.py`** - **role:** defines `arctrainer` class, subclass `pl.lightningmodule` responsible managing training loop. - **why check:** errors related training logic, training steps, validation steps, integration pytorch lightning, would stem file. - **`gpt2_arc/src/models/gpt2.py`** - **role:** implements `gpt2arc` model, including architecture forward pass. - **why check:** error pertains model initialization, forward propagation, layer-specific issues, primary file inspect. - **`gpt2_arc/src/data/arc_dataset.py`** - **role:** contains `arcdataset` class responsible data loading preprocessing. - **why check:** issues related data handling, dataset initialization, data preprocessing, data loader configuration, would originate here. - **`gpt2_arc/src/config.py`** - **role:** defines configuration dataclasses like `config`, `modelconfig`, `trainingconfig`. - **why check:** misconfigurations incorrect parameter settings affect training behavior would defined file. 2. **utility support files:** - **`gpt2_arc/src/utils/results_collector.py`** - **role:** implements `resultscollector` class aggregating managing training results. - **why check:** errors related result logging, metric collection, summary generation would found here. - **`gpt2_arc/src/utils/experiment_tracker.py`** - **role:** manages experiment tracking, possibly integrating tools like weights &amp; biases. - **why check:** error involves experiment tracking, logging configurations, integrations external tracking tools, file pertinent. 3. **additional considerations:** - **`gpt2_arc/benchmark.py` `gpt2_arc/src/evaluate.py`** - **role:** files focused benchmarking evaluation, respectively, might still interact training components. - **why check:** error indirectly involves evaluation metrics benchmarking training, reviewing files could provide insights. - **mock fixture implementations `test_train.py`:** - **role:** test file uses fixtures mocks extensively simulate different components. - **why check:** ensure mocks correctly mimic behavior actual classes fixtures set appropriately. errors test setup lead misleading test failures. 4. **logging configuration:** - **logging configuration `test_train.py`:** - **role:** test file sets logging levels configurations. - **why check:** misconfigured logging obscure error messages lead unexpected behaviors testing. 5. **dependencies environment:** - **external libraries:** - ensure dependencies like `pytorch_lightning`, `torch`, libraries correctly installed compatible codebase. - **environment variables paths:** - verify `sys.path` manipulations environment settings test file correctly point necessary modules path conflicts. **summary:** effectively debug resolve errors `test_train.py`: - **start source files tested** (`train.py`, `trainer.py`, `gpt2.py`, `arc_dataset.py`, `config.py`) identify underlying issues training pipeline. - **examine utility files** (`results_collector.py` `experiment_tracker.py`) problems related logging result management. - **review test setup itself**, ensuring mocks fixtures accurately represent real components setup-related errors. systematically inspecting areas, pinpoint root cause errors implement effective fixes.</file><file name="debug tips/test_integration_experiment.md">debugging errors `test_integration_experiment.py` test file, several parts codebase likely provide valuable insights. based imports structure project, following files relevant diagnosing fixing potential issues: 1. **data handling preprocessing:** - **`gpt2_arc/src/data/arc_dataset.py`** - **why:** file contains `arcdataset` class, crucial data loading preprocessing. errors related data formatting, missing fields, incorrect data types often originate here. - **key sections review:** - `__init__` method: ensure dataset initialized correctly provided data sources. - `_process_arckit_data` `_process_synthetic_data` methods: verify data `arckit` processed expected. - debug logging statements might help trace data issues. 2. **model definition:** - **`gpt2_arc/src/models/gpt2.py`** - **why:** file defines `gpt2arc` model related components like `attention`, `feedforward`, `transformerblock`. errors related model architecture, layer configurations, forward passes stem here. - **key sections review:** - `gpt2arc` class initialization: check layers correctly instantiated right configurations. - forward methods: ensure data flows correctly model without shape mismatches issues. - custom configurations modifications standard gpt-2 architecture. 3. **training logic:** - **`gpt2_arc/src/training/trainer.py`** - **why:** file contains `arctrainer` class, manages training loop, loss calculations, metric updates. issues like improper training steps, incorrect loss functions, metric logging problems found here. - **key sections review:** - `__init__` method: ensure datasets, model, configurations correctly set up. - training step methods: verify loss calculations backpropagation implemented correctly. - integration pytorch lightning: check compatibility correct usage lightning's `trainer`. 4. **configuration management:** - **`gpt2_arc/src/config.py`** - **why:** file defines configuration classes like `config`, `modelconfig`, `trainingconfig`. misconfigurations, incorrect hyperparameters missing configuration fields, lead errors data loading, model initialization, training. - **key sections review:** - default values data types configuration parameters. - methods manipulate validate configurations. - integration points configurations passed components like model trainer. 5. **results collection logging:** - **`gpt2_arc/src/utils/results_collector.py`** - **why:** file manages collection summarization training validation results. errors related metric logging, result storage, summary generation originate here. - **key sections review:** - methods updating retrieving metrics. - serialization saving results. - integration components ensure metrics correctly passed stored. 6. **additional considerations:** - **`arckit` library:** - **why:** test setup relies `arckit` library load task data. issues tasks loaded structured, could affect tests. - **action:** ensure `arckit` correctly installed task ids used tests (`"007bbfb7"`) valid accessible. - **pytorch lightning integration:** - **files involved:** listed explicitly, test uses pytorch lightning's `trainer`. ensure integrations lightning correctly implemented `arctrainer` class callbacks like `modelcheckpoint` properly configured. - **logging debug statements:** - **why:** test includes several `print` statements debugging. ensure logs provide meaningful information interfere test flow. 7. **common error scenarios file associations:** - **import errors:** - **files check:** `arc_dataset.py`, `gpt2.py`, `trainer.py`, `config.py`, `results_collector.py` - **reason:** missing incorrect imports usually point issues modules. - **attribute errors missing methods:** - **files check:** `gpt2.py`, `trainer.py`, `arc_dataset.py` - **reason:** ensure necessary methods attributes defined correctly named. - **data shape mismatches:** - **files check:** `arc_dataset.py`, `gpt2.py` - **reason:** verify data shapes consistent throughout data pipeline model. - **configuration mismatches:** - **files check:** `config.py`, `trainer.py`, `gpt2.py` - **reason:** ensure components receive use configurations correctly. 8. **next steps:** - **review relevant files:** start examining files listed above, focusing sections likely related error. - **add detailed logging:** already present, consider adding detailed logging within files trace flow data identify things might going wrong. - **isolate issue:** determine whether error related data loading, model initialization, training steps, configuration. help narrow file focus on. - **run tests incrementally:** use pytest's verbose mode selectively run tests get context failure occurs. identify specific file section causing issue need assistance, feel free share relevant code snippets adding files chat. allow targeted help resolving problem.</file><file name="debug tips/test_benchmark.md">encountering error `gpt2_arc/tests/test_benchmark.py` test suite, essential identify relevant files could provide insights issue. here's prioritized list files examine, along brief explanations likely involved: 1. **`gpt2_arc/benchmark.py`** - **reason:** primary module tested. functions like `benchmark_model` `main` directly imported invoked test cases. issues functions (e.g., logic errors, incorrect handling inputs/outputs) likely manifest testing. 2. **`gpt2_arc/src/models/gpt2.py`** - **reason:** `gpt2arc` class core component mocked used tests. errors related model configuration, initialization, methods (like `forward`) affect benchmark tests. 3. **`gpt2_arc/src/config.py`** - **reason:** `modelconfig` dataclass imported potentially used within `benchmark.py` model modules. misconfigurations incorrect parameter settings lead unexpected behaviors benchmarking. 4. **`gpt2_arc/src/data/arc_dataset.py`** - **reason:** dataset (`arcdataset`) mocked tests, underlying issues data processing, loading, preprocessing actual implementation cause tests fail behave unpredictably. 5. **`gpt2_arc/src/utils/experiment_tracker.py`** - **reason:** `benchmark_model` related functions utilize `experimenttracker` logging tracking experiments, bugs exceptions within utility propagate tests. 6. **`gpt2_arc/src/utils/results_collector.py`** - **reason:** similar `experiment_tracker.py`, results collection part benchmarking process, issues `resultscollector` affect output validation tests. 7. **`gpt2_arc/src/training/train.py` &amp; `gpt2_arc/src/training/trainer.py`** - **reason:** directly invoked provided test code, modules may indirectly involved `benchmark_model` interacts training routines utilizes components scripts. 8. **external dependencies (e.g., `torch`, `pytest`, `unittest.mock`)** - **reason:** although less likely, issues external libraries mocked tests also lead errors. ensure versions compatible mocks correctly set up. ### steps diagnose error: 1. **examine error message:** - start looking exact error message traceback provided test fails. often point directly problematic file line number. 2. **check `benchmark.py`:** - since main module test, review functions `benchmark_model` `main` logical errors incorrect handling inputs outputs. 3. **validate mocks fixtures:** - ensure mocks (e.g., `mock_model`, `mock_dataset`, `mock_dataloader`) accurately represent behavior real objects. incorrect mocking lead misleading test results. 4. **review dependencies `gpt2_arc/src/models/gpt2.py`:** - look issues `gpt2arc` class, especially methods invoked benchmarking, `forward`. 5. **inspect configuration `gpt2_arc/src/config.py`:** - verify necessary configurations correctly set mismatches expected actual parameters. 6. **analyze data handling `gpt2_arc/src/data/arc_dataset.py`:** - ensure data loading preprocessing steps functioning intended. errors lead incorrect inputs fed model benchmarking. 7. **evaluate utility modules:** - check `experiment_tracker.py` `results_collector.py` bugs exceptions might interfere benchmarking process. 8. **run isolated tests:** - consider running individual tests components isolation pinpoint failure occurs. 9. **check environment issues:** - sometimes, errors arise testing environment, incompatible library versions insufficient resources (e.g., gpu availability). ensure environment matches expectations set tests. ### additional tips: - **enable verbose logging:** - add logging statements within `benchmark.py` related modules trace flow execution identify things might going wrong. - **use debugging tools:** - utilize debugging tools like `pdb` step test execution inspect state variables different points. - **review recent changes:** - tests passing previously, review recent changes codebase might introduced error. systematically examining files following diagnostic steps, able identify resolve error test suite effectively.</file><file name="debug tips/test_results_collector.md">debugging errors `test_results_collector.py` test suite, relevant files examine directly involved functionality tested. here's prioritized list files likely provide information help fix issues: 1. **`gpt2_arc/src/utils/results_collector.py`** - **reason:** primary module tested. errors initialization, metric updates, result handling likely originating here. - **key components check:** - `resultscollector` class implementation. - methods like `update_train_metrics`, `update_val_metrics`, `set_test_results`, `add_task_specific_result`, `get_summary`. - initialization logic, especially `experiment_id`, `timestamp`, `config` set up. 2. **`gpt2_arc/src/config.py`** - **reason:** test initializes `resultscollector` using configurations defined file. errors related configuration attributes (e.g., `n_embd`, `n_head`, `n_layer`, `batch_size`, etc.) may stem issues configuration classes. - **key components check:** - `config`, `modelconfig`, `trainingconfig` dataclasses. - methods default values manipulate validate configuration parameters. 3. **`gpt2_arc/src/utils/experiment_tracker.py`** - **reason:** `resultscollector` may internally utilize `experimenttracker` logging tracking experiments. issues experiment tracking could affect results collection process. - **key components check:** - `experimenttracker` class methods, especially related logging metrics handling experiment ids. - initialization interactions external services like wandb (if `use_wandb` enabled). 4. **dependencies external modules:** - **`gpt2_arc/src/utils/results_collector.py` dependencies:** - ensure utility functions classes used within `resultscollector` functioning correctly. - **environment configuration files:** - check environment-specific configurations dependencies might affect test execution. 5. **test environment setup:** - although test file primarily testing, ensure `setup` method correctly initializes necessary components. misconfigurations incorrect setups lead misleading test failures. **steps diagnose issue:** 1. **identify error message:** - start examining exact error message traceback failed test. often point directly problematic line code. 2. **trace source:** - use traceback trace back source file line number error originated. help determine whether issue within `results_collector.py`, `config.py`, another related module. 3. **review recent changes:** - tests previously passing, consider recent changes made related modules might introduced error. 4. **check dependency issues:** - ensure dependencies correctly installed compatible other, especially updates packages like `torch`, `pytorch_lightning`, others used project. 5. **isolate problem:** - temporarily simplify isolate parts `resultscollector` identify specific method component causing failure. systematically reviewing files following diagnostic steps, able identify fix error test suite.</file></source>