{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8-QMJgz1LmwE",
      "metadata": {
        "id": "8-QMJgz1LmwE"
      },
      "source": [
        "Required: https://youtu.be/IDxrMbXPVTA?si=PHfGry-HQj__3Xne"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0L9IwEdxzd-H",
      "metadata": {
        "id": "0L9IwEdxzd-H"
      },
      "source": [
        "# STUFF YOU CHANGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "6okItopOumX_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6okItopOumX_",
        "outputId": "5359a138-ed90-4dd4-9289-3e70fb5c7bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Configuration:\n",
            "Parent Directory: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Use Best Parameters: True\n",
            "Manual Parameters: {'n_embd': 32, 'n_head': 4, 'n_layer': 6, 'batch_size': 64, 'learning_rate': 0.0001, 'max_epochs': 50}\n",
            "Number of Optuna Trials: 100\n",
            "Hyperparameter Ranges:\n",
            "  n_embd: 64 to 256\n",
            "  n_head: 2 to 16\n",
            "  n_layer: 2 to 16\n",
            "  batch_size: 1 to 256\n",
            "  learning_rate: 1e-08 to 0.01\n",
            "  max_epochs: 3 to 50\n",
            "Configuration saved to config.json\n",
            "\n",
            "GPU Available: False\n"
          ]
        }
      ],
      "source": [
        "# ========== User-Defined Parameters (Top of Notebook) ==========\n",
        "\n",
        "# 1. Base directory for storing date-based experiment result folders in Google Drive.\n",
        "parent_dir = \"/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\"\n",
        "\n",
        "# 2. Boolean flag to choose between best hyperparameters from previous experiments (True) or manual parameters (False).\n",
        "use_best_params = True  # Set to False to use manual parameters\n",
        "\n",
        "# 3. Dictionary of manually set hyperparameters used when use_best_params is False.\n",
        "#    Includes model architecture and training settings.\n",
        "manual_params = {\n",
        "    \"n_embd\": 32,     # Embedding dimension\n",
        "    \"n_head\": 4,       # Number of attention heads\n",
        "    \"n_layer\": 6,     # Number of transformer layers\n",
        "    \"batch_size\": 64,  # Batch size for training\n",
        "    \"learning_rate\": 0.0001,  # Learning rate\n",
        "    \"max_epochs\": 50   # Maximum number of epochs for training\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning search space settings\n",
        "# 4. Number of Optuna trials for hyperparameter tuning.\n",
        "#    More trials can lead to better optimization but increase computation time.\n",
        "n_trials = 100\n",
        "\n",
        "# 5. Range for embedding dimension in hyperparameter search space.\n",
        "n_embd_min, n_embd_max = 64, 256\n",
        "\n",
        "# 6. Range for number of attention heads in transformer model during tuning.\n",
        "n_head_min, n_head_max = 2, 16\n",
        "\n",
        "# 7. Range for number of transformer layers in model architecture during optimization.\n",
        "n_layer_min, n_layer_max = 2, 16\n",
        "\n",
        "# 8. Range of batch sizes to explore. Larger batches can speed up training but may require more memory.\n",
        "batch_size_min, batch_size_max = 1, 256\n",
        "\n",
        "# 9. Range for learning rate in hyperparameter search space. Crucial for model convergence and performance.\n",
        "learning_rate_min, learning_rate_max = 1e-8, 1e-2\n",
        "\n",
        "# 10. Range for number of training epochs to consider during hyperparameter optimization.\n",
        "max_epochs_min, max_epochs_max = 3, 50\n",
        "\n",
        "# These parameters allow flexible experimentation with different model configurations and training settings,\n",
        "# enabling comprehensive exploration of the hyperparameter space for optimal model performance.\n",
        "\n",
        "# Print configurations for verification\n",
        "print(\"Current Configuration:\")\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"Use Best Parameters: {use_best_params}\")\n",
        "print(f\"Manual Parameters: {manual_params}\")\n",
        "print(f\"Number of Optuna Trials: {n_trials}\")\n",
        "print(\"Hyperparameter Ranges:\")\n",
        "print(f\"  n_embd: {n_embd_min} to {n_embd_max}\")\n",
        "print(f\"  n_head: {n_head_min} to {n_head_max}\")\n",
        "print(f\"  n_layer: {n_layer_min} to {n_layer_max}\")\n",
        "print(f\"  batch_size: {batch_size_min} to {batch_size_max}\")\n",
        "print(f\"  learning_rate: {learning_rate_min} to {learning_rate_max}\")\n",
        "print(f\"  max_epochs: {max_epochs_min} to {max_epochs_max}\")\n",
        "\n",
        "# Save configuration\n",
        "import json\n",
        "\n",
        "def save_config(config, filename=\"config.json\"):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "config = {\n",
        "    \"parent_dir\": parent_dir,\n",
        "    \"use_best_params\": use_best_params,\n",
        "    \"manual_params\": manual_params,\n",
        "    \"tuning\": {\n",
        "        \"n_trials\": n_trials,\n",
        "        \"n_embd\": (n_embd_min, n_embd_max),\n",
        "        \"n_head\": (n_head_min, n_head_max),\n",
        "        \"n_layer\": (n_layer_min, n_layer_max),\n",
        "        \"batch_size\": (batch_size_min, batch_size_max),\n",
        "        \"learning_rate\": (learning_rate_min, learning_rate_max),\n",
        "        \"max_epochs\": (max_epochs_min, max_epochs_max)\n",
        "    }\n",
        "}\n",
        "\n",
        "save_config(config)\n",
        "print(f\"Configuration saved to config.json\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"\\nGPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2-qicsswzi-S",
      "metadata": {
        "id": "2-qicsswzi-S"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DgxlLFBTg8Kt",
      "metadata": {
        "id": "DgxlLFBTg8Kt"
      },
      "source": [
        "### 1. Set up the Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "u8k0Yxzd4RCb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8k0Yxzd4RCb",
        "outputId": "aa0390e3-e53c-46ca-9f77-ac45d076335a"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "pRx7QZEdg8Kt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRx7QZEdg8Kt",
        "outputId": "9b102db3-e7ac-4294-868d-f3bfb9cb33c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '/content/arc-neural-reasoning-model/gpt2_arc/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: optuna in /usr/local/python/3.12.1/lib/python3.12/site-packages (4.0.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.12/site-packages (from optuna) (24.1)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna) (4.64.0)\n",
            "Requirement already satisfied: PyYAML in /home/codespace/.local/lib/python3.12/site-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/local/python/3.12.1/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from alembic>=1.5.0->optuna) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/codespace/.local/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna) (2.1.5)\n",
            "Requirement already satisfied: optuna-dashboard in /usr/local/python/3.12.1/lib/python3.12/site-packages (0.16.2)\n",
            "Collecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: bottle>=0.13.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna-dashboard) (0.13.1)\n",
            "Requirement already satisfied: optuna>=3.1.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna-dashboard) (4.0.0)\n",
            "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from optuna-dashboard) (24.1)\n",
            "Requirement already satisfied: scikit-learn in /home/codespace/.local/lib/python3.12/site-packages (from optuna-dashboard) (1.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.12/site-packages (from pyngrok) (6.0.2)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna>=3.1.0->optuna-dashboard) (1.13.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna>=3.1.0->optuna-dashboard) (6.8.2)\n",
            "Requirement already satisfied: numpy in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna>=3.1.0->optuna-dashboard) (1.26.4)\n",
            "Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna>=3.1.0->optuna-dashboard) (2.0.35)\n",
            "Requirement already satisfied: tqdm in /usr/local/python/3.12.1/lib/python3.12/site-packages (from optuna>=3.1.0->optuna-dashboard) (4.64.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from scikit-learn->optuna-dashboard) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->optuna-dashboard) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from scikit-learn->optuna-dashboard) (3.5.0)\n",
            "Requirement already satisfied: Mako in /usr/local/python/3.12.1/lib/python3.12/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (1.3.5)\n",
            "Requirement already satisfied: typing-extensions>=4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (4.12.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from sqlalchemy>=1.3.0->optuna>=3.1.0->optuna-dashboard) (3.1.1)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /home/codespace/.local/lib/python3.12/site-packages (from Mako->alembic>=1.5.0->optuna>=3.1.0->optuna-dashboard) (2.1.5)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n",
            "Authtoken saved to configuration file: /home/codespace/.config/ngrok/ngrok.yml                      \n",
            "Requirement already satisfied: tensorboard in /usr/local/python/3.12.1/lib/python3.12/site-packages (2.17.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (2.1.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (1.66.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (3.7)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (1.26.4)\n",
            "Requirement already satisfied: packaging in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (5.28.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard) (73.0.1)\n",
            "Requirement already satisfied: six>1.9 in /home/codespace/.local/lib/python3.12/site-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from tensorboard) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/codespace/.local/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Collecting watchdog\n",
            "  Downloading watchdog-5.0.2-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n",
            "Downloading watchdog-5.0.2-py3-none-manylinux2014_x86_64.whl (78 kB)\n",
            "Installing collected packages: watchdog\n",
            "Successfully installed watchdog-5.0.2\n"
          ]
        }
      ],
      "source": [
        "#!rm -rf /content/arc-neural-reasoning-model/\n",
        "#!git clone https://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@github.com/ImmortalDemonGod/arc-neural-reasoning-model.git\n",
        "!pip install -r /workspaces/arc-neural-reasoning-model/gpt2_arc/requirements.txt\n",
        "!pip install optuna\n",
        "#!pip install jupyterlab jupyterlab-optuna\n",
        "# Install the required packages\n",
        "!pip install optuna-dashboard pyngrok\n",
        "!ngrok config add-authtoken 2NCEuxuUBMj6zsdTokQHkYJ4AZz_3E7e2pyW87otgFg3UdSC3\n",
        "!pip install tensorboard\n",
        "!pip install watchdog"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "bhpQwlBojS1S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhpQwlBojS1S",
        "outputId": "a2b3d0e5-37d3-421a-ff20-b9e2bd721d49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspaces/arc-neural-reasoning-model/EXPERIMENTAL\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "%cd /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
        "#!rm -rf /content/arc-neural-reasoning-model/arc_sat_solver\n",
        "#!rm -rf /content/arc-neural-reasoning-model/benchmark_results\n",
        "#!rm -rf /content/arc-neural-reasoning-model/checkpoints\n",
        "#!rm -rf /content/arc-neural-reasoning-model/tmp\n",
        "#!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iz4dsrstg8Ku",
      "metadata": {
        "id": "Iz4dsrstg8Ku"
      },
      "source": [
        "### 2. Run hyperparameter tuning (in the background) click the **links** to see the dashboards:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6zIQhT9m9_wv",
      "metadata": {
        "id": "6zIQhT9m9_wv"
      },
      "source": [
        "make sure to set the directory in your drive to save the files in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "DVIRPnKcPV92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVIRPnKcPV92",
        "outputId": "cd8d8350-a4d5-4b47-cc05-40e0eefc7b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date in YYYYMMDD format\n",
        "current_date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "\n",
        "# Create a folder named after the current date\n",
        "date_folder = os.path.join(parent_dir, current_date)\n",
        "if not os.path.exists(date_folder):\n",
        "    os.makedirs(date_folder)\n",
        "\n",
        "# Change into the newly created folder\n",
        "%cd {date_folder}\n",
        "\n",
        "# Now all your operations will save to /content/YYYYMMDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7xyBrsMGg8Kv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xyBrsMGg8Kv",
        "outputId": "fe9c106f-a2b6-49df-e656-8141182e9fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna dashboard is accessible at: NgrokTunnel: \"https://5b4a-35-240-146-94.ngrok-free.app\" -> \"http://localhost:8081\"\n",
            "TensorBoard is accessible at: NgrokTunnel: \"https://90f7-35-240-146-94.ngrok-free.app\" -> \"http://localhost:6006\"\n"
          ]
        }
      ],
      "source": [
        "# Now run the optimization command using the new date folder for storage in the background\n",
        "# The user-defined hyperparameters are passed here dynamically\n",
        "!nohup python /content/arc-neural-reasoning-model/gpt2_arc/src/optimize_hyperparameters.py \\\n",
        "  --n_trials {n_trials} \\\n",
        "  --storage sqlite:///{date_folder}/optuna_results.db \\\n",
        "  --n_embd_min {n_embd_min} --n_embd_max {n_embd_max} \\\n",
        "  --n_head_min {n_head_min} --n_head_max {n_head_max} \\\n",
        "  --n_layer_min {n_layer_min} --n_layer_max {n_layer_max} \\\n",
        "  --batch_size_min {batch_size_min} --batch_size_max {batch_size_max} \\\n",
        "  --learning_rate_min {learning_rate_min} --learning_rate_max {learning_rate_max} \\\n",
        "  --max_epochs_min {max_epochs_min} --max_epochs_max {max_epochs_max} > /dev/null 2>&1 &\n",
        "\n",
        "# Import ngrok and set up the tunnel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the ngrok tunnel for port 8081\n",
        "public_url = ngrok.connect(8081)\n",
        "print(f\"Optuna dashboard is accessible at: {public_url}\")\n",
        "\n",
        "# Run the Optuna dashboard in the background without blocking\n",
        "!nohup optuna-dashboard --port 8081 sqlite:///{date_folder}/optuna_results.db > /dev/null 2>&1 &\n",
        "\n",
        "# Import necessary modules\n",
        "import os\n",
        "\n",
        "# Specify the folder for logs (using date_folder)\n",
        "log_dir = f\"{date_folder}/runs\"\n",
        "\n",
        "# Run TensorBoard in the background without blocking\n",
        "!nohup tensorboard --logdir {log_dir} --port 6006 > /dev/null 2>&1 &\n",
        "\n",
        "# Set up the ngrok tunnel for TensorBoard\n",
        "public_url_tb = ngrok.connect(6006)\n",
        "print(f\"TensorBoard is accessible at: {public_url_tb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb75DkIFg8Kw",
      "metadata": {
        "id": "gb75DkIFg8Kw"
      },
      "source": [
        "### 3. Get the best hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda44504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fda44504",
        "outputId": "6c6196cb-1072-4d71-a081-3367966411ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Available studies in the database:\n",
            "Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\n",
            "Details: 'Record does not exist.'\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import json\n",
        "\n",
        "# Set Optuna storage and study details\n",
        "storage_name = f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "study_name = \"gpt2_arc_optimization\"\n",
        "\n",
        "try:\n",
        "    # List all study names in the database\n",
        "    study_summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
        "    print(\"Available studies in the database:\")\n",
        "    for study_summary in study_summaries:\n",
        "        print(f\"- {study_summary.study_name}\")\n",
        "\n",
        "    # Load the specified study\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
        "    best_params = study.best_params\n",
        "    print(\"Best hyperparameters:\")\n",
        "    print(json.dumps(best_params, indent=2))\n",
        "\n",
        "    # Save the best parameters to a JSON file\n",
        "    with open(f\"{date_folder}/best_hyperparameters.json\", \"w\") as f:\n",
        "        json.dump(best_params, f)\n",
        "\n",
        "except KeyError as e:\n",
        "    print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
        "    print(f\"Details: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ba232c",
      "metadata": {
        "id": "f2ba232c"
      },
      "source": [
        "### 4. Use the best hyperparameters for longer training (manually set max epochs!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "m8uLi0U5l6QL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8uLi0U5l6QL",
        "outputId": "cdc0d029-04a1-4ceb-df7b-3b0a4a85627e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/best_hyperparameters.json not found. Using manual parameters.\n",
            "Starting training process in the background...\n",
            "Training process started with PID: 7126\n",
            "You can monitor the training progress in /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/training_output.log\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Load best hyperparameters from the JSON file if use_best_params is True\n",
        "if use_best_params:\n",
        "    try:\n",
        "        with open(f\"{date_folder}/best_hyperparameters.json\", \"r\") as f:\n",
        "            best_params = json.load(f)\n",
        "        # Extract hyperparameters from the JSON file\n",
        "        params = {\n",
        "            \"n_embd\": best_params.get(\"n_embd\", manual_params[\"n_embd\"]),\n",
        "            \"n_head\": best_params.get(\"n_head\", manual_params[\"n_head\"]),\n",
        "            \"n_layer\": best_params.get(\"n_layer\", manual_params[\"n_layer\"]),\n",
        "            \"batch_size\": best_params.get(\"batch_size\", manual_params[\"batch_size\"]),\n",
        "            \"learning_rate\": best_params.get(\"learning_rate\", manual_params[\"learning_rate\"]),\n",
        "            \"max_epochs\": best_params.get(\"max_epochs\", manual_params[\"max_epochs\"])\n",
        "        }\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {date_folder}/best_hyperparameters.json not found. Using manual parameters.\")\n",
        "        params = manual_params\n",
        "else:\n",
        "    # Use manually defined parameters\n",
        "    params = manual_params\n",
        "\n",
        "# Build the arguments for the training command\n",
        "train_args = [\n",
        "    \"python\", \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/training/train.py\",\n",
        "    \"--n-embd\", str(params[\"n_embd\"]),\n",
        "    \"--n-head\", str(params[\"n_head\"]),\n",
        "    \"--n-layer\", str(params[\"n_layer\"]),\n",
        "    \"--batch-size\", str(params[\"batch_size\"]),\n",
        "    \"--learning-rate\", str(params[\"learning_rate\"]),\n",
        "    \"--max-epochs\", str(params[\"max_epochs\"]),\n",
        "    \"--use-gpu\",\n",
        "    \"--project\", \"arc-scaling-test\"\n",
        "]\n",
        "\n",
        "# Execute the training script in the background\n",
        "print(\"Starting training process in the background...\")\n",
        "with open(f\"{date_folder}/training_output.log\", \"w\") as log_file:\n",
        "    process = subprocess.Popen(train_args, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(f\"Training process started with PID: {process.pid}\")\n",
        "print(f\"You can monitor the training progress in {date_folder}/training_output.log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5062c297",
      "metadata": {
        "id": "5062c297"
      },
      "source": [
        "### 5. Evaluate the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0pwrd3HMBnhW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pwrd3HMBnhW",
        "outputId": "e7020464-f37e-4d1c-ffed-06bfeb029552"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Watching for new models in directory: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Watching for new checkpoints and final models in all subdirectories...\n",
            "This script will continue running until you stop it manually.\n",
            "You can stop it by interrupting the process when training is complete.\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: []\n",
            "Current models: set()\n",
            "New models to evaluate: set()\n",
            "New checkpoint detected: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt\n",
            "Evaluating model: arc_model-epoch=00-val_loss=0.73.ckpt with command: ['python', '/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py', '--model_checkpoint', '/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt', '--batch_size', '32', '--output_dir', '20240924/evaluation_results', '--wandb_project', 'arc-evaluation', '--wandb_run_name', 'scaling-test-evaluation-epoch00-val_loss0.73']\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: ['/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt']\n",
            "Current models: {'/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt'}\n",
            "New models to evaluate: {'/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt'}\n",
            "Evaluating model: arc_model-epoch=00-val_loss=0.73.ckpt with command: ['python', '/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py', '--model_checkpoint', '/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt', '--batch_size', '32', '--output_dir', '20240924/evaluation_results', '--wandb_project', 'arc-evaluation', '--wandb_run_name', 'scaling-test-evaluation-epoch00-val_loss0.73']\n",
            "Error during evaluation of arc_model-epoch=00-val_loss=0.73.ckpt: Command '['python', '/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py', '--model_checkpoint', '/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt', '--batch_size', '32', '--output_dir', '20240924/evaluation_results', '--wandb_project', 'arc-evaluation', '--wandb_run_name', 'scaling-test-evaluation-epoch00-val_loss0.73']' returned non-zero exit status 1.\n",
            "Standard Output:\n",
            "DEBUG: Starting ARCDataset initialization\n",
            "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
            "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
            "DEBUG: Processed data length: 400\n",
            "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
            "DEBUG: First train item: {'input': array([[8, 6],\n",
            "       [6, 4]]), 'output': array([[8, 6, 8, 6, 8, 6],\n",
            "       [6, 4, 6, 4, 6, 4],\n",
            "       [6, 8, 6, 8, 6, 8],\n",
            "       [4, 6, 4, 6, 4, 6],\n",
            "       [8, 6, 8, 6, 8, 6],\n",
            "       [6, 4, 6, 4, 6, 4]])}\n",
            "Number of train samples: 1363\n",
            "Number of test samples: 419\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mscaling-test-evaluation-epoch00-val_loss0.73\u001b[0m at: \u001b[34mhttps://wandb.ai/arc-abolition/arc-evaluation/runs/7e7jgvwc\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20240924_184441-7e7jgvwc/logs\u001b[0m\n",
            "\n",
            "Standard Error:\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\n",
            "DEBUG:git.util:Failed checking if running in CYGWIN due to: FileNotFoundError(2, 'No such file or directory')\n",
            "DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/workspaces/arc-neural-reasoning-model, stdin=None, shell=False, universal_newlines=False)\n",
            "DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/workspaces/arc-neural-reasoning-model, stdin=None, shell=False, universal_newlines=False)\n",
            "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
            "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
            "wandb: Currently logged in as: military-ingram (arc-abolition). Use `wandb login --relogin` to force relogin\n",
            "DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/workspaces/arc-neural-reasoning-model, stdin=<valid stream>, shell=False, universal_newlines=False)\n",
            "wandb: - Waiting for wandb.init()...\n",
            "wandb: \\ Waiting for wandb.init()...\n",
            "wandb: Tracking run with wandb version 0.18.1\n",
            "wandb: Run data is saved locally in /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/wandb/run-20240924_184441-7e7jgvwc\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run scaling-test-evaluation-epoch00-val_loss0.73\n",
            "wandb: ⭐️ View project at https://wandb.ai/arc-abolition/arc-evaluation\n",
            "wandb: 🚀 View run at https://wandb.ai/arc-abolition/arc-evaluation/runs/7e7jgvwc\n",
            "/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.model_checkpoint, map_location='cpu')\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\", line 126, in <module>\n",
            "    main(args)\n",
            "  File \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\", line 87, in main\n",
            "    model.load_state_dict(checkpoint['state_dict'])\n",
            "  File \"/home/codespace/.local/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 2215, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for GPT2ARC:\n",
            "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"blocks.0.attention.key.weight\", \"blocks.0.attention.key.bias\", \"blocks.0.attention.query.weight\", \"blocks.0.attention.query.bias\", \"blocks.0.attention.value.weight\", \"blocks.0.attention.value.bias\", \"blocks.0.attention.proj.weight\", \"blocks.0.attention.proj.bias\", \"blocks.0.feed_forward.net.0.weight\", \"blocks.0.feed_forward.net.0.bias\", \"blocks.0.feed_forward.net.2.weight\", \"blocks.0.feed_forward.net.2.bias\", \"blocks.0.ln1.weight\", \"blocks.0.ln1.bias\", \"blocks.0.ln2.weight\", \"blocks.0.ln2.bias\", \"blocks.1.attention.key.weight\", \"blocks.1.attention.key.bias\", \"blocks.1.attention.query.weight\", \"blocks.1.attention.query.bias\", \"blocks.1.attention.value.weight\", \"blocks.1.attention.value.bias\", \"blocks.1.attention.proj.weight\", \"blocks.1.attention.proj.bias\", \"blocks.1.feed_forward.net.0.weight\", \"blocks.1.feed_forward.net.0.bias\", \"blocks.1.feed_forward.net.2.weight\", \"blocks.1.feed_forward.net.2.bias\", \"blocks.1.ln1.weight\", \"blocks.1.ln1.bias\", \"blocks.1.ln2.weight\", \"blocks.1.ln2.bias\", \"blocks.2.attention.key.weight\", \"blocks.2.attention.key.bias\", \"blocks.2.attention.query.weight\", \"blocks.2.attention.query.bias\", \"blocks.2.attention.value.weight\", \"blocks.2.attention.value.bias\", \"blocks.2.attention.proj.weight\", \"blocks.2.attention.proj.bias\", \"blocks.2.feed_forward.net.0.weight\", \"blocks.2.feed_forward.net.0.bias\", \"blocks.2.feed_forward.net.2.weight\", \"blocks.2.feed_forward.net.2.bias\", \"blocks.2.ln1.weight\", \"blocks.2.ln1.bias\", \"blocks.2.ln2.weight\", \"blocks.2.ln2.bias\", \"blocks.3.attention.key.weight\", \"blocks.3.attention.key.bias\", \"blocks.3.attention.query.weight\", \"blocks.3.attention.query.bias\", \"blocks.3.attention.value.weight\", \"blocks.3.attention.value.bias\", \"blocks.3.attention.proj.weight\", \"blocks.3.attention.proj.bias\", \"blocks.3.feed_forward.net.0.weight\", \"blocks.3.feed_forward.net.0.bias\", \"blocks.3.feed_forward.net.2.weight\", \"blocks.3.feed_forward.net.2.bias\", \"blocks.3.ln1.weight\", \"blocks.3.ln1.bias\", \"blocks.3.ln2.weight\", \"blocks.3.ln2.bias\", \"blocks.4.attention.key.weight\", \"blocks.4.attention.key.bias\", \"blocks.4.attention.query.weight\", \"blocks.4.attention.query.bias\", \"blocks.4.attention.value.weight\", \"blocks.4.attention.value.bias\", \"blocks.4.attention.proj.weight\", \"blocks.4.attention.proj.bias\", \"blocks.4.feed_forward.net.0.weight\", \"blocks.4.feed_forward.net.0.bias\", \"blocks.4.feed_forward.net.2.weight\", \"blocks.4.feed_forward.net.2.bias\", \"blocks.4.ln1.weight\", \"blocks.4.ln1.bias\", \"blocks.4.ln2.weight\", \"blocks.4.ln2.bias\", \"blocks.5.attention.key.weight\", \"blocks.5.attention.key.bias\", \"blocks.5.attention.query.weight\", \"blocks.5.attention.query.bias\", \"blocks.5.attention.value.weight\", \"blocks.5.attention.value.bias\", \"blocks.5.attention.proj.weight\", \"blocks.5.attention.proj.bias\", \"blocks.5.feed_forward.net.0.weight\", \"blocks.5.feed_forward.net.0.bias\", \"blocks.5.feed_forward.net.2.weight\", \"blocks.5.feed_forward.net.2.bias\", \"blocks.5.ln1.weight\", \"blocks.5.ln1.bias\", \"blocks.5.ln2.weight\", \"blocks.5.ln2.bias\", \"ln_f.weight\", \"ln_f.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"model.conv1.weight\", \"model.conv1.bias\", \"model.blocks.0.attention.key.weight\", \"model.blocks.0.attention.key.bias\", \"model.blocks.0.attention.query.weight\", \"model.blocks.0.attention.query.bias\", \"model.blocks.0.attention.value.weight\", \"model.blocks.0.attention.value.bias\", \"model.blocks.0.attention.proj.weight\", \"model.blocks.0.attention.proj.bias\", \"model.blocks.0.feed_forward.net.0.weight\", \"model.blocks.0.feed_forward.net.0.bias\", \"model.blocks.0.feed_forward.net.2.weight\", \"model.blocks.0.feed_forward.net.2.bias\", \"model.blocks.0.ln1.weight\", \"model.blocks.0.ln1.bias\", \"model.blocks.0.ln2.weight\", \"model.blocks.0.ln2.bias\", \"model.blocks.1.attention.key.weight\", \"model.blocks.1.attention.key.bias\", \"model.blocks.1.attention.query.weight\", \"model.blocks.1.attention.query.bias\", \"model.blocks.1.attention.value.weight\", \"model.blocks.1.attention.value.bias\", \"model.blocks.1.attention.proj.weight\", \"model.blocks.1.attention.proj.bias\", \"model.blocks.1.feed_forward.net.0.weight\", \"model.blocks.1.feed_forward.net.0.bias\", \"model.blocks.1.feed_forward.net.2.weight\", \"model.blocks.1.feed_forward.net.2.bias\", \"model.blocks.1.ln1.weight\", \"model.blocks.1.ln1.bias\", \"model.blocks.1.ln2.weight\", \"model.blocks.1.ln2.bias\", \"model.blocks.2.attention.key.weight\", \"model.blocks.2.attention.key.bias\", \"model.blocks.2.attention.query.weight\", \"model.blocks.2.attention.query.bias\", \"model.blocks.2.attention.value.weight\", \"model.blocks.2.attention.value.bias\", \"model.blocks.2.attention.proj.weight\", \"model.blocks.2.attention.proj.bias\", \"model.blocks.2.feed_forward.net.0.weight\", \"model.blocks.2.feed_forward.net.0.bias\", \"model.blocks.2.feed_forward.net.2.weight\", \"model.blocks.2.feed_forward.net.2.bias\", \"model.blocks.2.ln1.weight\", \"model.blocks.2.ln1.bias\", \"model.blocks.2.ln2.weight\", \"model.blocks.2.ln2.bias\", \"model.blocks.3.attention.key.weight\", \"model.blocks.3.attention.key.bias\", \"model.blocks.3.attention.query.weight\", \"model.blocks.3.attention.query.bias\", \"model.blocks.3.attention.value.weight\", \"model.blocks.3.attention.value.bias\", \"model.blocks.3.attention.proj.weight\", \"model.blocks.3.attention.proj.bias\", \"model.blocks.3.feed_forward.net.0.weight\", \"model.blocks.3.feed_forward.net.0.bias\", \"model.blocks.3.feed_forward.net.2.weight\", \"model.blocks.3.feed_forward.net.2.bias\", \"model.blocks.3.ln1.weight\", \"model.blocks.3.ln1.bias\", \"model.blocks.3.ln2.weight\", \"model.blocks.3.ln2.bias\", \"model.blocks.4.attention.key.weight\", \"model.blocks.4.attention.key.bias\", \"model.blocks.4.attention.query.weight\", \"model.blocks.4.attention.query.bias\", \"model.blocks.4.attention.value.weight\", \"model.blocks.4.attention.value.bias\", \"model.blocks.4.attention.proj.weight\", \"model.blocks.4.attention.proj.bias\", \"model.blocks.4.feed_forward.net.0.weight\", \"model.blocks.4.feed_forward.net.0.bias\", \"model.blocks.4.feed_forward.net.2.weight\", \"model.blocks.4.feed_forward.net.2.bias\", \"model.blocks.4.ln1.weight\", \"model.blocks.4.ln1.bias\", \"model.blocks.4.ln2.weight\", \"model.blocks.4.ln2.bias\", \"model.blocks.5.attention.key.weight\", \"model.blocks.5.attention.key.bias\", \"model.blocks.5.attention.query.weight\", \"model.blocks.5.attention.query.bias\", \"model.blocks.5.attention.value.weight\", \"model.blocks.5.attention.value.bias\", \"model.blocks.5.attention.proj.weight\", \"model.blocks.5.attention.proj.bias\", \"model.blocks.5.feed_forward.net.0.weight\", \"model.blocks.5.feed_forward.net.0.bias\", \"model.blocks.5.feed_forward.net.2.weight\", \"model.blocks.5.feed_forward.net.2.bias\", \"model.blocks.5.ln1.weight\", \"model.blocks.5.ln1.bias\", \"model.blocks.5.ln2.weight\", \"model.blocks.5.ln2.bias\", \"model.ln_f.weight\", \"model.ln_f.bias\". \n",
            "\n",
            "Error during evaluation of arc_model-epoch=00-val_loss=0.73.ckpt: Command '['python', '/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py', '--model_checkpoint', '/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt', '--batch_size', '32', '--output_dir', '20240924/evaluation_results', '--wandb_project', 'arc-evaluation', '--wandb_run_name', 'scaling-test-evaluation-epoch00-val_loss0.73']' returned non-zero exit status 1.\n",
            "Standard Output:\n",
            "DEBUG: Starting ARCDataset initialization\n",
            "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
            "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
            "DEBUG: Processed data length: 400\n",
            "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
            "DEBUG: First train item: {'input': array([[8, 6],\n",
            "       [6, 4]]), 'output': array([[8, 6, 8, 6, 8, 6],\n",
            "       [6, 4, 6, 4, 6, 4],\n",
            "       [6, 8, 6, 8, 6, 8],\n",
            "       [4, 6, 4, 6, 4, 6],\n",
            "       [8, 6, 8, 6, 8, 6],\n",
            "       [6, 4, 6, 4, 6, 4]])}\n",
            "Number of train samples: 1363\n",
            "Number of test samples: 419\n",
            "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mscaling-test-evaluation-epoch00-val_loss0.73\u001b[0m at: \u001b[34mhttps://wandb.ai/arc-abolition/arc-evaluation/runs/1flst0g9\u001b[0m\n",
            "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20240924_184444-1flst0g9/logs\u001b[0m\n",
            "\n",
            "Standard Error:\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\n",
            "DEBUG:git.util:Failed checking if running in CYGWIN due to: FileNotFoundError(2, 'No such file or directory')\n",
            "DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/workspaces/arc-neural-reasoning-model, stdin=None, shell=False, universal_newlines=False)\n",
            "DEBUG:git.cmd:Popen(['git', 'rev-parse', '--show-toplevel'], cwd=/workspaces/arc-neural-reasoning-model, stdin=None, shell=False, universal_newlines=False)\n",
            "wandb: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n",
            "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): api.wandb.ai:443\n",
            "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
            "DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 \"POST /graphql HTTP/1.1\" 200 None\n",
            "wandb: Currently logged in as: military-ingram (arc-abolition). Use `wandb login --relogin` to force relogin\n",
            "DEBUG:git.cmd:Popen(['git', 'cat-file', '--batch-check'], cwd=/workspaces/arc-neural-reasoning-model, stdin=<valid stream>, shell=False, universal_newlines=False)\n",
            "wandb: Tracking run with wandb version 0.18.1\n",
            "wandb: Run data is saved locally in /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/wandb/run-20240924_184444-1flst0g9\n",
            "wandb: Run `wandb offline` to turn off syncing.\n",
            "wandb: Syncing run scaling-test-evaluation-epoch00-val_loss0.73\n",
            "wandb: ⭐️ View project at https://wandb.ai/arc-abolition/arc-evaluation\n",
            "wandb: 🚀 View run at https://wandb.ai/arc-abolition/arc-evaluation/runs/1flst0g9\n",
            "/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py:73: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(args.model_checkpoint, map_location='cpu')\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\", line 126, in <module>\n",
            "    main(args)\n",
            "  File \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\", line 87, in main\n",
            "    model.load_state_dict(checkpoint['state_dict'])\n",
            "  File \"/home/codespace/.local/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 2215, in load_state_dict\n",
            "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
            "RuntimeError: Error(s) in loading state_dict for GPT2ARC:\n",
            "\tMissing key(s) in state_dict: \"conv1.weight\", \"conv1.bias\", \"blocks.0.attention.key.weight\", \"blocks.0.attention.key.bias\", \"blocks.0.attention.query.weight\", \"blocks.0.attention.query.bias\", \"blocks.0.attention.value.weight\", \"blocks.0.attention.value.bias\", \"blocks.0.attention.proj.weight\", \"blocks.0.attention.proj.bias\", \"blocks.0.feed_forward.net.0.weight\", \"blocks.0.feed_forward.net.0.bias\", \"blocks.0.feed_forward.net.2.weight\", \"blocks.0.feed_forward.net.2.bias\", \"blocks.0.ln1.weight\", \"blocks.0.ln1.bias\", \"blocks.0.ln2.weight\", \"blocks.0.ln2.bias\", \"blocks.1.attention.key.weight\", \"blocks.1.attention.key.bias\", \"blocks.1.attention.query.weight\", \"blocks.1.attention.query.bias\", \"blocks.1.attention.value.weight\", \"blocks.1.attention.value.bias\", \"blocks.1.attention.proj.weight\", \"blocks.1.attention.proj.bias\", \"blocks.1.feed_forward.net.0.weight\", \"blocks.1.feed_forward.net.0.bias\", \"blocks.1.feed_forward.net.2.weight\", \"blocks.1.feed_forward.net.2.bias\", \"blocks.1.ln1.weight\", \"blocks.1.ln1.bias\", \"blocks.1.ln2.weight\", \"blocks.1.ln2.bias\", \"blocks.2.attention.key.weight\", \"blocks.2.attention.key.bias\", \"blocks.2.attention.query.weight\", \"blocks.2.attention.query.bias\", \"blocks.2.attention.value.weight\", \"blocks.2.attention.value.bias\", \"blocks.2.attention.proj.weight\", \"blocks.2.attention.proj.bias\", \"blocks.2.feed_forward.net.0.weight\", \"blocks.2.feed_forward.net.0.bias\", \"blocks.2.feed_forward.net.2.weight\", \"blocks.2.feed_forward.net.2.bias\", \"blocks.2.ln1.weight\", \"blocks.2.ln1.bias\", \"blocks.2.ln2.weight\", \"blocks.2.ln2.bias\", \"blocks.3.attention.key.weight\", \"blocks.3.attention.key.bias\", \"blocks.3.attention.query.weight\", \"blocks.3.attention.query.bias\", \"blocks.3.attention.value.weight\", \"blocks.3.attention.value.bias\", \"blocks.3.attention.proj.weight\", \"blocks.3.attention.proj.bias\", \"blocks.3.feed_forward.net.0.weight\", \"blocks.3.feed_forward.net.0.bias\", \"blocks.3.feed_forward.net.2.weight\", \"blocks.3.feed_forward.net.2.bias\", \"blocks.3.ln1.weight\", \"blocks.3.ln1.bias\", \"blocks.3.ln2.weight\", \"blocks.3.ln2.bias\", \"blocks.4.attention.key.weight\", \"blocks.4.attention.key.bias\", \"blocks.4.attention.query.weight\", \"blocks.4.attention.query.bias\", \"blocks.4.attention.value.weight\", \"blocks.4.attention.value.bias\", \"blocks.4.attention.proj.weight\", \"blocks.4.attention.proj.bias\", \"blocks.4.feed_forward.net.0.weight\", \"blocks.4.feed_forward.net.0.bias\", \"blocks.4.feed_forward.net.2.weight\", \"blocks.4.feed_forward.net.2.bias\", \"blocks.4.ln1.weight\", \"blocks.4.ln1.bias\", \"blocks.4.ln2.weight\", \"blocks.4.ln2.bias\", \"blocks.5.attention.key.weight\", \"blocks.5.attention.key.bias\", \"blocks.5.attention.query.weight\", \"blocks.5.attention.query.bias\", \"blocks.5.attention.value.weight\", \"blocks.5.attention.value.bias\", \"blocks.5.attention.proj.weight\", \"blocks.5.attention.proj.bias\", \"blocks.5.feed_forward.net.0.weight\", \"blocks.5.feed_forward.net.0.bias\", \"blocks.5.feed_forward.net.2.weight\", \"blocks.5.feed_forward.net.2.bias\", \"blocks.5.ln1.weight\", \"blocks.5.ln1.bias\", \"blocks.5.ln2.weight\", \"blocks.5.ln2.bias\", \"ln_f.weight\", \"ln_f.bias\". \n",
            "\tUnexpected key(s) in state_dict: \"model.conv1.weight\", \"model.conv1.bias\", \"model.blocks.0.attention.key.weight\", \"model.blocks.0.attention.key.bias\", \"model.blocks.0.attention.query.weight\", \"model.blocks.0.attention.query.bias\", \"model.blocks.0.attention.value.weight\", \"model.blocks.0.attention.value.bias\", \"model.blocks.0.attention.proj.weight\", \"model.blocks.0.attention.proj.bias\", \"model.blocks.0.feed_forward.net.0.weight\", \"model.blocks.0.feed_forward.net.0.bias\", \"model.blocks.0.feed_forward.net.2.weight\", \"model.blocks.0.feed_forward.net.2.bias\", \"model.blocks.0.ln1.weight\", \"model.blocks.0.ln1.bias\", \"model.blocks.0.ln2.weight\", \"model.blocks.0.ln2.bias\", \"model.blocks.1.attention.key.weight\", \"model.blocks.1.attention.key.bias\", \"model.blocks.1.attention.query.weight\", \"model.blocks.1.attention.query.bias\", \"model.blocks.1.attention.value.weight\", \"model.blocks.1.attention.value.bias\", \"model.blocks.1.attention.proj.weight\", \"model.blocks.1.attention.proj.bias\", \"model.blocks.1.feed_forward.net.0.weight\", \"model.blocks.1.feed_forward.net.0.bias\", \"model.blocks.1.feed_forward.net.2.weight\", \"model.blocks.1.feed_forward.net.2.bias\", \"model.blocks.1.ln1.weight\", \"model.blocks.1.ln1.bias\", \"model.blocks.1.ln2.weight\", \"model.blocks.1.ln2.bias\", \"model.blocks.2.attention.key.weight\", \"model.blocks.2.attention.key.bias\", \"model.blocks.2.attention.query.weight\", \"model.blocks.2.attention.query.bias\", \"model.blocks.2.attention.value.weight\", \"model.blocks.2.attention.value.bias\", \"model.blocks.2.attention.proj.weight\", \"model.blocks.2.attention.proj.bias\", \"model.blocks.2.feed_forward.net.0.weight\", \"model.blocks.2.feed_forward.net.0.bias\", \"model.blocks.2.feed_forward.net.2.weight\", \"model.blocks.2.feed_forward.net.2.bias\", \"model.blocks.2.ln1.weight\", \"model.blocks.2.ln1.bias\", \"model.blocks.2.ln2.weight\", \"model.blocks.2.ln2.bias\", \"model.blocks.3.attention.key.weight\", \"model.blocks.3.attention.key.bias\", \"model.blocks.3.attention.query.weight\", \"model.blocks.3.attention.query.bias\", \"model.blocks.3.attention.value.weight\", \"model.blocks.3.attention.value.bias\", \"model.blocks.3.attention.proj.weight\", \"model.blocks.3.attention.proj.bias\", \"model.blocks.3.feed_forward.net.0.weight\", \"model.blocks.3.feed_forward.net.0.bias\", \"model.blocks.3.feed_forward.net.2.weight\", \"model.blocks.3.feed_forward.net.2.bias\", \"model.blocks.3.ln1.weight\", \"model.blocks.3.ln1.bias\", \"model.blocks.3.ln2.weight\", \"model.blocks.3.ln2.bias\", \"model.blocks.4.attention.key.weight\", \"model.blocks.4.attention.key.bias\", \"model.blocks.4.attention.query.weight\", \"model.blocks.4.attention.query.bias\", \"model.blocks.4.attention.value.weight\", \"model.blocks.4.attention.value.bias\", \"model.blocks.4.attention.proj.weight\", \"model.blocks.4.attention.proj.bias\", \"model.blocks.4.feed_forward.net.0.weight\", \"model.blocks.4.feed_forward.net.0.bias\", \"model.blocks.4.feed_forward.net.2.weight\", \"model.blocks.4.feed_forward.net.2.bias\", \"model.blocks.4.ln1.weight\", \"model.blocks.4.ln1.bias\", \"model.blocks.4.ln2.weight\", \"model.blocks.4.ln2.bias\", \"model.blocks.5.attention.key.weight\", \"model.blocks.5.attention.key.bias\", \"model.blocks.5.attention.query.weight\", \"model.blocks.5.attention.query.bias\", \"model.blocks.5.attention.value.weight\", \"model.blocks.5.attention.value.bias\", \"model.blocks.5.attention.proj.weight\", \"model.blocks.5.attention.proj.bias\", \"model.blocks.5.feed_forward.net.0.weight\", \"model.blocks.5.feed_forward.net.0.bias\", \"model.blocks.5.feed_forward.net.2.weight\", \"model.blocks.5.feed_forward.net.2.bias\", \"model.blocks.5.ln1.weight\", \"model.blocks.5.ln1.bias\", \"model.blocks.5.ln2.weight\", \"model.blocks.5.ln2.bias\", \"model.ln_f.weight\", \"model.ln_f.bias\". \n",
            "\n",
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Found checkpoint files: ['/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt']\n",
            "Current models: {'/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt'}\n",
            "New models to evaluate: {'/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240924/checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt'}\n",
            "Skipping already evaluated model: arc_model-epoch=00-val_loss=0.73.ckpt\n",
            "Checkpoint and final model evaluation completed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "import time\n",
        "from watchdog.observers import Observer\n",
        "from watchdog.events import FileSystemEventHandler\n",
        "import subprocess\n",
        "\n",
        "# Set W&B API key (replace with your actual API key)\n",
        "wandb_api_key = \"2b06e99af167044b281668f6edd388c633aba1a0\"  # Replace with your W&B API key\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "\n",
        "# Define the date_folder variable appropriately\n",
        "# For example, you can set it based on the current date\n",
        "from datetime import datetime\n",
        "date_folder = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "# Directory containing the model files\n",
        "model_dir = \"/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\"  # Ensure this path is correct\n",
        "print(f\"Watching for new models in directory: {model_dir}\")\n",
        "output_dir = f\"{date_folder}/evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "wandb_project = \"arc-evaluation\"\n",
        "\n",
        "# Set of evaluated models\n",
        "evaluated_models = set()\n",
        "\n",
        "class CheckpointHandler(FileSystemEventHandler):\n",
        "    def on_created(self, event):\n",
        "        if event.is_directory:\n",
        "            return\n",
        "        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):\n",
        "            print(f\"New checkpoint detected: {event.src_path}\")\n",
        "            self.evaluate_model(event.src_path)\n",
        "\n",
        "    def evaluate_model(self, model_path):\n",
        "        model_file = os.path.basename(model_path)\n",
        "\n",
        "        if model_file in evaluated_models:\n",
        "            print(f\"Skipping already evaluated model: {model_file}\")\n",
        "            return  # Skip if the model was already evaluated\n",
        "\n",
        "        # Extract epoch and val_loss from the filename for run_name\n",
        "        # Assuming the filename pattern arc_model-epoch=<number>-val_loss=<number>.ckpt\n",
        "        try:\n",
        "            parts = model_file.replace('.ckpt', '').split('-')\n",
        "            epoch = None\n",
        "            val_loss = None\n",
        "            for part in parts:\n",
        "                if part.startswith('epoch='):\n",
        "                    epoch = part.split('=')[1]\n",
        "                elif part.startswith('val_loss='):\n",
        "                    val_loss = part.split('=')[1]\n",
        "            if epoch is not None and val_loss is not None:\n",
        "                run_name = f\"scaling-test-evaluation-epoch{epoch}-val_loss{val_loss}\"\n",
        "            else:\n",
        "                run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing run name from filename {model_file}: {e}\")\n",
        "            run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "\n",
        "        eval_command = [\n",
        "            \"python\", \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\",\n",
        "            \"--model_checkpoint\", model_path,\n",
        "            \"--batch_size\", \"32\",\n",
        "            \"--output_dir\", output_dir,\n",
        "            \"--wandb_project\", wandb_project,\n",
        "            \"--wandb_run_name\", run_name\n",
        "        ]\n",
        "\n",
        "        print(f\"Evaluating model: {model_file} with command: {eval_command}\")\n",
        "        try:\n",
        "            # Run the evaluation command and capture stdout and stderr\n",
        "            result = subprocess.run(\n",
        "                eval_command,\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True  # Automatically decode bytes to string\n",
        "            )\n",
        "            print(f\"Successfully evaluated model: {model_file}\")\n",
        "            print(\"Evaluation Output:\")\n",
        "            print(result.stdout)  # Print the standard output from evaluate.py\n",
        "            if result.stderr:\n",
        "                print(\"Evaluation Errors/Warnings:\")\n",
        "                print(result.stderr)  # Print any errors or warnings from evaluate.py\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during evaluation of {model_file}: {e}\")\n",
        "            print(\"Standard Output:\")\n",
        "            print(e.stdout)  # Print the standard output even if there's an error\n",
        "            print(\"Standard Error:\")\n",
        "            print(e.stderr)  # Print the error messages\n",
        "        except Exception as ex:\n",
        "            print(f\"An unexpected error occurred while evaluating {model_file}: {ex}\")\n",
        "\n",
        "        evaluated_models.add(model_file)\n",
        "\n",
        "def get_all_checkpoint_files(directory):\n",
        "    print(f\"Checking directory for .ckpt and .pth files: {directory}\")\n",
        "    checkpoint_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])\n",
        "    print(f\"Found checkpoint files: {checkpoint_files}\")\n",
        "    return checkpoint_files\n",
        "\n",
        "# Set up and start the watchdog observer\n",
        "event_handler = CheckpointHandler()\n",
        "observer = Observer()\n",
        "observer.schedule(event_handler, model_dir, recursive=True)\n",
        "observer.start()\n",
        "\n",
        "print(\"Watching for new checkpoints and final models in all subdirectories...\")\n",
        "print(\"This script will continue running until you stop it manually.\")\n",
        "print(\"You can stop it by interrupting the process when training is complete.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Check for any new models\n",
        "        current_models = set(f for f in get_all_checkpoint_files(model_dir))\n",
        "        new_models = current_models - evaluated_models\n",
        "\n",
        "        print(f\"Current models: {current_models}\")\n",
        "        print(f\"New models to evaluate: {new_models}\")\n",
        "\n",
        "        for model_path in new_models:\n",
        "            event_handler.evaluate_model(model_path)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    observer.stop()\n",
        "\n",
        "observer.join()\n",
        "print(\"Checkpoint and final model evaluation completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L25zvW0ig8K1",
      "metadata": {
        "id": "L25zvW0ig8K1"
      },
      "source": [
        "### 6. Analyze the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJhLC5Ttg8K1",
      "metadata": {
        "id": "mJhLC5Ttg8K1"
      },
      "outputs": [],
      "source": [
        "#import json\n",
        "\n",
        "# Load and print evaluation results\n",
        "#with open(\"./evaluation_results/scaling-test-evaluation_results.json\", \"r\") as f:\n",
        "#    results = json.load(f)\n",
        "\n",
        "#print(\"Evaluation Results:\")\n",
        "#print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOEvyN3LCVdV",
      "metadata": {
        "id": "LOEvyN3LCVdV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0L9IwEdxzd-H",
        "Iz4dsrstg8Ku",
        "gb75DkIFg8Kw",
        "f2ba232c",
        "L25zvW0ig8K1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
