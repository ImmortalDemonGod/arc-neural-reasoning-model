{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0e19b3a3",
      "metadata": {},
      "source": [
        "Required: https://youtu.be/IDxrMbXPVTA?si=PHfGry-HQj__3Xne"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0L9IwEdxzd-H",
      "metadata": {
        "id": "0L9IwEdxzd-H"
      },
      "source": [
        "# STUFF YOU CHANGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6okItopOumX_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6okItopOumX_",
        "outputId": "5359a138-ed90-4dd4-9289-3e70fb5c7bbf"
      },
      "outputs": [],
      "source": [
        "# ========== User-Defined Parameters (Top of Notebook) ==========\n",
        "\n",
        "dev_mode = True  # Set to True for development mode (Miguel's coding machine)\n",
        "\n",
        "# 1. Base directory for storing date-based experiment result folders.\n",
        "import os\n",
        "\n",
        "# Define the base directory for the arc-neural-reasoning-model\n",
        "arc_model_dir = \"/content/arc-neural-reasoning-model/\"\n",
        "parent_dir = \"/content/drive/MyDrive/ArcGPT/\"\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"ARC Model Directory: {arc_model_dir}\")\n",
        "# 2. Boolean flag to choose between best hyperparameters from previous experiments (True) or manual parameters (False).\n",
        "use_best_params = False  # Set to False to use manual parameters\n",
        "\n",
        "# 3. Dictionary of manually set hyperparameters used when use_best_params is False.\n",
        "#    Includes model architecture and training settings.\n",
        "manual_params = {\n",
        "    \"n_embd\": 4,     # Embedding dimension\n",
        "    \"n_head\": 4,       # Number of attention heads\n",
        "    \"n_layer\": 4,     # Number of transformer layers\n",
        "    \"batch_size\": 16,  # Batch size for training\n",
        "    \"learning_rate\": 1e-4,  # Learning rate\n",
        "    \"max_epochs\": 20   # Maximum number of epochs for training\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning search space settings\n",
        "# 4. Number of Optuna trials for hyperparameter tuning.\n",
        "#    More trials can lead to better optimization but increase computation time.\n",
        "n_trials = 100\n",
        "\n",
        "# 5. Range for embedding dimension in hyperparameter search space.\n",
        "n_embd_min, n_embd_max = 64, 256\n",
        "\n",
        "# 6. Range for number of attention heads in transformer model during tuning.\n",
        "n_head_min, n_head_max = 2, 16\n",
        "\n",
        "# 7. Range for number of transformer layers in model architecture during optimization.\n",
        "n_layer_min, n_layer_max = 2, 16\n",
        "\n",
        "# 8. Range of batch sizes to explore. Larger batches can speed up training but may require more memory.\n",
        "batch_size_min, batch_size_max = 1, 256\n",
        "\n",
        "# 9. Range for learning rate in hyperparameter search space. Crucial for model convergence and performance.\n",
        "learning_rate_min, learning_rate_max = 1e-8, 1e-2\n",
        "\n",
        "# 10. Range for number of training epochs to consider during hyperparameter optimization.\n",
        "max_epochs_min, max_epochs_max = 3, 50\n",
        "\n",
        "# 11. Range for n_head exponent in hyperparameter search space.\n",
        "n_head_exp_min, n_head_exp_max = 1, 3\n",
        "\n",
        "# 12. Range for n_embd multiplier in hyperparameter search space.\n",
        "n_embd_multiplier_min, n_embd_multiplier_max = 4, 16\n",
        "\n",
        "# These parameters allow flexible experimentation with different model configurations and training settings,\n",
        "# enabling comprehensive exploration of the hyperparameter space for optimal model performance.\n",
        "\n",
        "# Validate manual parameters\n",
        "def validate_manual_params(params):\n",
        "    assert params[\"n_embd\"] % params[\"n_head\"] == 0, f\"n_embd ({params['n_embd']}) must be divisible by n_head ({params['n_head']})\"\n",
        "    assert params[\"n_embd\"] >= params[\"n_head\"], f\"n_embd ({params['n_embd']}) must be greater than or equal to n_head ({params['n_head']})\"\n",
        "    assert params[\"n_layer\"] > 0, f\"n_layer ({params['n_layer']}) must be positive\"\n",
        "    print(\"Manual parameters validated successfully\")\n",
        "\n",
        "\n",
        "# Validate the manual parameters\n",
        "validate_manual_params(manual_params)\n",
        "\n",
        "# Print configurations for verification\n",
        "print(\"Current Configuration:\")\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"Use Best Parameters: {use_best_params}\")\n",
        "print(f\"Manual Parameters: {manual_params}\")\n",
        "print(f\"Number of Optuna Trials: {n_trials}\")\n",
        "print(\"Hyperparameter Ranges:\")\n",
        "print(f\"  n_embd: {n_embd_min} to {n_embd_max}\")\n",
        "print(f\"  n_head: {n_head_min} to {n_head_max}\")\n",
        "print(f\"  n_layer: {n_layer_min} to {n_layer_max}\")\n",
        "print(f\"  batch_size: {batch_size_min} to {batch_size_max}\")\n",
        "print(f\"  learning_rate: {learning_rate_min} to {learning_rate_max}\")\n",
        "print(f\"  max_epochs: {max_epochs_min} to {max_epochs_max}\")\n",
        "\n",
        "# Save configuration\n",
        "import json\n",
        "\n",
        "def save_config(config, filename=\"config.json\"):\n",
        "    full_path = os.path.join(parent_dir, filename)\n",
        "    os.makedirs(parent_dir, exist_ok=True)\n",
        "    with open(full_path, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "config = {\n",
        "    \"parent_dir\": parent_dir,\n",
        "    \"use_best_params\": use_best_params,\n",
        "    \"manual_params\": manual_params,\n",
        "    \"tuning\": {\n",
        "        \"n_trials\": n_trials,\n",
        "        \"n_embd\": (n_embd_min, n_embd_max),\n",
        "        \"n_head\": (n_head_min, n_head_max),\n",
        "        \"n_layer\": (n_layer_min, n_layer_max),\n",
        "        \"batch_size\": (batch_size_min, batch_size_max),\n",
        "        \"learning_rate\": (learning_rate_min, learning_rate_max),\n",
        "        \"max_epochs\": (max_epochs_min, max_epochs_max)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"\\nGPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2-qicsswzi-S",
      "metadata": {
        "id": "2-qicsswzi-S"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DgxlLFBTg8Kt",
      "metadata": {
        "id": "DgxlLFBTg8Kt"
      },
      "source": [
        "### 1. Set up the Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u8k0Yxzd4RCb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8k0Yxzd4RCb",
        "outputId": "aa0390e3-e53c-46ca-9f77-ac45d076335a"
      },
      "outputs": [],
      "source": [
        "if dev_mode != True:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        save_config(config)\n",
        "        print(f\"Configuration saved to config.json\")\n",
        "\n",
        "    #general error\n",
        "    except:\n",
        "        print(\"Google Colab not detected. Skipping drive mount.\")\n",
        "\n",
        "    %cd /content/\n",
        "    !rm -rf /content/arc-neural-reasoning-model/\n",
        "    !git clone https://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@github.com/ImmortalDemonGod/arc-neural-reasoning-model.git\n",
        "    !pip install -r /content/arc-neural-reasoning-model/gpt2_arc/requirements.txt\n",
        "    !pip install optuna\n",
        "    !pip install jupyterlab jupyterlab-optuna\n",
        "    # Install the required packages\n",
        "    !pip install optuna-dashboard pyngrok\n",
        "    !ngrok config add-authtoken 2NCEuxuUBMj6zsdTokQHkYJ4AZz_3E7e2pyW87otgFg3UdSC3\n",
        "    !pip install tensorboard\n",
        "    !pip install watchdog\n",
        "    !pip install numpy\n",
        "    !rm -rf /content/arc-neural-reasoning-model/arc_sat_solver\n",
        "    !rm -rf /content/arc-neural-reasoning-model/benchmark_results\n",
        "    !rm -rf /content/arc-neural-reasoning-model/checkpoints\n",
        "    !rm -rf /content/arc-neural-reasoning-model/tmp\n",
        "    %cd /content/arc-neural-reasoning-model/\n",
        "    !pip install -e .\n",
        "else:\n",
        "    print(\"Development mode is enabled. Using Miguel's coding machine.\")\n",
        "    arc_model_dir = \"/workspaces/arc-neural-reasoning-model/\"\n",
        "    parent_dir = \"/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\"\n",
        "    %cd /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
        "    save_config(config)\n",
        "    print(f\"Configuration saved to config.json\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iz4dsrstg8Ku",
      "metadata": {
        "id": "Iz4dsrstg8Ku"
      },
      "source": [
        "### 2. Run hyperparameter tuning (in the background) click the **links** to see the dashboards:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6zIQhT9m9_wv",
      "metadata": {
        "id": "6zIQhT9m9_wv"
      },
      "source": [
        "make sure to set the directory in your drive to save the files in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DVIRPnKcPV92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVIRPnKcPV92",
        "outputId": "cd8d8350-a4d5-4b47-cc05-40e0eefc7b0c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date in YYYYMMDD format\n",
        "current_date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "\n",
        "# Create a folder named after the current date\n",
        "date_folder = os.path.join(parent_dir, current_date)\n",
        "if not os.path.exists(date_folder):\n",
        "    os.makedirs(date_folder)\n",
        "\n",
        "# Change into the newly created folder\n",
        "%cd {date_folder}\n",
        "\n",
        "# Now all your operations will save to /content/YYYYMMDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c281536",
      "metadata": {},
      "outputs": [],
      "source": [
        "kill_process_by_name(\"optuna-dashboard\")\n",
        "kill_process_by_name(\"tensorboard\")\n",
        "kill_process_by_name(\"ngrok\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7xyBrsMGg8Kv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xyBrsMGg8Kv",
        "outputId": "fe9c106f-a2b6-49df-e656-8141182e9fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No optuna-dashboard processes were found running.\n",
            "No tensorboard processes were found running.\n",
            "No ngrok processes were found running.\n",
            "Running hyperparameter tuning in the background...\n",
            "Arc Model Directory: /workspaces/arc-neural-reasoning-model/\n",
            "Date Folder: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240926\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<Popen: returncode: None args: ['python', '/workspaces/arc-neural-reasoning-...>"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (2.2.3) or chardet (5.2.0)/charset_normalizer (3.3.2) doesn't match a supported version!\n",
            "  warnings.warn(\n",
            "2024-09-26 22:31:31.747074: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-26 22:31:31.750458: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
            "2024-09-26 22:31:31.759734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2024-09-26 22:31:31.774258: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2024-09-26 22:31:31.778566: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2024-09-26 22:31:31.791578: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "DEBUG:tensorflow:Falling back to TensorFlow client; we recommended you install the Cloud TPU client directly with pip install cloud-tpu-client.\n",
            "2024-09-26 22:31:32.945431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n",
            "DEBUG:h5py._conv:Creating converter from 7 to 5\n",
            "DEBUG:h5py._conv:Creating converter from 5 to 7\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Function to kill processes by name\n",
        "def kill_process_by_name(process_name):\n",
        "    try:\n",
        "        # Find and kill the process using pkill (Linux/Unix/Mac) or taskkill (Windows)\n",
        "        if os.name == \"posix\":  # For Unix-based systems\n",
        "            subprocess.run([\"pkill\", \"-f\", process_name], check=True)\n",
        "        elif os.name == \"nt\":  # For Windows systems\n",
        "            subprocess.run([\"taskkill\", \"/IM\", process_name, \"/F\"], check=True)\n",
        "        print(f\"Successfully terminated any running {process_name} processes.\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"No {process_name} processes were found running.\")\n",
        "\n",
        "# Kill existing Optuna, TensorBoard, and ngrok processes\n",
        "\n",
        "kill_process_by_name(\"tensorboard\")\n",
        "kill_process_by_name(\"ngrok\")\n",
        "kill_process_by_name(\"optuna-dashboard\")\n",
        "\n",
        "# Run the optimization command using the new date folder for storage in the background\n",
        "# The user-defined hyperparameters are passed here dynamically\n",
        "print(\"Running hyperparameter tuning in the background...\")\n",
        "print(f\"Arc Model Directory: {arc_model_dir}\")\n",
        "print(f\"Date Folder: {date_folder}\")\n",
        "optimize_command = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/optimize_hyperparameters.py\"),\n",
        "    \"--n_trials\", str(n_trials),\n",
        "    \"--storage\", f\"sqlite:///{date_folder}/optuna_results.db\",\n",
        "    \"--n_embd_min\", str(n_embd_min), \"--n_embd_max\", str(n_embd_max),\n",
        "    \"--n_head_min\", str(n_head_min), \"--n_head_max\", str(n_head_max),\n",
        "    \"--n_layer_min\", str(n_layer_min), \"--n_layer_max\", str(n_layer_max),\n",
        "    \"--batch_size_min\", str(batch_size_min), \"--batch_size_max\", str(batch_size_max),\n",
        "    \"--learning_rate_min\", str(learning_rate_min), \"--learning_rate_max\", str(learning_rate_max),\n",
        "    \"--max_epochs_min\", str(max_epochs_min), \"--max_epochs_max\", str(max_epochs_max),\n",
        "    \"--n_head_exp_min\", str(n_head_exp_min), \"--n_head_exp_max\", str(n_head_exp_max),\n",
        "    \"--n_embd_multiplier_min\", str(n_embd_multiplier_min), \"--n_embd_multiplier_max\", str(n_embd_multiplier_max)\n",
        "]\n",
        "subprocess.Popen(optimize_command)\n",
        "\n",
        "\n",
        "# Import ngrok and set up the tunnel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the ngrok tunnel for port 8081\n",
        "public_url = ngrok.connect(8081)\n",
        "print(f\"Optuna dashboard is accessible at: {public_url}\")\n",
        "\n",
        "# Run the Optuna dashboard in the background without blocking\n",
        "optuna_command = [\n",
        "    \"optuna-dashboard\", \"--port\", \"8081\", f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "]\n",
        "subprocess.Popen(optuna_command)\n",
        "\n",
        "# Specify the folder for logs (using date_folder)\n",
        "log_dir = f\"{date_folder}/runs\"\n",
        "\n",
        "# Run TensorBoard in the background without blocking\n",
        "tensorboard_command = [\n",
        "    \"tensorboard\", \"--logdir\", log_dir, \"--port\", \"6006\"\n",
        "]\n",
        "subprocess.Popen(tensorboard_command)\n",
        "\n",
        "# Set up the ngrok tunnel for TensorBoard\n",
        "public_url_tb = ngrok.connect(6006)\n",
        "print(f\"TensorBoard is accessible at: {public_url_tb}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb75DkIFg8Kw",
      "metadata": {
        "id": "gb75DkIFg8Kw"
      },
      "source": [
        "### 3. Get the best hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda44504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fda44504",
        "outputId": "6c6196cb-1072-4d71-a081-3367966411ac"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "# Set Optuna storage and study details\n",
        "storage_name = f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "study_name = \"gpt2_arc_optimization\"\n",
        "\n",
        "try:\n",
        "    # List all study names in the database\n",
        "    study_summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
        "    print(\"Available studies in the database:\")\n",
        "    for study_summary in study_summaries:\n",
        "        print(f\"- {study_summary.study_name}\")\n",
        "\n",
        "    # Load the specified study\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
        "    best_params = study.best_params\n",
        "    print(\"Best hyperparameters:\")\n",
        "    print(json.dumps(best_params, indent=2))\n",
        "\n",
        "    # Save the best parameters to a JSON file\n",
        "    with open(f\"{date_folder}/best_hyperparameters.json\", \"w\") as f:\n",
        "        json.dump(best_params, f)\n",
        "\n",
        "except KeyError as e:\n",
        "    print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
        "    print(f\"Details: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ba232c",
      "metadata": {
        "id": "f2ba232c"
      },
      "source": [
        "### 4. Use the best hyperparameters for longer training (manually set max epochs!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m8uLi0U5l6QL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8uLi0U5l6QL",
        "outputId": "cdc0d029-04a1-4ceb-df7b-3b0a4a85627e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Load best hyperparameters from the JSON file if use_best_params is True\n",
        "if use_best_params:\n",
        "    try:\n",
        "        with open(f\"{date_folder}/best_hyperparameters.json\", \"r\") as f:\n",
        "            best_params = json.load(f)\n",
        "        # Extract hyperparameters from the JSON file\n",
        "        params = {\n",
        "            \"n_embd\": best_params.get(\"n_embd\", manual_params[\"n_embd\"]),\n",
        "            \"n_head\": best_params.get(\"n_head\", manual_params[\"n_head\"]),\n",
        "            \"n_layer\": best_params.get(\"n_layer\", manual_params[\"n_layer\"]),\n",
        "            \"batch_size\": best_params.get(\"batch_size\", manual_params[\"batch_size\"]),\n",
        "            \"learning_rate\": best_params.get(\"learning_rate\", manual_params[\"learning_rate\"]),\n",
        "            \"max_epochs\": best_params.get(\"max_epochs\", manual_params[\"max_epochs\"])\n",
        "        }\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {date_folder}/best_hyperparameters.json not found. Using manual parameters.\")\n",
        "        params = manual_params\n",
        "else:\n",
        "    # Use manually defined parameters\n",
        "    params = manual_params\n",
        "\n",
        "# Build the arguments for the training command\n",
        "train_args = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/training/train.py\"),\n",
        "    \"--n-embd\", str(params[\"n_embd\"]),\n",
        "    \"--n-head\", str(params[\"n_head\"]),\n",
        "    \"--n-layer\", str(params[\"n_layer\"]),\n",
        "    \"--batch-size\", str(params[\"batch_size\"]),\n",
        "    \"--learning-rate\", str(params[\"learning_rate\"]),\n",
        "    \"--max-epochs\", str(params[\"max_epochs\"]),\n",
        "    \"--use-gpu\",\n",
        "    \"--project\", \"arc-scaling-test\"\n",
        "]\n",
        "\n",
        "# Execute the training script in the background\n",
        "print(\"Starting training process in the background...\")\n",
        "with open(f\"{date_folder}/training_output.log\", \"w\") as log_file:\n",
        "    process = subprocess.Popen(train_args, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(f\"Training process started with PID: {process.pid}\")\n",
        "print(f\"You can monitor the training progress in {date_folder}/training_output.log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5062c297",
      "metadata": {
        "id": "5062c297"
      },
      "source": [
        "### 5. Evaluate the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pwrd3HMBnhW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pwrd3HMBnhW",
        "outputId": "e7020464-f37e-4d1c-ffed-06bfeb029552"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import time\n",
        "from watchdog.observers import Observer\n",
        "from watchdog.events import FileSystemEventHandler\n",
        "import subprocess\n",
        "\n",
        "# Set W&B API key (replace with your actual API key)\n",
        "wandb_api_key = \"2b06e99af167044b281668f6edd388c633aba1a0\"  # Replace with your W&B API key\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "\n",
        "# Define the date_folder variable appropriately\n",
        "# For example, you can set it based on the current date\n",
        "from datetime import datetime\n",
        "#date_folder = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "# Directory containing the model files\n",
        "model_dir = f\"{date_folder}\"\n",
        "print(f\"Watching for new models in directory: {model_dir}\")\n",
        "output_dir = \"evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "wandb_project = \"arc-evaluation\"\n",
        "\n",
        "# Set of evaluated models\n",
        "evaluated_models = set()\n",
        "\n",
        "# Load previously evaluated models from a file\n",
        "evaluated_models_file = os.path.join(output_dir, \"evaluated_models.txt\")\n",
        "if os.path.exists(evaluated_models_file):\n",
        "    with open(evaluated_models_file, \"r\") as f:\n",
        "        evaluated_models.update(line.strip() for line in f)\n",
        "    print(f\"Loaded evaluated models from {evaluated_models_file}\")\n",
        "\n",
        "class CheckpointHandler(FileSystemEventHandler):\n",
        "    def on_created(self, event):\n",
        "        if event.is_directory:\n",
        "            return\n",
        "        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):\n",
        "            print(f\"New checkpoint detected: {event.src_path}\")\n",
        "            self.evaluate_model(event.src_path)\n",
        "\n",
        "    def evaluate_model(self, model_path):\n",
        "        model_file = os.path.basename(model_path)\n",
        "\n",
        "        if model_file in evaluated_models:\n",
        "            print(f\"Skipping already evaluated model: {model_file}\")\n",
        "            return  # Skip if the model was already evaluated\n",
        "\n",
        "        # Extract epoch and val_loss from the filename for run_name\n",
        "        # Assuming the filename pattern arc_model-epoch=<number>-val_loss=<number>.ckpt\n",
        "        try:\n",
        "            parts = model_file.replace('.ckpt', '').split('-')\n",
        "            epoch = None\n",
        "            val_loss = None\n",
        "            for part in parts:\n",
        "                if part.startswith('epoch='):\n",
        "                    epoch = part.split('=')[1]\n",
        "                elif part.startswith('val_loss='):\n",
        "                    val_loss = part.split('=')[1]\n",
        "            if epoch is not None and val_loss is not None:\n",
        "                run_name = f\"scaling-test-evaluation-epoch{epoch}-val_loss{val_loss}\"\n",
        "            else:\n",
        "                run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing run name from filename {model_file}: {e}\")\n",
        "            run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "\n",
        "        eval_command = [\n",
        "            \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/evaluate.py\"),\n",
        "            \"--model_checkpoint\", model_path,\n",
        "            \"--batch_size\", \"32\",\n",
        "            \"--output_dir\", output_dir,\n",
        "            \"--wandb_project\", wandb_project,\n",
        "            \"--wandb_run_name\", run_name\n",
        "        ]\n",
        "        print(f\"Evaluating model: {model_file} with command: {' '.join(eval_command)}\")\n",
        "\n",
        "        print(f\"Evaluating model: {model_file} with command: {eval_command}\")\n",
        "        try:\n",
        "            # Run the evaluation command and capture stdout and stderr\n",
        "            result = subprocess.run(\n",
        "                eval_command,\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True  # Automatically decode bytes to string\n",
        "            )\n",
        "            print(f\"Successfully evaluated model: {model_file}\")\n",
        "            print(\"Evaluation Output:\")\n",
        "            print(result.stdout)  # Print the standard output from evaluate.py\n",
        "            if result.stderr:\n",
        "                print(\"Evaluation Errors/Warnings:\")\n",
        "                print(result.stderr)  # Print any errors or warnings from evaluate.py\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during evaluation of {model_file}: {e}\")\n",
        "            print(\"Standard Output:\")\n",
        "            print(e.stdout)  # Print the standard output even if there's an error\n",
        "            print(\"Standard Error:\")\n",
        "            print(e.stderr)  # Print the error messages\n",
        "        except Exception as ex:\n",
        "            print(f\"An unexpected error occurred while evaluating {model_file}: {ex}\")\n",
        "\n",
        "        evaluated_models.add(model_file)\n",
        "        \n",
        "        # Save the evaluated model to the file\n",
        "        with open(evaluated_models_file, \"a\") as f:\n",
        "            f.write(model_file + \"\\n\")\n",
        "\n",
        "def get_all_checkpoint_files(directory):\n",
        "    print(f\"Checking directory for .ckpt and .pth files: {directory}\")\n",
        "    checkpoint_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])\n",
        "    print(f\"Found checkpoint files: {checkpoint_files}\")\n",
        "    return checkpoint_files\n",
        "\n",
        "# Set up and start the watchdog observer\n",
        "event_handler = CheckpointHandler()\n",
        "observer = Observer()\n",
        "observer.schedule(event_handler, model_dir, recursive=True)\n",
        "observer.start()\n",
        "\n",
        "print(\"Watching for new checkpoints and final models in all subdirectories...\")\n",
        "print(\"This script will continue running until you stop it manually.\")\n",
        "print(\"You can stop it by interrupting the process when training is complete.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Check for any new models\n",
        "        current_models = set(f for f in get_all_checkpoint_files(model_dir))\n",
        "        new_models = current_models - evaluated_models\n",
        "\n",
        "        print(f\"Current models: {current_models}\")\n",
        "        print(f\"New models to evaluate: {new_models}\")\n",
        "\n",
        "        for model_path in new_models:\n",
        "            event_handler.evaluate_model(model_path)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    observer.stop()\n",
        "\n",
        "observer.join()\n",
        "print(\"Checkpoint and final model evaluation completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L25zvW0ig8K1",
      "metadata": {
        "id": "L25zvW0ig8K1"
      },
      "source": [
        "### 6. Analyze the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJhLC5Ttg8K1",
      "metadata": {
        "id": "mJhLC5Ttg8K1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Define the directory containing evaluation results\n",
        "results_dir = Path(\"./evaluation_results/\")\n",
        "\n",
        "# 2. Retrieve all JSON files\n",
        "json_files = list(results_dir.glob(\"*.json\"))\n",
        "print(f\"Found {len(json_files)} JSON files.\")\n",
        "\n",
        "# 3. Function to extract timestamp from filename\n",
        "def extract_timestamp(filename):\n",
        "    \"\"\"\n",
        "    Extracts the timestamp from the filename.\n",
        "    Expected format: ..._YYYYMMDD_HHMMSS.json\n",
        "    \"\"\"\n",
        "    pattern = r\"_(\\d{8}_\\d{6})\\.json$\"\n",
        "    match = re.search(pattern, filename)\n",
        "    if match:\n",
        "        return pd.to_datetime(match.group(1), format=\"%Y%m%d_%H%M%S\")\n",
        "    else:\n",
        "        return pd.NaT  # Not a Time if pattern doesn't match\n",
        "\n",
        "# 4. Load and compile aggregate results\n",
        "data = []\n",
        "\n",
        "for file in json_files:\n",
        "    with open(file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    # Extract aggregate results\n",
        "    aggregate = results.get(\"aggregate_results\", {})\n",
        "    \n",
        "    # Extract timestamp from filename\n",
        "    timestamp = extract_timestamp(file.name)\n",
        "    \n",
        "    # Combine data\n",
        "    record = {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"test_loss\": aggregate.get(\"test_loss\"),\n",
        "        \"test_accuracy\": aggregate.get(\"test_accuracy\"),\n",
        "        \"test_diff_accuracy\": aggregate.get(\"test_diff_accuracy\"),\n",
        "        \"complete_task_accuracy\": aggregate.get(\"complete_task_accuracy\")\n",
        "    }\n",
        "    \n",
        "    data.append(record)\n",
        "\n",
        "print(f\"Compiled {len(data)} records.\")\n",
        "\n",
        "# 5. Create DataFrame for aggregate results\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Drop records with missing timestamps\n",
        "df = df.dropna(subset=[\"timestamp\"])\n",
        "\n",
        "# Sort by timestamp\n",
        "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "print(\"Aggregate Results DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# 6. (Optional) Handle individual metrics\n",
        "individual_data = []\n",
        "\n",
        "for file in json_files:\n",
        "    with open(file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "    \n",
        "    # Extract individual metrics\n",
        "    individual = results.get(\"individual_metrics\", {})\n",
        "    \n",
        "    # Extract timestamp from filename\n",
        "    timestamp = extract_timestamp(file.name)\n",
        "    \n",
        "    for metric_id, metrics in individual.items():\n",
        "        record = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"metric_id\": metric_id,\n",
        "            \"test_accuracy\": metrics.get(\"test_accuracy\"),\n",
        "            \"test_diff_accuracy\": metrics.get(\"test_diff_accuracy\")\n",
        "        }\n",
        "        individual_data.append(record)\n",
        "\n",
        "individual_df = pd.DataFrame(individual_data)\n",
        "\n",
        "# Drop records with missing timestamps\n",
        "individual_df = individual_df.dropna(subset=[\"timestamp\"])\n",
        "\n",
        "# Convert timestamp to datetime if not already\n",
        "if individual_df[\"timestamp\"].dtype == object:\n",
        "    individual_df[\"timestamp\"] = pd.to_datetime(individual_df[\"timestamp\"])\n",
        "\n",
        "print(\"Individual Metrics DataFrame:\")\n",
        "print(individual_df.head())\n",
        "\n",
        "# 7. Save DataFrames to CSV (Optional)\n",
        "df.to_csv(\"aggregate_evaluation_results.csv\", index=False)\n",
        "individual_df.to_csv(\"individual_evaluation_metrics.csv\", index=False)\n",
        "\n",
        "# 8. Plot Test Accuracy Over Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['timestamp'], df['test_accuracy'], marker='o', linestyle='-', label='Test Accuracy')\n",
        "plt.title('Test Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Diff Accuracy Over Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['timestamp'], df['test_diff_accuracy'], marker='o', linestyle='-', color='green', label='Test Diff Accuracy')\n",
        "plt.title('Test Diff Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Test Diff Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Complete Task Accuracy Over Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['timestamp'], df['complete_task_accuracy'], marker='o', linestyle='-', color='red', label='Complete Task Accuracy')\n",
        "plt.title('Complete Task Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Complete Task Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Heatmap: Task Accuracy Over Time\n",
        "\n",
        "# Pivot the individual_df to have 'metric_id' as rows and 'timestamp' as columns, with 'test_accuracy' as values\n",
        "heatmap_data = individual_df.pivot_table(index='metric_id', columns='timestamp', values='test_accuracy')\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(heatmap_data, cmap=\"coolwarm\", cbar_kws={'label': 'Test Accuracy'}, annot=False)\n",
        "plt.title('Task Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Task (Metric ID)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOEvyN3LCVdV",
      "metadata": {
        "id": "LOEvyN3LCVdV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0L9IwEdxzd-H",
        "Iz4dsrstg8Ku",
        "gb75DkIFg8Kw",
        "f2ba232c",
        "L25zvW0ig8K1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
