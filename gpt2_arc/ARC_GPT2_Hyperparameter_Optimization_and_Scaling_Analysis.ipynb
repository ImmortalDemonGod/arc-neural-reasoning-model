{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8-QMJgz1LmwE",
      "metadata": {
        "id": "8-QMJgz1LmwE"
      },
      "outputs": [],
      "source": [
        "Required: https://youtu.be/IDxrMbXPVTA?si=PHfGry-HQj__3Xne"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0L9IwEdxzd-H",
      "metadata": {
        "id": "0L9IwEdxzd-H"
      },
      "source": [
        "# STUFF YOU CHANGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "6okItopOumX_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6okItopOumX_",
        "outputId": "5359a138-ed90-4dd4-9289-3e70fb5c7bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent Directory: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "ARC Model Directory: /workspaces/arc-neural-reasoning-model/\n",
            "Current Configuration:\n",
            "Parent Directory: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Use Best Parameters: False\n",
            "Manual Parameters: {'n_embd': 2, 'n_head': 4, 'n_layer': 4, 'batch_size': 16, 'learning_rate': 0.0001, 'max_epochs': 20}\n",
            "Number of Optuna Trials: 100\n",
            "Hyperparameter Ranges:\n",
            "  n_embd: 64 to 256\n",
            "  n_head: 2 to 16\n",
            "  n_layer: 2 to 16\n",
            "  batch_size: 1 to 256\n",
            "  learning_rate: 1e-08 to 0.01\n",
            "  max_epochs: 3 to 50\n",
            "Configuration saved to config.json\n",
            "\n",
            "GPU Available: False\n"
          ]
        }
      ],
      "source": [
        "# ========== User-Defined Parameters (Top of Notebook) ==========\n",
        "\n",
        "# 1. Base directory for storing date-based experiment result folders.\n",
        "import os\n",
        "\n",
        "# Define the base directory for the arc-neural-reasoning-model\n",
        "arc_model_dir = \"/workspaces/arc-neural-reasoning-model/\"\n",
        "parent_dir = \"/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\"\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"ARC Model Directory: {arc_model_dir}\")\n",
        "# 2. Boolean flag to choose between best hyperparameters from previous experiments (True) or manual parameters (False).\n",
        "use_best_params = False  # Set to False to use manual parameters\n",
        "\n",
        "# 3. Dictionary of manually set hyperparameters used when use_best_params is False.\n",
        "#    Includes model architecture and training settings.\n",
        "manual_params = {\n",
        "    \"n_embd\": 2,     # Embedding dimension\n",
        "    \"n_head\": 4,       # Number of attention heads\n",
        "    \"n_layer\": 4,     # Number of transformer layers\n",
        "    \"batch_size\": 16,  # Batch size for training\n",
        "    \"learning_rate\": 1e-4,  # Learning rate\n",
        "    \"max_epochs\": 20   # Maximum number of epochs for training\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning search space settings\n",
        "# 4. Number of Optuna trials for hyperparameter tuning.\n",
        "#    More trials can lead to better optimization but increase computation time.\n",
        "n_trials = 100\n",
        "\n",
        "# 5. Range for embedding dimension in hyperparameter search space.\n",
        "n_embd_min, n_embd_max = 64, 256\n",
        "\n",
        "# 6. Range for number of attention heads in transformer model during tuning.\n",
        "n_head_min, n_head_max = 2, 16\n",
        "\n",
        "# 7. Range for number of transformer layers in model architecture during optimization.\n",
        "n_layer_min, n_layer_max = 2, 16\n",
        "\n",
        "# 8. Range of batch sizes to explore. Larger batches can speed up training but may require more memory.\n",
        "batch_size_min, batch_size_max = 1, 256\n",
        "\n",
        "# 9. Range for learning rate in hyperparameter search space. Crucial for model convergence and performance.\n",
        "learning_rate_min, learning_rate_max = 1e-8, 1e-2\n",
        "\n",
        "# 10. Range for number of training epochs to consider during hyperparameter optimization.\n",
        "max_epochs_min, max_epochs_max = 3, 50\n",
        "\n",
        "# 11. Range for n_head exponent in hyperparameter search space.\n",
        "n_head_exp_min, n_head_exp_max = 1, 3\n",
        "\n",
        "# 12. Range for n_embd multiplier in hyperparameter search space.\n",
        "n_embd_multiplier_min, n_embd_multiplier_max = 4, 16\n",
        "\n",
        "# These parameters allow flexible experimentation with different model configurations and training settings,\n",
        "# enabling comprehensive exploration of the hyperparameter space for optimal model performance.\n",
        "\n",
        "# Print configurations for verification\n",
        "print(\"Current Configuration:\")\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"Use Best Parameters: {use_best_params}\")\n",
        "print(f\"Manual Parameters: {manual_params}\")\n",
        "print(f\"Number of Optuna Trials: {n_trials}\")\n",
        "print(\"Hyperparameter Ranges:\")\n",
        "print(f\"  n_embd: {n_embd_min} to {n_embd_max}\")\n",
        "print(f\"  n_head: {n_head_min} to {n_head_max}\")\n",
        "print(f\"  n_layer: {n_layer_min} to {n_layer_max}\")\n",
        "print(f\"  batch_size: {batch_size_min} to {batch_size_max}\")\n",
        "print(f\"  learning_rate: {learning_rate_min} to {learning_rate_max}\")\n",
        "print(f\"  max_epochs: {max_epochs_min} to {max_epochs_max}\")\n",
        "\n",
        "# Save configuration\n",
        "import json\n",
        "\n",
        "def save_config(config, filename=\"config.json\"):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "config = {\n",
        "    \"parent_dir\": parent_dir,\n",
        "    \"use_best_params\": use_best_params,\n",
        "    \"manual_params\": manual_params,\n",
        "    \"tuning\": {\n",
        "        \"n_trials\": n_trials,\n",
        "        \"n_embd\": (n_embd_min, n_embd_max),\n",
        "        \"n_head\": (n_head_min, n_head_max),\n",
        "        \"n_layer\": (n_layer_min, n_layer_max),\n",
        "        \"batch_size\": (batch_size_min, batch_size_max),\n",
        "        \"learning_rate\": (learning_rate_min, learning_rate_max),\n",
        "        \"max_epochs\": (max_epochs_min, max_epochs_max)\n",
        "    }\n",
        "}\n",
        "\n",
        "save_config(config)\n",
        "print(f\"Configuration saved to config.json\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"\\nGPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2-qicsswzi-S",
      "metadata": {
        "id": "2-qicsswzi-S"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DgxlLFBTg8Kt",
      "metadata": {
        "id": "DgxlLFBTg8Kt"
      },
      "source": [
        "### 1. Set up the Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u8k0Yxzd4RCb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8k0Yxzd4RCb",
        "outputId": "aa0390e3-e53c-46ca-9f77-ac45d076335a"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pRx7QZEdg8Kt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRx7QZEdg8Kt",
        "outputId": "9b102db3-e7ac-4294-868d-f3bfb9cb33c8"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/arc-neural-reasoning-model/\n",
        "#!git clone https://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@github.com/ImmortalDemonGod/arc-neural-reasoning-model.git\n",
        "#!pip install -r /workspaces/arc-neural-reasoning-model/gpt2_arc/requirements.txt\n",
        "#!pip install optuna\n",
        "#!pip install jupyterlab jupyterlab-optuna\n",
        "# Install the required packages\n",
        "#!pip install optuna-dashboard pyngrok\n",
        "#!ngrok config add-authtoken 2NCEuxuUBMj6zsdTokQHkYJ4AZz_3E7e2pyW87otgFg3UdSC3\n",
        "#!pip install tensorboard\n",
        "#!pip install watchdog\n",
        "#!pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bhpQwlBojS1S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhpQwlBojS1S",
        "outputId": "a2b3d0e5-37d3-421a-ff20-b9e2bd721d49"
      },
      "outputs": [],
      "source": [
        "%cd /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
        "#!rm -rf /content/arc-neural-reasoning-model/arc_sat_solver\n",
        "#!rm -rf /content/arc-neural-reasoning-model/benchmark_results\n",
        "#!rm -rf /content/arc-neural-reasoning-model/checkpoints\n",
        "#!rm -rf /content/arc-neural-reasoning-model/tmp\n",
        "#!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iz4dsrstg8Ku",
      "metadata": {
        "id": "Iz4dsrstg8Ku"
      },
      "source": [
        "### 2. Run hyperparameter tuning (in the background) click the **links** to see the dashboards:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6zIQhT9m9_wv",
      "metadata": {
        "id": "6zIQhT9m9_wv"
      },
      "source": [
        "make sure to set the directory in your drive to save the files in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "DVIRPnKcPV92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVIRPnKcPV92",
        "outputId": "cd8d8350-a4d5-4b47-cc05-40e0eefc7b0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240926\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/codespace/.local/lib/python3.12/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
            "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date in YYYYMMDD format\n",
        "current_date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "\n",
        "# Create a folder named after the current date\n",
        "date_folder = os.path.join(parent_dir, current_date)\n",
        "if not os.path.exists(date_folder):\n",
        "    os.makedirs(date_folder)\n",
        "\n",
        "# Change into the newly created folder\n",
        "%cd {date_folder}\n",
        "\n",
        "# Now all your operations will save to /content/YYYYMMDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "7xyBrsMGg8Kv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xyBrsMGg8Kv",
        "outputId": "fe9c106f-a2b6-49df-e656-8141182e9fed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optuna dashboard is accessible at: NgrokTunnel: \"https://96fa-20-163-40-131.ngrok-free.app\" -> \"http://localhost:8081\"\n",
            "TensorBoard is accessible at: NgrokTunnel: \"https://bb17-20-163-40-131.ngrok-free.app\" -> \"http://localhost:6006\"\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "TensorFlow installation not found - running with reduced feature set.\n",
            "\n",
            "NOTE: Using experimental fast data loading logic. To disable, pass\n",
            "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
            "    https://github.com/tensorflow/tensorboard/issues/4784\n",
            "\n",
            "Listening on http://127.0.0.1:8081/\n",
            "Hit Ctrl-C to quit.\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/codespace/.python/current/bin/optuna-dashboard\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna_dashboard/_cli.py\", line 140, in main\n",
            "    run_wsgiref(app, args.host, args.port, args.quiet)\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna_dashboard/_cli.py\", line 43, in run_wsgiref\n",
            "    httpd = make_server(host, port, app, server_class=ThreadedWSGIServer)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/wsgiref/simple_server.py\", line 154, in make_server\n",
            "    server = server_class((host, port), handler_class)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 457, in __init__\n",
            "    self.server_bind()\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/wsgiref/simple_server.py\", line 50, in server_bind\n",
            "    HTTPServer.server_bind(self)\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/http/server.py\", line 136, in server_bind\n",
            "    socketserver.TCPServer.server_bind(self)\n",
            "  File \"/usr/local/python/3.12.1/lib/python3.12/socketserver.py\", line 473, in server_bind\n",
            "    self.socket.bind(self.server_address)\n",
            "OSError: [Errno 98] Address already in use\n",
            "I0926 20:23:42.091755 127648093177600 plugin.py:429] Monitor runs begin\n",
            "E0926 20:23:42.362953 127648508822144 program.py:300] TensorBoard could not bind to port 6006, it was already in use\n",
            "ERROR: TensorBoard could not bind to port 6006, it was already in use\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/requests/__init__.py:109: RequestsDependencyWarning: urllib3 (1.26.20) or chardet (5.2.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
            "  warnings.warn(\n",
            "[I 2024-09-26 20:23:44,416] Using an existing study with name 'gpt2_arc_optimization' instead of creating a new one.\n",
            "INFO:__main__:Starting optimization with 100 trials\n",
            "INFO:__main__:Starting trial 1\n",
            "DEBUG:__main__:Suggested n_head: 4 (2^2)\n",
            "DEBUG:__main__:Adjusted n_embd: 32\n",
            "DEBUG:__main__:Suggested n_layer: 9\n",
            "DEBUG:__main__:Validating hyperparameters: n_embd=32, n_head=4, n_layer=9\n",
            "DEBUG:__main__:Hyperparameters validated successfully\n",
            "DEBUG:__main__:Model config: ModelConfig(n_embd=32, n_head=4, n_layer=9, dropout=0.1)\n",
            "DEBUG:__main__:Full config: Config(model=ModelConfig(n_embd=32, n_head=4, n_layer=9, dropout=0.1), training=TrainingConfig(batch_size=159, learning_rate=1.187260781692586e-07, max_epochs=9, use_gpu=True, log_level='INFO'), evaluation=EvaluationConfig(perfect_accuracy_threshold=0.999999))\n",
            "DEBUG:__main__:Loading data\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Starting ARCDataset initialization\n",
            "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
            "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
            "DEBUG: Processed data length: 400\n",
            "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
            "DEBUG: First train item: {'input': array([[0, 7, 7],\n",
            "       [7, 7, 7],\n",
            "       [0, 7, 7]]), 'output': array([[0, 0, 0, 0, 7, 7, 0, 7, 7],\n",
            "       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n",
            "       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n",
            "       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n",
            "       [7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
            "       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n",
            "       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n",
            "       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n",
            "       [0, 0, 0, 0, 7, 7, 0, 7, 7]])}\n",
            "Number of train samples: 1302\n",
            "Number of test samples: 416\n",
            "DEBUG: Starting ARCDataset initialization\n",
            "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
            "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
            "DEBUG: Processed data length: 400\n",
            "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
            "DEBUG: First train item: {'input': array([[8, 6],\n",
            "       [6, 4]]), 'output': array([[8, 6, 8, 6, 8, 6],\n",
            "       [6, 4, 6, 4, 6, 4],\n",
            "       [6, 8, 6, 8, 6, 8],\n",
            "       [4, 6, 4, 6, 4, 6],\n",
            "       [8, 6, 8, 6, 8, 6],\n",
            "       [6, 4, 6, 4, 6, 4]])}\n",
            "Number of train samples: 1363\n",
            "Number of test samples: 419\n",
            "DEBUG: Initialized self.results['train'] as <class 'dict'>\n",
            "DEBUG: Optuna trial TensorBoard logger initialized. Log dir: runs/experiment_optuna_trial_1/version_0\n",
            "DEBUG: Trainer created for Optuna trial with TensorBoard logger\n",
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:__main__:Data loaded. Train set size: 1302, Validation set size: 1363\n",
            "DEBUG:__main__:Creating model and trainer\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=32, n_head=4\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=32, n_head=4\n",
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "DEBUG:__main__:Trainer created with config: TrainerState(status=<TrainerStatus.INITIALIZING: 'initializing'>, fn=None, stage=None)\n",
            "DEBUG:__main__:Starting training\n",
            "\n",
            "  | Name  | Type    | Params | Mode \n",
            "------------------------------------------\n",
            "0 | model | GPT2ARC | 114 K  | train\n",
            "------------------------------------------\n",
            "114 K     Trainable params\n",
            "0         Non-trainable params\n",
            "114 K     Total params\n",
            "0.459     Total estimated model params size (MB)\n",
            "121       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "DEBUG:fsspec.local:open file: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240926/runs/experiment_optuna_trial_1/version_0/hparams.yaml\n",
            "DEBUG:gpt2_arc.src.training.trainer:DEBUG: Test dataloader created with 9 batches\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:gpt2_arc.src.training.trainer:Validation step - Batch type: <class 'list'>, length: 3\n",
            "DEBUG:gpt2_arc.src.models.gpt2:GPT2ARC input shape: torch.Size([159, 1, 30, 30]), dtype: torch.float32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After conv1 shape: torch.Size([159, 32, 30, 30])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Reshaped for transformer blocks: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 1 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 2 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 3 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 4 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 5 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 6 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 7 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 8 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 9 shape: torch.Size([159, 900, 32])\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_lightning/utilities/data.py:78: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 159. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "DEBUG:gpt2_arc.src.training.trainer:Validation step - Batch type: <class 'list'>, length: 3\n",
            "DEBUG:gpt2_arc.src.models.gpt2:GPT2ARC input shape: torch.Size([159, 1, 30, 30]), dtype: torch.float32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After conv1 shape: torch.Size([159, 32, 30, 30])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Reshaped for transformer blocks: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Logged validation loss: 3.3868985176086426 for epoch 0\n",
            "Sanity Checking DataLoader 0:  50%|█████     | 1/2 [00:12<00:12,  0.08it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 1 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 2 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 3 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 4 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 5 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 6 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 7 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 8 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 9 shape: torch.Size([159, 900, 32])\n",
            "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_lightning/loops/fit_loop.py:298: The number of training batches (9) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DEBUG: Logged validation loss: 3.3587708473205566 for epoch 0\n",
            "Epoch 0:   0%|          | 0/9 [00:00<?, ?it/s]                             "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEBUG:gpt2_arc.src.training.trainer:Training step - Batch type: <class 'list'>, length: 3\n",
            "DEBUG:gpt2_arc.src.models.gpt2:GPT2ARC input shape: torch.Size([159, 1, 30, 30]), dtype: torch.float32\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After conv1 shape: torch.Size([159, 32, 30, 30])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Reshaped for transformer blocks: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 1 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:FeedForward output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock output shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:After block 2 shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:TransformerBlock input shape: torch.Size([159, 900, 32])\n",
            "DEBUG:gpt2_arc.src.models.gpt2:Attention input shape: torch.Size([159, 900, 32])\n"
          ]
        }
      ],
      "source": [
        "import subprocess\n",
        "\n",
        "# Run the optimization command using the new date folder for storage in the background\n",
        "# The user-defined hyperparameters are passed here dynamically\n",
        "optimize_command = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/optimize_hyperparameters.py\"),\n",
        "    \"--n_trials\", str(n_trials),\n",
        "    \"--storage\", f\"sqlite:///{date_folder}/optuna_results.db\",\n",
        "    \"--n_embd_min\", str(n_embd_min), \"--n_embd_max\", str(n_embd_max),\n",
        "    \"--n_head_min\", str(n_head_min), \"--n_head_max\", str(n_head_max),\n",
        "    \"--n_layer_min\", str(n_layer_min), \"--n_layer_max\", str(n_layer_max),\n",
        "    \"--batch_size_min\", str(batch_size_min), \"--batch_size_max\", str(batch_size_max),\n",
        "    \"--learning_rate_min\", str(learning_rate_min), \"--learning_rate_max\", str(learning_rate_max),\n",
        "    \"--max_epochs_min\", str(max_epochs_min), \"--max_epochs_max\", str(max_epochs_max),\n",
        "    \"--n_head_exp_min\", str(n_head_exp_min), \"--n_head_exp_max\", str(n_head_exp_max),\n",
        "    \"--n_embd_multiplier_min\", str(n_embd_multiplier_min), \"--n_embd_multiplier_max\", str(n_embd_multiplier_max)\n",
        "]\n",
        "subprocess.Popen(optimize_command)\n",
        "\n",
        "# Import ngrok and set up the tunnel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the ngrok tunnel for port 8081\n",
        "public_url = ngrok.connect(8081)\n",
        "print(f\"Optuna dashboard is accessible at: {public_url}\")\n",
        "\n",
        "# Run the Optuna dashboard in the background without blocking\n",
        "optuna_command = [\n",
        "    \"optuna-dashboard\", \"--port\", \"8081\", f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "]\n",
        "subprocess.Popen(optuna_command)\n",
        "\n",
        "# Specify the folder for logs (using date_folder)\n",
        "log_dir = f\"{date_folder}/runs\"\n",
        "\n",
        "# Run TensorBoard in the background without blocking\n",
        "tensorboard_command = [\n",
        "    \"tensorboard\", \"--logdir\", log_dir, \"--port\", \"6006\"\n",
        "]\n",
        "subprocess.Popen(tensorboard_command)\n",
        "\n",
        "# Set up the ngrok tunnel for TensorBoard\n",
        "public_url_tb = ngrok.connect(6006)\n",
        "print(f\"TensorBoard is accessible at: {public_url_tb}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb75DkIFg8Kw",
      "metadata": {
        "id": "gb75DkIFg8Kw"
      },
      "source": [
        "### 3. Get the best hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda44504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fda44504",
        "outputId": "6c6196cb-1072-4d71-a081-3367966411ac"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from gpt2_arc.src.optimize_hyperparameters import run_optimization\n",
        "\n",
        "# Set Optuna storage and study details\n",
        "storage_name = f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "study_name = \"gpt2_arc_optimization\"\n",
        "\n",
        "try:\n",
        "    # List all study names in the database\n",
        "    study_summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
        "    print(\"Available studies in the database:\")\n",
        "    for study_summary in study_summaries:\n",
        "        print(f\"- {study_summary.study_name}\")\n",
        "\n",
        "    # Load the specified study\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
        "    best_params = study.best_params\n",
        "    print(\"Best hyperparameters:\")\n",
        "    print(json.dumps(best_params, indent=2))\n",
        "\n",
        "    # Save the best parameters to a JSON file\n",
        "    with open(f\"{date_folder}/best_hyperparameters.json\", \"w\") as f:\n",
        "        json.dump(best_params, f)\n",
        "\n",
        "except KeyError as e:\n",
        "    print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
        "    print(f\"Details: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ba232c",
      "metadata": {
        "id": "f2ba232c"
      },
      "source": [
        "### 4. Use the best hyperparameters for longer training (manually set max epochs!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "m8uLi0U5l6QL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8uLi0U5l6QL",
        "outputId": "cdc0d029-04a1-4ceb-df7b-3b0a4a85627e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training process in the background...\n",
            "Training process started with PID: 196685\n",
            "You can monitor the training progress in /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240926/training_output.log\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Load best hyperparameters from the JSON file if use_best_params is True\n",
        "if use_best_params:\n",
        "    try:\n",
        "        with open(f\"{date_folder}/best_hyperparameters.json\", \"r\") as f:\n",
        "            best_params = json.load(f)\n",
        "        # Extract hyperparameters from the JSON file\n",
        "        params = {\n",
        "            \"n_embd\": best_params.get(\"n_embd\", manual_params[\"n_embd\"]),\n",
        "            \"n_head\": best_params.get(\"n_head\", manual_params[\"n_head\"]),\n",
        "            \"n_layer\": best_params.get(\"n_layer\", manual_params[\"n_layer\"]),\n",
        "            \"batch_size\": best_params.get(\"batch_size\", manual_params[\"batch_size\"]),\n",
        "            \"learning_rate\": best_params.get(\"learning_rate\", manual_params[\"learning_rate\"]),\n",
        "            \"max_epochs\": best_params.get(\"max_epochs\", manual_params[\"max_epochs\"])\n",
        "        }\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {date_folder}/best_hyperparameters.json not found. Using manual parameters.\")\n",
        "        params = manual_params\n",
        "else:\n",
        "    # Use manually defined parameters\n",
        "    params = manual_params\n",
        "\n",
        "# Build the arguments for the training command\n",
        "train_args = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/training/train.py\"),\n",
        "    \"--n-embd\", str(params[\"n_embd\"]),\n",
        "    \"--n-head\", str(params[\"n_head\"]),\n",
        "    \"--n-layer\", str(params[\"n_layer\"]),\n",
        "    \"--batch-size\", str(params[\"batch_size\"]),\n",
        "    \"--learning-rate\", str(params[\"learning_rate\"]),\n",
        "    \"--max-epochs\", str(params[\"max_epochs\"]),\n",
        "    \"--use-gpu\",\n",
        "    \"--project\", \"arc-scaling-test\"\n",
        "]\n",
        "\n",
        "# Execute the training script in the background\n",
        "print(\"Starting training process in the background...\")\n",
        "with open(f\"{date_folder}/training_output.log\", \"w\") as log_file:\n",
        "    process = subprocess.Popen(train_args, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(f\"Training process started with PID: {process.pid}\")\n",
        "print(f\"You can monitor the training progress in {date_folder}/training_output.log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5062c297",
      "metadata": {
        "id": "5062c297"
      },
      "source": [
        "### 5. Evaluate the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pwrd3HMBnhW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pwrd3HMBnhW",
        "outputId": "e7020464-f37e-4d1c-ffed-06bfeb029552"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import time\n",
        "from watchdog.observers import Observer\n",
        "from watchdog.events import FileSystemEventHandler\n",
        "import subprocess\n",
        "\n",
        "# Set W&B API key (replace with your actual API key)\n",
        "wandb_api_key = \"2b06e99af167044b281668f6edd388c633aba1a0\"  # Replace with your W&B API key\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "\n",
        "# Define the date_folder variable appropriately\n",
        "# For example, you can set it based on the current date\n",
        "from datetime import datetime\n",
        "date_folder = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "# Directory containing the model files\n",
        "model_dir = os.path.join(arc_model_dir, \"EXPERIMENTAL\")  # Ensure this path is correct\n",
        "print(f\"Watching for new models in directory: {model_dir}\")\n",
        "output_dir = \"evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "wandb_project = \"arc-evaluation\"\n",
        "\n",
        "# Set of evaluated models\n",
        "evaluated_models = set()\n",
        "\n",
        "class CheckpointHandler(FileSystemEventHandler):\n",
        "    def on_created(self, event):\n",
        "        if event.is_directory:\n",
        "            return\n",
        "        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):\n",
        "            print(f\"New checkpoint detected: {event.src_path}\")\n",
        "            self.evaluate_model(event.src_path)\n",
        "\n",
        "    def evaluate_model(self, model_path):\n",
        "        model_file = os.path.basename(model_path)\n",
        "\n",
        "        if model_file in evaluated_models:\n",
        "            print(f\"Skipping already evaluated model: {model_file}\")\n",
        "            return  # Skip if the model was already evaluated\n",
        "\n",
        "        # Extract epoch and val_loss from the filename for run_name\n",
        "        # Assuming the filename pattern arc_model-epoch=<number>-val_loss=<number>.ckpt\n",
        "        try:\n",
        "            parts = model_file.replace('.ckpt', '').split('-')\n",
        "            epoch = None\n",
        "            val_loss = None\n",
        "            for part in parts:\n",
        "                if part.startswith('epoch='):\n",
        "                    epoch = part.split('=')[1]\n",
        "                elif part.startswith('val_loss='):\n",
        "                    val_loss = part.split('=')[1]\n",
        "            if epoch is not None and val_loss is not None:\n",
        "                run_name = f\"scaling-test-evaluation-epoch{epoch}-val_loss{val_loss}\"\n",
        "            else:\n",
        "                run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing run name from filename {model_file}: {e}\")\n",
        "            run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "\n",
        "        eval_command = [\n",
        "            \"python\", \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\",\n",
        "            \"--model_checkpoint\", model_path,\n",
        "            \"--batch_size\", \"32\",\n",
        "            \"--output_dir\", output_dir,\n",
        "            \"--wandb_project\", wandb_project,\n",
        "            \"--wandb_run_name\", run_name\n",
        "        ]\n",
        "\n",
        "        print(f\"Evaluating model: {model_file} with command: {eval_command}\")\n",
        "        try:\n",
        "            # Run the evaluation command and capture stdout and stderr\n",
        "            result = subprocess.run(\n",
        "                eval_command,\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True  # Automatically decode bytes to string\n",
        "            )\n",
        "            print(f\"Successfully evaluated model: {model_file}\")\n",
        "            print(\"Evaluation Output:\")\n",
        "            print(result.stdout)  # Print the standard output from evaluate.py\n",
        "            if result.stderr:\n",
        "                print(\"Evaluation Errors/Warnings:\")\n",
        "                print(result.stderr)  # Print any errors or warnings from evaluate.py\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during evaluation of {model_file}: {e}\")\n",
        "            print(\"Standard Output:\")\n",
        "            print(e.stdout)  # Print the standard output even if there's an error\n",
        "            print(\"Standard Error:\")\n",
        "            print(e.stderr)  # Print the error messages\n",
        "        except Exception as ex:\n",
        "            print(f\"An unexpected error occurred while evaluating {model_file}: {ex}\")\n",
        "\n",
        "        evaluated_models.add(model_file)\n",
        "\n",
        "def get_all_checkpoint_files(directory):\n",
        "    print(f\"Checking directory for .ckpt and .pth files: {directory}\")\n",
        "    checkpoint_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])\n",
        "    print(f\"Found checkpoint files: {checkpoint_files}\")\n",
        "    return checkpoint_files\n",
        "\n",
        "# Set up and start the watchdog observer\n",
        "event_handler = CheckpointHandler()\n",
        "observer = Observer()\n",
        "observer.schedule(event_handler, model_dir, recursive=True)\n",
        "observer.start()\n",
        "\n",
        "print(\"Watching for new checkpoints and final models in all subdirectories...\")\n",
        "print(\"This script will continue running until you stop it manually.\")\n",
        "print(\"You can stop it by interrupting the process when training is complete.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Check for any new models\n",
        "        current_models = set(f for f in get_all_checkpoint_files(model_dir))\n",
        "        new_models = current_models - evaluated_models\n",
        "\n",
        "        print(f\"Current models: {current_models}\")\n",
        "        print(f\"New models to evaluate: {new_models}\")\n",
        "\n",
        "        for model_path in new_models:\n",
        "            event_handler.evaluate_model(model_path)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    observer.stop()\n",
        "\n",
        "observer.join()\n",
        "print(\"Checkpoint and final model evaluation completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L25zvW0ig8K1",
      "metadata": {
        "id": "L25zvW0ig8K1"
      },
      "source": [
        "### 6. Analyze the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJhLC5Ttg8K1",
      "metadata": {
        "id": "mJhLC5Ttg8K1"
      },
      "outputs": [],
      "source": [
        "#import json\n",
        "\n",
        "# Load and print evaluation results\n",
        "#with open(\"./evaluation_results/scaling-test-evaluation_results.json\", \"r\") as f:\n",
        "#    results = json.load(f)\n",
        "\n",
        "#print(\"Evaluation Results:\")\n",
        "#print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOEvyN3LCVdV",
      "metadata": {
        "id": "LOEvyN3LCVdV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0L9IwEdxzd-H",
        "Iz4dsrstg8Ku",
        "gb75DkIFg8Kw",
        "f2ba232c",
        "L25zvW0ig8K1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
