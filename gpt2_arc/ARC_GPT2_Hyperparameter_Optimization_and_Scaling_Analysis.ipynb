{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8-QMJgz1LmwE",
      "metadata": {
        "id": "8-QMJgz1LmwE"
      },
      "outputs": [],
      "source": [
        "Required: https://youtu.be/IDxrMbXPVTA?si=PHfGry-HQj__3Xne"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0L9IwEdxzd-H",
      "metadata": {
        "id": "0L9IwEdxzd-H"
      },
      "source": [
        "# STUFF YOU CHANGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "6okItopOumX_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6okItopOumX_",
        "outputId": "5359a138-ed90-4dd4-9289-3e70fb5c7bbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parent Directory: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "ARC Model Directory: /workspaces/arc-neural-reasoning-model/\n",
            "Manual parameters validated successfully\n",
            "Current Configuration:\n",
            "Parent Directory: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
            "Use Best Parameters: False\n",
            "Manual Parameters: {'n_embd': 4, 'n_head': 4, 'n_layer': 4, 'batch_size': 16, 'learning_rate': 0.0001, 'max_epochs': 20}\n",
            "Number of Optuna Trials: 100\n",
            "Hyperparameter Ranges:\n",
            "  n_embd: 64 to 256\n",
            "  n_head: 2 to 16\n",
            "  n_layer: 2 to 16\n",
            "  batch_size: 1 to 256\n",
            "  learning_rate: 1e-08 to 0.01\n",
            "  max_epochs: 3 to 50\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'config.json'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[33], line 104\u001b[0m\n\u001b[1;32m     87\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(config, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     89\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparent_dir\u001b[39m\u001b[38;5;124m\"\u001b[39m: parent_dir,\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_best_params\u001b[39m\u001b[38;5;124m\"\u001b[39m: use_best_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    101\u001b[0m     }\n\u001b[1;32m    102\u001b[0m }\n\u001b[0;32m--> 104\u001b[0m \u001b[43msave_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConfiguration saved to config.json\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# Check GPU availability\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[33], line 86\u001b[0m, in \u001b[0;36msave_config\u001b[0;34m(config, filename)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_config\u001b[39m(config, filename\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.json\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 86\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     87\u001b[0m         json\u001b[38;5;241m.\u001b[39mdump(config, f, indent\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
            "File \u001b[0;32m~/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.json'"
          ]
        }
      ],
      "source": [
        "# ========== User-Defined Parameters (Top of Notebook) ==========\n",
        "\n",
        "# 1. Base directory for storing date-based experiment result folders.\n",
        "import os\n",
        "\n",
        "# Define the base directory for the arc-neural-reasoning-model\n",
        "arc_model_dir = \"/workspaces/arc-neural-reasoning-model/\"\n",
        "parent_dir = \"/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\"\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"ARC Model Directory: {arc_model_dir}\")\n",
        "# 2. Boolean flag to choose between best hyperparameters from previous experiments (True) or manual parameters (False).\n",
        "use_best_params = False  # Set to False to use manual parameters\n",
        "\n",
        "# 3. Dictionary of manually set hyperparameters used when use_best_params is False.\n",
        "#    Includes model architecture and training settings.\n",
        "manual_params = {\n",
        "    \"n_embd\": 4,     # Embedding dimension\n",
        "    \"n_head\": 4,       # Number of attention heads\n",
        "    \"n_layer\": 4,     # Number of transformer layers\n",
        "    \"batch_size\": 16,  # Batch size for training\n",
        "    \"learning_rate\": 1e-4,  # Learning rate\n",
        "    \"max_epochs\": 20   # Maximum number of epochs for training\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning search space settings\n",
        "# 4. Number of Optuna trials for hyperparameter tuning.\n",
        "#    More trials can lead to better optimization but increase computation time.\n",
        "n_trials = 100\n",
        "\n",
        "# 5. Range for embedding dimension in hyperparameter search space.\n",
        "n_embd_min, n_embd_max = 64, 256\n",
        "\n",
        "# 6. Range for number of attention heads in transformer model during tuning.\n",
        "n_head_min, n_head_max = 2, 16\n",
        "\n",
        "# 7. Range for number of transformer layers in model architecture during optimization.\n",
        "n_layer_min, n_layer_max = 2, 16\n",
        "\n",
        "# 8. Range of batch sizes to explore. Larger batches can speed up training but may require more memory.\n",
        "batch_size_min, batch_size_max = 1, 256\n",
        "\n",
        "# 9. Range for learning rate in hyperparameter search space. Crucial for model convergence and performance.\n",
        "learning_rate_min, learning_rate_max = 1e-8, 1e-2\n",
        "\n",
        "# 10. Range for number of training epochs to consider during hyperparameter optimization.\n",
        "max_epochs_min, max_epochs_max = 3, 50\n",
        "\n",
        "# 11. Range for n_head exponent in hyperparameter search space.\n",
        "n_head_exp_min, n_head_exp_max = 1, 3\n",
        "\n",
        "# 12. Range for n_embd multiplier in hyperparameter search space.\n",
        "n_embd_multiplier_min, n_embd_multiplier_max = 4, 16\n",
        "\n",
        "# These parameters allow flexible experimentation with different model configurations and training settings,\n",
        "# enabling comprehensive exploration of the hyperparameter space for optimal model performance.\n",
        "\n",
        "# Validate manual parameters\n",
        "def validate_manual_params(params):\n",
        "    assert params[\"n_embd\"] % params[\"n_head\"] == 0, f\"n_embd ({params['n_embd']}) must be divisible by n_head ({params['n_head']})\"\n",
        "    assert params[\"n_embd\"] >= params[\"n_head\"], f\"n_embd ({params['n_embd']}) must be greater than or equal to n_head ({params['n_head']})\"\n",
        "    assert params[\"n_layer\"] > 0, f\"n_layer ({params['n_layer']}) must be positive\"\n",
        "    print(\"Manual parameters validated successfully\")\n",
        "\n",
        "\n",
        "# Validate the manual parameters\n",
        "validate_manual_params(manual_params)\n",
        "\n",
        "# Print configurations for verification\n",
        "print(\"Current Configuration:\")\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"Use Best Parameters: {use_best_params}\")\n",
        "print(f\"Manual Parameters: {manual_params}\")\n",
        "print(f\"Number of Optuna Trials: {n_trials}\")\n",
        "print(\"Hyperparameter Ranges:\")\n",
        "print(f\"  n_embd: {n_embd_min} to {n_embd_max}\")\n",
        "print(f\"  n_head: {n_head_min} to {n_head_max}\")\n",
        "print(f\"  n_layer: {n_layer_min} to {n_layer_max}\")\n",
        "print(f\"  batch_size: {batch_size_min} to {batch_size_max}\")\n",
        "print(f\"  learning_rate: {learning_rate_min} to {learning_rate_max}\")\n",
        "print(f\"  max_epochs: {max_epochs_min} to {max_epochs_max}\")\n",
        "\n",
        "# Save configuration\n",
        "import json\n",
        "\n",
        "def save_config(config, filename=\"config.json\"):\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "config = {\n",
        "    \"parent_dir\": parent_dir,\n",
        "    \"use_best_params\": use_best_params,\n",
        "    \"manual_params\": manual_params,\n",
        "    \"tuning\": {\n",
        "        \"n_trials\": n_trials,\n",
        "        \"n_embd\": (n_embd_min, n_embd_max),\n",
        "        \"n_head\": (n_head_min, n_head_max),\n",
        "        \"n_layer\": (n_layer_min, n_layer_max),\n",
        "        \"batch_size\": (batch_size_min, batch_size_max),\n",
        "        \"learning_rate\": (learning_rate_min, learning_rate_max),\n",
        "        \"max_epochs\": (max_epochs_min, max_epochs_max)\n",
        "    }\n",
        "}\n",
        "\n",
        "save_config(config)\n",
        "print(f\"Configuration saved to config.json\")\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"\\nGPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2-qicsswzi-S",
      "metadata": {
        "id": "2-qicsswzi-S"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DgxlLFBTg8Kt",
      "metadata": {
        "id": "DgxlLFBTg8Kt"
      },
      "source": [
        "### 1. Set up the Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u8k0Yxzd4RCb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u8k0Yxzd4RCb",
        "outputId": "aa0390e3-e53c-46ca-9f77-ac45d076335a"
      },
      "outputs": [],
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pRx7QZEdg8Kt",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRx7QZEdg8Kt",
        "outputId": "9b102db3-e7ac-4294-868d-f3bfb9cb33c8"
      },
      "outputs": [],
      "source": [
        "#!rm -rf /content/arc-neural-reasoning-model/\n",
        "#!git clone https://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@github.com/ImmortalDemonGod/arc-neural-reasoning-model.git\n",
        "#!pip install -r /workspaces/arc-neural-reasoning-model/gpt2_arc/requirements.txt\n",
        "#!pip install optuna\n",
        "#!pip install jupyterlab jupyterlab-optuna\n",
        "# Install the required packages\n",
        "#!pip install optuna-dashboard pyngrok\n",
        "#!ngrok config add-authtoken 2NCEuxuUBMj6zsdTokQHkYJ4AZz_3E7e2pyW87otgFg3UdSC3\n",
        "#!pip install tensorboard\n",
        "#!pip install watchdog\n",
        "#!pip install numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bhpQwlBojS1S",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhpQwlBojS1S",
        "outputId": "a2b3d0e5-37d3-421a-ff20-b9e2bd721d49"
      },
      "outputs": [],
      "source": [
        "%cd /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
        "#!rm -rf /content/arc-neural-reasoning-model/arc_sat_solver\n",
        "#!rm -rf /content/arc-neural-reasoning-model/benchmark_results\n",
        "#!rm -rf /content/arc-neural-reasoning-model/checkpoints\n",
        "#!rm -rf /content/arc-neural-reasoning-model/tmp\n",
        "#!pip install -e ."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iz4dsrstg8Ku",
      "metadata": {
        "id": "Iz4dsrstg8Ku"
      },
      "source": [
        "### 2. Run hyperparameter tuning (in the background) click the **links** to see the dashboards:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6zIQhT9m9_wv",
      "metadata": {
        "id": "6zIQhT9m9_wv"
      },
      "source": [
        "make sure to set the directory in your drive to save the files in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DVIRPnKcPV92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVIRPnKcPV92",
        "outputId": "cd8d8350-a4d5-4b47-cc05-40e0eefc7b0c"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date in YYYYMMDD format\n",
        "current_date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "\n",
        "# Create a folder named after the current date\n",
        "date_folder = os.path.join(parent_dir, current_date)\n",
        "if not os.path.exists(date_folder):\n",
        "    os.makedirs(date_folder)\n",
        "\n",
        "# Change into the newly created folder\n",
        "%cd {date_folder}\n",
        "\n",
        "# Now all your operations will save to /content/YYYYMMDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7xyBrsMGg8Kv",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xyBrsMGg8Kv",
        "outputId": "fe9c106f-a2b6-49df-e656-8141182e9fed"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "# Run the optimization command using the new date folder for storage in the background\n",
        "# The user-defined hyperparameters are passed here dynamically\n",
        "optimize_command = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/optimize_hyperparameters.py\"),\n",
        "    \"--n_trials\", str(n_trials),\n",
        "    \"--storage\", f\"sqlite:///{date_folder}/optuna_results.db\",\n",
        "    \"--n_embd_min\", str(n_embd_min), \"--n_embd_max\", str(n_embd_max),\n",
        "    \"--n_head_min\", str(n_head_min), \"--n_head_max\", str(n_head_max),\n",
        "    \"--n_layer_min\", str(n_layer_min), \"--n_layer_max\", str(n_layer_max),\n",
        "    \"--batch_size_min\", str(batch_size_min), \"--batch_size_max\", str(batch_size_max),\n",
        "    \"--learning_rate_min\", str(learning_rate_min), \"--learning_rate_max\", str(learning_rate_max),\n",
        "    \"--max_epochs_min\", str(max_epochs_min), \"--max_epochs_max\", str(max_epochs_max),\n",
        "    \"--n_head_exp_min\", str(n_head_exp_min), \"--n_head_exp_max\", str(n_head_exp_max),\n",
        "    \"--n_embd_multiplier_min\", str(n_embd_multiplier_min), \"--n_embd_multiplier_max\", str(n_embd_multiplier_max)\n",
        "]\n",
        "subprocess.Popen(optimize_command)\n",
        "\n",
        "# Import ngrok and set up the tunnel\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start the ngrok tunnel for port 8081\n",
        "public_url = ngrok.connect(8081)\n",
        "print(f\"Optuna dashboard is accessible at: {public_url}\")\n",
        "\n",
        "# Run the Optuna dashboard in the background without blocking\n",
        "optuna_command = [\n",
        "    \"optuna-dashboard\", \"--port\", \"8081\", f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "]\n",
        "subprocess.Popen(optuna_command)\n",
        "\n",
        "# Specify the folder for logs (using date_folder)\n",
        "log_dir = f\"{date_folder}/runs\"\n",
        "\n",
        "# Run TensorBoard in the background without blocking\n",
        "tensorboard_command = [\n",
        "    \"tensorboard\", \"--logdir\", log_dir, \"--port\", \"6006\"\n",
        "]\n",
        "subprocess.Popen(tensorboard_command)\n",
        "\n",
        "# Set up the ngrok tunnel for TensorBoard\n",
        "public_url_tb = ngrok.connect(6006)\n",
        "print(f\"TensorBoard is accessible at: {public_url_tb}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb75DkIFg8Kw",
      "metadata": {
        "id": "gb75DkIFg8Kw"
      },
      "source": [
        "### 3. Get the best hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda44504",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fda44504",
        "outputId": "6c6196cb-1072-4d71-a081-3367966411ac"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "from gpt2_arc.src.optimize_hyperparameters import run_optimization\n",
        "\n",
        "# Set Optuna storage and study details\n",
        "storage_name = f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "study_name = \"gpt2_arc_optimization\"\n",
        "\n",
        "try:\n",
        "    # List all study names in the database\n",
        "    study_summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
        "    print(\"Available studies in the database:\")\n",
        "    for study_summary in study_summaries:\n",
        "        print(f\"- {study_summary.study_name}\")\n",
        "\n",
        "    # Load the specified study\n",
        "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
        "    best_params = study.best_params\n",
        "    print(\"Best hyperparameters:\")\n",
        "    print(json.dumps(best_params, indent=2))\n",
        "\n",
        "    # Save the best parameters to a JSON file\n",
        "    with open(f\"{date_folder}/best_hyperparameters.json\", \"w\") as f:\n",
        "        json.dump(best_params, f)\n",
        "\n",
        "except KeyError as e:\n",
        "    print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
        "    print(f\"Details: {str(e)}\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ba232c",
      "metadata": {
        "id": "f2ba232c"
      },
      "source": [
        "### 4. Use the best hyperparameters for longer training (manually set max epochs!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m8uLi0U5l6QL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m8uLi0U5l6QL",
        "outputId": "cdc0d029-04a1-4ceb-df7b-3b0a4a85627e"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "\n",
        "# Load best hyperparameters from the JSON file if use_best_params is True\n",
        "if use_best_params:\n",
        "    try:\n",
        "        with open(f\"{date_folder}/best_hyperparameters.json\", \"r\") as f:\n",
        "            best_params = json.load(f)\n",
        "        # Extract hyperparameters from the JSON file\n",
        "        params = {\n",
        "            \"n_embd\": best_params.get(\"n_embd\", manual_params[\"n_embd\"]),\n",
        "            \"n_head\": best_params.get(\"n_head\", manual_params[\"n_head\"]),\n",
        "            \"n_layer\": best_params.get(\"n_layer\", manual_params[\"n_layer\"]),\n",
        "            \"batch_size\": best_params.get(\"batch_size\", manual_params[\"batch_size\"]),\n",
        "            \"learning_rate\": best_params.get(\"learning_rate\", manual_params[\"learning_rate\"]),\n",
        "            \"max_epochs\": best_params.get(\"max_epochs\", manual_params[\"max_epochs\"])\n",
        "        }\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {date_folder}/best_hyperparameters.json not found. Using manual parameters.\")\n",
        "        params = manual_params\n",
        "else:\n",
        "    # Use manually defined parameters\n",
        "    params = manual_params\n",
        "\n",
        "# Build the arguments for the training command\n",
        "train_args = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/training/train.py\"),\n",
        "    \"--n-embd\", str(params[\"n_embd\"]),\n",
        "    \"--n-head\", str(params[\"n_head\"]),\n",
        "    \"--n-layer\", str(params[\"n_layer\"]),\n",
        "    \"--batch-size\", str(params[\"batch_size\"]),\n",
        "    \"--learning-rate\", str(params[\"learning_rate\"]),\n",
        "    \"--max-epochs\", str(params[\"max_epochs\"]),\n",
        "    \"--use-gpu\",\n",
        "    \"--project\", \"arc-scaling-test\"\n",
        "]\n",
        "\n",
        "# Execute the training script in the background\n",
        "print(\"Starting training process in the background...\")\n",
        "with open(f\"{date_folder}/training_output.log\", \"w\") as log_file:\n",
        "    process = subprocess.Popen(train_args, stdout=log_file, stderr=subprocess.STDOUT)\n",
        "\n",
        "print(f\"Training process started with PID: {process.pid}\")\n",
        "print(f\"You can monitor the training progress in {date_folder}/training_output.log\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5062c297",
      "metadata": {
        "id": "5062c297"
      },
      "source": [
        "### 5. Evaluate the trained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pwrd3HMBnhW",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pwrd3HMBnhW",
        "outputId": "e7020464-f37e-4d1c-ffed-06bfeb029552"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import wandb\n",
        "import time\n",
        "from watchdog.observers import Observer\n",
        "from watchdog.events import FileSystemEventHandler\n",
        "import subprocess\n",
        "\n",
        "# Set W&B API key (replace with your actual API key)\n",
        "wandb_api_key = \"2b06e99af167044b281668f6edd388c633aba1a0\"  # Replace with your W&B API key\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "\n",
        "# Define the date_folder variable appropriately\n",
        "# For example, you can set it based on the current date\n",
        "from datetime import datetime\n",
        "date_folder = datetime.now().strftime(\"%Y%m%d\")\n",
        "\n",
        "# Directory containing the model files\n",
        "model_dir = os.path.join(arc_model_dir, \"EXPERIMENTAL\")  # Ensure this path is correct\n",
        "print(f\"Watching for new models in directory: {model_dir}\")\n",
        "output_dir = \"evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "wandb_project = \"arc-evaluation\"\n",
        "\n",
        "# Set of evaluated models\n",
        "evaluated_models = set()\n",
        "\n",
        "class CheckpointHandler(FileSystemEventHandler):\n",
        "    def on_created(self, event):\n",
        "        if event.is_directory:\n",
        "            return\n",
        "        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):\n",
        "            print(f\"New checkpoint detected: {event.src_path}\")\n",
        "            self.evaluate_model(event.src_path)\n",
        "\n",
        "    def evaluate_model(self, model_path):\n",
        "        model_file = os.path.basename(model_path)\n",
        "\n",
        "        if model_file in evaluated_models:\n",
        "            print(f\"Skipping already evaluated model: {model_file}\")\n",
        "            return  # Skip if the model was already evaluated\n",
        "\n",
        "        # Extract epoch and val_loss from the filename for run_name\n",
        "        # Assuming the filename pattern arc_model-epoch=<number>-val_loss=<number>.ckpt\n",
        "        try:\n",
        "            parts = model_file.replace('.ckpt', '').split('-')\n",
        "            epoch = None\n",
        "            val_loss = None\n",
        "            for part in parts:\n",
        "                if part.startswith('epoch='):\n",
        "                    epoch = part.split('=')[1]\n",
        "                elif part.startswith('val_loss='):\n",
        "                    val_loss = part.split('=')[1]\n",
        "            if epoch is not None and val_loss is not None:\n",
        "                run_name = f\"scaling-test-evaluation-epoch{epoch}-val_loss{val_loss}\"\n",
        "            else:\n",
        "                run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing run name from filename {model_file}: {e}\")\n",
        "            run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "\n",
        "        eval_command = [\n",
        "            \"python\", \"/workspaces/arc-neural-reasoning-model/gpt2_arc/src/evaluate.py\",\n",
        "            \"--model_checkpoint\", model_path,\n",
        "            \"--batch_size\", \"32\",\n",
        "            \"--output_dir\", output_dir,\n",
        "            \"--wandb_project\", wandb_project,\n",
        "            \"--wandb_run_name\", run_name\n",
        "        ]\n",
        "\n",
        "        print(f\"Evaluating model: {model_file} with command: {eval_command}\")\n",
        "        try:\n",
        "            # Run the evaluation command and capture stdout and stderr\n",
        "            result = subprocess.run(\n",
        "                eval_command,\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True  # Automatically decode bytes to string\n",
        "            )\n",
        "            print(f\"Successfully evaluated model: {model_file}\")\n",
        "            print(\"Evaluation Output:\")\n",
        "            print(result.stdout)  # Print the standard output from evaluate.py\n",
        "            if result.stderr:\n",
        "                print(\"Evaluation Errors/Warnings:\")\n",
        "                print(result.stderr)  # Print any errors or warnings from evaluate.py\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during evaluation of {model_file}: {e}\")\n",
        "            print(\"Standard Output:\")\n",
        "            print(e.stdout)  # Print the standard output even if there's an error\n",
        "            print(\"Standard Error:\")\n",
        "            print(e.stderr)  # Print the error messages\n",
        "        except Exception as ex:\n",
        "            print(f\"An unexpected error occurred while evaluating {model_file}: {ex}\")\n",
        "\n",
        "        evaluated_models.add(model_file)\n",
        "\n",
        "def get_all_checkpoint_files(directory):\n",
        "    print(f\"Checking directory for .ckpt and .pth files: {directory}\")\n",
        "    checkpoint_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])\n",
        "    print(f\"Found checkpoint files: {checkpoint_files}\")\n",
        "    return checkpoint_files\n",
        "\n",
        "# Set up and start the watchdog observer\n",
        "event_handler = CheckpointHandler()\n",
        "observer = Observer()\n",
        "observer.schedule(event_handler, model_dir, recursive=True)\n",
        "observer.start()\n",
        "\n",
        "print(\"Watching for new checkpoints and final models in all subdirectories...\")\n",
        "print(\"This script will continue running until you stop it manually.\")\n",
        "print(\"You can stop it by interrupting the process when training is complete.\")\n",
        "\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(10)\n",
        "\n",
        "        # Check for any new models\n",
        "        current_models = set(f for f in get_all_checkpoint_files(model_dir))\n",
        "        new_models = current_models - evaluated_models\n",
        "\n",
        "        print(f\"Current models: {current_models}\")\n",
        "        print(f\"New models to evaluate: {new_models}\")\n",
        "\n",
        "        for model_path in new_models:\n",
        "            event_handler.evaluate_model(model_path)\n",
        "\n",
        "except KeyboardInterrupt:\n",
        "    observer.stop()\n",
        "\n",
        "observer.join()\n",
        "print(\"Checkpoint and final model evaluation completed.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L25zvW0ig8K1",
      "metadata": {
        "id": "L25zvW0ig8K1"
      },
      "source": [
        "### 6. Analyze the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJhLC5Ttg8K1",
      "metadata": {
        "id": "mJhLC5Ttg8K1"
      },
      "outputs": [],
      "source": [
        "#import json\n",
        "\n",
        "# Load and print evaluation results\n",
        "#with open(\"./evaluation_results/scaling-test-evaluation_results.json\", \"r\") as f:\n",
        "#    results = json.load(f)\n",
        "\n",
        "#print(\"Evaluation Results:\")\n",
        "#print(json.dumps(results, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOEvyN3LCVdV",
      "metadata": {
        "id": "LOEvyN3LCVdV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "0L9IwEdxzd-H",
        "Iz4dsrstg8Ku",
        "gb75DkIFg8Kw",
        "f2ba232c",
        "L25zvW0ig8K1"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
