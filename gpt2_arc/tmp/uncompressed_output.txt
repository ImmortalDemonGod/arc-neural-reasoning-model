<source type="local_directory" path="/workspaces/arc-neural-reasoning-model/gpt2_arc">
<file name="benchmark.py">
# gp2_arc/benchmark.py
import sys
import os

# Add the project root directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch._dynamo
import csv
import uuid
from datetime import datetime
import os
import torch
from torch.utils.data import DataLoader
import arckit
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import ModelConfig
import time
from torch.amp import autocast
import psutil
import logging
import argparse
import statistics
import numpy as np
from scipy import stats

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Dynamically adjustable baseline values for CPU, GPU, and MPS
BASELINES = {
    'cpu': {'total_time': 1.6391, 'grids_per_second': 199.27},
    'cuda': {'total_time': 0.0481, 'grids_per_second': 13774.98},
    'mps': {'total_time': 0.0481, 'grids_per_second': 13774.98}  # Updated baselines for MPS
}

def benchmark_model(model, dataset, batch_size=1, num_batches=1, num_runs=1, device_type='cpu', precision='medium', model_checkpoint=None):
    print(f"Starting benchmark_model with parameters: batch_size={batch_size}, num_batches={num_batches}, device_type={device_type}, precision={precision}, model_checkpoint={model_checkpoint}")

    if device_type not in ['cpu', 'cuda', 'mps']:
        raise ValueError("Invalid device type")

    if len(dataset) == 0:
        raise ValueError("Dataset is empty")

    checkpoint_used = False
    checkpoint_info = {}
    if model_checkpoint:
        checkpoint = torch.load(model_checkpoint)
        model_config = ModelConfig(**checkpoint['config'])
        model = GPT2ARC(model_config)
        state_dict = {k.replace("model.", ""): v for k, v in checkpoint['state_dict'].items()}
        model.load_state_dict(state_dict)
        model.to(device_type)
        model.eval()
        checkpoint_used = True
        checkpoint_info = {
            'checkpoint_path': model_checkpoint,
            'config': checkpoint['config'],
            'state_dict_keys': list(checkpoint['state_dict'].keys())
        }
    run_id = str(uuid.uuid4())
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    practical_threshold = 20.0  # Define a threshold for practical significance
    total_time_runs = []
    grids_per_second_runs = []

    cpu_usages = []
    memory_usages = []

    run_results = []  # Initialize run_results to store each run's data
    gpu_usages = []  # Initialize gpu_usages to store GPU utilization data

    # Set the float32 matmul precision
    torch.set_float32_matmul_precision(precision)

    # Select device based on the argument (including support for MPS)
    device = torch.device("cuda" if device_type == "cuda" and torch.cuda.is_available() else
                          "mps" if device_type == "mps" and torch.backends.mps.is_available() else "cpu")
    model = model.to(device)
    torch._dynamo.config.suppress_errors = True
    if device.type == "cpu":
        compiled_model = model  # Use the model directly for CPU
    else:
        try:
            if device.type != "mps":
                compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
            else:
                compiled_model = model  # Use the model directly for MPS
        except ImportError as e:
            logger.warning(f"Compilation failed with error: {e}. Falling back to eager execution.")
            compiled_model = model

    try:
        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=ARCDataset.collate_fn)
        total_time = 0.0
        total_grids = 0

        for i, batch in enumerate(dataloader):
            if i &gt;= num_batches:
                break
            print(f"Processing batch {i+1}/{num_batches}")

            logger.debug(f"Batch content before unpacking: {batch}")
            if len(batch) != 3:
                raise ValueError(f"Unexpected batch format. Expected 3 items, got {len(batch)}")
            inputs, outputs, task_ids = batch

            print(f"Inputs type: {type(inputs)}")
            if hasattr(inputs, 'shape'):
                print(f"Inputs shape: {inputs.shape}")
            else:
                print("Inputs shape: N/A")
            print(f"Outputs type: {type(outputs)}, shape: {outputs.shape if torch.is_tensor(outputs) else 'N/A'}")
            print(f"Task IDs: {task_ids}")

            if inputs is None or not isinstance(inputs, torch.Tensor):
                raise ValueError(f"Expected inputs to be a torch.Tensor, got {type(inputs)}")

            if inputs.numel() == 0:
                raise ValueError("Inputs tensor is empty")
            print(f"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}, Task IDs: {task_ids}")
            
            if inputs.dim() == 2:
                # If inputs is 2D (batch_size, sequence_length), reshape it to 4D
                height = width = int(inputs.size(1)**0.5)
                inputs = inputs.view(inputs.size(0), 1, height, width)
            elif inputs.dim() == 3:
                # If inputs is 3D (batchText preprocessing completed with XML structure 
preserved.

Compressed Token Count: 49279
Uncompressed Token Count: 58656

compressed_output.txt and uncompressed_output.txt have 
been created in the working directory.

An error occurred: 
    Pyperclip could not find a copy/paste mechanism for
your system.
    For more information, please visit https://pypercli
p.readthedocs.io/en/latest/index.html#not-implemented-e
rror 

Please check your input and try again.
Processing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00
Traceback (most recent call last):
  File "/workspaces/arc-neural-reasoning-model/tmp/1filellm/onefilellm.py", line 678, in <module>
    main()
  File "/workspaces/arc-neural-reasoning-model/tmp/1filellm/onefilellm.py", line 669, in main
    pyperclip.copy(uncompressed_text)
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyperclip/__init__.py", line 659, in lazy_load_stub_copy
    return copy(text)
           ^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyperclip/__init__.py", line 336, in __call__
    raise PyperclipException(EXCEPT_MSG)
pyperclip.PyperclipException: 
    Pyperclip could not find a copy/paste mechanism for your system.
    For more information, please visit https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error 
oking the model with inputs and attention_mask")
                with torch.no_grad():
                    if device.type == 'cuda':
                        with autocast(device_type=device.type, dtype=torch.float16):
                            compiled_model(inputs, attention_mask)
                    else:
                        compiled_model(inputs, attention_mask)

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                end_time = time.time()

                batch_time = end_time - start_time
                print(f"Batch time: {batch_time}")
                if batch_time &lt;= 0:
                    print(f"WARNING: Invalid batch time: {batch_time}. Skipping this batch.")
                    continue
                total_time += batch_time
                total_grids += len(inputs)
    except Exception as e:
        logger.error(f"An error occurred during benchmarking: {e}")
        raise

    print(f"Benchmark completed. Total time: {total_time}, Total grids: {total_grids}")
    # Calculate average and standard deviation for the runs
    num_runs = len(total_time_runs)
    avg_total_time = np.mean(total_time_runs)
    std_total_time = np.std(total_time_runs)
    avg_grids_per_second = np.mean(grids_per_second_runs)
    std_grids_per_second = np.std(grids_per_second_runs)
    if total_time &gt; 0:
        grids_per_second = total_grids / total_time
    else:
        grids_per_second = 0.0  # Avoid division by zero
        logger.warning("Total time is zero. Setting grids_per_second to 0.0 to avoid division by zero.")

    logger.info(f"Total Time: {total_time:.4f} seconds, Grids per Second: {grids_per_second:.2f}")
    
    # Store the results of the run
    run_results.append({
        'run_id': run_id,
        'datetime': current_time,
        'total_time': total_time,
        'grids_per_second': grids_per_second,
        'cpu_usage': np.mean(cpu_usages),
        'memory_usage': np.mean(memory_usages),
        'gpu_usage': np.mean(gpu_usages) if gpu_usages else None,
        'batch_size': batch_size,
        'num_batches': num_batches,
        'device': device.type,
        'n_embd': model.config.n_embd,
        'n_head': model.config.n_head,
        'n_layer': model.config.n_layer,
        'precision': precision,  # Add precision here
        'checkpoint_used': checkpoint_used,
        'checkpoint_info': checkpoint_info
    })

    total_time_runs.append(total_time)
    grids_per_second_runs.append(grids_per_second)

    if total_time &lt;= 0 or total_grids &lt;= 0:
        logger.warning(f"ERROR: Invalid total time ({total_time}) or total grids ({total_grids}). Check the benchmark implementation.")
        return 0.0, 0.0  # Return sensible defaults instead of infinity

    avg_total_time = total_time
    avg_grids_per_second = total_grids / total_time if total_time &gt; 0 else 0.0

    logger.info(f"Total Time: {avg_total_time:.4f} seconds, Grids per Second: {avg_grids_per_second:.2f}")


    # Perform statistical analysis (confidence intervals, effect size, etc.)
    confidence_level = 0.95
    z_score = stats.norm.ppf((1 + confidence_level) / 2)

    ci_total_time = z_score * (std_total_time / np.sqrt(num_runs))
    ci_grids_per_second = z_score * (std_grids_per_second / np.sqrt(num_runs))

    effect_size_time = (avg_total_time - BASELINES[device.type]['total_time']) / std_total_time
    effect_size_grids = (avg_grids_per_second - BASELINES[device.type]['grids_per_second']) / std_grids_per_second

    # Calculate improvements and regressions based on averages
    time_improvement = BASELINES[device.type]['total_time'] - avg_total_time
    time_improvement_percent = (time_improvement / BASELINES[device.type]['total_time']) * 100
    time_regression = avg_total_time - BASELINES[device.type]['total_time']
    time_regression_percent = (time_regression / BASELINES[device.type]['total_time']) * 100

    grids_per_second_improvement = avg_grids_per_second - BASELINES[device.type]['grids_per_second']
    grids_per_second_improvement_percent = (grids_per_second_improvement / BASELINES[device.type]['grids_per_second']) * 100
    grids_per_second_regression = BASELINES[device.type]['grids_per_second'] - avg_grids_per_second
    grids_per_second_regression_percent = (grids_per_second_regression / BASELINES[device.type]['grids_per_second']) * 100

    # Determine if there was an improvement
    improvement_time = avg_total_time &lt; BASELINES[device.type]['total_time']
    improvement_grids = avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']

    # Log improvements or regressions based on averages
    if avg_total_time &lt; BASELINES[device.type]['total_time']:
        logger.info(f"Improvement in average total time: -{time_improvement:.4f} seconds ({time_improvement_percent:.2f}%)")
    else:
        logger.info(f"Regression in average total time: +{time_regression:.4f} seconds ({time_regression_percent:.2f}%)")

    if avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']:
        logger.info(f"Improvement in average grids per second: +{grids_per_second_improvement:.2f} ({grids_per_second_improvement_percent:.2f}%)")
    else:
        logger.info(f"Regression in average grids per second: -{grids_per_second_regression:.2f} ({grids_per_second_regression_percent:.2f}%)")

    # Update practical significance checks
    practical_significance_time = time_improvement_percent &gt;= practical_threshold
    practical_significance_grids = grids_per_second_improvement_percent &gt;= practical_threshold

    # Log practical significance
    if improvement_time:
        if practical_significance_time:
            logger.info("The improvement in average total time is practically significant.")
        else:
            logger.info("The improvement in average total time is not practically significant.")
    else:
        if practical_significance_time:
            logger.info("The regression in average total time is practically significant.")
        else:
            logger.info("The regression in average total time is not practically significant.")

    if improvement_grids:
        if practical_significance_grids:
            logger.info("The improvement in average grids per second is practically significant.")
        else:
            logger.info("The improvement in average grids per second is not practically significant.")
    else:
        if practical_significance_grids:
            logger.info("The regression in average grids per second is practically significant.")
        else:
            logger.info("The regression in average grids per second is not practically significant.")

    # Perform a one-sample t-test
    t_stat_time, p_value_time = stats.ttest_1samp(total_time_runs, BASELINES[device.type]['total_time'])
    t_stat_grids, p_value_grids = stats.ttest_1samp(grids_per_second_runs, BASELINES[device.type]['grids_per_second'])

    logger.info(f"T-Test for total time: t-statistic = {t_stat_time:.4f}, p-value = {p_value_time:.4f}")
    logger.info(f"T-Test for grids per second: t-statistic = {t_stat_grids:.4f}, p-value = {p_value_grids:.4f}")

    # Log the results including confidence intervals
    logger.info(f"Run Summary:")
    logger.info(f" • Avg Total Time: {avg_total_time:.4f}s (CI 95%: ±{ci_total_time:.4f}s)")
    logger.info(f" • Avg Grids per Second: {avg_grids_per_second:.2f} (CI 95%: ±{ci_grids_per_second:.2f})")
    logger.info(f" • Effect Size (Total Time): {effect_size_time:.4f}, Effect Size (Grids per Second): {effect_size_grids:.4f}")

    # Determine if there was an improvement
    improvement_time = avg_total_time &lt; BASELINES[device.type]['total_time']
    improvement_grids = avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']
    csv_file_path = 'benchmark_results.csv'
    file_exists = os.path.isfile(csv_file_path)
    with open(csv_file_path, 'a', newline='') as csvfile:
        fieldnames = [
            'run_id', 'datetime', 'run', 'total_time', 'grids_per_second', 'cpu_usage', 'memory_usage',
            'batch_size', 'num_batches', 'device', 'n_embd', 'n_head', 'n_layer', 'gpu_usage', 'precision',
            'checkpoint_used', 'checkpoint_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        for result in run_results:
            writer.writerow(result)

    # Write statistical summary to CSV
    stats_csv_file_path = 'benchmark_statistics.csv'
    stats_file_exists = os.path.isfile(stats_csv_file_path)
    with open(stats_csv_file_path, 'a', newline='') as csvfile:
        fieldnames = [
            'run_id', 'datetime', 'avg_total_time', 'std_total_time', 'ci_total_time',
            'avg_grids_per_second', 'std_grids_per_second', 'ci_grids_per_second',
            'effect_size_time', 'effect_size_grids', 'percent_change_time', 'percent_change_grids',
            't_stat_time', 'p_value_time', 't_stat_grids', 'p_value_grids',
            'improvement_time', 'improvement_grids',
            'practical_significance_time', 'practical_significance_grids', 'precision',
            'checkpoint_used', 'checkpoint_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not stats_file_exists:
            writer.writeheader()
        writer.writerow({
            'run_id': run_id,
            'datetime': current_time,
            'avg_total_time': avg_total_time,
            'std_total_time': std_total_time,
            'ci_total_time': ci_total_time,
            'avg_grids_per_second': avg_grids_per_second,
            'std_grids_per_second': std_grids_per_second,
            'ci_grids_per_second': ci_grids_per_second,
            'effect_size_time': effect_size_time,
            'effect_size_grids': effect_size_grids,
            'percent_change_time': time_improvement_percent if improvement_time else time_regression_percent,
            'percent_change_grids': grids_per_second_improvement_percent if improvement_grids else grids_per_second_regression_percent,
            't_stat_time': t_stat_time,
            'p_value_time': p_value_time,
            't_stat_grids': t_stat_grids,
            'p_value_grids': p_value_grids,
            'improvement_time': improvement_time,
            'improvement_grids': improvement_grids,
            'practical_significance_time': practical_significance_time,
            'practical_significance_grids': practical_significance_grids,
            'precision': precision,  # Add precision here
            'checkpoint_used': checkpoint_used,
            'checkpoint_info': checkpoint_info
        })

    print(f"Benchmark completed. Final results - avg_time: {avg_total_time}, avg_grids: {avg_grids_per_second}")
    return avg_total_time, avg_grids_per_second


def main(args):
    print(f"Starting main function with args: {args}")
    # Set the float32 matmul precision
    torch.set_float32_matmul_precision(args.precision)
    train_set, _ = arckit.load_data()
    full_dataset = ARCDataset(train_set, is_test=False)

    # Create the model configuration
    model_config = ModelConfig(n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer)
    model = GPT2ARC(model_config)

    # Run the benchmark for different configurations
    for run_num in range(args.num_full_runs):
        logger.info(f"Starting full benchmark run {run_num + 1}/{args.num_full_runs}")
        avg_time, avg_grids = benchmark_model(
            model, full_dataset, batch_size=args.batch_size, num_batches=args.num_batches, num_runs=args.num_runs, device_type=args.device, precision=args.precision, model_checkpoint=args.model_checkpoint
        )
        logger.info(f"Full run {run_num + 1} - Avg Time: {avg_time:.4f}s, Avg Grids per Second: {avg_grids:.2f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Benchmark the GPT2ARC model.")
    parser.add_argument('--model_checkpoint', type=str, help='Path to the model checkpoint')
    parser.add_argument('--num-runs', type=int, default=20, help='Number of runs for each configuration')
    parser.add_argument('--num-full-runs', type=int, default=1, help='Number of full configurations to run')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for each run')
    parser.add_argument('--num-batches', type=int, default=10, help='Number of batches per run')
    parser.add_argument('--n-embd', type=int, default=64, help='Number of embeddings for the model')
    parser.add_argument('--n-head', type=int, default=2, help='Number of attention heads')
    parser.add_argument('--n-layer', type=int, default=1, help='Number of layers')
    parser.add_argument('--device', choices=['cpu', 'cuda', 'mps'], default='cpu', help='Device to run the benchmark on (cpu, cuda, or mps)')
    parser.add_argument('--precision', choices=['highest', 'high', 'medium'], default='highest', help='Precision level for float32 matrix multiplications')
    
    args = parser.parse_args()
    main(args)

</file>
<file name="setup.py">
from setuptools import setup

setup()

</file>
<file name="requirements.txt">
aider-chat
torch&gt;=2.0.0
transformers&gt;=4.0.0
pytorch-lightning&gt;=2.0.0
numpy&gt;=1.20.0
pytest&gt;=6.0
pytest-cov&gt;=2.0
black&gt;=20.8b1
isort&gt;=5.0
flake8&gt;=3.9
ruff
scipy
psutil
ultralytics-thop
mypy
pynvml
tox
tensorboard
arckit
urllib3&gt;=1.26.0
chardet&gt;=5.0.0
wandb
</file>
<file name="README.md">
# GPT-2 ARC Neural Reasoning Model

This project implements a neural reasoning model based on the GPT-2 architecture to solve tasks from the Abstraction and Reasoning Corpus (ARC) challenge.

## Features

- **Data Handling**: Utilizes a custom `ArcDataset` class for handling and preprocessing ARC data.
- **Model Architecture**: Implements a `GPT2ARC` model leveraging the pre-trained GPT-2 architecture.
- **Training**: Includes a `train.py` script for training the model using PyTorch Lightning, with support for logging and checkpointing.
- **Testing**: Comprehensive test suite using `pytest` to ensure model and data integrity.

## Installation

Clone the repository and install the required packages:

```bash
git clone https://github.com/yourusername/arc-neural-reasoning-model.git
cd arc-neural-reasoning-model
pip install -e .
```

For development, install the extra dependencies:

```bash
pip install -e ".[dev]"
```

## Usage

### Training the Model

To train the model, use the following command:

```
python src/train.py --train_data path/to/train_data --val_data path/to/val_data --batch_size 32 --learning_rate 1e-4 --max_epochs 10 --use_gpu
```

Adjust the parameters as needed. The trained model checkpoints will be saved in the `checkpoints` directory.

### Evaluating the Model

To evaluate a trained model on a test set, use the following command:

```
python src/evaluate.py --test_data path/to/test_data --model_checkpoint path/to/model_checkpoint.ckpt --batch_size 32
```

This will output the evaluation metrics for the model on the test dataset.

## Running Tests

To run the tests, use the following command:

```
pytest -v
```

This will run all tests and display the results, including test coverage.

## Contributing

[Add contribution guidelines here]

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

</file>
<file name="tests/test_model_evaluation.py">
# gpt2_arc/tests/test_model_evaluation.py
import sys
import os

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
sys.path.insert(0, project_root)
print("Current PYTHONPATH:", sys.path)

import sys
import os
import json
import pytest
import torch
from pytest_mock import mocker
from src.models.gpt2 import GPT2ARC
from src.config import Config, ModelConfig, TrainingConfig
from torch.utils.data import DataLoader
from src.utils.helpers import differential_pixel_accuracy
from src.training.trainer import ARCTrainer

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)
print(f"Updated Python path: {sys.path}")

@pytest.fixture
def trainer():
    model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    return ARCTrainer(model, None, None, config)
import logging
from unittest.mock import Mock


# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@pytest.fixture
def model(mocker):
    mock_model = mocker.Mock()
    mock_model.eval = mocker.Mock()
    mock_model.side_effect = lambda inputs, attention_mask=None: torch.randn(1, 4, 2, 2)
    logger.debug(f"Created mock model")
    return mock_model

@pytest.fixture
def inputs():
    # Use a predetermined input
    inputs = torch.tensor([[[[1.0, 0.0], [0.0, 1.0]]]])
    logger.debug(f"Input shape: {inputs.shape}, dtype: {inputs.dtype}")
    return inputs

@pytest.fixture
def targets():
    # Use a predetermined target
    targets = torch.tensor([[[1, 0], [0, 1]]])
    logger.debug(f"Targets shape: {targets.shape}, dtype: {targets.dtype}")
    return targets

@pytest.fixture
def attention_mask():
    mask = torch.ones(1, 4)
    logger.debug(f"Attention mask shape: {mask.shape}, dtype: {mask.dtype}")
    return mask

@pytest.fixture
def dataloader(inputs, targets, attention_mask):
    dataset = list(zip(inputs, targets, attention_mask))
    loader = DataLoader(dataset, batch_size=1)
    logger.debug(f"Dataloader created with {len(loader)} batches")
    return loader

def test_no_grad_calculation(model, inputs, attention_mask):
    logger.debug("Starting test_no_grad_calculation")
    with torch.no_grad():
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Output shape: {outputs.shape}, requires_grad: {outputs.requires_grad}")
        assert not outputs.requires_grad, "Gradients should not be tracked in evaluation mode."

def test_data_loop_for_evaluation(model, dataloader):
    logger.debug("Starting test_data_loop_for_evaluation")
    model.eval()
    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Batch {batch_idx}: Input shape: {inputs.shape}, Output shape: {outputs.shape}")
        assert outputs is not None, f"Model returned None for batch {batch_idx}"
        assert outputs.shape == (1, 4, 2, 2), f"Expected output shape (1, 4, 2, 2), got {outputs.shape}"

def test_model_predictions(model, inputs, attention_mask):
    logger.debug("Starting test_model_predictions")
    outputs = model(inputs, attention_mask=attention_mask)
    logger.debug(f"Model output shape: {outputs.shape}, dtype: {outputs.dtype}")
    initial_output = model(inputs, attention_mask=attention_mask)
    logger.debug(f"Initial output shape: {initial_output.shape}")
    
    # Change input and check if output changes
    modified_inputs = inputs + 1
    modified_output = model(modified_inputs, attention_mask=attention_mask)
    logger.debug(f"Modified output shape: {modified_output.shape}")
    
    assert not torch.allclose(initial_output, modified_output), "Output should change when input changes"

@pytest.mark.skip(reason="Needs to be checked against a known value from the ARC data")
def test_standard_pixel_accuracy(model, inputs, targets):
    logger.debug("Starting test_standard_pixel_accuracy")
    outputs = model(inputs)
    logger.debug(f"Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
    outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
    predicted = outputs.argmax(dim=1)
    accuracy = (predicted == targets).float().mean().item()
    logger.debug(f"Calculated accuracy: {accuracy}")
    assert 0.0 &lt;= accuracy &lt;= 1.0, f"Accuracy should be between 0 and 1, got {accuracy}"
    
    # Test with known values
    known_outputs = torch.FloatTensor([[[[0.9, 0.1], [0.1, 0.9]]]])
    known_targets = torch.tensor([[[0, 1], [1, 0]]])
    known_accuracy = (known_outputs.argmax(dim=1) == known_targets).float().mean().item()
    logger.debug(f"Known accuracy: {known_accuracy}")
    assert known_accuracy == 1.0, f"Expected known accuracy to be 1.0, got {known_accuracy}"

def test_differential_pixel_accuracy(model, inputs, targets):
    logger.debug("Starting test_differential_pixel_accuracy")
    outputs = model(inputs)
    logger.debug(f"Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
    outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
    predicted = outputs.argmax(dim=1)
    diff_accuracy, _, _ = differential_pixel_accuracy(inputs, targets, predicted)
    logger.debug(f"Calculated differential accuracy: {diff_accuracy}")
    assert 0.0 &lt;= diff_accuracy &lt;= 1.0, f"Differential pixel accuracy should be between 0 and 1, got {diff_accuracy}"

    # Test with known values
    known_inputs = torch.tensor([[[[1, 0], [0, 1]]]])
    known_targets = torch.tensor([[[0, 1], [1, 0]]])
    known_predicted = torch.tensor([[[0, 1], [1, 0]]])
    known_diff_accuracy, known_total_diff, known_correct_diff = differential_pixel_accuracy(known_inputs, known_targets, known_predicted)
    logger.debug(f"Known differential accuracy: {known_diff_accuracy}, Total diff: {known_total_diff}, Correct diff: {known_correct_diff}")
    assert known_diff_accuracy == 1.0, f"Expected known differential accuracy to be 1.0, got {known_diff_accuracy}"

def test_task_accuracies_tracking(model, dataloader, is_training=False):
    logger.debug("Starting test_task_accuracies_tracking")
    task_accuracies = {}
    model.eval()
    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Batch {batch_idx}: Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
        outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
        accuracy = (outputs.argmax(dim=1) == targets).float().mean().item()
        task_id = getattr(dataloader, 'task_id', 'default_task')
        if task_id not in task_accuracies:
            task_accuracies[task_id] = {'train': [], 'test': []}
        task_accuracies[task_id]['train' if is_training else 'test'].append(accuracy)
        logger.debug(f"Task accuracies after batch {batch_idx}: {task_accuracies}")
    assert task_accuracies, "Task accuracies dictionary should not be empty"
    assert 'default_task' in task_accuracies, "Default task should be logged in task accuracies"
    assert 'test' in task_accuracies['default_task'], "Test accuracies should be logged for default task"

def test_final_metric_calculation(model, dataloader, attention_mask):
    logger.debug("Starting test_final_metric_calculation")
    model.eval()
    total_loss, total_accuracy = 0, 0
    num_batches = 0
    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Batch {batch_idx}: Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
        outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
        loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(1)), targets.view(-1))
        total_loss += loss.item()
        accuracy = (outputs.argmax(dim=1) == targets).float().mean().item()
        logger.debug(f"Batch {batch_idx}: Loss: {loss.item()}, Accuracy: {accuracy}")
        total_accuracy += accuracy
        num_batches += 1
    avg_loss = total_loss / num_batches
    avg_accuracy = total_accuracy / num_batches
    logger.debug(f"Final metrics - Average loss: {avg_loss}, Average accuracy: {avg_accuracy}")
    assert avg_loss &gt;= 0, f"Average loss should be non-negative, got {avg_loss}"
    assert 0.0 &lt;= avg_accuracy &lt;= 1.0, f"Average accuracy should be between 0 and 1, got {avg_accuracy}"

def test_return_of_evaluation_results(model, dataloader, mocker):
    logger.debug("Starting test_return_of_evaluation_results")
    # Simulate a simple evaluation result
    model.evaluate = lambda dataloader: {'loss': 0.5, 'accuracy': 0.75}
    results = model.evaluate(dataloader)
    logger.debug(f"Evaluation results: {results}")
    assert "loss" in results and "accuracy" in results, "Evaluation results should return loss and accuracy."
    assert isinstance(results["loss"], float), f"Loss should be a float, got {type(results['loss'])}"
    assert 0.0 &lt;= results["accuracy"] &lt;= 1.0, f"Accuracy should be between 0 and 1, got {results['accuracy']}"
def test_validation_step_with_incorrect_batch_format(trainer):
    """Test that the validation_step raises a ValueError for an incorrect batch format."""

    # Create a batch with an incorrect format (e.g., a list)
    incorrect_batch = [
        torch.randint(0, 10, (2, 900)),  # Random input data
        # Labels are missing
    ]

    logger.debug(f"Testing with incorrect batch format: {type(incorrect_batch)}")
    with pytest.raises(ValueError, match="Batch must contain inputs and labels."):
        trainer.validation_step(incorrect_batch, 0)

def test_model_loading_from_checkpoint(mocker):
    logger.debug("Starting test_model_loading_from_checkpoint")
    
    # Load the model checkpoint
    checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt"
    logger.debug(f"Attempting to load checkpoint from: {checkpoint_path}")

    # Check if the checkpoint file exists
    if not os.path.isfile(checkpoint_path):
        pytest.skip(f"Checkpoint file not found: {checkpoint_path}")

    try:
        checkpoint = torch.load(checkpoint_path)
        logger.debug(f"Checkpoint loaded successfully. Keys: {checkpoint.keys()}")
    except FileNotFoundError as e:
        logger.error(f"Failed to load checkpoint: {str(e)}")
        pytest.fail(f"Failed to load checkpoint: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        pytest.fail(f"Unexpected error: {str(e)}")

    # Extract and print the config from the checkpoint
    if 'config' in checkpoint:
        config_dict = checkpoint['config']
        logger.debug(f"Config found in checkpoint: {json.dumps(config_dict, indent=2)}")
    else:
        logger.error("Config not found in checkpoint")
        pytest.fail("Config not found in checkpoint")

    # Reconstruct ModelConfig
    try:
        model_config = ModelConfig(
            n_embd=config_dict['n_embd'],
            n_head=config_dict['n_head'],
            n_layer=config_dict['n_layer'],
            dropout=config_dict['dropout']
        )
        logger.debug(f"ModelConfig reconstructed: {model_config}")
    except KeyError as e:
        logger.error(f"Missing key in config_dict: {str(e)}")
        pytest.fail(f"Failed to reconstruct ModelConfig: {str(e)}")

    # Initialize the model
    try:
        model = GPT2ARC(model_config)
        logger.debug("Model initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize model: {str(e)}")
        pytest.fail(f"Failed to initialize model: {str(e)}")

    # Load the state dict
    try:
        state_dict = {k.replace("model.", ""): v for k, v in checkpoint['state_dict'].items()}
        model.load_state_dict(state_dict)
        logger.debug("State dict loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load state dict: {str(e)}")
        pytest.fail(f"Failed to load state dict: {str(e)}")

    # Ensure the model is in evaluation mode
    model.eval()
    assert not model.training, "Model should be in evaluation mode after calling eval()"
    
    logger.debug("Completed test_model_loading_from_checkpoint")


def test_checkpoint_contains_model_config():
    checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt"
    logger.debug(f"Checking for checkpoint file at: {checkpoint_path}")

    if not os.path.isfile(checkpoint_path):
        logger.warning(f"Checkpoint file not found: {checkpoint_path}")
        pytest.skip(f"Checkpoint file not found: {checkpoint_path}")

    try:
        checkpoint = torch.load(checkpoint_path)
        # Log the keys in the checkpoint
        logger.debug(f"Checkpoint keys: {checkpoint.keys()}")
    except FileNotFoundError as e:
        logger.error(f"FileNotFoundError: {str(e)}")
        pytest.fail(f"FileNotFoundError: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        pytest.fail(f"Unexpected error: {str(e)}")

    # Check for model configuration
    assert 'config' in checkpoint, "Model configuration not found in checkpoint."
    model_config = checkpoint['config']
    logger.debug(f"Model configuration found in checkpoint: {model_config}")
    print("Model configuration found in checkpoint:", model_config)

</file>
<file name="tests/test_checkpoint_loading.py">
import torch
import pytest

def test_actual_checkpoint_loading():
    # Path to the actual checkpoint file
    checkpoint_path = 'final_model_4fe9801e-c839-454f-a46c-6e94e3c04e81.pth'
    
    # Load the checkpoint
    checkpoint = torch.load(checkpoint_path)
    
    # Print the keys for debugging purposes
    print("Checkpoint keys:", checkpoint.keys())
    
    # Check if the checkpoint contains expected keys
    expected_keys = ['conv1.weight', 'conv1.bias', 'blocks.0.attention.key.weight']
    for key in expected_keys:
        assert key in checkpoint, f"Checkpoint does not contain expected key: {key}"

if __name__ == "__main__":
    pytest.main([__file__])

</file>
<file name="tests/test_integration_experiment.py">
# gpt2_arc/tests/test_integration_experiment.py
import sys
import os

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

import pytest
import torch
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.utils.results_collector import ResultsCollector
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

import arckit

@pytest.fixture
def setup_experiment():
    # Load a sample task from arckit
    task_id = "007bbfb7"  # Example task ID
    task_data = arckit.load_single(task_id)
    train_data = task_data.train
    val_data = task_data.test
    print(f"DEBUG: task_data type: {type(task_data)}")
    print(f"DEBUG: task_data attributes: {dir(task_data)}")
    print(f"DEBUG: train_data type: {type(train_data)}")
    print(f"DEBUG: train_data content: {train_data}")
    print(f"DEBUG: val_data type: {type(val_data)}")
    print(f"DEBUG: val_data content: {val_data}")
    train_dataset = ARCDataset([{"train": train_data, "test": val_data}])
    val_dataset = ARCDataset([{"train": train_data, "test": val_data}])

    # Model and config setup
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    training_config = TrainingConfig(batch_size=1, learning_rate=1e-4, max_epochs=1)
    config = Config(model=model_config, training=training_config)
    model = GPT2ARC(config=model_config)

    # Trainer setup
    trainer = ARCTrainer(model=model, train_dataset=train_dataset, val_dataset=val_dataset, config=config)
    return trainer, config

def test_full_experiment_run(setup_experiment):
    trainer, config = setup_experiment

    # PyTorch Lightning Trainer
    pl_trainer = Trainer(
        max_epochs=config.training.max_epochs,
        logger=TensorBoardLogger("tb_logs", name="arc_model_test"),
        callbacks=[ModelCheckpoint(dirpath="checkpoints", save_top_k=1, monitor="val_loss")],
        enable_checkpointing=True,
        enable_progress_bar=False,
        fast_dev_run=True
    )

    # Run training
    pl_trainer.fit(trainer)

    # Verify results
    results_summary = trainer.results_collector.get_summary()
    assert results_summary["experiment_id"] is not None
    assert "final_train_loss" in results_summary
    assert "final_val_loss" in results_summary

@pytest.mark.parametrize("invalid_data", [
    ({"input": [[0] * 30 for _ in range(30)]}),  # Missing output
    ({"output": [[0] * 30 for _ in range(30)]}),  # Missing input
])
def test_invalid_data_handling(invalid_data):
    with pytest.raises(ValueError):
        ARCDataset([invalid_data])

def test_model_convergence_issue(setup_experiment):
    trainer, config = setup_experiment
    trainer.config.training.learning_rate = 1e-10  # Set an inappropriate learning rate

    # PyTorch Lightning Trainer
    pl_trainer = Trainer(
        max_epochs=config.training.max_epochs,
        logger=False,
        enable_checkpointing=False,
        enable_progress_bar=False,
        fast_dev_run=True
    )

    # Run training
    pl_trainer.fit(trainer)

    # Verify that the model did not converge
    results_summary = trainer.results_collector.get_summary()
    assert results_summary["final_train_loss"] is not None
    assert results_summary["final_train_loss"] &gt; 1.0  # Assuming a high loss indicates non-convergence

</file>
<file name="tests/test_results_collector.py">
# gpt2_arc/tests/test_results_collector.py
import unittest
from gpt2_arc.src.utils.results_collector import ResultsCollector
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig

class TestResultsCollector(unittest.TestCase):
    def setUp(self):
        model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
        training_config = TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=10)
        config = Config(model=model_config, training=training_config)
        self.results_collector = ResultsCollector(config)

    def test_initialization(self):
        self.assertIsNotNone(self.results_collector.experiment_id)
        self.assertIsNotNone(self.results_collector.timestamp)
        self.assertEqual(self.results_collector.config['model']['n_embd'], 96)

    def test_update_train_metrics(self):
        self.results_collector.update_train_metrics(1, {"loss": 0.5})
        self.assertIn(1, self.results_collector.results["train"])
        self.assertEqual(self.results_collector.results["train"][1]["loss"], 0.5)

    def test_update_val_metrics(self):
        self.results_collector.update_val_metrics(1, {"loss": 0.3})
        self.assertIn(1, self.results_collector.results["validation"])
        self.assertEqual(self.results_collector.results["validation"][1]["loss"], 0.3)

    def test_set_test_results(self):
        self.results_collector.set_test_results({"accuracy": 0.8})
        self.assertEqual(self.results_collector.results["test"]["accuracy"], 0.8)

    def test_add_task_specific_result(self):
        self.results_collector.add_task_specific_result("task_1", {"accuracy": 0.9})
        self.assertIn("task_1", self.results_collector.task_specific_results)
        self.assertEqual(self.results_collector.task_specific_results["task_1"]["accuracy"], 0.9)

    def test_get_summary(self):
        summary = self.results_collector.get_summary()
        self.assertEqual(summary["experiment_id"], self.results_collector.experiment_id)

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_differential_pixel_accuracy.py">
# gpt2_arc/tests/test_differential_pixel_accuracy.py
import sys
import os

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

import torch
from gpt2_arc.src.utils.helpers import differential_pixel_accuracy
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import ModelConfig
from gpt2_arc.src.data.arc_dataset import ARCDataset
import arckit

def test_identical_inputs_and_targets():
    input_tensor = torch.tensor([[1, 2], [3, 4]])
    target_tensor = torch.tensor([[1, 2], [3, 4]])
    prediction_tensor = torch.tensor([[1, 2], [3, 4]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for identical input and target"

def test_completely_different_inputs_and_targets():
    input_tensor = torch.tensor([[1, 1], [1, 1]])
    target_tensor = torch.tensor([[0, 0], [0, 0]])
    prediction_tensor = torch.tensor([[0, 0], [0, 0]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for correct prediction of all differing pixels"

def test_partial_differences():
    input_tensor = torch.tensor([[1, 2], [3, 4]])
    target_tensor = torch.tensor([[1, 0], [3, 0]])
    prediction_tensor = torch.tensor([[1, 0], [3, 4]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 0.5, "Expected accuracy of 0.5 for partial correct predictions"

def test_empty_tensors():
    input_tensor = torch.tensor([])
    target_tensor = torch.tensor([])
    prediction_tensor = torch.tensor([])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for empty tensors"

def test_single_pixel_difference():
    input_tensor = torch.tensor([[1]])
    target_tensor = torch.tensor([[0]])
    prediction_tensor = torch.tensor([[0]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for single pixel difference"

def test_differential_pixel_accuracy_with_arckit_data():
    print("Starting test_differential_pixel_accuracy_with_arckit_data")
    task_id = "007bbfb7"
    task_data = arckit.load_single(task_id)

    print(f"Loaded task data: {task_data}")
    print(f"Debug: task_data type: {type(task_data)}")
    print(f"Debug: task_data attributes: {dir(task_data)}")

    dataset = ARCDataset([task_data])  # Wrap in list to simulate multiple tasks
    input_tensor, target_tensor, _ = dataset[0]

    print(f"Dataset input tensor shape: {input_tensor.shape}")
    print(f"Dataset target tensor shape: {target_tensor.shape}")

    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    model = GPT2ARC(model_config)
    model.eval()

    print("Model initialized and set to eval mode")

    with torch.no_grad():
        prediction_tensor = model(input_tensor.unsqueeze(0))

    print(f"Model prediction tensor shape: {prediction_tensor.shape}")

    # Reverse scaling for evaluation
    original_input = task_data.train[0][0]
    original_target = task_data.train[0][1]
    
    print(f"Original input shape: {original_input.shape}")
    print(f"Original target shape: {original_target.shape}")

    prediction_np = prediction_tensor.squeeze().argmax(dim=0).numpy()
    print(f"Prediction numpy array shape: {prediction_np.shape}")

    reversed_prediction = dataset.reverse_scaling(original_input, prediction_np)
    print(f"Reversed prediction shape: {reversed_prediction.shape}")

    # Convert back to tensors for differential_pixel_accuracy
    # Ensure all tensors have the same shape
    input_tensor = torch.tensor(original_input, dtype=torch.float32).resize_(original_target.shape)
    target_tensor = torch.tensor(original_target, dtype=torch.float32)
    prediction_tensor = torch.tensor(reversed_prediction, dtype=torch.float32).resize_(original_target.shape)

    print(f"Final input tensor shape: {input_tensor.shape}")
    print(f"Final target tensor shape: {target_tensor.shape}")
    print(f"Final prediction tensor shape: {prediction_tensor.shape}")

    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    print(f"Differential Pixel Accuracy for task {task_id}: {accuracy}")

    assert 0 &lt;= accuracy &lt;= 1, f"Accuracy should be between 0 and 1, but got {accuracy}"

# Run the tests
if __name__ == "__main__":
    test_identical_inputs_and_targets()
    test_completely_different_inputs_and_targets()
    test_partial_differences()
    test_empty_tensors()
    test_single_pixel_difference()
    print("All tests passed!")

</file>
<file name="tests/__init__.py">

</file>
<file name="tests/test_arc_dataset.py">
# gpt2_arc/tests/test_arc_dataset.py

import os
import numpy as np
import pytest
import torch
import random
import logging
import arckit
from torch.utils.data import DataLoader
from src.data.arc_dataset import ARCDataset, set_debug_mode

# Set up logging for tests
logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)

@pytest.fixture(scope="module")
def debug_mode():
    set_debug_mode(True)
    yield
    set_debug_mode(False)
from unittest.mock import Mock
from arckit.data import TaskSet


@pytest.fixture
def sample_data():
    return [
        {'input': [[1, 0], [0, 1]], 'output': [[0, 1], [1, 0]]},
        {'input': [[0, 1], [1, 0]], 'output': [[1, 0], [0, 1]]}
    ]


@pytest.fixture
def mock_taskset():
    mock_task = Mock()
    mock_task.id = "mock_task_1"
    mock_task.train = [
        (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])),
        (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]]))
    ]
    mock_task.test = [
        (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]]))
    ]
    
    mock_taskset = Mock(spec=TaskSet)
    mock_taskset.tasks = [mock_task]
    return mock_taskset
def test_arc_dataset_initialization(sample_data, debug_mode):
    dataset = ARCDataset(sample_data, debug=True)
    logger.debug(f"Dataset length: {len(dataset)}, expected: {len(sample_data)}")
    assert len(dataset) == len(sample_data), "Dataset length mismatch"
    
    input_grid, output_grid, *_ = dataset[0]
    logger.debug(f"Input grid shape: {input_grid.shape}, expected: (1, 30, 30)")
    logger.debug(f"Output grid shape: {output_grid.shape}, expected: (1, 30, 30)")
    
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    
    # Update the shape check to match the new preprocessing logic
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Verify that the original data is preserved in the center of the padded grid
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    
    logger.debug(f"Center input:\n{center_input}")
    logger.debug(f"Center output:\n{center_output}")
    
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"
    dataset = ARCDataset(sample_data)
    assert len(dataset) == 2, "Dataset should have 2 samples"
    
    input_grid, output_grid, *_ = dataset[0]
    
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    
    # Update the shape check to match the new preprocessing logic
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Verify that the original data is preserved in the center of the padded grid
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"

#Skip
@pytest.mark.skip(reason="Skipping test for synthetic data because test is problematic")
def test_arc_dataset_synthetic_data(debug_mode):
    synthetic_data_path = "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/syntheticARC/tasks"
    assert os.path.isdir(synthetic_data_path), f"Directory does not exist: {synthetic_data_path}"
    train_dataset = ARCDataset(synthetic_data_path, is_test=False, debug=True)
    test_dataset = ARCDataset(synthetic_data_path, is_test=True, debug=True)

    assert len(train_dataset) &gt; 0, "Synthetic train dataset should not be empty"
    assert len(test_dataset) &gt; 0, "Synthetic test dataset should not be empty"
    logger.debug(f"Loaded {len(train_dataset.data)} synthetic tasks")
    logger.debug(f"Total train dataset length: {len(train_dataset)}")
    logger.debug(f"Total test dataset length: {len(test_dataset)}")

    total_train = sum(len(task['train']) for task in train_dataset.data)
    total_test = sum(len(task['test']) for task in test_dataset.data)
    logger.debug(f"Total train samples: {total_train}")
    logger.debug(f"Total test samples: {total_test}")

    for i, task in enumerate(train_dataset.data):
        print(f"Task {i} - Train samples: {len(task['train'])}, Test samples: {len(task['test'])}")

    assert len(train_dataset) == total_train, f"Train dataset length ({len(train_dataset)}) should match total train samples ({total_train})"
    assert len(test_dataset) == total_test, f"Test dataset length ({len(test_dataset)}) should match total test samples ({total_test})"

    if len(train_dataset) == 0:
        pytest.skip("Train dataset is empty; skipping random sample tests.")

    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Test dataset size: {len(test_dataset)}")
    
    if len(train_dataset) &lt; 3:
        pytest.skip("Not enough data in the train dataset for random sampling tests.")
    
    # Test a few random samples from the train dataset
    for i in range(3):
        idx = random.choice(range(len(train_dataset)))
        try:
            print(f"\nTrain Sample {i + 1}:")
            print(f"Generated index: {idx}")
            input_grid, output_grid = train_dataset[idx]
            print(f"Input grid shape: {input_grid.shape}")
            print(f"Output grid shape: {output_grid.shape}")
        except IndexError as e:
            print(f"Error: Attempted to access index {idx} which is out of range. Train dataset size is {len(train_dataset)}.")
            pytest.fail(f"Generated index {idx} out of range for train dataset size {len(train_dataset)}: {str(e)}")

    # Verify grid sizes
    max_h, max_w = train_dataset.max_grid_size
    assert max_h &gt; 0 and max_w &gt; 0, "Grid size should be positive"
    print(f"Maximum grid size: {train_dataset.max_grid_size}")

    # Verify access to train and test splits
    assert len(train_dataset.data) &gt; 0, "Dataset should contain at least one task"
    assert 'train' in train_dataset.data[0], "Each task should have a 'train' split"
    assert 'test' in train_dataset.data[0], "Each task should have a 'test' split"

    print(f"Train dataset length: {len(train_dataset)}")
    print(f"Test dataset length: {len(test_dataset)}")




def test_arc_dataset_getitem(sample_data):
    dataset = ARCDataset(sample_data)
    input_grid, output_grid, *_ = dataset[0]

    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"

    # Check if the original data is preserved in the center
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"


def test_arc_dataset_len(sample_data):
    print("Debugging: Entering test_arc_dataset_len")
    print(f"Debugging: sample_data = {sample_data}")
    dataset = ARCDataset(sample_data)
    print(f"Debugging: len(dataset) = {len(dataset)}, len(sample_data) = {len(sample_data)}")
    assert len(dataset) == len(sample_data), "Dataset length should match input data length"
    print("Debugging: Exiting test_arc_dataset_len")


def test_arc_dataset_invalid_data(sample_data):
    invalid_data = [{"input": [1, 0], "output": [[0, 1], [1, 0]]}]
    with pytest.raises(ValueError):
        ARCDataset(invalid_data)

    invalid_data = [{"input": [[1, 0], [0, 1]], "output": "not a list"}]
    with pytest.raises(ValueError):
        ARCDataset(invalid_data)

def test_arc_dataset_preprocess_grid(sample_data):
    dataset = ARCDataset(sample_data, num_symbols=10)
    input_grid, output_grid, *_ = dataset[0]

    print(f"Input grid shape: {input_grid.shape}")
    print(f"Output grid shape: {output_grid.shape}")
    print(f"Input grid content:\n{input_grid}")
    print(f"Output grid content:\n{output_grid}")

    # Check that the grids are indeed 3D
    assert input_grid.ndim == 3, f"Expected 3D input grid, got {input_grid.ndim}D"
    assert output_grid.ndim == 3, f"Expected 3D output grid, got {output_grid.ndim}D"

    # Check the shape (1, 30, 30)
    assert input_grid.shape == (1, 30, 30), f"Preprocessed grid should have shape (1, 30, 30), but got {input_grid.shape}"
    assert output_grid.shape == (1, 30, 30), f"Preprocessed grid should have shape (1, 30, 30), but got {output_grid.shape}"

    # Check if the original data is preserved in the center
    expected_input = torch.zeros((1, 30, 30))
    expected_input[0, 14:16, 14:16] = torch.tensor([[1., 0.], [0., 1.]])

    expected_output = torch.zeros((1, 30, 30))
    expected_output[0, 14:16, 14:16] = torch.tensor([[0., 1.], [1., 0.]])

    print(f"Expected input:\n{expected_input}")
    print(f"Expected output:\n{expected_output}")

    assert torch.allclose(input_grid, expected_input), "Input grid data mismatch"
    assert torch.allclose(output_grid, expected_output), "Output grid data mismatch"

@pytest.fixture
def mock_taskset():
    mock_task = Mock()
    mock_task.id = "mock_task_1"
    mock_task.train = [
        (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])),
        (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]]))
    ]
    mock_task.test = [
        (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]]))
    ]
    
    mock_taskset = Mock(spec=TaskSet)
    mock_taskset.tasks = [mock_task]
    return mock_taskset
def test_collate_fn_output():
    sample_data = [
        {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]},
        {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]},
    ]
    dataset = ARCDataset(sample_data)
    dataloader = DataLoader(dataset, batch_size=2, collate_fn=ARCDataset.collate_fn)
    batch = next(iter(dataloader))

    assert isinstance(batch, list), "Collate function should return a list"
    assert len(batch) == 3, "Collate function should return a list with 3 elements"
    assert isinstance(batch[0], torch.Tensor), "First element should be a tensor (inputs)"
    assert isinstance(batch[1], torch.Tensor), "Second element should be a tensor (outputs)"
    assert batch[0].shape == (2, 1, 30, 30), "Input tensor should have shape (batch_size, 1, 30, 30)"
    assert batch[1].shape == (2, 1, 30, 30), "Output tensor should have shape (batch_size, 1, 30, 30)"
    assert batch[0].dtype == torch.float32, "Input tensor should be of type float32"
    assert batch[1].dtype == torch.float32, "Output tensor should be of type float32"

def test_getitem_output():
    sample_data = [
        {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]},
    ]
    dataset = ARCDataset(sample_data)
    input_grid, output_grid, *_ = dataset[0]

    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    assert input_grid.dtype == torch.float32, "Input grid should be float32"
    assert output_grid.dtype == torch.float32, "Output grid should be float32"

#Skip
@pytest.mark.skip(reason="Skipping because test is problematic")
def test_arc_dataset_taskset_initialization(mock_taskset):
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logger = logging.getLogger(__name__)
    
    logger.debug(f"Mock TaskSet: {mock_taskset}")
    logger.debug(f"Mock TaskSet attributes: {dir(mock_taskset)}")
    
    print(f"Mock task train data: {mock_taskset.tasks[0].train}")
    print(f"Mock task test data: {mock_taskset.tasks[0].test}")
    dataset = ARCDataset(mock_taskset)
    
    logger.debug(f"Dataset length: {len(dataset)}")
    print(f"Dataset length: {len(dataset)}, Expected: 3")
    
    assert len(dataset) == 3, "Dataset should have 3 samples (2 train + 1 test)"
    input_grid, output_grid, *_ = dataset[0]
    print(f"Input grid shape: {input_grid.shape}, Expected: (1, 30, 30)")
    print(f"Output grid shape: {output_grid.shape}, Expected: (1, 30, 30)")
    
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Check if the original data is preserved in the center
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    print(f"Center input: {center_input}")
    print(f"Center output: {center_output}")
    
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logger = logging.getLogger(__name__)
    
    logger.debug(f"Mock TaskSet: {mock_taskset}")
    logger.debug(f"Mock TaskSet attributes: {dir(mock_taskset)}")
    
    dataset = ARCDataset(mock_taskset)
    
    logger.debug(f"Dataset length: {len(dataset)}")
    
    assert len(dataset) == 3, "Dataset should have 3 samples (2 train + 1 test)"
    input_grid, output_grid = dataset[0]
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Check if the original data is preserved in the center
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"

from torch.utils.data import DataLoader

def test_arc_dataset_collate_fn(sample_data):
    logger.debug("Starting test_arc_dataset_collate_fn")
    dataset = ARCDataset(sample_data)
    dataloader = DataLoader(dataset, batch_size=2, collate_fn=ARCDataset.collate_fn)
    batch = next(iter(dataloader))
    input_batch, output_batch, *_ = batch
    logger.debug(f"Collated batch shapes - inputs: {input_batch.shape}, outputs: {output_batch.shape}")
    assert input_batch.shape == (2, 1, 30, 30), "Batched input should have shape (2, 1, 30, 30)"
    assert output_batch.shape == (2, 1, 30, 30), "Batched output should have shape (2, 1, 30, 30)"
    logger.debug("Completed test_arc_dataset_collate_fn")

def test_arc_dataset_variable_size_grids(sample_data):
    logger.debug("Starting test_arc_dataset_variable_size_grids")
    variable_data = sample_data + [{"input": [[1, 0, 2], [0, 2, 1], [2, 1, 0]], "output": [[2, 1, 0], [1, 0, 2], [0, 2, 1]]}]
    dataset = ARCDataset(variable_data)
    
    # Check first sample (2x2)
    input_grid_1, output_grid_1, *_ = dataset[0]
    assert input_grid_1.shape == (1, 30, 30), "First sample should have shape (1, 30, 30)"
    assert output_grid_1.shape == (1, 30, 30), "First sample should have shape (1, 30, 30)"
    
    # Check center of first sample (2x2)
    center_input_1 = input_grid_1[0, 14:16, 14:16]
    center_output_1 = output_grid_1[0, 14:16, 14:16]
    assert torch.allclose(center_input_1, torch.tensor([[1., 0.], [0., 1.]])), "First sample input data not preserved correctly"
    assert torch.allclose(center_output_1, torch.tensor([[0., 1.], [1., 0.]])), "First sample output data not preserved correctly"
    
    # Check third sample (3x3)
    input_grid_2, output_grid_2, *_ = dataset[2]
    assert input_grid_2.shape == (1, 30, 30), "Third sample should have shape (1, 30, 30)"
    assert output_grid_2.shape == (1, 30, 30), "Third sample should have shape (1, 30, 30)"
    
    # Check center of third sample (3x3)
    center_input_2 = input_grid_2[0, 13:16, 13:16]
    center_output_2 = output_grid_2[0, 13:16, 13:16]
    assert torch.allclose(center_input_2, torch.tensor([[1., 0., 2.], [0., 2., 1.], [2., 1., 0.]])), f"Third sample input data not preserved correctly. Got:\n{center_input_2}"
    assert torch.allclose(center_output_2, torch.tensor([[2., 1., 0.], [1., 0., 2.], [0., 2., 1.]])), f"Third sample output data not preserved correctly. Got:\n{center_output_2}"
    
    logger.debug("Completed test_arc_dataset_variable_size_grids")

def test_arc_dataset_with_arckit_data_get_task_id():
    # Load data using arckit
    train_set, _ = arckit.load_data()

    # Initialize the dataset
    dataset = ARCDataset(train_set, is_test=False)

    # Check that __getitem__ returns the correct structure
    input_grid, output_grid, task_id = dataset[0]
    assert isinstance(input_grid, torch.Tensor), "Input grid should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output grid should be a torch.Tensor"
    assert isinstance(task_id, str), "Task ID should be a string"

    # Test the collate_fn
    batch = [dataset[i] for i in range(2)]  # Create a batch of two samples
    collated_inputs, collated_outputs, collated_task_ids = ARCDataset.collate_fn(batch)

    assert len(collated_task_ids) == 2, "Batch size should be 2"
    assert collated_inputs.shape[0] == 2, "Batch size should be 2"
    assert collated_outputs.shape[0] == 2, "Batch size should be 2"

</file>
<file name="tests/test_gpt2.py">
# gpt2_arc/tests/test_gpt2.py
import logging

import pytest
import torch
from src.config import ModelConfig
from src.models.gpt2 import GPT2ARC, Attention, FeedForward, TransformerBlock

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


@pytest.fixture
def model():
    config = ModelConfig()
    return GPT2ARC(config)


def test_gpt2arc_initialization(model):
    assert isinstance(model, GPT2ARC)
    assert hasattr(model, "conv1")  # Check for conv1 instead of token_embedding
    assert hasattr(model, "blocks")
    assert hasattr(model, "ln_f")
    assert hasattr(model, "config")


def test_gpt2arc_forward_pass(model):
    batch_size = 2
    height = 30
    width = 30
    input_ids = torch.randn(batch_size, 1, height, width)  # Simulate image-like input
    attention_mask = torch.ones((batch_size, height * width))

    output = model(input_ids, attention_mask)

    assert isinstance(output, torch.Tensor)
    assert output.shape == (batch_size, height * width, model.config.n_embd)

    logger.debug(f"Output shape: {output.shape}")


def test_gpt2arc_output_values(model):
    logger.debug("Testing GPT2ARC output values")
    batch_size = 1
    channels = 1
    height = 30
    width = 30
    input_ids = torch.randn(batch_size, channels, height, width)  # Simulate image-like input
    attention_mask = torch.ones((batch_size, height * width))

    output = model(input_ids, attention_mask)

    assert not torch.isnan(output).any(), "Output contains NaN values"


def test_gpt2arc_forward_pass(model):
    batch_size = 2
    channels = 1
    height = 30
    width = 30
    input_ids = torch.randn(batch_size, channels, height, width)  # Simulate image-like input
    attention_mask = torch.ones((batch_size, height * width))

    output_with_mask = model(input_ids, attention_mask)
    output_without_mask = model(input_ids)

    logger.debug(
        f"Difference between outputs: {(output_with_mask - output_without_mask).abs().mean()}"
    )


def test_attention_module():
    logger.debug("Testing Attention module")
    attention = Attention(n_embd=768, n_head=12)
    x = torch.randn(2, 10, 768)
    output = attention(x)
    assert output.shape == x.shape
    logger.debug(f"Attention input shape: {x.shape}, output shape: {output.shape}")


def test_feedforward_module():
    logger.debug("Testing FeedForward module")
    ff = FeedForward(n_embd=768)
    x = torch.randn(2, 10, 768)
    output = ff(x)
    assert output.shape == x.shape
    logger.debug(f"FeedForward input shape: {x.shape}, output shape: {output.shape}")


def test_transformer_block():
    logger.debug("Testing TransformerBlock")
    block = TransformerBlock(n_embd=768, n_head=12)
    x = torch.randn(2, 10, 768)
    output = block(x)
    assert output.shape == x.shape
    logger.debug(
        f"TransformerBlock input shape: {x.shape}, output shape: {output.shape}"
    )

</file>
<file name="tests/test_train.py">
# gpt2_arc/tests/test_train.py
import os
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))
import os
import sys

import pytest
import logging

logger = logging.getLogger(__name__)

def set_logging_level(level=logging.ERROR):
    logger = logging.getLogger()
    logger.setLevel(level)

# Add the project root to the PYTHONPATH
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))
import argparse
from unittest.mock import ANY, MagicMock, patch

import pytorch_lightning as pl
import torch

from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.train import main
from gpt2_arc.src.training.trainer import ARCTrainer


@pytest.fixture
def mock_args():
    args = argparse.Namespace()
    args.train_data = "mock_train_data.json"
    args.val_data = "mock_val_data.json"
    args.batch_size = 32
    args.learning_rate = 1e-4
    args.max_epochs = 10
    args.use_gpu = False
    args.no_logging = False
    args.no_checkpointing = False
    args.no_progress_bar = False
    args.log_level = "INFO"  # Add log_level attribute
    args.fast_dev_run = False  # Add fast_dev_run attribute
    args.project = "test_project"  # Add a project attribute to mock_args
    return args


@pytest.fixture
def mock_dataset():
    dataset = MagicMock(spec=ARCDataset)
    dataset.data = [{"input": "mock input", "output": "mock output"}]
    dataset.__len__.return_value = 100
    return dataset


from src.config import Config, ModelConfig, TrainingConfig


@pytest.fixture
def model():
    config = Config(model=ModelConfig(), training=TrainingConfig())
    return GPT2ARC(config.model)


@pytest.fixture
def trainer():
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    return ARCTrainer(model, None, None, config)


@pytest.fixture
def mock_pl_trainer():
    return MagicMock(spec=pl.Trainer)


# Existing GPT2ARC model tests


def test_gpt2arc_initialization(model):
    assert isinstance(model, GPT2ARC)
    assert hasattr(model, "conv1")  # Check for conv1 instead of token_embedding
    assert hasattr(model, "blocks")
    assert hasattr(model, "ln_f")
    assert hasattr(model, "config")


def test_gpt2arc_forward_pass(model):
    batch_size = 2
    height = width = 30
    seq_length = height * width
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output_with_mask = model(input_ids, attention_mask)
    output_without_mask = model(input_ids)

    assert isinstance(output_with_mask, torch.Tensor)
    assert output_with_mask.shape == (batch_size, seq_length, model.config.n_embd)
    assert isinstance(output_without_mask, torch.Tensor)
    assert output_without_mask.shape == (batch_size, seq_length, model.config.n_embd)

    logger.debug(f"Difference between outputs: {(output_with_mask - output_without_mask).abs().mean()}")


def test_gpt2arc_output_values(model):
    logger.debug("Testing GPT2ARC output values")
    batch_size = 1
    height = width = 30
    seq_length = height * width
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output = model(input_ids, attention_mask)

    assert not torch.isnan(output).any(), "Output contains NaN values"
    assert not torch.isinf(output).any(), "Output contains infinity values"


def test_gpt2arc_attention_mask(model):
    batch_size = 2
    channels = 1
    height = 30
    width = 30
    input_ids = torch.randint(0, 2, (batch_size, channels, height, width))
    attention_mask = torch.zeros((batch_size, height * width))
    attention_mask[:, :450] = 1  # Only attend to first half of the pixels
    output_with_mask = model(input_ids, attention_mask)
    output_without_mask = model(input_ids)
    assert not torch.allclose(output_with_mask, output_without_mask), "Attention mask should affect the output"


# New tests for train.py


def test_logging(mock_args, mock_dataset, model, mock_pl_trainer):
    print("Entering test_logging")
    with patch(
        "gpt2_arc.src.training.train.ARCDataset", return_value=mock_dataset
    ), patch("gpt2_arc.src.training.train.GPT2ARC", return_value=model), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
    ) as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer", return_value=mock_pl_trainer
    ), patch("gpt2_arc.src.training.train.TensorBoardLogger") as mock_logger, patch(
        "gpt2_arc.src.training.train.ModelCheckpoint"
    ), patch("torch.utils.data.DataLoader") as mock_dataloader:
        mock_dataloader.return_value = MagicMock()

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "1234",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }
        mock_trainer_instance.results_collector = mock_results_collector

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_logger.assert_called_once_with("tb_logs", name="arc_model")


def test_fit_call(mock_args, mock_dataset, model):
    mock_pl_trainer = MagicMock()
    mock_pl_trainer.fit = MagicMock()
    print("Entering test_fit_call")
    with patch(
        "gpt2_arc.src.training.train.ARCDataset", return_value=mock_dataset
    ), patch("gpt2_arc.src.training.train.GPT2ARC", return_value=model), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ) as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer", return_value=mock_pl_trainer
    ), patch("gpt2_arc.src.training.train.TensorBoardLogger"), patch(
        "gpt2_arc.src.training.train.ModelCheckpoint"
    ), patch("torch.utils.data.DataLoader", new_callable=MagicMock) as mock_dataloader:
        mock_dataloader.return_value = MagicMock()

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_pl_trainer.fit.assert_called_once_with(mock_trainer_instance)


def test_data_loading(mock_args):
    with patch(
        "gpt2_arc.src.data.arc_dataset.ARCDataset.__init__", return_value=None
    ) as mock_init:
        ARCDataset(mock_args.train_data)
        mock_init.assert_called_once_with(mock_args.train_data)


def test_trainer_initialization(model, mock_dataset):
    config = Config(model=ModelConfig(), training=TrainingConfig())
    trainer = ARCTrainer(
        model=model, train_dataset=mock_dataset, val_dataset=mock_dataset, config=config
    )
    assert isinstance(trainer, ARCTrainer)
    assert trainer.model == model
    assert trainer.train_dataset == mock_dataset
    assert trainer.val_dataset == mock_dataset
    assert trainer.batch_size == 32
    assert trainer.lr == 1e-4


@pytest.mark.parametrize("batch_size", [1, 1000000])
def test_batch_size_extremes(mock_args, batch_size):
    model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=batch_size, learning_rate=5e-4, max_epochs=10))
    mock_args.batch_size = batch_size
    mock_args.no_logging = True
    mock_args.no_checkpointing = True
    mock_args.no_progress_bar = True
    mock_args.use_gpu = False
    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch("gpt2_arc.src.training.trainer.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ) as mock_trainer, patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        main(mock_args)

        mock_trainer.assert_called_with(
            max_epochs=config.training.max_epochs,
            logger=False,
            callbacks=None,
            enable_checkpointing=False,
            enable_progress_bar=False,
            fast_dev_run=False,  # Include fast_dev_run in the expected call
            gradient_clip_val=1.0,
            accelerator='cpu'
        )


@pytest.mark.parametrize("learning_rate", [1e-10, 1000])
def test_learning_rate_extremes(mock_args, learning_rate):
    set_logging_level(logging.WARNING)  # Suppress INFO and DEBUG messages
    mock_args.learning_rate = learning_rate
    logger.debug(f"Testing with learning_rate: {learning_rate}")
    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ), patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "1234",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }
        main(mock_args)  # Should not raise an exception


def test_non_existent_train_data(mock_args):
    mock_args.train_data = "non_existent_path.json"
    with pytest.raises(FileNotFoundError):
        if not os.path.exists(mock_args.train_data):
            raise FileNotFoundError(f"File not found: {mock_args.train_data}")
        main(mock_args)


def test_gpu_not_available(mock_args):
    mock_args.use_gpu = True
    mock_args.no_logging = False
    mock_args.no_checkpointing = False
    mock_args.no_progress_bar = False
    with patch("torch.cuda.is_available", return_value=False), patch(
        "gpt2_arc.src.training.train.ARCDataset"
    ), patch("gpt2_arc.src.training.train.GPT2ARC"), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch("gpt2_arc.src.training.train.pl.Trainer") as mock_trainer, \
         patch("gpt2_arc.src.utils.results_collector.ResultsCollector.get_summary") as mock_get_summary:

        # Mock the get_summary method to return a serializable dictionary
        mock_get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }
        # Use a simple function instead of MagicMock for main
        def simple_main(args):
            pass

        simple_main(mock_args)
        mock_trainer.assert_called_with(
            max_epochs=mock_args.max_epochs,
            logger=ANY,
            callbacks=ANY,
            enable_checkpointing=True,
            enable_progress_bar=True,
            fast_dev_run=False,
            gradient_clip_val=1.0,
            accelerator='cpu'
        )


from hypothesis import HealthCheck, given, settings
from hypothesis import strategies as st


@settings(suppress_health_check=[HealthCheck.function_scoped_fixture], deadline=None)
@given(batch_size=st.integers(min_value=1, max_value=1024))
def test_valid_batch_sizes(mock_args, batch_size):
    mock_args.batch_size = batch_size
    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ), patch("gpt2_arc.src.training.train.ResultsCollector.get_summary", return_value={
        "experiment_id": "test_id",
        "timestamp": "2023-10-01 12:00:00",
        "final_train_loss": 0.1,
        "final_val_loss": 0.2,
        "test_accuracy": 0.95,
        "config": {"model": {}, "training": {}}
    }), patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        main(mock_args)  # Should not raise an exception


@settings(suppress_health_check=[HealthCheck.function_scoped_fixture], deadline=None)
@given(
    learning_rate=st.floats(
        min_value=1e-6, max_value=1.0, allow_nan=False, allow_infinity=False
    )
)
def test_valid_learning_rates(mock_args, learning_rate):
    mock_args.learning_rate = learning_rate
    import glob
    import os

    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ) as mock_trainer, patch(
        "torch.utils.data.DataLoader"
    ) as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        try:
            # Set up the ARCTrainer mock instance
            mock_trainer_instance = mock_ARCTrainer.return_value

            # Create a mock ResultsCollector with a real get_summary() method
            mock_results_collector = MagicMock()
            mock_results_collector.get_summary.return_value = {
                "experiment_id": "test_id",
                "timestamp": "2023-10-01 12:00:00",
                "final_train_loss": 0.1,
                "final_val_loss": 0.2,
                "test_accuracy": 0.95,
                "config": {"model": {}, "training": {}}
            }
            mock_results_collector.config = {"model": {}, "training": {}}

            # Assign the mock ResultsCollector to the trainer instance
            mock_trainer_instance.results_collector = mock_results_collector

            main(mock_args)  # Should not raise an exception
        finally:
            # Ensure cleanup of generated files
            for file in glob.glob("results/summary_*.json"):
                os.remove(file)


def test_end_to_end_training(mock_args, tmp_path):
    model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=5e-4, max_epochs=2))
    checkpoint_dir = tmp_path / "checkpoints"
    checkpoint_dir.mkdir()
    mock_args.checkpoint_dir = str(checkpoint_dir)

    with patch("gpt2_arc.src.training.train.ARCDataset"), \
         patch("gpt2_arc.src.training.train.GPT2ARC"), \
         patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, \
         patch("gpt2_arc.src.training.train.pl.Trainer") as mock_trainer, \
         patch("gpt2_arc.src.training.train.ModelCheckpoint") as mock_checkpoint, \
         patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_trainer.return_value.fit.assert_called_once()
        mock_checkpoint.assert_called_once()


def test_tensorboard_logging(mock_args, tmp_path):
    log_dir = tmp_path / "tb_logs"
    log_dir.mkdir()

    with patch("gpt2_arc.src.training.train.ARCDataset"), \
         patch("gpt2_arc.src.training.train.GPT2ARC"), \
         patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, \
         patch("gpt2_arc.src.training.train.pl.Trainer"), \
         patch("gpt2_arc.src.training.train.TensorBoardLogger") as mock_logger, \
         patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_logger.assert_called_once_with("tb_logs", name="arc_model")


# Additional test for GPT2ARC model in training context


def test_arctrainer_forward_pass(trainer):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output = trainer(input_ids, attention_mask)

    assert isinstance(output, torch.Tensor)
    assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd)

def test_arctrainer_training_step(trainer):
    batch_size = 2
    height = width = 30  # 30x30 grid
    seq_length = height * width
    vocab_size = 10  # Use a small vocab size for testing
    batch = (
        torch.randint(0, vocab_size, (batch_size, seq_length)).long(),  # inputs
        torch.ones((batch_size, seq_length)).float(),                   # labels
        torch.randint(0, vocab_size, (batch_size, seq_length)).long()   # task_ids
    )
    pl_trainer = MagicMock()
    pl_trainer.validate = MagicMock()
    pl_trainer.validate(trainer, dataloaders=[batch])

@pytest.mark.parametrize("batch_format", ["tuple", "dict"])
def test_arctrainer_batch_format(trainer, batch_format):
    batch_size = 2
    height = width = 30  # 30x30 grid
    seq_length = height * width
    vocab_size = 10  # Use a small vocab size for testing

    if batch_format == "tuple":
        batch = (
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            torch.ones((batch_size, seq_length)).float(),
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        )
    else:
        batch = {
            "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            "attention_mask": torch.ones((batch_size, seq_length)).float(),
            "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        }

    loss = trainer.training_step(batch, 0)

    assert isinstance(loss, torch.Tensor)
    assert loss.shape == torch.Size([])  # Loss should be a scalar
    assert not torch.isnan(loss).any(), "Loss contains NaN values"
    assert not torch.isinf(loss).any(), "Loss contains infinity values"

</file>
<file name="tests/test_benchmark.py">
# gpt2_arc/tests/test_benchmark.py

import pytest
import torch
import numpy as np
from unittest.mock import MagicMock, patch
from benchmark import benchmark_model, main, BASELINES
from src.config import ModelConfig
from src.models.gpt2 import GPT2ARC

# Mock classes and fixtures

@pytest.fixture
def mock_model():
    model = MagicMock(spec=GPT2ARC)
    model.forward = MagicMock()  # Ensure forward is a mock
    return model

@pytest.fixture
def mock_dataset():
    dataset = MagicMock()
    dataset.__getitem__.return_value = (
        torch.randn(1, 30, 30),  # inputs
        torch.randint(0, 10, (1, 30, 30)),  # outputs
        "task_1"  # task_id
    )
    dataset.__len__.return_value = 100
    return dataset

@pytest.fixture
def mock_dataloader():
    dataloader = MagicMock()
    dataloader.__iter__.return_value = iter([
        (
            torch.randn(32, 1, 30, 30),  # inputs
            torch.randn(32, 1, 30, 30),  # outputs
            f"task_{i}"                  # task_ids
        )
        for i in range(10)
    ])
    return dataloader


# Tests for benchmark_model function

def test_benchmark_model_basic(mock_model, mock_dataset, mock_dataloader):
    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader), \
         patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=False), \
         patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=False):
        avg_time, avg_grids = benchmark_model(mock_model, mock_dataset)
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))
    assert avg_time &gt; 0
    assert avg_grids &gt; 0

@pytest.mark.parametrize("batch_size,num_batches,num_runs", [
    (16, 5, 10),
    (64, 20, 5),
    (128, 2, 3)
])
def test_benchmark_model_parameters(mock_model, mock_dataset, mock_dataloader, batch_size, num_batches, num_runs):
    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader), \
         patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=False), \
         patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=False):
        avg_time, avg_grids = benchmark_model(
            mock_model, mock_dataset, batch_size=batch_size, num_batches=num_batches, num_runs=num_runs
        )
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))

def test_benchmark_model_cuda(mock_model, mock_dataset, mock_dataloader):
    with patch('benchmark.torch.cuda.is_available', return_value=True), \
         patch('benchmark.torch.cuda.synchronize'), \
         patch('benchmark.DataLoader', return_value=mock_dataloader), \
         patch('benchmark.torch.compile', return_value=mock_model):
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA is not available on this system")
        
        try:
            avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='cuda')
        except AssertionError as e:
            if "Torch not compiled with CUDA enabled" in str(e):
                pytest.skip("PyTorch not compiled with CUDA support")
            else:
                raise
        
        assert isinstance(avg_time, float)
        assert isinstance(avg_grids, float)
        assert avg_time &gt;= 0
        assert avg_grids &gt;= 0

def test_benchmark_model_mps(mock_model, mock_dataset, mock_dataloader):
    with patch('benchmark.torch.backends.mps.is_available', return_value=True), \
         patch('benchmark.DataLoader', return_value=mock_dataloader):
        avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='mps')
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))

def test_benchmark_model_error_handling(mock_model, mock_dataset):
    with pytest.raises(ValueError, match="Invalid device type"):
        benchmark_model(mock_model, mock_dataset, device_type='invalid_device')

# Tests for main function

@pytest.fixture
def mock_argparse():
    with patch('benchmark.argparse.ArgumentParser') as mock_argparse:
        mock_args = MagicMock()
        mock_args.num_runs = 5
        mock_args.num_full_runs = 1
        mock_args.batch_size = 32
        mock_args.num_batches = 10
        mock_args.n_embd = 64
        mock_args.n_head = 2
        mock_args.n_layer = 1
        mock_args.device = 'cpu'
        mock_args.precision = 'highest'
        mock_argparse.return_value.parse_args.return_value = mock_args
        yield mock_argparse

def test_main_function(mock_argparse, mock_dataset, mock_model):
    with patch('benchmark.arckit.load_data', return_value=(mock_dataset, None)), \
         patch('benchmark.ARCDataset', return_value=mock_dataset), \
         patch('benchmark.GPT2ARC', return_value=mock_model), \
         patch('benchmark.benchmark_model', return_value=(1.0, 100.0)):
        main(mock_argparse.return_value.parse_args())

# Performance tests

@pytest.mark.benchmark(group="benchmark_model")
def test_benchmark_model_performance(benchmark, mock_model):
    # Create a mock dataset with one item
    mock_dataset = MagicMock()
    mock_dataset.__len__.return_value = 1
    mock_dataset.__getitem__.return_value = (
        torch.randn(1, 30, 30),  # input
        torch.randint(0, 10, (1, 30, 30)),  # output
        "task_1"  # task_id
    )

    # Create a mock dataloader that returns the mock dataset item
    mock_dataloader = MagicMock()
    mock_dataloader.__iter__.return_value = iter([mock_dataset.__getitem__()])

    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader), \
         patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=False), \
         patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=False):
        result = benchmark(
            benchmark_model,
            mock_model,
            mock_dataset,
            batch_size=1,
            num_batches=1,
            device_type='cpu',
            precision='medium',
            model_checkpoint=None
        )

    assert isinstance(result, tuple), f"Expected tuple, got {type(result)}"
    assert len(result) == 2, f"Expected tuple of length 2, got length {len(result)}"
    
    avg_time, grids_per_second = result
    print(f"Benchmark result - Average Time: {avg_time}, Grids per Second: {grids_per_second}")
    
    assert isinstance(avg_time, float), f"Expected float for avg_time, got {type(avg_time)}"
    assert isinstance(grids_per_second, float), f"Expected float for grids_per_second, got {type(grids_per_second)}"
    assert avg_time &gt;= 0, f"Average time should be non-negative, got {avg_time}"
    assert grids_per_second &gt;= 0, f"Grids per second should be non-negative, got {grids_per_second}"

    if avg_time &gt; 0:
        assert grids_per_second &gt; 0, f"Grids per second should be positive when avg_time &gt; 0, got {grids_per_second}"

# Edge case tests

def test_benchmark_model_empty_dataset(mock_model):
    empty_dataset = MagicMock()
    empty_dataset.__len__.return_value = 0

    with pytest.raises(ValueError, match="Dataset is empty"):
        benchmark_model(mock_model, empty_dataset)

def test_benchmark_model_single_item_dataset(mock_model):
    single_item_dataset = MagicMock()
    single_item_dataset.__len__.return_value = 1
    mock_dataloader = MagicMock()
    mock_dataloader.__iter__.return_value = iter([
        (torch.randn(1, 1, 30, 30), torch.randn(1, 1, 30, 30), "task_1")
    ])
    
    with patch('benchmark.DataLoader', return_value=mock_dataloader):
        avg_time, avg_grids = benchmark_model(mock_model, single_item_dataset, batch_size=1, num_batches=1)
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))

# Error handling tests

def test_benchmark_model_with_correct_data(mock_model, mock_dataset, mock_dataloader):
    with patch('benchmark.DataLoader', return_value=mock_dataloader):
        avg_time, avg_grids = benchmark_model(mock_model, mock_dataset)
        
        assert isinstance(avg_time, float), "avg_time should be a float"
        assert isinstance(avg_grids, float), "avg_grids should be a float"
        assert avg_time &gt; 0, "avg_time should be positive"
        assert avg_grids &gt; 0, "avg_grids should be positive"

def test_benchmark_model_model_error(mock_model, mock_dataset, mock_dataloader):
    # Mock the model's forward method to raise a RuntimeError during execution
    mock_model.forward = MagicMock(side_effect=RuntimeError("Model execution failed"))
    
    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader):
        with pytest.raises(RuntimeError, match="Model execution failed"):
            print("DEBUG: Invoking benchmark_model")
            benchmark_model(mock_model, mock_dataset, device_type='cpu')
        # Ensure the forward method is called
        assert mock_model.forward.call_count &gt; 0, "DEBUG: forward method was not called"
        print(f"DEBUG: forward method call count: {mock_model.forward.call_count}")

#skip
@pytest.mark.skip(reason="I dont want to crash my computer")
def test_benchmark_model_out_of_memory(mock_model, mock_dataset, mock_dataloader):
    mock_model.side_effect = torch.cuda.OutOfMemoryError("CUDA out of memory")
    
    with patch('benchmark.DataLoader', return_value=mock_dataloader), \
         patch('benchmark.torch.cuda.is_available', return_value=True), \
         pytest.raises(torch.cuda.OutOfMemoryError, match="CUDA out of memory"):
        benchmark_model(mock_model, mock_dataset, device_type='cuda')

# Precision tests

@pytest.fixture
def mock_torch():
    return MagicMock()

@pytest.mark.parametrize("precision", ['highest', 'high', 'medium'])
def test_benchmark_model_precision(mock_model, mock_dataset, mock_torch, precision):
    with patch('gpt2_arc.benchmark.DataLoader') as mock_dataloader_class:
        mock_dataloader = MagicMock()
        mock_dataloader.__iter__.return_value = iter([
            (
                torch.randn(1, 1, 30, 30),  # inputs
                torch.randint(0, 10, (1, 30, 30)),  # outputs
                "task_1"  # task_id
            )
        ])
        mock_dataloader_class.return_value = mock_dataloader

        with patch('gpt2_arc.benchmark.torch.set_float32_matmul_precision') as mock_set_precision:
            benchmark_model(mock_model, mock_dataset, precision=precision)
    
    mock_set_precision.assert_called_once_with(precision)

# CSV output tests

def test_csv_output(mock_model, mock_dataset, mock_dataloader, tmp_path):
    csv_file = tmp_path / "benchmark_results.csv"
    stats_csv_file = tmp_path / "benchmark_statistics.csv"
    
    with patch('benchmark.DataLoader', return_value=mock_dataloader), \
         patch('benchmark.csv.writer') as mock_csv_writer:
        benchmark_model(mock_model, mock_dataset)
    
    assert mock_csv_writer.call_count == 2  # One for results, one for statistics

# Test suite execution

if __name__ == '__main__':
    pytest.main(['-v', '--cov=benchmark', '--cov-report=term-missing'])

</file>
<file name="tests/test_end_to_end.py">
# gpt2_arc/tests/test_end_to_end.py
import pytest
import torch
import numpy as np
from src.data.arc_dataset import ARCDataset
from src.models.gpt2 import GPT2ARC
from src.training.trainer import ARCTrainer
from src.config import Config, ModelConfig, TrainingConfig
import pytorch_lightning as pl
import time
import logging
import os
from thop import profile, clever_format  # Import THOP
from pytest import approx

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@pytest.fixture
def arc_data_path():
    # Adjust this path to the location of your ARC dataset JSON file
    return "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/syntheticARC/tasks/1c786137.json"

import arckit

def test_end_to_end():
    logger.debug("Starting end-to-end test")

    try:
        # Load data using arckit
        logger.debug("Loading data using arckit")
        train_set, eval_set = arckit.load_data()
        
        # Create datasets using ARCDataset
        logger.debug("Creating train and validation datasets")
        full_dataset = ARCDataset(train_set, is_test=False)
        # Use a smaller subset of the dataset
        subset_size = int(0.1 * len(full_dataset))  # Use 10% of the dataset
        train_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size])
        val_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size])
        logger.debug(f"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}")

        # Create a custom collate function to handle the data format
        def collate_fn(batch):
            inputs = [item[0].to(torch.float32) for item in batch]  # Convert to float32
            outputs = [item[1].to(torch.float32) for item in batch]  # Convert to float32
            logger.debug(f"Batch input dtypes before stack: {[item[0].dtype for item in batch]}")
            logger.debug(f"Batch output dtypes before stack: {[item[1].dtype for item in batch]}")

            # Inputs and outputs are already tensors, so we just need to stack them
            input_stack = torch.stack(inputs)
            output_stack = torch.stack(outputs)

            # Log data types after stacking
            logger.debug(f"Collate function input_stack dtype: {input_stack.dtype}")
            logger.debug(f"Collate function output_stack dtype: {output_stack.dtype}")

            # Create a dummy attention mask (all ones)
            attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32)

            logger.debug(f"Collate function attention_mask dtype: {attention_mask.dtype}")
            
            # Generate dummy task_ids for each item in the batch
            task_ids = [f"task_{i}" for i in range(len(batch))]
            
            return input_stack, attention_mask, output_stack, task_ids
            logger.debug(f"Batch output dtypes before stack: {[item[1].dtype for item in batch]}")

            # Inputs and outputs are already tensors, so we just need to stack them
            input_stack = torch.stack(inputs)
            output_stack = torch.stack(outputs)

            # Create a dummy attention mask (all ones)
            attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32)

            logger.debug(f"Collate function input dtype: {input_stack.dtype}")
            return input_stack, attention_mask, output_stack

        # Initialize model
        logger.debug("Initializing model")
        model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)  # Use smaller model configuration
        model = GPT2ARC(model_config).to(torch.float32)
        logger.debug(f"Model initialized with config: {model_config}")

        # # THOP Profiling - Commented out due to TypeError with MPS Tensors
        # logger.debug("Profiling model with THOP")
        # dummy_input = torch.randn(1, 1, 28, 28, dtype=torch.float32)  # Example input shape
        # macs, params = profile(model, inputs=(dummy_input,))
        # macs, params = clever_format([macs, params], "%.3f")
        # logger.info(f"MACs: {macs}, Parameters: {params}")

        # Initialize trainer
        logger.debug("Initializing trainer")
        config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))  # Reduce epochs to 2
        trainer = ARCTrainer(model, train_dataset, val_dataset, config)
        trainer.train_dataloader = lambda: torch.utils.data.DataLoader(train_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0)
        trainer.val_dataloader = lambda: torch.utils.data.DataLoader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0)
        trainer.test_dataloader = lambda: torch.utils.data.DataLoader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0)
        logger.debug(f"Trainer initialized with config: {config}")

        # Create PyTorch Lightning trainer
        logger.debug("Creating PyTorch Lightning trainer")
        # Measure training time
        start_time = time.time()
        
        pl_trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False
        )
        logger.debug("PyTorch Lightning trainer created")

        # Evaluate model before training to get initial accuracy
        logger.info("Evaluating model before training")
        initial_val_results = pl_trainer.test(trainer, verbose=False)
        logger.debug(f"Initial validation results: {initial_val_results}")
        initial_accuracy = initial_val_results[0].get('test_accuracy')
        initial_loss = initial_val_results[0].get('test_loss')

        print(f"Initial validation results: {initial_val_results}")
        assert initial_accuracy is not None, "Initial validation results missing 'test_accuracy'"
        assert initial_loss is not None, "Initial validation results missing 'test_loss'"
        logger.info(f"Initial validation accuracy: {initial_accuracy}, Initial loss: {initial_loss}")
        print(f"Initial validation accuracy: {initial_accuracy}, Initial loss: {initial_loss}")
        logger.debug("Starting model training")
        pl_trainer.fit(trainer)
        end_time = time.time()
        training_time = end_time - start_time
        logger.info(f"Total training time: {training_time:.2f} seconds")
        logger.debug("Model training completed")

        # Check that loss decreased
        train_losses = trainer.train_losses
        logger.info(f"Training losses: {train_losses}")
        assert train_losses[-1] &lt; train_losses[0], f"Training loss did not decrease. Initial loss: {train_losses[0]}, Final loss: {train_losses[-1]}"
        
        # Check that the final loss is lower than the initial loss
        assert train_losses[-1] &lt; train_losses[0], "Final training loss should be lower than initial loss"

        # Check that the average loss per epoch decreases
        epoch_losses = [sum(train_losses[i:i+33])/33 for i in range(0, len(train_losses), 33)]
        assert all(epoch_losses[i] &gt; epoch_losses[i+1] for i in range(len(epoch_losses)-1)), "Average training loss per epoch did not consistently decrease"

        # Evaluate model after training
        logger.debug("Evaluating model after training")
        final_val_results = pl_trainer.test(trainer, verbose=False)
        final_accuracy = final_val_results[0]['test_accuracy']
        final_loss = final_val_results[0]['test_loss']
        logger.info(f"Final validation accuracy: {final_accuracy}, Final loss: {final_loss}")
        print(f"Final validation accuracy: {final_accuracy}, Final loss: {final_loss}")

        # Check that validation accuracy improved
        assert final_accuracy &gt; initial_accuracy, f"Validation accuracy did not improve. Initial accuracy: {initial_accuracy}, Final accuracy: {final_accuracy}"

        logger.info(f"Final training loss: {train_losses[-1]:.4f}")
        logger.info(f"Validation accuracy: {final_accuracy:.4f}")

        # Check model parameters
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        assert total_params &gt; 0, "Model has no parameters"
        assert trainable_params &gt; 0, "Model has no trainable parameters"
        assert trainable_params == total_params, "Not all parameters are trainable"

        logger.debug(f"Total parameters: {total_params}")
        logger.debug(f"Trainable parameters: {trainable_params}")

        logger.debug("End-to-end test completed successfully")
    except Exception as e:
        logger.error(f"End-to-end test failed with error: {str(e)}")
        raise

def test_evaluation_process_with_arckit_data():
    logger.debug("Starting evaluation process test with arckit data")

    # Load data using arckit
    _ , evaluation_data = arckit.load_data()

    # Log the structure of evaluation data
    logger.debug(f"Evaluation data structure: {evaluation_data}")
    logger.debug(f"Evaluation data structure: {evaluation_data}")
    test_dataset = ARCDataset(evaluation_data, is_test=True)

    # Initialize the model and trainer
    model_configuration = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    model = GPT2ARC(model_configuration)
    training_configuration = Config(model=model_configuration, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))
    trainer = ARCTrainer(model, None, test_dataset, training_configuration)

    # Run the evaluation
    lightning_trainer = pl.Trainer(logger=False, enable_checkpointing=False, enable_progress_bar=False)
    evaluation_results = lightning_trainer.test(trainer)

    # Access the test results from the trainer
    evaluation_results = trainer.test_results

    # Log the evaluation results
    logger.debug(f"Evaluation results: {evaluation_results}")
    for result in evaluation_results:
        task_ids = result.get('task_ids', [])
        if not task_ids:
            logger.error(f"Missing task_ids in result: {result}")
        else:
            for task_id in task_ids:
                logger.info(f"Task {task_id}: Loss={result['test_loss']}, Accuracy={result['test_accuracy']}")

    # Check for duplicate metrics
    unique_task_ids = set(task_id for result in evaluation_results for task_id in result.get('task_ids', []))
    print("All task IDs:", [task_id for result in evaluation_results for task_id in result.get('task_ids', [])])
    print("Unique task IDs:", unique_task_ids)
    print(f"Number of evaluation results: {len(evaluation_results)}")
    print(f"Number of unique task IDs: {len(unique_task_ids)}")

    if len(unique_task_ids) != len(evaluation_results):
        print("Warning: Number of unique task IDs doesn't match number of evaluation results")
        duplicate_tasks = [task_id for task_id in unique_task_ids if sum(task_id in result.get('task_ids', []) for result in evaluation_results) &gt; 1]
        print(f"Duplicate task IDs: {duplicate_tasks}")
        for task_id in duplicate_tasks:
            print(f"Results for task {task_id}:")
            for result in evaluation_results:
                if task_id in result.get('task_ids', []):
                    print(result)
    print(f"Unique task IDs: {unique_task_ids}")
    print(f"Evaluation results: {evaluation_results}")
    assert len(unique_task_ids) &gt; 0, "No tasks were evaluated"

    logger.debug("Completed evaluation process test with arckit data")

</file>
<file name="tests/test_pytest_error_fixer.py">
# gpt2_arc/tests/test_pytest_error_fixer.py
import os
import json
import pytest
from unittest.mock import patch, MagicMock
from pytest_error_fixer import PytestErrorFixer

# Reusable fixtures for test setup
@pytest.fixture
def error_fixer(tmp_path):
    # Initialize PytestErrorFixer with a temporary directory for the progress log
    fixer = PytestErrorFixer("test_project_dir")
    fixer.progress_log = tmp_path / "test_progress_log.json"
    fixer.error_log = tmp_path / "test_error_log.json"
    return fixer

@pytest.fixture
def sample_errors():
    # Sample errors for testing
    return {
        "gpt2_arc/test_file.py": [
            "test_function AssertionError: assert 1 == 2",
            "test_another_function TypeError: unsupported operand type(s) for +: 'int' and 'str'"
        ]
    }

# 1. Test for progress log initialization
def test_init_progress_log(error_fixer):
    error_fixer.init_progress_log()
    assert os.path.exists(error_fixer.progress_log)
    with open(error_fixer.progress_log, 'r') as f:
        assert json.load(f) == []  # Ensure the log is empty upon initialization

# 2. Test for logging progress in the progress log
def test_log_progress(error_fixer):
    error_fixer.init_progress_log()
    error_fixer.log_progress("fixed", "test error", "test_file.py")
    with open(error_fixer.progress_log, 'r') as f:
        log = json.load(f)
        assert len(log) == 1
        assert log[0] == {"error": "test error", "file": "test_file.py", "status": "fixed"}

# 3. Test for running full test suite and capturing output
@patch('subprocess.run')
def test_run_full_test(mock_run, error_fixer):
    # Mock the output of subprocess.run to simulate pytest execution
    mock_run.return_value = MagicMock(stdout="Test output", stderr="Test error")
    stdout, stderr = error_fixer.run_full_test()
    
    # Assert that stdout and stderr are captured correctly
    assert stdout == "Test output"
    assert stderr == "Test error"
    mock_run.assert_called_once()

# 4. Test for parsing errors from pytest output
def test_parse_errors(error_fixer):
    # Simulate pytest output with multiple errors
    sample_output = """
    gpt2_arc/test_file.py::test_function FAILED
    gpt2_arc/another_file.py::test_another_function FAILED
    """
    errors = error_fixer.parse_errors(sample_output)
    
    # Verify that errors are correctly parsed and associated with the right test files
    assert "gpt2_arc/test_file.py" in errors
    assert "gpt2_arc/another_file.py" in errors
    assert "test_function FAILED" in errors["gpt2_arc/test_file.py"]
    assert "test_another_function FAILED" in errors["gpt2_arc/another_file.py"]

# 5. Test for saving and loading errors to/from a JSON file
def test_save_and_load_errors(error_fixer, sample_errors):
    # Save errors to a file
    error_fixer.save_errors(sample_errors)
    
    # Load errors back and verify they match the original data
    loaded_errors = error_fixer.load_errors()
    assert loaded_errors == sample_errors

# 6. Test for predicting relevant files using aider's output
@patch.object(PytestErrorFixer, 'coder')
def test_predict_relevant_files(mock_coder, error_fixer):
    # Mock aider's file prediction output
    mock_coder.run.return_value = "The files likely involved are gpt2_arc/file1.py and gpt2_arc/file2.py"
    
    # Predict files for a test error
    files = error_fixer.predict_relevant_files("test error")
    
    # Assert that the correct files are predicted
    assert files == ["gpt2_arc/file1.py", "gpt2_arc/file2.py"]
    mock_coder.run.assert_called_once()

# 7. Test for fixing errors and retrying if needed
@patch('subprocess.run')
@patch.object(PytestErrorFixer, 'coder')
def test_fix_error(mock_coder, mock_run, error_fixer):
    # Simulate failed and successful pytest runs
    mock_run.side_effect = [
        MagicMock(stdout="Test failed", stderr="Error occurred"),
        MagicMock(stdout="Test PASSED", stderr="")
    ]
    
    # Simulate aider suggesting fixes
    mock_coder.run.return_value = "Suggested fix"
    
    # Run the fix_error method and verify it retries and eventually succeeds
    result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function")
    
    # Assert that the error is eventually fixed
    assert result == True
    assert mock_run.call_count == 2
    mock_coder.run.assert_called_once()

# 8. Edge case: Test for handling invalid error output (additional coverage)
def test_parse_errors_invalid_format(error_fixer):
    invalid_output = "This is not a valid pytest output"
    errors = error_fixer.parse_errors(invalid_output)
    assert errors == {}

# 9. Edge case: Test for retry exhaustion when errors remain unfixed
@patch('subprocess.run')
@patch.object(PytestErrorFixer, 'coder')
def test_retry_exhaustion(mock_coder, mock_run, error_fixer):
    # Simulate constant failure in pytest runs
    mock_run.side_effect = [
        MagicMock(stdout="Test failed", stderr="Error occurred")
    ] * 3  # Retry the maximum number of times
    
    mock_coder.run.return_value = "Suggested fix"
    
    # Run the fix_error method and ensure it retries up to the max limit
    result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function")
    
    # Assert that the retries are exhausted
    assert result == False
    assert mock_run.call_count == 3  # Ensure the retry mechanism works
    mock_coder.run.assert_called_once()

</file>
<file name="tests/test_trainer.py">
# gpt2_arc/tests/test_trainer.py
import pytest
import torch
from src.config import Config, ModelConfig, TrainingConfig
from src.data.arc_dataset import ARCDataset
from src.models.gpt2 import GPT2ARC
from src.training.trainer import ARCTrainer


@pytest.fixture
def sample_data():
    return [
        {
            "train": [
                {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}
            ],
            "test": [
                {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]}
            ]
        }
    ]


@pytest.fixture
def model():
    config = ModelConfig()
    return GPT2ARC(config)


@pytest.fixture
def trainer(model, sample_data):
    config = Config(model=ModelConfig(), training=TrainingConfig())
    train_dataset = ARCDataset(sample_data)
    val_dataset = ARCDataset(sample_data)
    trainer = ARCTrainer(model, train_dataset, val_dataset, config)
    trainer.logged_metrics = {}
    trainer.config.training.log_level = "INFO"  # Add this line
    trainer.log = lambda name, value, on_step=None, on_epoch=None, prog_bar=None, logger=None: trainer.logged_metrics.update({name: value})
    return trainer


def test_arctrainer_initialization(trainer):
    assert isinstance(trainer, ARCTrainer)
    assert hasattr(trainer, "model")
    assert hasattr(trainer, "train_dataset")
    assert hasattr(trainer, "val_dataset")


def test_arctrainer_forward_pass(trainer):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output = trainer(input_ids, attention_mask)

    assert isinstance(output, torch.Tensor)
    assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd)


@pytest.mark.parametrize("batch_format", ["tuple", "dict"])
def test_arctrainer_training_step(trainer, batch_format):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    vocab_size = 10  # Use a small vocab size for testing
    if batch_format == "tuple":
        batch = (
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            torch.ones((batch_size, seq_length)).float(),
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        )
    else:
        batch = {
            "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            "attention_mask": torch.ones((batch_size, seq_length)).float(),
            "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        }
    loss = trainer.training_step(batch, 0)

    assert isinstance(loss, torch.Tensor)
    assert loss.shape == torch.Size([])
    assert not torch.isnan(loss).any(), "Loss contains NaN values"
    assert not torch.isinf(loss).any(), "Loss contains infinity values"


def test_training_step_with_list_input():
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=2, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    trainer = ARCTrainer(model, None, None, config)

    batch_size = 2
    vocab_size = 10

    # Create a batch as a tuple of length 3 (input_ids, attention_mask, labels)
    inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float()
    inputs_flat = inputs.view(batch_size, -1)

    attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float()

    labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long()
    labels_flat = labels.view(batch_size, -1)

    batch = (
        inputs_flat,       # input_ids
        attention_mask,    # attention_mask
        labels_flat        # labels
    )

    loss = trainer.training_step(batch, 0)

    assert isinstance(loss, torch.Tensor), "Loss should be a torch.Tensor"
    assert loss.shape == torch.Size([]), "Loss should be a scalar"
    assert not torch.isnan(loss).any(), "Loss should not be NaN"
    assert not torch.isinf(loss).any(), "Loss should not be infinity"

def test_validation_step_with_list_input():
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=2, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    trainer = ARCTrainer(model, None, None, config)

    batch_size = 2
    vocab_size = 10

    # Create inputs and labels
    inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float()
    inputs_flat = inputs.view(batch_size, -1)  # Flatten inputs

    labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long()
    labels_flat = labels.view(batch_size, -1)  # Flatten labels

    # Create an attention mask
    attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float()

    # Create batch as a tuple of length 3
    batch = (
        inputs_flat,       # input_ids
        attention_mask,    # attention_mask
        labels_flat        # labels
    )

    trainer.validation_step(batch, 0)

    assert "val_loss" in trainer.logged_metrics, "Validation loss should be logged"
    assert isinstance(trainer.logged_metrics["val_loss"], float), "Logged validation loss should be a float"

@pytest.mark.parametrize("batch_format", ["tuple", "dict"])
def test_arctrainer_validation_step(trainer, batch_format):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    vocab_size = 10  # Use a small vocab size for testing
    if batch_format == "tuple":
        batch = (
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            torch.ones((batch_size, seq_length)).float(),
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        )
    else:
        batch = {
            "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            "attention_mask": torch.ones((batch_size, seq_length)).float(),
            "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        }
    trainer.validation_step(batch, 0)

    # Check if val_loss is logged
    assert "val_loss" in trainer.logged_metrics


def test_arctrainer_configure_optimizers(trainer):
    optimizers, schedulers = trainer.configure_optimizers()
    assert any(isinstance(opt, torch.optim.Adam) for opt in optimizers), "Expected an Adam optimizer"
    assert any(sch['scheduler'].__class__.__name__ == 'StepLR' for sch in schedulers), "Expected a StepLR scheduler"


def test_arctrainer_train_dataloader(trainer):
    dataloader = trainer.train_dataloader()
    assert isinstance(dataloader, torch.utils.data.DataLoader)
    assert len(dataloader.dataset) == len(trainer.train_dataset)


def test_arctrainer_val_dataloader(trainer):
    dataloader = trainer.val_dataloader()
    assert isinstance(dataloader, torch.utils.data.DataLoader)
    assert len(dataloader.dataset) == len(trainer.val_dataset)
def test_arctrainer_test_step_with_task_ids(trainer):
    batch_size = 2
    height = width = 30
    num_symbols = 10
    
    # Create a mock batch
    inputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).float()
    outputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).long()
    task_ids = ['task1', 'task2']
    
    batch = (inputs, outputs, task_ids)
    
    # Run the test step
    result = trainer.test_step(batch, 0)
    
    # Check if the result contains the expected keys
    assert 'test_loss' in result
    assert 'test_accuracy' in result
    assert 'task_ids' in result
    
    # Check if 'test_loss', 'test_accuracy', 'test_diff_accuracy' are logged
    assert 'test_loss' in trainer.logged_metrics
    assert 'test_accuracy' in trainer.logged_metrics
    assert 'test_diff_accuracy' in trainer.logged_metrics

    # Check if task-specific metrics were logged
    for task_id in task_ids:
        assert f'{task_id}_test_loss' in trainer.logged_metrics
        assert f'{task_id}_test_accuracy' in trainer.logged_metrics
        assert f'{task_id}_test_diff_accuracy' in trainer.logged_metrics

</file>
<file name="src/evaluate.py">
# gpt2_arc/src/evaluate.py
import sys
import os
import json
import argparse
import pytorch_lightning as pl
import torch
import wandb
from datetime import datetime

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

from gpt2_arc.src.config import Config, ModelConfig
import arckit
import logging
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.utils.helpers import differential_pixel_accuracy

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def evaluate(model, test_dataset, batch_size=32):
    trainer = ARCTrainer(model, None, test_dataset, config=Config())
    pl_trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu')
    results = pl_trainer.test(trainer)
    
    # Collect individual task metrics
    individual_metrics = []
    for result in results:
        task_id = result.get('task_id', 'unknown')
        individual_metrics.append((task_id, result))

    # Log individual task metrics
    logger.info("Individual Task Metrics:")
    for task_id, metrics in individual_metrics:
        logger.info(f"Task {task_id}: {metrics}")

    return results[0], individual_metrics

def load_config_from_json(json_path):
    with open(json_path, 'r') as f:
        data = json.load(f)
    return data['config']

def save_results(results, individual_metrics, output_dir, model_name):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{model_name}_eval_results_{timestamp}.json"
    output_path = os.path.join(output_dir, filename)
    
    with open(output_path, 'w') as f:
        json.dump({
            "aggregate_results": results,
            "individual_metrics": individual_metrics
        }, f, indent=2)
    
    logger.info(f"Results saved to {output_path}")
    return output_path

def main(args):
    # Initialize wandb
    wandb.init(project=args.wandb_project, name=args.wandb_run_name)

    # Load the test data using arckit
    _, test_set = arckit.load_data()
    test_data = ARCDataset(test_set)

    # Load the checkpoint
    checkpoint = torch.load(args.model_checkpoint)

    # Load the configuration from the JSON file
    config_path = f"results/experiment_{args.model_checkpoint.split('_')[-1].replace('.pth', '.json')}"
    config_dict = load_config_from_json(config_path)
    model_config = ModelConfig(
        n_embd=config_dict['model']['n_embd'],
        n_head=config_dict['model']['n_head'],
        n_layer=config_dict['model']['n_layer'],
        dropout=config_dict['model']['dropout']
    )

    # Initialize the model with the checkpoint configuration
    model = GPT2ARC(model_config)
    model.load_state_dict(checkpoint)
    model.eval()
    logger.info("Model set to evaluation mode")

    # Evaluate the model
    results, individual_metrics = evaluate(model, test_data, args.batch_size)

    logger.info("Evaluation Results:")
    for metric, value in results.items():
        print(f"{metric}: {value}")
        wandb.log({f"eval/{metric}": value})

    # Save results locally
    model_name = os.path.basename(args.model_checkpoint).split('.')[0]
    results_path = save_results(results, individual_metrics, args.output_dir, model_name)

    # Log results file to wandb
    wandb.save(results_path)

    wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the ARC Neural Reasoning Model")
    parser.add_argument("--model_checkpoint", type=str, required=True, help="Path to the model checkpoint")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for evaluation")
    parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="Directory to save evaluation results")
    parser.add_argument("--wandb_project", type=str, default="arc-evaluation", help="Weights &amp; Biases project name")
    parser.add_argument("--wandb_run_name", type=str, default=None, help="Weights &amp; Biases run name")

    args = parser.parse_args()
    
    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)
    
    main(args)

</file>
<file name="src/__init__.py">
# This file allows the src directory to be recognized as a package.

</file>
<file name="src/optimize_hyperparameters.py">
# gpt2_arc/src/optimize_hyperparameters.py
import optuna
import logging
import sys
import os
import torch
import pytorch_lightning as pl
import psutil
import gc

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.data.arc_dataset import ARCDataset
import arckit

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def log_memory_usage():
    process = psutil.Process(os.getpid())
    memory_info = process.memory_info()
    logger.info(f"Memory usage: {memory_info.rss / 1024 / 1024:.2f} MB")

def objective(trial):
    logger.info(f"Starting trial {trial.number}")
    log_memory_usage()
    
    # Suggest hyperparameters
    model_config = ModelConfig(
        n_embd=trial.suggest_int("n_embd", 32, 128),
        n_head=trial.suggest_int("n_head", 2, 4),
        n_layer=trial.suggest_int("n_layer", 1, 3)
    )
    training_config = TrainingConfig(
        batch_size=trial.suggest_int("batch_size", 16, 64),
        learning_rate=trial.suggest_float("learning_rate", 1e-5, 1e-2, log=True),
        max_epochs=trial.suggest_int("max_epochs", 5, 20)
    )
    config = Config(model=model_config, training=training_config)
    
    logger.info(f"Trial {trial.number} config: {config}")
    
    try:
        # Load data
        train_set, eval_set = arckit.load_data()
        train_data = ARCDataset(train_set)
        val_data = ARCDataset(eval_set)
        logger.info(f"Loaded {len(train_data)} training samples and {len(val_data)} validation samples")
        
        # Create model and trainer
        model = GPT2ARC(config.model)
        arc_trainer = ARCTrainer(model, train_data, val_data, config)

        # Set up PyTorch Lightning trainer
        trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            accelerator='gpu' if torch.cuda.is_available() else 'cpu',
            devices=1,
            logger=False,  # Disable logging to keep things simple
            enable_checkpointing=False,  # Disable checkpointing for simplicity
            accumulate_grad_batches=4,  # Add gradient accumulation
        )

        # Train and evaluate
        logger.info(f"Starting training for trial {trial.number}")
        log_memory_usage()
        trainer.fit(arc_trainer)
        
        # Test the model
        log_memory_usage()
        test_results = trainer.test(arc_trainer)
        val_accuracy = test_results[0]['test_accuracy']  # Assuming test_accuracy is logged

        logger.info(f"Trial {trial.number} completed with validation accuracy: {val_accuracy}")
        log_memory_usage()
        
        # Clean up to free memory
        del trainer, arc_trainer, model
        torch.cuda.empty_cache()
        gc.collect()
        log_memory_usage()
        
        return val_accuracy
    
    except Exception as e:
        logger.error(f"Error in trial {trial.number}: {str(e)}")
        log_memory_usage()
        raise optuna.exceptions.TrialPruned()

def run_optimization(n_trials=100):
    study_name = "gpt2_arc_optimization"
    storage_name = "sqlite:///optuna_results.db"
    
    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        load_if_exists=True,
        direction="maximize"
    )
    
    logger.info(f"Starting optimization with {n_trials} trials")
    study.optimize(objective, n_trials=n_trials)
    
    logger.info("Optimization completed")
    if study.best_trial:
        logger.info(f"Best trial: {study.best_trial.number}")
        logger.info(f"Best value: {study.best_value}")
        logger.info("Best hyperparameters:")
        for key, value in study.best_params.items():
            logger.info(f"  {key}: {value}")
    else:
        logger.warning("No successful trials found.")

if __name__ == "__main__":
    run_optimization(n_trials=10)  # Start with a small number of trials for testing

</file>
<file name="src/config.py">
# gpt2_arc/src/config.py
from dataclasses import dataclass, asdict


@dataclass
class ModelConfig:
    n_embd: int = 768
    n_head: int = 12
    n_layer: int = 12
    dropout: float = 0.1


@dataclass
class TrainingConfig:
    batch_size: int = 32
    learning_rate: float = 1e-4
    max_epochs: int = 10
    use_gpu: bool = True
    log_level: str = "INFO"  # Add log_level attribute with default value


from dataclasses import field


@dataclass
class Config:
    def to_dict(self):
        return asdict(self)
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)

</file>
<file name="src/utils/experiment_tracker.py">
# gpt2_arc/src/utils/experiment_tracker.py
import wandb
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
from typing import Dict, Any, Optional

class ExperimentTracker:
    def __init__(self, config: Dict[str, Any], project: str, entity: Optional[str] = None, use_wandb: bool = False):
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = config.to_dict() if hasattr(config, 'to_dict') else self._config_to_dict(config)
        self.project = project
        self.entity = entity
        self.run = None
        self.use_wandb = use_wandb
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False

        self.results = {
            "train": [],
            "validation": [],
            "test": {}
        }
        self.metrics = {}
        self.task_specific_results = {}
        self.environment = self._get_environment_info()
        self.checkpoint_path = None

        # Add debug logging
        print(f"ExperimentTracker initialized with config: {json.dumps(self.config, indent=2)}")
        print(f"Project: {project}, Entity: {entity}")
        print(f"use_wandb: {self.use_wandb}")

    def _get_environment_info(self) -&gt; Dict[str, str]:
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    def _config_to_dict(self, config):
        if isinstance(config, dict):
            return {k: self._config_to_dict(v) for k, v in config.items()}
        elif hasattr(config, '__dict__'):
            return {k: self._config_to_dict(v) for k, v in config.__dict__.items() if not k.startswith('_')}
        else:
            return config
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False
        
        if not self.use_wandb:
            print("Using local logging only")

    def finish(self):
        if self.use_wandb and self.run:
            try:
                wandb.finish()
                print("Wandb run finished")
            except Exception as e:
                print(f"Error finishing wandb run: {str(e)}")

    def log_metric(self, name: str, value: float, step: Optional[int] = None):
        if self.use_wandb:
            try:
                wandb.log({name: value}, step=step)
                print(f"Logged metric to wandb: {name}={value}, step={step}")
            except Exception as e:
                print(f"Error logging metric to wandb: {str(e)}")
        
        # Always log locally as a fallback
        print(f"Logged metric locally: {name}={value}, step={step}")

    def update_train_metrics(self, epoch: int, metrics: Dict[str, float]):
        if "train" not in self.results:
            self.results["train"] = []
        while len(self.results["train"]) &lt;= epoch:
            self.results["train"].append({})
        self.results["train"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"train": metrics}, step=epoch)

    def update_val_metrics(self, epoch: int, metrics: Dict[str, float]):
        if "validation" not in self.results:
            self.results["validation"] = []
        while len(self.results["validation"]) &lt;= epoch:
            self.results["validation"].append({})
        self.results["validation"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"validation": metrics}, step=epoch)

    def set_test_results(self, metrics: Dict[str, float]):
        self.results["test"] = metrics
        if self.use_wandb:
            wandb.log({"test": metrics})

    def add_task_specific_result(self, task_id: str, metrics: Dict[str, float]):
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        self.task_specific_results[task_id].update(metrics)
        if self.use_wandb:
            wandb.log({f"task_{task_id}": metrics})

    def set_final_metrics(self, metrics: Dict[str, float]):
        self.metrics = metrics
        if self.use_wandb:
            wandb.log(metrics)

    def set_checkpoint_path(self, path: str):
        self.checkpoint_path = path
        if self.use_wandb:
            wandb.save(path)

    def save_to_json(self, filepath: str):
        try:
            directory = os.path.dirname(filepath)
            if directory and not os.path.exists(directory):
                os.makedirs(directory)
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path
            }
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"Results saved to {filepath}")
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    def _ensure_directory_exists(self, directory: str):
        if not os.path.exists(directory):
            os.makedirs(directory)

    def get_summary(self) -&gt; Dict[str, Any]:
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_accuracy": self.results["test"].get("accuracy"),
            "config": self._serialize_config(self.config)
        }
        return {k: self._make_serializable(v) for k, v in summary.items()}

    def _make_serializable(self, obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

    def _serialize_config(self, config):
        return {k: self._make_serializable(v) for k, v in config.items()}

# Add a simple test
if __name__ == "__main__":
    config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": True}
    tracker = ExperimentTracker(config, project="test-project")
    tracker.start()
    tracker.log_metric("accuracy", 0.85, step=1)
    tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8})
    tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75})
    tracker.set_test_results({"loss": 0.55, "accuracy": 0.82})
    tracker.add_task_specific_result("task_1", {"accuracy": 0.9})
    tracker.set_final_metrics({"best_accuracy": 0.85})
    tracker.set_checkpoint_path("model_checkpoint.pth")
    tracker.save_to_json("results.json")
    tracker.finish()

</file>
<file name="src/utils/__init__.py">

</file>
<file name="src/utils/results_collector.py">
# gpt2_arc/src/utils/results_collector.py
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
from typing import Dict, Any

class ResultsCollector:
    def __init__(self, config):
        """Initialize the ResultsCollector with a given configuration."""
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = asdict(config)
        self.results = {
            "train": {},
            "validation": {},
            "test": {}
        }
        print(f"DEBUG: Initialized self.results['train'] as {type(self.results['train'])}")
        self._log_results_type("After initialization")
        self.metrics = {}
        self.task_specific_results = {}
        self.environment = self._get_environment_info()
        self.checkpoint_path = None

    def _get_environment_info(self) -&gt; Dict[str, str]:
        """Retrieve environment information such as Python and PyTorch versions."""
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    def _log_results_type(self, context: str):
        """Log the type of self.results['train'] for debugging."""
    def update_train_metrics(self, epoch: int, metrics: Dict[str, float]):
        print(f"DEBUG: self.results['train'] is of type {type(self.results['train'])}")
        """Update training metrics for a specific epoch."""
        self._log_results_type("Before checking 'train' in results")
        if "train" not in self.results:
            self.results["train"] = {}
        self._log_results_type("Before type check")
        if not isinstance(self.results["train"], dict):
            raise TypeError(f"Expected self.results['train'] to be a dict, but got {type(self.results['train'])}")
        self._log_results_type("Before setting default")
        print(f"DEBUG: Before setting default, self.results['train'] is of type {type(self.results['train'])}")
        self.results["train"].setdefault(epoch, {})
        self._log_results_type("After setting default")
        print(f"DEBUG: After setting default, self.results['train'] is of type {type(self.results['train'])}")
        self.results["train"][epoch].update(metrics)

    def update_val_metrics(self, epoch: int, metrics: Dict[str, float]):
        """Update validation metrics for a specific epoch."""
        if "validation" not in self.results:
            self.results["validation"] = {}
        self.results["validation"][epoch] = metrics

    def set_test_results(self, metrics: Dict[str, float]):
        """Set the test results metrics."""
        self.results["test"] = metrics

    def add_task_specific_result(self, task_id: str, metrics: Dict[str, float]):
        """Add task-specific results for a given task ID."""
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        self.task_specific_results[task_id].update(metrics)

    def set_final_metrics(self, metrics: Dict[str, float]):
        """Set the final metrics after training."""
        self.metrics = metrics

    def set_checkpoint_path(self, path: str):
        """Set the path to the model checkpoint."""
        self.checkpoint_path = path

    def save_to_json(self, filepath: str):
        """Save the collected results to a JSON file."""
        try:
            self._ensure_directory_exists(os.path.dirname(filepath))
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path
            }
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    def _ensure_directory_exists(self, directory: str):
        """Ensure that the directory exists; create it if it does not."""
        if not os.path.exists(directory):
            os.makedirs(directory)

    def get_summary(self) -&gt; Dict[str, Any]:
        """Get a summary of the results."""
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_accuracy": self.results["test"].get("accuracy"),
            "config": self._serialize_config(self.config)
        }
        return {k: self._make_serializable(v) for k, v in summary.items()}

    def _make_serializable(self, obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

    def _serialize_config(self, config):
        return {k: self._make_serializable(v) for k, v in config.items()}

</file>
<file name="src/utils/helpers.py">
# gpt2_arc/src/utils/helpers.py
import torch

def differential_pixel_accuracy(input, target, prediction):
    print(f"Differential pixel accuracy - Input shape: {input.shape}, Target shape: {target.shape}, Prediction shape: {prediction.shape}")
    
    assert isinstance(input, torch.Tensor) and isinstance(target, torch.Tensor) and isinstance(prediction, torch.Tensor), "All inputs must be torch.Tensor"
    assert input.numel() == target.numel() == prediction.numel(), "Input, target, and prediction must have the same number of elements"

    input = input.view_as(target)
    prediction = prediction.view_as(target)
    
    print(f"Reshaped - Input: {input.shape}, Target: {target.shape}, Prediction: {prediction.shape}")

    input_target_diff = input != target
    correct_diff_predictions = (prediction == target) &amp; input_target_diff

    total_diff_pixels = input_target_diff.sum().item()
    correct_diff_pixels = correct_diff_predictions.sum().item()

    print(f"Total different pixels: {total_diff_pixels}")
    print(f"Correctly predicted different pixels: {correct_diff_pixels}")

    if total_diff_pixels &gt; 0:
        accuracy = correct_diff_pixels / total_diff_pixels
    else:
        accuracy = 1.0  # If no pixels differ, consider it 100% accurate

    print(f"Calculated accuracy: {accuracy}")
    return accuracy, input_target_diff, correct_diff_predictions

</file>
<file name="src/models/gpt2.py">
# gpt2_arc/src/models/gpt2.py

import logging

import torch
import torch.nn.functional as F
from torch import nn
import torch.nn.init as init

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class Attention(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        self.n_head = n_head
        self.n_embd = n_embd
        self.key = nn.Linear(n_embd, n_embd)
        self.query = nn.Linear(n_embd, n_embd)
        self.value = nn.Linear(n_embd, n_embd)
        self.proj = nn.Linear(n_embd, n_embd)
        logger.debug(f"Initialized Attention with n_embd={n_embd}, n_head={n_head}")

    def forward(self, x, mask=None):
        B, T, C = x.size()
        if not torch._dynamo.is_compiling():
            logger.debug(f"Attention input shape: {x.shape}")
        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32)))
        if mask is not None:
            att = att.masked_fill(mask[:, None, None, :] == 0, float("-inf"))
        att = F.softmax(att, dim=-1)
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        output = self.proj(y)
        if not torch._dynamo.is_compiling():
            logger.debug(f"Attention output shape: {output.shape}")
        return output


class FeedForward(nn.Module):
    def __init__(self, n_embd):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(n_embd, 4 * n_embd), nn.ReLU(), nn.Linear(4 * n_embd, n_embd)
        )
        logger.debug(f"Initialized FeedForward with n_embd={n_embd}")

    def forward(self, x):
        if not torch._dynamo.is_compiling():
            logger.debug(f"FeedForward input shape: {x.shape}")
        output = self.net(x)
        if not torch._dynamo.is_compiling():
            logger.debug(f"FeedForward output shape: {output.shape}")
        return output


class TransformerBlock(nn.Module):
    def __init__(self, n_embd, n_head):
        super().__init__()
        self.attention = Attention(n_embd, n_head)
        self.feed_forward = FeedForward(n_embd)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        logger.debug(
            f"Initialized TransformerBlock with n_embd={n_embd}, n_head={n_head}"
        )

    def forward(self, x, mask=None):
        if not torch._dynamo.is_compiling():
            logger.debug(f"TransformerBlock input shape: {x.shape}")
        x = x + self.attention(self.ln1(x), mask)
        x = x + self.feed_forward(self.ln2(x))
        if not torch._dynamo.is_compiling():
            logger.debug(f"TransformerBlock output shape: {x.shape}")
        return x


from dataclasses import dataclass
from ..config import ModelConfig

@dataclass
class ModelConfig:
    n_embd: int = 768
    n_head: int = 12
    n_layer: int = 12
    dropout: float = 0.1


class GPT2ARC(nn.Module):
    def __init__(self, config: ModelConfig):
        super().__init__()
        self.config = config
        # Replace token embedding with a convolutional layer
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=self.config.n_embd, kernel_size=3, padding=1).to(torch.float32)
        self.blocks = nn.ModuleList(
            [
                TransformerBlock(self.config.n_embd, self.config.n_head)
                for _ in range(self.config.n_layer)
            ]
        )
        self.ln_f = nn.LayerNorm(self.config.n_embd)
        
        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Conv2d):
            # Calculate fan_in for Conv2d
            fan_in = module.in_channels * module.kernel_size[0] * module.kernel_size[1]
            std = 1.0 / fan_in**0.5
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        elif isinstance(module, nn.Linear):
            fan_in = module.in_features
            std = 1.0 / fan_in**0.5
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        # No initialization for nn.LayerNorm, using default

    def forward(self, input_ids, attention_mask=None):
        if not torch._dynamo.is_compiling():
            logger.debug(f"GPT2ARC input shape: {input_ids.shape}, dtype: {input_ids.dtype}")
        
        # Check if input_ids is already in the correct shape
        if input_ids.dim() == 4:
            x = input_ids.float()
        else:
            # Reshape input_ids to [batch_size, 1, height, width]
            batch_size = input_ids.size(0)
            seq_length = input_ids.size(1)
            height = width = int(seq_length ** 0.5)
            x = input_ids.float().view(batch_size, 1, height, width)
        
        x = self.conv1(x)
        logger.debug(f"After conv1 shape: {x.shape}")
        b, c, h, w = x.size()
        x = x.view(b, c, h * w)  # Flatten spatial dimensions
        x = x.permute(0, 2, 1)  # Rearrange to (batch_size, sequence_length, channels)
        logger.debug(f"Reshaped for transformer blocks: {x.shape}")

        for i, block in enumerate(self.blocks):
            x = block(x, attention_mask)
            logger.debug(f"After block {i+1} shape: {x.shape}")
        
        x = self.ln_f(x)
        return x

</file>
<file name="src/models/__init__.py">

</file>
<file name="src/data/__init__.py">

</file>
<file name="src/data/arc_dataset.py">
# gp2_arc/src/data/arc_dataset.py
import os
import json
import random
from typing import Union, List, Dict, Tuple
import numpy as np
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
import logging

try:
    from arckit.data import TaskSet, Task
except ImportError:
    TaskSet = None

logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)  # Set to ERROR by default

# Create a handler that writes to stderr
handler = logging.StreamHandler()
handler.setLevel(logging.ERROR)

# Create a formatting for the logs
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)

# Add the handler to the logger
logger.addHandler(handler)

# Function to set debug mode
def set_debug_mode(debug=False):
    if debug:
        logger.setLevel(logging.DEBUG)
        handler.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.ERROR)
        handler.setLevel(logging.ERROR)

class ARCDataset(Dataset):
    def __init__(
        self,
        data_source: Union[str, List[Dict], 'TaskSet', Tuple[Union[List, 'TaskSet'], str]],
        is_test: bool = False,
        num_symbols: int = 10,
        test_split: float = 0.2,
        debug=False,
    ):
        print("DEBUG: Starting ARCDataset initialization")
        print(f"DEBUG: data_source type: {type(data_source)}")
        print(f"DEBUG: data_source content: {data_source}")
        logger.debug(f"Initializing ARCDataset with data_source: {data_source}")
        # Add logging to verify task_id presence
        if isinstance(data_source, list):
            for item in data_source:
                if isinstance(item, Task):
                    logger.debug(f"Processing Task object with ID: {item.id}")
                elif 'task_id' not in item:
                    logger.warning(f"Missing task_id in data item: {item}")
        self.debug_attr = "test"  # Simple attribute for testing
        set_debug_mode(debug)  # Set debug mode based on parameter
        logger.debug(f"Initializing ARCDataset with data_source type: {type(data_source)}")
        if isinstance(data_source, str):
            logger.debug(f"Data source path: {data_source}")
            if os.path.isdir(data_source):
                logger.debug("Processing synthetic data from directory")
                self.data = self._process_synthetic_data(data_source)
            elif os.path.isfile(data_source):
                logger.debug("Processing JSON data from file")
                with open(data_source, 'r') as f:
                    raw_data = json.load(f)
                self.data = self._process_json_data(raw_data)
            else:
                raise FileNotFoundError(f"Data source file or directory not found: {data_source}")
        elif isinstance(data_source, list):
            logger.debug("Processing list data")
            self.data = self._process_list_data(data_source)
        elif isinstance(data_source, tuple):
            logger.debug("Processing combined data")
            self.data = self._combine_data(*data_source)
        elif TaskSet is not None and isinstance(data_source, TaskSet):
            logger.debug("Processing ARCkit data")
            self.data = self._process_arckit_data(data_source)
        else:
            logger.error(f"Invalid data_source type: {type(data_source)}")
            raise ValueError("Data source must be either a file path, a list of tasks, or a TaskSet")

        print(f"DEBUG: Processed data length: {len(self.data)}")
        if self.data:
            print(f"DEBUG: First item keys: {self.data[0].keys()}")
            if 'train' in self.data[0]:
                print(f"DEBUG: First train item: {self.data[0]['train'][0] if self.data[0]['train'] else 'No train data'}")
        logger.debug(f"Number of tasks: {len(self.data)}")
        logger.debug(f"First task structure: {self.data[0].keys()}")
        if self.data and 'train' in self.data[0] and self.data[0]['train']:
            first_train_sample = self.data[0]['train'][0]
            if isinstance(first_train_sample, dict):
                logger.debug(f"First train sample structure: {first_train_sample.keys()}")
            else:
                logger.debug(f"First train sample is not a dict. Type: {type(first_train_sample)}, Content: {first_train_sample}")
        else:
            logger.debug("No train samples found in first task")
        logger.debug(f"First train input shape: {np.array(self.data[0]['train'][0]['input']).shape}")
        self.is_test = is_test
        self.num_symbols = num_symbols
        self.test_split = test_split
        logger.debug(f"test_split set to: {self.test_split}")
        self.test_split = test_split
        self.samples = []
        if TaskSet is not None and isinstance(data_source, TaskSet):
            for task in data_source.tasks:
                self.samples.extend(task.train)
                self.samples.extend(task.test)
        
        if isinstance(data_source, str):
            if os.path.isdir(data_source):
                self.data = self._process_synthetic_data(data_source)
            elif os.path.isfile(data_source):
                with open(data_source, 'r') as f:
                    raw_data = json.load(f)
                self.data = self._process_json_data(raw_data)
            else:
                raise FileNotFoundError(f"Data source file or directory not found: {data_source}")
        elif isinstance(data_source, list):
            self.data = self._process_list_data(data_source)
        elif isinstance(data_source, tuple):
            self.data = self._combine_data(*data_source)
        elif TaskSet is not None and isinstance(data_source, TaskSet):
            self.data = self._process_arckit_data(data_source)
        else:
            logger.error(f"Invalid data_source type: {type(data_source)}")
            raise ValueError("Data source must be either a file path, a list of tasks, or a TaskSet")
        
        print(f"Number of train samples: {sum(len(task['train']) for task in self.data)}")
        print(f"Number of test samples: {sum(len(task['test']) for task in self.data)}")
        self.max_grid_size = self._compute_max_grid_size()
        self._validate_data()

    def _process_json_data(self, raw_data: List[Dict]) -&gt; List[Dict]:
        processed_data = []
        for task in raw_data:
            logger.debug(f"Processing task: {task}")
            processed_task = {
                "train": [
                    {"input": np.array(example["input"]), "output": np.array(example["output"])}
                    for example in task["train"]
                ],
                "test": [
                    {"input": np.array(example["input"]), "output": np.array(example["output"])}
                    for example in task["test"]
                ]
            }
            processed_data.append(processed_task)
        # Flatten the data structure
        flattened_data = []
        for task in processed_data:
            flattened_data.extend(task['train'])
            flattened_data.extend(task['test'])
        
        return flattened_data

    def _validate_data(self):
        for task in self.data:
            for split in ["train", "test"]:
                if split in task:
                    for sample in task[split]:
                        if not ("input" in sample and "output" in sample):
                            raise ValueError(f"Each sample must contain 'input' and 'output'. Task: {task.get('id', 'unknown')}")
        print("Data validation passed.")

    def _compute_max_grid_size(self):
        max_h, max_w = 0, 0
        for task in self.data:
            for split in ['train', 'test']:
                for sample in task[split]:
                    if isinstance(sample['input'], torch.Tensor):
                        if sample['input'].dim() == 3:
                            h, w = sample['input'].shape[1], sample['input'].shape[2]
                        elif sample['input'].dim() == 2:
                            h, w = sample['input'].shape
                        else:
                            raise ValueError(f"Unexpected tensor dimensions: {sample['input'].dim()}")
                    elif isinstance(sample['input'], np.ndarray):
                        if sample['input'].ndim == 2:
                            h, w = sample['input'].shape
                        elif sample['input'].ndim == 3:
                            h, w = sample['input'].shape[1], sample['input'].shape[2]
                        else:
                            raise ValueError(f"Unexpected ndarray dimensions: {sample['input'].ndim}")
                    elif isinstance(sample['input'], list):
                        h, w = len(sample['input']), len(sample['input'][0])
                    else:
                        raise TypeError(f"Unexpected input type: {type(sample['input'])}")
                    
                    max_h = max(max_h, h)
                    max_w = max(max_w, w)
        
        logger.debug(f"Computed max grid size: ({max_h}, {max_w})")
        return (max_h, max_w)

    def _combine_data(self, official_data, synthetic_data_path):
        official_processed = self._process_arckit_data(official_data) if TaskSet is not None and isinstance(official_data, TaskSet) else official_data
        synthetic_processed = self._process_synthetic_data(synthetic_data_path)
        return official_processed + synthetic_processed

    def _process_synthetic_data(self, directory: str) -&gt; List[Dict]:
        processed_data = []
        for filename in os.listdir(directory):
            if filename.endswith('.json'):
                with open(os.path.join(directory, filename), 'r') as f:
                    task_data = json.load(f)
                    processed_data.append(self._process_single_task(task_data))
        return processed_data

    def _process_single_task(self, task_data: Union[Dict, List]) -&gt; Dict:
        logger.debug(f"Inside _process_single_task, test_split is: {self.test_split}")
        if isinstance(task_data, dict):
            train_examples = task_data.get("train", [])
            test_examples = task_data.get("test", [])
        elif isinstance(task_data, list):
            split_idx = int(len(task_data) * (1 - self.test_split))
            train_examples = task_data[:split_idx]
            test_examples = task_data[split_idx:]
        else:
            raise ValueError("Task data must be either a dictionary or a list")

        return {
            "train": [self._preprocess_grid(example) for example in train_examples],
            "test": [self._preprocess_grid(example) for example in test_examples]
        }

    def _process_arckit_data(self, taskset: 'TaskSet') -&gt; List[Dict]:
        processed_data = []
        logger.debug(f"Processing TaskSet with {len(taskset.tasks)} tasks")
        for task in taskset.tasks:
            logger.debug(f"Task ID: {task.id}")
        for task in taskset.tasks:
            logger.debug(f"Processing task: {task.id}")
            logger.debug(f"Task ID: {task.id}")
            logger.debug(f"Train samples: {len(task.train)}, Test samples: {len(task.test)}")
            processed_task = {
                "id": task.id,
                "train": [
                    {"input": np.array(ex[0]), "output": np.array(ex[1])}
                    for ex in task.train
                ],
                "test": [
                    {"input": np.array(ex[0]), "output": np.array(ex[1])}
                    for ex in task.test
                ]
            }
            processed_data.append(processed_task)
            logger.debug(f"Processed task {task.id}: Train samples: {len(processed_task['train'])}, Test samples: {len(processed_task['test'])}")
        logger.debug(f"Processed {len(processed_data)} tasks")
        return processed_data

    def __len__(self) -&gt; int:
        if self.is_test:
            total_samples = sum(len(task['test']) for task in self.data)
        else:
            total_samples = sum(len(task['train']) for task in self.data)
        logger.debug(f"Total samples in dataset: {total_samples}")
        return total_samples

    def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:
        logger.debug(f"ARCDataset __getitem__ called with idx {idx}")
        if idx &lt; 0 or idx &gt;= len(self):
            raise IndexError(f"Index {idx} out of range (total samples: {len(self)})")

        current_idx = 0
        for task in self.data:
            split = 'test' if self.is_test else 'train'
            if idx &lt; current_idx + len(task[split]):
                sample = task[split][idx - current_idx]
                input_grid = self._preprocess_grid(sample["input"])
                output_grid = self._preprocess_grid(sample["output"])
                logger.debug(f"Returning input shape: {input_grid.shape}, output shape: {output_grid.shape}")
                logger.debug(f"__getitem__ input dtype: {input_grid.dtype}, output dtype: {output_grid.dtype}")
                task_id = task.get("id", -1)  # Default to -1 if no id is found
                return input_grid, output_grid, task_id
            current_idx += len(task[split])

        raise RuntimeError("Unexpected error in __getitem__")

    def _validate_data(self):
        for task in self.data:
            for split in ["train", "test"]:
                if split not in task:
                    continue
                for idx, sample in enumerate(task[split]):
                    if "input" not in sample or "output" not in sample:
                        raise KeyError(f"Sample {idx} in task {split} set is missing 'input' or 'output' key")
                    input_data = sample["input"]
                    output_data = sample["output"]
                    if not (isinstance(input_data, (list, np.ndarray)) and isinstance(output_data, (list, np.ndarray))):
                        logger.warning(f"Sample {idx} in task {split} set 'input' or 'output' must be a list or numpy array")
                        continue
                    if isinstance(input_data, list):
                        input_data = np.array(input_data)
                    if isinstance(output_data, list):
                        output_data = np.array(output_data)
                    if input_data.ndim != 2 or output_data.ndim != 2:
                        raise ValueError(f"Sample {idx} in task {split} set 'input' and 'output' must be 2D lists")
                    if np.any(input_data &gt;= self.num_symbols) or np.any(output_data &gt;= self.num_symbols):
                        logger.warning(f"Sample {idx} in task {split} set contains invalid symbols (&gt;= {self.num_symbols})")

    def _compute_grid_size_stats(self):
        max_height, max_width = 0, 0
        for task in self.data:
            for split in ["train", "test"]:
                for sample in task[split]:
                    max_height = max(max_height, sample["input"].shape[0], sample["output"].shape[0])
                    max_width = max(max_width, sample["input"].shape[1], sample["output"].shape[1])
        self.max_grid_size = (max_height, max_width)

    def _compute_symbol_frequencies(self):
        symbol_counts = np.zeros(self.num_symbols, dtype=int)
        for task in self.data:
            for split in ["train", "test"]:
                for sample in task[split]:
                    symbol_counts += np.bincount(sample["input"].flatten(), minlength=self.num_symbols)
                    symbol_counts += np.bincount(sample["output"].flatten(), minlength=self.num_symbols)
        return symbol_counts / symbol_counts.sum()
    def _preprocess_grid(self, grid: Union[dict, np.ndarray]) -&gt; torch.Tensor:
        if isinstance(grid, dict):
            input_grid = np.array(grid['input'])
            logger.debug(f"Original grid shape: {input_grid.shape}")
            logger.debug(f"Original grid content:\n{input_grid}")
        elif isinstance(grid, np.ndarray):
            input_grid = grid
            logger.debug(f"Original grid shape: {input_grid.shape}")
            logger.debug(f"Original grid content:\n{input_grid}")
        else:
            raise ValueError(f"Unexpected grid type: {type(grid)}")

        # Pad the grid to 30x30
        padded_grid = self._pad_grid(input_grid, height=30, width=30)

        # Convert to tensor and add channel dimension
        grid_tensor = torch.tensor(padded_grid, dtype=torch.float32).unsqueeze(0)

        logger.debug(f"Preprocessed grid shape: {grid_tensor.shape}")
        logger.debug(f"Preprocessed grid content:\n{grid_tensor}")

        return grid_tensor
    def kronecker_scale(self, X, target_height=30, target_width=30):
        print(f"Kronecker scaling input shape: {X.shape}")
        h, w = X.shape
        scale_h = target_height / h
        scale_w = target_width / w
        d = int(np.floor(min(scale_h, scale_w)))
        
        X_scaled = np.kron(X, np.ones((d, d)))
        print(f"Kronecker scaled output shape: {X_scaled.shape}")
        return X_scaled

    def pad_grid(self, X, target_height=30, target_width=30):
        print(f"Padding input shape: {X.shape}")
        h, w = X.shape
        pad_h = (target_height - h) // 2
        pad_w = (target_width - w) // 2
        padded = np.pad(X, ((pad_h, target_height - h - pad_h), 
                            (pad_w, target_width - w - pad_w)), 
                        mode='constant')
        print(f"Padded output shape: {padded.shape}")
        return padded

    def reverse_scaling(self, X_orig, X_pred):
        print(f"Reverse scaling - Original shape: {X_orig.shape}, Prediction shape: {X_pred.shape}")
        h, w = X_orig.shape
        # Reshape X_pred to 2D if it's 1D
        if X_pred.ndim == 1:
            X_pred = X_pred.reshape((int(np.sqrt(X_pred.size)), -1))
        
        X_pred_cropped = X_pred[:h, :w]  # Crop to original size
        
        if h == X_pred.shape[0] and w == X_pred.shape[1]:
            print("No rescaling needed")
            return X_pred_cropped
        
        # Calculate the downscale factor
        d_h = X_pred_cropped.shape[0] // h
        d_w = X_pred_cropped.shape[1] // w
        
        # Ensure the dimensions are compatible for reshaping
        if d_h &gt; 0 and d_w &gt; 0:
            try:
                X_rev = X_pred_cropped.reshape(h, d_h, w, d_w).mean(axis=(1, 3))
            except ValueError as e:
                print(f"Error during reshaping: {e}")
                print(f"X_pred_cropped shape: {X_pred_cropped.shape}, h: {h}, w: {w}, d_h: {d_h}, d_w: {d_w}")
                raise
        else:
            print(f"Invalid downscale factors: d_h={d_h}, d_w={d_w}")
            raise ValueError("Invalid dimensions for reverse scaling")
        # Resize the result to match the original target shape
        result = np.resize(X_rev.round().astype(int), X_orig.shape)
        print(f"Reverse scaled output shape: {result.shape}")
        return result

    def _scale_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray:
        return grid  # No scaling, preserve original size

    def _pad_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray:
        h, w = grid.shape
        pad_h = (height - h) // 2
        pad_w = (width - w) // 2
        return np.pad(grid, ((pad_h, height - h - pad_h), (pad_w, width - w - pad_w)), mode='constant')
    def _process_list_data(self, data_source):
        print(f"DEBUG: Processing {len(data_source)} items")
        processed_data = []
        for idx, item in enumerate(data_source):
            print(f"DEBUG: Processing item {idx}")
            print(f"DEBUG: Item type: {type(item)}")
            print(f"DEBUG: Item content: {item}")

            if isinstance(item, Task):
                processed_item = {
                    "train": [{"input": np.array(ex[0]), "output": np.array(ex[1])} for ex in item.train],
                    "test": [{"input": np.array(ex[0]), "output": np.array(ex[1])} for ex in item.test]
                }
            elif 'train' in item and 'test' in item:
                processed_item = {
                    "train": [{"input": np.array(sample["input"]), "output": np.array(sample["output"])} for sample in item['train']],
                    "test": [{"input": np.array(sample["input"]), "output": np.array(sample["output"])} for sample in item['test']]
                }
            elif 'input' in item and 'output' in item:
                processed_item = {
                    "train": [{"input": np.array(item["input"]), "output": np.array(item["output"])}],
                    "test": []
                }
            else:
                raise ValueError("Unexpected item format in data_source.")
        
            processed_data.append(processed_item)
    
        return processed_data


    @staticmethod
    def collate_fn(batch):
        print(f"Collating batch of size: {len(batch)}")
        if not batch:
            print("Warning: Empty batch received")
            return torch.tensor([]), torch.tensor([]), []
        
        try:
            inputs, outputs, task_ids = zip(*batch)
        except ValueError as e:
            print(f"Error unpacking batch: {e}")
            print(f"Batch content: {batch}")
            # Return empty tensors and list if unpacking fails
            return torch.tensor([]), torch.tensor([]), []

        print(f"Input shapes: {[i.shape for i in inputs]}")
        print(f"Output shapes: {[o.shape for o in outputs]}")

        # Find max dimensions in the batch
        max_h = max(i.size(1) for i in inputs)
        max_w = max(i.size(2) for i in inputs)

        print(f"Max dimensions: height={max_h}, width={max_w}")

        # Pad inputs and outputs to max size in the batch
        padded_inputs = torch.stack([F.pad(i, (0, max_w - i.size(2), 0, max_h - i.size(1))) for i in inputs])
        padded_outputs = torch.stack([F.pad(o, (0, max_w - o.size(2), 0, max_h - o.size(1))) for o in outputs])

        print(f"Padded input shape: {padded_inputs.shape}")
        print(f"Padded output shape: {padded_outputs.shape}")

        return [padded_inputs, padded_outputs, list(task_ids)]

</file>
<file name="src/training/__init__.py">

</file>
<file name="src/training/trainer.py">
# gpt2_arc/src/training/trainer.py
import pytorch_lightning as pl
import torch
import logging
from torch import nn, optim
import time
from typing import Any, Dict, Optional
from collections import deque
from torch.optim.lr_scheduler import LambdaLR
from ..config import Config
from ..utils.helpers import differential_pixel_accuracy
from ..utils.results_collector import ResultsCollector
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
import os

logger = logging.getLogger(__name__)


class ARCTrainer(pl.LightningModule):
    def __init__(self, model, train_dataset, val_dataset, config: Config):
        super().__init__()
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config
        self.batch_size = config.training.batch_size
        self.lr = config.training.learning_rate
        self.train_losses = []
        self.logged_metrics = {}
        self.test_outputs = []  # Store test outputs for aggregation
        self.test_results = []  # Initialize test results for storing test outcomes
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.results_collector = ResultsCollector(config)
        log_dir = f"runs/experiment_{self.results_collector.experiment_id}"
        os.makedirs(log_dir, exist_ok=True)  # Ensure the directory exists
        self.writer = SummaryWriter(log_dir)
        print(f"DEBUG: TensorBoard writer initialized for experiment {self.results_collector.experiment_id}")
        print(f"DEBUG: Logs will be written to {log_dir}")

    def training_step(self, batch, batch_idx):
        logger.debug(f"Training step - Batch type: {type(batch)}, length: {len(batch)}")
        
        if isinstance(batch, (list, tuple)) and len(batch) &gt;= 2:
            inputs, labels = batch[:2]
            task_ids = batch[2] if len(batch) &gt; 2 else None
        elif isinstance(batch, dict):
            inputs = batch.get("input_ids")
            labels = batch.get("labels")
            task_ids = batch.get("task_ids")
        else:
            raise ValueError(f"Unexpected batch format: {type(batch)}. Content: {batch}")

        # Ensure inputs and labels are the correct type
        inputs = inputs.float()
        labels = labels.long()

        outputs = self(inputs)
        loss = self.compute_loss(outputs, labels)
        
        if hasattr(self, 'log'):
            self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.results_collector.update_train_metrics(self.current_epoch, {"loss": loss.item()})
        
        try:
            self.writer.add_scalar('train/loss', loss.item(), self.current_epoch * len(self.train_dataloader()) + batch_idx)
            print(f"DEBUG: Logged training loss: {loss.item()} at step {self.current_epoch * len(self.train_dataloader()) + batch_idx}")
        except Exception as e:
            print(f"DEBUG: Error logging training step: {str(e)}")
        
        return loss

    def validation_step(self, batch, batch_idx, dataloader_idx=0):
        logger.debug(f"Validation step - Batch type: {type(batch)}, length: {len(batch)}")
        
        if isinstance(batch, (list, tuple)):
            if len(batch) &lt; 2:
                logger.error(f"Missing inputs or labels in batch. Inputs: {batch[0] if len(batch) &gt; 0 else None}, Labels: {batch[1] if len(batch) &gt; 1 else None}")
                raise ValueError("Batch must contain inputs and labels.")
            inputs, labels = batch[:2]
            task_ids = batch[2] if len(batch) &gt; 2 else None
        elif isinstance(batch, dict):
            inputs = batch.get("input_ids")
            labels = batch.get("labels")
            task_ids = batch.get("task_ids")
            if inputs is None or labels is None:
                logger.error(f"Missing inputs or labels in batch. Inputs: {inputs}, Labels: {labels}")
                raise ValueError("Batch must contain inputs and labels.")
        else:
            logger.error(f"Unexpected batch format: {type(batch)}. Content: {batch}")
            raise ValueError(f"Unexpected batch format: {type(batch)}. Content: {batch}")

        # Ensure inputs and labels are the correct type
        inputs = inputs.float()
        labels = labels.long()

        outputs = self(inputs)
        loss = self.compute_loss(outputs, labels)
        
        if hasattr(self, 'log'):
            self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.logged_metrics["val_loss"] = loss.item()
        self.results_collector.update_val_metrics(self.current_epoch, {"loss": loss.item()})
        
        try:
            self.writer.add_scalar('val/loss', loss.item(), self.current_epoch)
            print(f"DEBUG: Logged validation loss: {loss.item()} for epoch {self.current_epoch}")
        except Exception as e:
            print(f"DEBUG: Error logging validation step: {str(e)}")
        
        return loss

    def test_step(self, batch, batch_idx):
        """
        The `test_step` function processes a batch of data for testing a model, computes metrics such as
        loss and accuracy, logs the results, and stores them for further analysis.
        
        :param batch: The `batch` parameter in the `test_step` function is expected to contain input data,
        attention masks (optional), target outputs, and task IDs. The function processes the batch based on
        its format and performs inference using a model. It calculates loss, accuracy, and differential
        pixel accuracy metrics for evaluation
        :param batch_idx: Batch index is used to keep track of the current batch being processed during
        testing. It helps in identifying and logging information specific to each batch, such as loss and
        accuracy values. The batch index is typically an integer value that increments for each batch
        processed during testing
        :return: The `test_step` method returns a dictionary named `result` containing the keys 'loss',
        'accuracy', 'task_ids', and 'test_loss'. Additionally, it logs various metrics such as test_loss,
        test_accuracy, and test_diff_accuracy. The method also appends the result to `self.test_outputs` and
        calculates task success for TSR, logging it as well.
        """
        logger.debug(f"Test step - Batch type: {type(batch)}, length: {len(batch)}")

        if isinstance(batch, list) and len(batch) == 3:
            inputs, outputs, task_ids = batch
            attention_mask = None
        elif isinstance(batch, tuple) and len(batch) == 3:
            inputs, outputs, task_ids = batch
            attention_mask = None
        elif isinstance(batch, tuple) and len(batch) == 4:
            inputs, attention_mask, outputs, task_ids = batch
            logger.debug(f"Task IDs in batch: {task_ids}")
        else:
            raise ValueError(f"Unexpected batch format: {type(batch)}. Content: {batch}")

        # Ensure inputs and outputs are the correct type
        inputs = inputs.float()
        outputs = outputs.long()

        # Create a dummy attention mask if it's None
        if attention_mask is None:
            attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32, device=inputs.device)

        model_outputs = self(inputs, attention_mask)
        loss = self.compute_loss(model_outputs, outputs)
        
        B, T, C = model_outputs.size()
        model_outputs = model_outputs.view(B, -1, C)
        predictions = torch.argmax(model_outputs, dim=-1)
        outputs = outputs.view(B, -1)
        
        accuracy = (predictions == outputs).float().mean()
        diff_accuracy, _, _ = differential_pixel_accuracy(inputs, outputs, predictions)

        # Collect metrics in a dictionary
        metrics = {
            'test_loss': loss.item() if isinstance(loss, torch.Tensor) else loss,
            'test_accuracy': accuracy.item() if isinstance(accuracy, torch.Tensor) else accuracy,
            'test_diff_accuracy': diff_accuracy.item() if isinstance(diff_accuracy, torch.Tensor) else diff_accuracy
        }

        # Return metrics
        if hasattr(self, 'log'):
            self.log("test_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log("test_accuracy", accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log("test_diff_accuracy", diff_accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)

        result = {
            'test_loss': metrics['test_loss'],
            'test_accuracy': metrics['test_accuracy'],
            'test_diff_accuracy': metrics['test_diff_accuracy'],
            'task_ids': task_ids,
        }

        # Log task-specific metrics
        for task_id in task_ids:
            self.log(f"{task_id}_test_loss", metrics['test_loss'], on_step=False, on_epoch=True, prog_bar=True, logger=True)
            self.log(f"{task_id}_test_accuracy", metrics['test_accuracy'], on_step=False, on_epoch=True, prog_bar=True, logger=True)
            self.log(f"{task_id}_test_diff_accuracy", metrics['test_diff_accuracy'], on_step=False, on_epoch=True, prog_bar=True, logger=True)

        try:
            self.writer.add_scalar('test/loss', metrics['test_loss'], self.current_epoch)
            self.writer.add_scalar('test/accuracy', metrics['test_accuracy'], self.current_epoch)
            self.writer.add_scalar('test/diff_accuracy', metrics['test_diff_accuracy'], self.current_epoch)
            print(f"DEBUG: Logged test metrics for epoch {self.current_epoch}: loss={metrics['test_loss']}, accuracy={metrics['test_accuracy']}, diff_accuracy={metrics['test_diff_accuracy']}")
        except Exception as e:
            print(f"DEBUG: Error logging test step: {str(e)}")

        self.test_results.append(result)
        return result
        
    def on_validation_epoch_end(self):
        # Compute average validation loss
        val_loss = self.trainer.callback_metrics.get('val_loss')
        if val_loss is not None:
            avg_val_loss = val_loss.item()
        else:
            avg_val_loss = float('inf')  # Default to a high value if val_loss is not available

        # Update best_val_loss and best_epoch
        if avg_val_loss &lt; self.best_val_loss:
            self.best_val_loss = avg_val_loss
            self.best_epoch = self.current_epoch

        # Log validation metrics
        self.log('val_loss', avg_val_loss)
        self.log('best_val_loss', self.best_val_loss)
        self.log('best_epoch', self.best_epoch)

        # Update the results collector
        self.results_collector.update_val_metrics(self.current_epoch, {
            "avg_loss": avg_val_loss,
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch
        })

        # Log additional information
        self.log('epoch', self.current_epoch)

    def configure_optimizers(self):
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        lr_scheduler = {
            'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95),
            'name': 'learning_rate',
        }
        return [optimizer], [lr_scheduler]

    def on_fit_end(self):
        self.results_collector.save_to_json(f"results/experiment_{self.results_collector.experiment_id}.json")
        try:
            self.writer.close()
            print("DEBUG: TensorBoard writer closed successfully.")
        except Exception as e:
            print(f"DEBUG: Error closing TensorBoard writer: {str(e)}")
        print("DEBUG: Results saved and TensorBoard writer closed.")

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)

    def val_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4)

    def test_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4)

    def compute_loss(self, outputs, labels):
        return nn.CrossEntropyLoss()(
            outputs.view(-1, outputs.size(-1)), labels.view(-1)
        )

    def forward(self, input_ids, attention_mask=None):
        return self.model(input_ids, attention_mask)
    def log_hyperparameters(self):
        hparams = {
            'learning_rate': self.config.training.learning_rate,
            'batch_size': self.config.training.batch_size,
            'n_embd': self.config.model.n_embd,
            'n_head': self.config.model.n_head,
            'n_layer': self.config.model.n_layer,
        }
        metric_dict = {
            'train_loss': 0,
            'val_loss': 0,
            'test_accuracy': 0,
        }
        try:
            self.writer.add_hparams(hparams, metric_dict)
            print(f"DEBUG: Successfully logged hyperparameters: {hparams}")
        except Exception as e:
            print(f"DEBUG: Error logging hyperparameters: {str(e)}")

</file>
<file name="src/training/train.py">
# gpt2_arc/src/training/train.py
import argparse
import sys
import logging
import os
import json
from unittest.mock import MagicMock
import optuna
import arckit

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
sys.path.insert(0, project_root)

#print("Current PYTHONPATH:", sys.path)

import pytorch_lightning as pl
import torch
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
import json
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.utils.experiment_tracker import ExperimentTracker
from gpt2_arc.src.utils.results_collector import ResultsCollector
import optuna
import os

# Set up logging
logger = logging.getLogger(__name__)

def main(args):
    logger.info("Starting main function")
    logger.debug(f"Command line arguments: {args}")

    try:
        if args.use_optuna:
            logger.info("Loading best hyperparameters from Optuna study")
            study = optuna.load_study(study_name=args.optuna_study_name, storage=args.optuna_storage)
            best_params = study.best_params
            logger.debug(f"Loaded best parameters: {best_params}")
            
            model_config = ModelConfig(
                n_embd=best_params.get("n_embd", 768),
                n_head=best_params.get("n_head", 12),
                n_layer=best_params.get("n_layer", 12)
            )
            training_config = TrainingConfig(
                batch_size=best_params.get("batch_size", 32),
                learning_rate=best_params.get("learning_rate", 1e-4),
                max_epochs=args.max_epochs  # Always use the user-provided max_epochs
            )
        else:
            logger.info("Using provided or default hyperparameters")
            model_config = ModelConfig(n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer)
            training_config = TrainingConfig(batch_size=args.batch_size, learning_rate=args.learning_rate, max_epochs=args.max_epochs)
        
        config = Config(model=model_config, training=training_config)
        logger.debug(f"Configuration: {config}")

        # Load data
        logger.info("Loading data")
        train_set, eval_set = arckit.load_data()
        train_data = ARCDataset(train_set)
        val_data = ARCDataset(eval_set)
        logger.debug(f"Train data size: {len(train_data)}, Validation data size: {len(val_data)}")

        # Load model configuration from JSON file
        config_path = f"results/experiment_{args.model_checkpoint.split('_')[-1].replace('.pth', '.json')}"
        with open(config_path, 'r') as f:
            config_data = json.load(f)

        model_config = ModelConfig(
            n_embd=config_data['config']['model']['n_embd'],
            n_head=config_data['config']['model']['n_head'],
            n_layer=config_data['config']['model']['n_layer'],
            dropout=config_data['config']['model']['dropout']
        )

        # Initialize model
        logger.info("Initializing model")
        model = GPT2ARC(config=model_config)
        logger.debug(f"Model structure: {model}")

        # Load the checkpoint if specified
        if args.model_checkpoint:
            logger.info(f"Loading model from checkpoint: {args.model_checkpoint}")
            checkpoint = torch.load(args.model_checkpoint)
            model.load_state_dict(checkpoint)

        # Initialize results collector
        results_collector = ResultsCollector(config)

        # Initialize experiment tracker
        tracker = ExperimentTracker(config, project=args.project)

        # Initialize trainer
        logger.info("Initializing trainer")
        trainer = ARCTrainer(
            model=model,
            train_dataset=train_data,
            val_dataset=val_data,
            config=config
        )
        trainer.log_hyperparameters()

        # Set up PyTorch Lightning trainer
        logger.info("Setting up PyTorch Lightning trainer")
        callbacks = []
        if not args.no_checkpointing:
            checkpoint_callback = ModelCheckpoint(
                dirpath="checkpoints",
                filename="arc_model-{epoch:02d}-{val_loss:.2f}",
                save_top_k=3,
                monitor="val_loss",
                mode="min",
            )
            callbacks.append(checkpoint_callback)

        tb_logger = False if args.no_logging else TensorBoardLogger("tb_logs", name="arc_model")

        pl_trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            logger=tb_logger,
            callbacks=callbacks if callbacks else None,
            enable_checkpointing=not args.no_checkpointing,
            enable_progress_bar=not args.no_progress_bar,
            fast_dev_run=args.fast_dev_run,
            gradient_clip_val=1.0,
            accelerator='gpu' if args.use_gpu and torch.cuda.is_available() else 'cpu',
            devices=1
        )

        # Train the model
        logger.info("Starting model training")
        pl_trainer.fit(trainer)

        # After training, run test
        logger.info("Running model evaluation")
        test_results = pl_trainer.test(trainer)
        if test_results:
            avg_test_loss = test_results[0]['test_loss']
            avg_test_accuracy = test_results[0]['test_accuracy']
            logger.info(f"Test results - Loss: {avg_test_loss}, Accuracy: {avg_test_accuracy}")
            trainer.results_collector.set_test_results({
                "test_loss": avg_test_loss,
                "test_accuracy": avg_test_accuracy
            })

        trainer.results_collector.set_final_metrics({
            "best_val_loss": trainer.best_val_loss,
            "best_epoch": trainer.best_epoch,
            "final_test_loss": avg_test_loss,
            "final_test_accuracy": avg_test_accuracy
        })

        # Save the final model
        logger.info("Saving final model")
        model_path = f"final_model_{trainer.results_collector.experiment_id}.pth"
        torch.save(trainer.model.state_dict(), model_path)
        trainer.results_collector.set_checkpoint_path(model_path)
        logger.debug(f"Model saved to: {model_path}")

        # Save results
        logger.info("Saving experiment results")
        results_path = f"results/experiment_{trainer.results_collector.experiment_id}.json"
        trainer.results_collector.save_to_json(results_path)
        logger.debug(f"Results saved to: {results_path}")

    except Exception as e:
        logger.error(f"An error occurred: {str(e)}", exc_info=True)
        if 'tracker' in locals():
            tracker.log_metric("training_interrupted", 1)
            tracker.log_metric("error_message", str(e))
        raise
    finally:
        if 'tracker' in locals():
            tracker.finish()


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train the ARC Neural Reasoning Model")
    parser.add_argument("--use-optuna", action="store_true", help="Use best hyperparameters from Optuna study")
    parser.add_argument("--optuna-study-name", type=str, default="gpt2_arc_optimization", help="Name of the Optuna study to load")
    parser.add_argument("--optuna-storage", type=str, default="sqlite:///optuna_results.db", help="Storage URL for the Optuna study")
    parser.add_argument("--n-embd", type=int, default=768, help="Embedding dimension")
    parser.add_argument("--n-head", type=int, default=12, help="Number of attention heads")
    parser.add_argument("--n-layer", type=int, default=12, help="Number of transformer layers")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size for training")
    parser.add_argument("--learning-rate", type=float, default=1e-4, help="Learning rate")
    parser.add_argument("--max-epochs", type=int, required=True, help="Maximum number of epochs")
    parser.add_argument("--use-gpu", action="store_true", help="Use GPU for training if available")
    parser.add_argument("--no-logging", action="store_true", help="Disable logging")
    parser.add_argument("--no-checkpointing", action="store_true", help="Disable checkpointing")
    parser.add_argument("--no-progress-bar", action="store_true", help="Disable progress bar")
    parser.add_argument("--fast-dev-run", action="store_true", help="Run a fast development test")
    parser.add_argument("--model_checkpoint", type=str, help="Path to the model checkpoint to resume training")
    parser.add_argument("--project", type=str, default="gpt2-arc", help="W&amp;B project name")
    parser.add_argument("--results-dir", type=str, default="./results", help="Directory to save results")
    parser.add_argument("--run-name", type=str, default="default_run", help="Name of the run for saving results")
    
    args = parser.parse_args()
    main(args)


</file>
<file name="debug tips/test_differential_pixel_accuracy.md">
To identify and fix errors in your `test_differential_pixel_accuracy.py` test script, it's essential to examine the files and modules that the test interacts with. Based on your provided code and the summaries of your repository files, the following files are the most likely candidates to investigate:

1. **`gpt2_arc/src/utils/helpers.py`**
   - **Reason:** This file contains the `differential_pixel_accuracy` function, which is central to all your test cases. If there's an error related to the accuracy computation, its implementation here is the first place to check.
   - **Action:** Review the implementation of `differential_pixel_accuracy` for potential bugs or inconsistencies. Ensure that it correctly handles different tensor shapes, data types, and edge cases like empty tensors.

2. **`gpt2_arc/src/models/gpt2.py`**
   - **Reason:** The `GPT2ARC` model is instantiated and used in one of your tests (`test_differential_pixel_accuracy_with_arckit_data`). Errors related to model initialization, prediction generation, or tensor shapes are likely rooted here.
   - **Action:** 
     - Verify that the `GPT2ARC` model is correctly defined, especially the forward pass.
     - Ensure that the model's output dimensions match the expected shapes used in the test.
     - Check for any potential issues in the model's layers (e.g., `Attention`, `FeedForward`, `TransformerBlock`) that might affect predictions.

3. **`gpt2_arc/src/config.py`**
   - **Reason:** The `ModelConfig` dataclass is used to configure the `GPT2ARC` model. Misconfigurations here can lead to unexpected behaviors or mismatches in model parameters.
   - **Action:** 
     - Ensure that all necessary configuration parameters are correctly defined and passed.
     - Check for consistency between the configuration used in tests and the model's requirements.

4. **`gpt2_arc/src/data/arc_dataset.py`**
   - **Reason:** The `ARCDataset` class is responsible for data loading and preprocessing, which are critical for generating valid input and target tensors for the tests.
   - **Action:** 
     - Verify that the data preprocessing methods (e.g., `_process_arckit_data`, `_preprocess_grid`) correctly handle the data.
     - Ensure that the dataset returns tensors of expected shapes and types.
     - Check the `reverse_scaling` method to confirm it accurately reverses any scaling applied during preprocessing.

5. **External Dependency: `arckit`**
   - **Reason:** Your test `test_differential_pixel_accuracy_with_arckit_data` relies on the `arckit` library to load task data. Issues with data loading or compatibility can stem from here.
   - **Action:** 
     - Ensure that the `arckit` library is correctly installed and compatible with your project.
     - Verify that the `task_id` used (`"007bbfb7"`) exists and that `arckit.load_single(task_id)` returns the expected data structure.
     - Check for any updates or changes in the `arckit` API that might affect data loading.

6. **Additional Considerations:**
   - **Environment and Dependencies:**
     - Ensure that all dependencies (e.g., PyTorch, `arckit`) are up-to-date and compatible with each other.
     - Verify that the Python environment has all necessary packages installed.
   - **Test Environment:**
     - Confirm that the test is being run in an environment where all relative paths and module imports are correctly resolved.
     - Check for any recent changes in the project structure that might affect import statements.

7. **Debugging Tips:**
   - **Verbose Logging:** Enhance your test functions with more detailed logging to pinpoint where the error occurs. For example, print shapes and data types of tensors before and after each operation.
   - **Isolate Tests:** Run individual test functions separately to identify which specific test is failing.
   - **Use Assertions Carefully:** Ensure that your assertions accurately reflect the expected outcomes. For example, floating-point comparisons might require a tolerance level instead of exact equality.

8. **If the Error Persists:**
   - **Provide Error Messages:** Sharing specific error messages or stack traces can help in diagnosing the issue more effectively.
   - **Check Version Control:** Review recent commits to identify changes that might have introduced the error.
   - **Consult Documentation:** Refer to the documentation of external libraries like `arckit` for any breaking changes or known issues.

By systematically examining these files and following the debugging steps, you should be able to identify and resolve the error in your test code effectively. If you need further assistance with specific files or error messages, feel free to share the relevant code snippets or details.
</file>
<file name="debug tips/test_model_evaluation.md">
To identify and fix errors in your `test_model_evaluation.py` file, it's essential to focus on the dependencies and modules that this test script interacts with. Here's a breakdown of the most relevant files in your repository that are likely to provide useful information for debugging:

1. **`src/models/gpt2.py`**
   - **Why:** This file defines the `GPT2ARC` class, which is a core component being tested. Any issues related to model architecture, forward pass, or specific layers (like `Attention`, `FeedForward`, or `TransformerBlock`) will originate here.
   - **What to Check:**
     - Initialization of the `GPT2ARC` model.
     - Implementation of the `forward` method.
     - Any custom layers or operations that might affect model outputs.

2. **`src/config.py`**
   - **Why:** This file contains the configuration classes (`Config`, `ModelConfig`, `TrainingConfig`) used to instantiate models and training parameters. Misconfigurations here can lead to unexpected behaviors or initialization errors in your tests.
   - **What to Check:**
     - Correct definitions and default values in the dataclasses.
     - Any dependencies or validations within the configuration classes.
     - Ensure that all required fields are being correctly passed and utilized.

3. **`src/training/trainer.py`**
   - **Why:** The `ARCTrainer` class is imported and used in your fixtures. Issues related to the training loop, validation steps, or how the trainer interacts with the model can manifest in your tests.
   - **What to Check:**
     - Initialization and setup of the `ARCTrainer`.
     - Implementation of methods like `validation_step`, which is explicitly tested.
     - Handling of incorrect batch formats and error raising mechanisms.

4. **`src/utils/helpers.py`**
   - **Why:** This file includes utility functions like `differential_pixel_accuracy`, which are directly used in your tests. Any bugs or unexpected behaviors in these helper functions can cause test failures.
   - **What to Check:**
     - Correct implementation of `differential_pixel_accuracy`.
     - Edge case handling and input validations within the helper functions.

5. **`src/data/arc_dataset.py`**
   - **Why:** Although not directly imported in your test script, the `DataLoader` relies on the `ARCDataset` class defined here. Issues with data preprocessing, batching, or dataset splitting can indirectly affect your tests.
   - **What to Check:**
     - Data loading and preprocessing logic.
     - Handling of different data sources and formats.
     - Any transformations applied to the data before it's fed into the model.

6. **Checkpoint Files (`checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt`)**
   - **Why:** Your tests involve loading model checkpoints. Problems with checkpoint integrity, missing keys, or incompatible configurations can lead to errors during model loading and evaluation.
   - **What to Check:**
     - Ensure that the checkpoint file exists and is accessible.
     - Verify that the checkpoint contains all necessary keys (`config`, `state_dict`, etc.).
     - Confirm that the `ModelConfig` in the checkpoint matches the expected configuration in your code.

7. **`src/utils/experiment_tracker.py` &amp; `src/utils/results_collector.py`**
   - **Why:** These utilities handle experiment tracking and results collection, which can influence how metrics and configurations are logged and stored. Issues here can affect the integrity of the metrics being tested.
   - **What to Check:**
     - Correct logging of metrics and configurations.
     - Proper serialization and deserialization of experiment data.
     - Error handling and edge case management in tracking methods.

8. **Logging Configuration in `test_model_evaluation.py`**
   - **Why:** Since your test script sets up logging, any misconfigurations here can obscure error messages or make debugging more challenging.
   - **What to Check:**
     - Ensure that the logging level is appropriately set (`DEBUG` in your case).
     - Verify that log messages are correctly formatted and informative.

### Steps to Diagnose and Fix Errors:

1. **Identify the Error Source:**
   - Look at the error message and traceback to pinpoint where the error originates. This will guide you to the relevant file(s).

2. **Check Dependencies:**
   - Once you know which part of the code is failing, inspect the corresponding file(s) mentioned above for potential issues.

3. **Validate Configurations:**
   - Ensure that all configurations passed to models and trainers are correct and complete.

4. **Verify Data Integrity:**
   - Make sure that the data being loaded and processed matches the expected format and structure required by the models and trainers.

5. **Inspect Checkpoints:**
   - Confirm that the checkpoint files are not corrupted and contain all necessary components to reconstruct the model and its state.

6. **Enhance Logging:**
   - Utilize the debug logs you've set up to gain more insights into the internal states and data flow during test execution.

By systematically reviewing these files and following the diagnostic steps, you should be able to identify and resolve errors in your `test_model_evaluation.py` script effectively.
</file>
<file name="debug tips/test_end_to_end.md">
When troubleshooting errors in your `test_end_to_end.py` script, several files within your repository are likely to provide the most relevant information to help you identify and fix the issue. Here's a breakdown of the key files to examine based on different parts of your test script:

### 1. **Data Handling and Preprocessing**

- **`src/data/arc_dataset.py`**
  - **Relevance:** This file contains the `ARCDataset` class, which is crucial for loading and preprocessing your ARC dataset. Errors related to data loading, dataset splitting, or preprocessing steps (like `_process_synthetic_data` or `_preprocess_grid`) will likely originate here.
  - **What to Check:**
    - Ensure the dataset paths are correct.
    - Verify the data processing methods are handling the data as expected.
    - Check for any issues in the `collate_fn` used for batching data.

- **`arckit` Module**
  - **Relevance:** Your test script uses `arckit.load_data()` to load the dataset. Issues with the data loading process or the structure of the loaded data would be tied to this module.
  - **What to Check:**
    - Ensure `arckit` is correctly installed and accessible.
    - Verify that the `load_data` function returns data in the expected format.

### 2. **Model Definition**

- **`src/models/gpt2.py`**
  - **Relevance:** This file defines the `GPT2ARC` model and its components (`Attention`, `FeedForward`, `TransformerBlock`). Errors related to model architecture, such as layer mismatches or incorrect configurations, will likely originate here.
  - **What to Check:**
    - Ensure the model configuration (`ModelConfig`) matches the expected architecture.
    - Verify that all layers are correctly defined and initialized.
    - Check for any type mismatches or tensor dimension issues within the model.

### 3. **Training Logic**

- **`src/training/trainer.py`**
  - **Relevance:** This file contains the `ARCTrainer` class, which manages the training loop, loss computation, and metric tracking. Errors during training, such as issues with the optimizer, loss functions, or training steps, will likely stem from here.
  - **What to Check:**
    - Ensure that the training configurations (`TrainingConfig`) are correctly set.
    - Verify the implementation of training and validation steps.
    - Check for any runtime errors during the forward or backward passes.

### 4. **Configuration Management**

- **`src/config.py`**
  - **Relevance:** This file defines the configuration data classes (`Config`, `ModelConfig`, `TrainingConfig`). Misconfigurations, such as incorrect hyperparameters or mismatched settings, can lead to errors during model initialization or training.
  - **What to Check:**
    - Ensure all configuration parameters are correctly set and passed to other components.
    - Verify that default values are appropriate and that any overrides are correctly applied.

### 5. **Utility Functions and Experiment Tracking**

- **`src/utils/experiment_tracker.py`**
  - **Relevance:** If your test script involves experiment tracking or logging metrics, issues here could affect the logging and tracking of your experiments.
  - **What to Check:**
    - Ensure that the experiment tracker is correctly initialized and configured.
    - Verify that metrics are being logged and saved as expected.

- **`src/utils/results_collector.py`**
  - **Relevance:** This file manages the collection and storage of results from training and evaluation. Errors related to result aggregation or storage will likely originate here.
  - **What to Check:**
    - Ensure that results are correctly collected and serialized.
    - Verify that there are no issues with saving or loading result data.

### 6. **Evaluation Process**

- **`src/evaluate.py`**
  - **Relevance:** Although not directly referenced in your test script, if evaluation logic is invoked or shared between scripts, issues here could affect the evaluation metrics.
  - **What to Check:**
    - Ensure that evaluation metrics are correctly computed.
    - Verify that the evaluation data is correctly processed and fed into the model.

### 7. **Other Potential Sources**

- **`benchmark.py` and `train.py`**
  - **Relevance:** While these are more likely related to running benchmarks or training outside of tests, any shared components or configurations could indirectly affect your tests.
  - **What to Check:**
    - Ensure that any shared utilities or configurations used by these scripts are consistent and error-free.

### **General Debugging Tips:**

1. **Logging:** Your test script has extensive logging enabled (`logging.basicConfig(level=logging.DEBUG)`). Review the debug logs to pinpoint where the error occurs. The logs provide step-by-step insights into the test execution flow.

2. **Assertions and Error Messages:** Pay close attention to the assertion statements and any error messages they produce. These can guide you to the exact point of failure.

3. **Dependencies:** Ensure all dependencies (like `arckit`, `torch`, `pytorch_lightning`, etc.) are correctly installed and compatible with each other.

4. **Environment Issues:** Sometimes, errors arise from the environment (e.g., incorrect CUDA setup, incompatible library versions). Verify that your environment matches the expected setup.

5. **Isolate the Issue:** If possible, try running individual components or smaller tests to isolate where the error is occurring. This can help narrow down the problematic file or section of code.

### **Next Steps:**

If after reviewing the above files you're still unable to identify the issue, consider the following:

- **Provide Specific Error Messages:** Sharing the exact error messages or stack traces can help in diagnosing the problem more accurately.
  
- **Add Relevant Files:** If the issue seems to originate from a specific file not listed here, feel free to add its content to the chat for a more in-depth analysis.

By systematically reviewing these files and following the debugging tips, you should be able to identify and resolve the errors in your end-to-end test script effectively.
</file>
<file name="debug tips/test_arc_dataset.md">
To effectively troubleshoot and fix errors in your `gpt2_arc/tests/test_arc_dataset.py` test suite, you should focus on the following key files in your repository:

1. **`gpt2_arc/src/data/arc_dataset.py`**
   - **Why:** This is the primary file where the `ARCDataset` class and the `set_debug_mode` function are defined. Since your tests are directly interacting with these components, any issues related to dataset initialization, data preprocessing, or utility functions will likely originate here.
   - **What to Look For:**
     - **Initialization Logic:** Ensure that the `__init__` method correctly handles different types of `data_source` inputs (`str`, `List[Dict]`, `TaskSet`, etc.).
     - **Data Processing Methods:** Check methods like `_process_synthetic_data`, `_process_arckit_data`, and `_preprocess_grid` for any logical errors or incorrect handling of data.
     - **Debug Mode Handling:** Verify that the `set_debug_mode` function correctly toggles the debug state and that debug-related logging or behavior in `ARCDataset` is functioning as expected.

2. **`gpt2_arc/src/utils/experiment_tracker.py`**
   - **Why:** While not directly referenced in your test file, utility classes like `ExperimentTracker` can influence the behavior of your dataset, especially if they are used for logging or tracking metrics during dataset processing.
   - **What to Look For:**
     - **Logging Configuration:** Ensure that logging is correctly set up and that it doesn't interfere with dataset operations.
     - **Serialization Methods:** Check methods like `_make_serializable` and `_serialize_config` to ensure that configurations are correctly handled, which can affect dataset initialization if configurations are passed around.

3. **`gpt2_arc/src/models/gpt2.py`**
   - **Why:** Although your tests focus on the dataset, models often interact closely with datasets during training and evaluation. Issues in model configurations or data handling within the model can indirectly affect dataset behavior.
   - **What to Look For:**
     - **Data Expectations:** Ensure that the model correctly expects the data shapes and types provided by `ARCDataset`.
     - **Integration Points:** Verify that any integration points between the model and dataset (if present) are correctly implemented.

4. **Dependencies and External Libraries (`arckit`)**
   - **Why:** Your tests import `TaskSet` from `arckit.data`, which suggests that `arckit` is an external dependency. Issues within this library can propagate to your dataset tests.
   - **What to Look For:**
     - **Compatibility:** Ensure that the version of `arckit` you are using is compatible with your dataset and that there are no known bugs affecting `TaskSet`.
     - **Mock Implementations:** Since you use `unittest.mock.Mock` for `TaskSet`, ensure that your mock accurately reflects the structure and behavior expected by `ARCDataset`.

5. **Test File Itself (`gpt2_arc/tests/test_arc_dataset.py`)**
   - **Why:** Sometimes, the issue might reside within the test logic rather than the implementation. Reviewing the test file can help identify incorrect assumptions or faulty test setups.
   - **What to Look For:**
     - **Test Fixtures:** Ensure that fixtures like `sample_data` and `mock_taskset` provide the correct data structures expected by `ARCDataset`.
     - **Assertions:** Verify that all assertions correctly reflect the intended behavior and that they are not overly restrictive or incorrectly specified.
     - **Skipped Tests:** Review why certain tests are skipped and determine if they need to be updated or fixed to be included in the test suite.

6. **Additional Configuration Files (`gpt2_arc/src/config.py`)**
   - **Why:** Configuration files often dictate how datasets and models are initialized and interacted with. Errors in configurations can lead to unexpected behaviors during testing.
   - **What to Look For:**
     - **Model and Dataset Configurations:** Ensure that all necessary configurations for the dataset are correctly defined and accessible.
     - **Defaults and Overrides:** Check how default configurations are set and how they can be overridden, ensuring consistency across different test scenarios.

**Summary:**

- **Primary Focus:** `gpt2_arc/src/data/arc_dataset.py`
- **Secondary Focus:** Utility files like `experiment_tracker.py`, model definitions in `gpt2.py`, and configuration files in `config.py`
- **Dependencies:** Ensure external libraries like `arckit` are functioning as expected
- **Test Integrity:** Verify the correctness of the test setups and assertions within `test_arc_dataset.py`

By systematically reviewing these areas, you should be able to identify and resolve errors within your test suite effectively.
</file>
<file name="debug tips/test_gpt2.md">
When encountering an error in your `gpt2_arc/tests/test_gpt2.py` test suite, the most relevant files to examine for troubleshooting are those that define the components being tested. Here's a breakdown of the primary files you should investigate:

1. **`gpt2_arc/src/models/gpt2.py`**
   - **Why:** This file contains the definitions for the `GPT2ARC` class as well as its constituent modules like `Attention`, `FeedForward`, and `TransformerBlock`. Since your tests are directly interacting with these classes (e.g., initializing `GPT2ARC`, performing forward passes, etc.), any issues related to model architecture, initialization, or forward computations would likely originate here.
   - **Key Sections to Check:**
     - `GPT2ARC` class initialization and attributes (`conv1`, `blocks`, `ln_f`, `config`).
     - Implementation details of `Attention`, `FeedForward`, and `TransformerBlock` modules.
     - Any custom methods or overrides that might affect the model's behavior during testing.

2. **`gpt2_arc/src/config.py`**
   - **Why:** The `ModelConfig` class from this file is used to configure the `GPT2ARC` model during initialization in your tests. Errors related to configuration parameters, default values, or the structure of the configuration can lead to issues in model instantiation or behavior.
   - **Key Sections to Check:**
     - Definition of `ModelConfig` and its fields.
     - Any methods or default values that set up the model's configuration.
     - Interactions between `ModelConfig` and other parts of the model (e.g., ensuring all necessary configuration parameters are correctly passed and utilized).

3. **Additional Files to Consider:**
   - **`gpt2_arc/src/utils/experiment_tracker.py` &amp; `gpt2_arc/src/utils/results_collector.py`:**
     - **Why:** If your tests involve tracking experiments or collecting results, issues in these utility classes might indirectly affect your tests. For instance, incorrect logging or result serialization could lead to unexpected behavior or errors during test execution.
   - **`gpt2_arc/src/data/arc_dataset.py`:**
     - **Why:** If your tests rely on specific data preprocessing or dataset structures, any bugs or changes in data handling could impact the inputs your tests use. Ensuring that data is correctly processed and fed into the model is crucial for accurate testing.

4. **Test-Specific Considerations:**
   - **Duplicate Test Function:**
     - **Issue:** In your `test_gpt2.py`, there are two functions named `test_gpt2arc_forward_pass`. Python does not support function overloading, so the second definition will overwrite the first. This could lead to unexpected test behaviors or skipped tests.
     - **Solution:** Rename one of the test functions to ensure each test has a unique name, such as `test_gpt2arc_forward_pass_with_mask` and `test_gpt2arc_forward_pass_without_mask`.

5. **Logging Output:**
   - **Why:** Since your test file is configured with `logging.DEBUG`, reviewing the log outputs can provide detailed insights into where the error might be occurring. Ensure that the logs are being captured and review them to identify any anomalies or error messages during test execution.

6. **Dependencies and Environment:**
   - **Why:** Ensure that all dependencies (like `torch`, `pytest`, etc.) are correctly installed and compatible with your codebase. Sometimes, version mismatches can lead to unexpected errors.
   - **Solution:** Verify your `requirements.txt` or environment configuration and ensure all packages are up-to-date and compatible.

7. **Version Control and Recent Changes:**
   - **Why:** If the error is new, reviewing recent commits or changes in the related files can help pinpoint the source of the issue.
   - **Solution:** Use `git` commands like `git blame` or `git diff` to identify recent modifications that might have introduced the error.

**Next Steps:**

1. **Identify the Specific Error:**
   - If you encounter an error message or traceback, use it to determine which part of the code is failing.
   
2. **Inspect Relevant Files:**
   - Focus on the files highlighted above, especially `gpt2_arc/src/models/gpt2.py` and `gpt2_arc/src/config.py`.

3. **Request Additional File Contents if Needed:**
   - If you determine that a specific section of a file might be causing the issue but need more details, feel free to ask me to include that file's content for further analysis.

By systematically reviewing these areas, you should be able to identify and resolve the error in your test suite effectively.
</file>
<file name="debug tips/test_pytest_error_fixer.md">
To effectively diagnose and fix errors in your `test_pytest_error_fixer.py` test script, you'll want to focus on several key files within your repository. Here's a breakdown of the most relevant files that can provide the necessary information:

1. **Primary Module Under Test:**
   - **`pytest_error_fixer.py`**: This is the main module being tested by your `test_pytest_error_fixer.py` script. Any errors in your tests are likely related to the implementation details within this file. Since you didn't list this file in your summaries, ensure it's available and consider sharing its contents if you need detailed assistance.

2. **Configuration Files:**
   - **`gpt2_arc/src/config.py`**: This file contains configuration classes (`ModelConfig` and `Config`) that might be used by `PytestErrorFixer`. Misconfigurations here can lead to issues in initializing or running the fixer.

3. **Utility Modules:**
   - **`gpt2_arc/src/utils/experiment_tracker.py`** and **`gpt2_arc/src/utils/results_collector.py`**: These utility classes (`ExperimentTracker` and `ResultsCollector`) might be dependencies for `PytestErrorFixer`. Errors in these utilities can propagate to your tests.

4. **Data Handling:**
   - **`gpt2_arc/src/data/arc_dataset.py`**: If `PytestErrorFixer` interacts with datasets or relies on data preprocessing, issues in this module can affect your tests.

5. **Model and Training Modules:**
   - **`gpt2_arc/src/models/gpt2.py`**: This file defines the `GPT2ARC` model and related classes. If `PytestErrorFixer` interacts with model components, ensure that there are no issues here.
   - **`gpt2_arc/src/training/train.py`** and **`gpt2_arc/src/training/trainer.py`**: These modules handle the training process. Any integration between `PytestErrorFixer` and the training pipeline should be verified.

6. **Evaluation and Benchmarking:**
   - **`gpt2_arc/src/evaluate.py`** and **`gpt2_arc/benchmark.py`**: These scripts are essential for evaluating model performance. Ensure that `PytestErrorFixer` correctly interacts with evaluation metrics if applicable.

7. **Experiment Tracking:**
   - **`gpt2_arc/src/utils/experiment_tracker.py`**: This utility is crucial for logging and tracking experiments. Any issues here can affect how errors and progress are logged by `PytestErrorFixer`.

8. **Results Collection:**
   - **`gpt2_arc/src/utils/results_collector.py`**: Similar to the experiment tracker, this module handles the collection and storage of results, which might be integral to how `PytestErrorFixer` operates.

### Next Steps:

- **Check `pytest_error_fixer.py`**: Start by reviewing the implementation of the `PytestErrorFixer` class in `pytest_error_fixer.py`. Look for any obvious issues or dependencies that might not be properly handled.
  
- **Verify Dependencies**: Ensure that all dependencies (`experiment_tracker.py`, `results_collector.py`, etc.) are correctly implemented and free from errors.
  
- **Review Configuration**: Double-check the configurations in `config.py` to ensure they align with the requirements of `PytestErrorFixer`.
  
- **Mock External Interactions**: In your tests, you’re using mocks for subprocess calls and the `coder` object. Ensure that these mocks accurately represent the behavior of the actual components.

- **Add Missing Files if Needed**: If you encounter issues that trace back to files not listed (like `pytest_error_fixer.py`), please add them to the chat so I can provide more targeted assistance.

By focusing on these files, you should be able to identify and resolve the errors in your test script effectively. If you need more detailed help, feel free to share the contents of `pytest_error_fixer.py` or any other relevant files.
</file>
<file name="debug tips/test_trainer.md">
When encountering an error in your `gpt2_arc/tests/test_trainer.py` file, the most relevant files to examine for debugging and fixing the issue are those that the test file directly interacts with or depends upon. Here's a breakdown of the key files and their roles:

1. **`src/config.py`**
   - **Classes to Check:**
     - `Config`
     - `ModelConfig`
     - `TrainingConfig`
   - **Relevance:** This file defines the configuration classes used to initialize models and trainers. Errors related to configuration parameters, default values, or initialization logic likely originate here.

2. **`src/data/arc_dataset.py`**
   - **Classes and Functions to Check:**
     - `ARCDataset`
     - `set_debug_mode`
     - `_process_synthetic_data`
     - `_process_arckit_data`
     - `_preprocess_grid`
   - **Relevance:** This file handles data preprocessing and dataset creation. Issues related to data loading, preprocessing steps, or dataset structure (e.g., unexpected data formats) would be found here.

3. **`src/models/gpt2.py`**
   - **Classes to Check:**
     - `GPT2ARC`
     - `Attention`
     - `FeedForward`
     - `TransformerBlock`
     - `ModelConfig`
   - **Relevance:** This file contains the GPT-2 model architecture and its components. Errors in the model's forward pass, layer configurations, or parameter settings are likely rooted in this file.

4. **`src/training/trainer.py`**
   - **Classes and Methods to Check:**
     - `ARCTrainer`
     - `training_step`
     - `validation_step`
     - `configure_optimizers`
     - `train_dataloader`
     - `val_dataloader`
     - `test_step`
   - **Relevance:** This is the core training module that orchestrates the training and validation processes. Issues related to the training loop, optimizer configuration, data loaders, or logging mechanisms would be addressed here.

5. **`src/utils/experiment_tracker.py`**
   - **Classes and Methods to Check:**
     - `ExperimentTracker`
     - `log_metric`
     - `update_train_metrics`
     - `update_val_metrics`
     - `set_test_results`
     - `save_to_json`
   - **Relevance:** If your tests involve tracking experiments or logging metrics, any errors related to metric logging, experiment initialization, or result serialization would involve this file.

6. **`src/utils/results_collector.py`**
   - **Classes and Methods to Check:**
     - `ResultsCollector`
     - `update_train_metrics`
     - `update_val_metrics`
     - `set_test_results`
     - `add_task_specific_result`
     - `save_to_json`
   - **Relevance:** Similar to `experiment_tracker.py`, this file manages the collection and storage of results. Errors in aggregating or storing test results would be pertinent here.

### Steps to Debug:

1. **Identify the Error Message:**
   - Start by looking at the exact error message and stack trace. This will often point directly to the file and line number where the issue originated.

2. **Trace Dependencies:**
   - Understand how `test_trainer.py` interacts with the other modules. For instance, if there's an issue during model initialization, focus on `src/config.py` and `src/models/gpt2.py`.

3. **Check Configurations:**
   - Ensure that the configuration objects (`Config`, `ModelConfig`, `TrainingConfig`) are correctly set up and that all required parameters are provided.

4. **Validate Data Handling:**
   - If the error is related to data loading or preprocessing, review the methods in `arc_dataset.py` to ensure data is being processed as expected.

5. **Inspect Model Architecture:**
   - For issues in the forward pass or model outputs, delve into `gpt2.py` to verify layer configurations and data flow within the model.

6. **Examine Training Logic:**
   - If the error occurs during training or validation steps, scrutinize the `ARCTrainer` class in `trainer.py`, focusing on methods like `training_step` and `validation_step`.

7. **Review Utility Functions:**
   - For issues related to logging or result collection, check the utility files to ensure metrics are being recorded and stored correctly.

### Additional Tips:

- **Use Debugging Tools:**
  - Incorporate debugging statements or use tools like `pdb` to step through the code and inspect variable states at different execution points.

- **Isolate the Issue:**
  - Temporarily simplify your tests or mock certain components to isolate where the error is occurring.

- **Check Dependencies and Versions:**
  - Ensure that all dependencies (e.g., PyTorch, PyTest) are up to date and compatible with your codebase.

- **Consult Documentation:**
  - Review the documentation for any third-party libraries or frameworks you’re using to ensure you're adhering to best practices and usage patterns.

By systematically examining these files and following a structured debugging approach, you should be able to identify and resolve the error in your `test_trainer.py` code.
</file>
<file name="debug tips/test_train.md">
When encountering an error in your `gpt2_arc/tests/test_train.py` test suite, the most relevant files to examine for debugging and resolving the issue are those that are directly imported and utilized within the test cases. Here's a breakdown of the key files you should focus on:

1. **Source Files Under Test:**
   
   - **`gpt2_arc/src/training/train.py`**
     - **Role:** Contains the `main` function that orchestrates the training process.
     - **Why Check:** Since your tests are invoking `main(args)`, any issues in how training is initiated or handled would likely originate here.
   
   - **`gpt2_arc/src/training/trainer.py`**
     - **Role:** Defines the `ARCTrainer` class, which is a subclass of `pl.LightningModule` responsible for managing the training loop.
     - **Why Check:** Errors related to the training logic, such as training steps, validation steps, or integration with PyTorch Lightning, would stem from this file.
   
   - **`gpt2_arc/src/models/gpt2.py`**
     - **Role:** Implements the `GPT2ARC` model, including its architecture and forward pass.
     - **Why Check:** If the error pertains to model initialization, forward propagation, or any layer-specific issues, this is the primary file to inspect.
   
   - **`gpt2_arc/src/data/arc_dataset.py`**
     - **Role:** Contains the `ARCDataset` class responsible for data loading and preprocessing.
     - **Why Check:** Issues related to data handling, such as dataset initialization, data preprocessing, or data loader configuration, would originate here.
   
   - **`gpt2_arc/src/config.py`**
     - **Role:** Defines configuration dataclasses like `Config`, `ModelConfig`, and `TrainingConfig`.
     - **Why Check:** Misconfigurations or incorrect parameter settings that affect training behavior would be defined in this file.

2. **Utility and Support Files:**
   
   - **`gpt2_arc/src/utils/results_collector.py`**
     - **Role:** Implements the `ResultsCollector` class for aggregating and managing training results.
     - **Why Check:** Errors related to result logging, metric collection, or summary generation would be found here.
   
   - **`gpt2_arc/src/utils/experiment_tracker.py`**
     - **Role:** Manages experiment tracking, possibly integrating with tools like Weights &amp; Biases.
     - **Why Check:** If the error involves experiment tracking, logging configurations, or integrations with external tracking tools, this file is pertinent.

3. **Additional Considerations:**
   
   - **`gpt2_arc/benchmark.py` and `gpt2_arc/src/evaluate.py`**
     - **Role:** While these files are more focused on benchmarking and evaluation, respectively, they might still interact with training components.
     - **Why Check:** If the error indirectly involves evaluation metrics or benchmarking during training, reviewing these files could provide insights.
   
   - **Mock and Fixture Implementations in `test_train.py`:**
     - **Role:** The test file itself uses fixtures and mocks extensively to simulate different components.
     - **Why Check:** Ensure that the mocks correctly mimic the behavior of the actual classes and that fixtures are set up appropriately. Errors in the test setup can lead to misleading test failures.

4. **Logging and Configuration:**
   
   - **Logging Configuration in `test_train.py`:**
     - **Role:** The test file sets up logging levels and configurations.
     - **Why Check:** Misconfigured logging can obscure error messages or lead to unexpected behaviors during testing.

5. **Dependencies and Environment:**
   
   - **External Libraries:**
     - Ensure that dependencies like `pytorch_lightning`, `torch`, and other libraries are correctly installed and compatible with your codebase.
   
   - **Environment Variables and Paths:**
     - Verify that the `sys.path` manipulations and environment settings in the test file correctly point to the necessary modules and that there are no path conflicts.

**Summary:**

To effectively debug and resolve errors in your `test_train.py`:

- **Start with the source files being tested** (`train.py`, `trainer.py`, `gpt2.py`, `arc_dataset.py`, and `config.py`) to identify any underlying issues in the training pipeline.
  
- **Examine utility files** (`results_collector.py` and `experiment_tracker.py`) for problems related to logging and result management.
  
- **Review the test setup itself**, ensuring that mocks and fixtures accurately represent the real components and that there are no setup-related errors.

By systematically inspecting these areas, you can pinpoint the root cause of the errors and implement effective fixes.
</file>
<file name="debug tips/test_integration_experiment.md">
When debugging errors in your `test_integration_experiment.py` test file, several parts of your codebase are likely to provide valuable insights. Based on the imports and the structure of your project, the following files are the most relevant for diagnosing and fixing potential issues:

1. **Data Handling and Preprocessing:**
   
   - **`gpt2_arc/src/data/arc_dataset.py`**
     - **Why:** This file contains the `ARCDataset` class, which is crucial for data loading and preprocessing. Errors related to data formatting, missing fields, or incorrect data types often originate here.
     - **Key Sections to Review:**
       - `__init__` method: Ensure that the dataset is being initialized correctly with the provided data sources.
       - `_process_arckit_data` and `_process_synthetic_data` methods: Verify that data from `arckit` is being processed as expected.
       - Any debug or logging statements that might help trace data issues.

2. **Model Definition:**
   
   - **`gpt2_arc/src/models/gpt2.py`**
     - **Why:** This file defines the `GPT2ARC` model and related components like `Attention`, `FeedForward`, and `TransformerBlock`. Errors related to model architecture, layer configurations, or forward passes will stem from here.
     - **Key Sections to Review:**
       - `GPT2ARC` class initialization: Check that all layers are correctly instantiated with the right configurations.
       - Forward methods: Ensure that data flows correctly through the model without shape mismatches or other issues.
       - Any custom configurations or modifications to the standard GPT-2 architecture.

3. **Training Logic:**
   
   - **`gpt2_arc/src/training/trainer.py`**
     - **Why:** This file contains the `ARCTrainer` class, which manages the training loop, loss calculations, and metric updates. Issues like improper training steps, incorrect loss functions, or metric logging problems will be found here.
     - **Key Sections to Review:**
       - `__init__` method: Ensure that datasets, model, and configurations are correctly set up.
       - Training step methods: Verify that loss calculations and backpropagation are implemented correctly.
       - Integration with PyTorch Lightning: Check compatibility and correct usage of Lightning's `Trainer`.

4. **Configuration Management:**
   
   - **`gpt2_arc/src/config.py`**
     - **Why:** This file defines configuration classes like `Config`, `ModelConfig`, and `TrainingConfig`. Misconfigurations, such as incorrect hyperparameters or missing configuration fields, can lead to errors during data loading, model initialization, or training.
     - **Key Sections to Review:**
       - Default values and data types for all configuration parameters.
       - Any methods that manipulate or validate configurations.
       - Integration points where configurations are passed to other components like the model or trainer.

5. **Results Collection and Logging:**
   
   - **`gpt2_arc/src/utils/results_collector.py`**
     - **Why:** This file manages the collection and summarization of training and validation results. Errors related to metric logging, result storage, or summary generation will originate here.
     - **Key Sections to Review:**
       - Methods for updating and retrieving metrics.
       - Serialization and saving of results.
       - Integration with other components to ensure that metrics are correctly passed and stored.

6. **Additional Considerations:**
   
   - **`arckit` Library:**
     - **Why:** Your test setup relies on the `arckit` library to load task data. If there are issues with how tasks are loaded or structured, it could affect your tests.
     - **Action:** Ensure that `arckit` is correctly installed and that the task IDs used in tests (`"007bbfb7"`) are valid and accessible.

   - **PyTorch Lightning Integration:**
     - **Files Involved:** While not listed explicitly, your test uses PyTorch Lightning's `Trainer`. Ensure that all integrations with Lightning are correctly implemented in your `ARCTrainer` class and that callbacks like `ModelCheckpoint` are properly configured.

   - **Logging and Debug Statements:**
     - **Why:** Your test includes several `print` statements for debugging. Ensure that these logs provide meaningful information and that they don't interfere with the test flow.

7. **Common Error Scenarios and File Associations:**

   - **Import Errors:**
     - **Files to Check:** `arc_dataset.py`, `gpt2.py`, `trainer.py`, `config.py`, `results_collector.py`
     - **Reason:** Missing or incorrect imports usually point to issues in these modules.

   - **Attribute Errors or Missing Methods:**
     - **Files to Check:** `gpt2.py`, `trainer.py`, `arc_dataset.py`
     - **Reason:** Ensure that all necessary methods and attributes are defined and correctly named.

   - **Data Shape Mismatches:**
     - **Files to Check:** `arc_dataset.py`, `gpt2.py`
     - **Reason:** Verify that the data shapes are consistent throughout the data pipeline and model.

   - **Configuration Mismatches:**
     - **Files to Check:** `config.py`, `trainer.py`, `gpt2.py`
     - **Reason:** Ensure that all components receive and use configurations correctly.

8. **Next Steps:**

   - **Review the Relevant Files:** Start by examining the files listed above, focusing on the sections most likely related to your error.
   
   - **Add Detailed Logging:** If not already present, consider adding more detailed logging within these files to trace the flow of data and identify where things might be going wrong.
   
   - **Isolate the Issue:** Determine whether the error is related to data loading, model initialization, training steps, or configuration. This will help narrow down which file to focus on.
   
   - **Run Tests Incrementally:** Use PyTest's verbose mode or selectively run tests to get more context about where the failure occurs.

If you identify that a specific file or section is causing the issue and need further assistance, feel free to share the relevant code snippets by adding those files to the chat. This will allow for more targeted help in resolving the problem.
</file>
<file name="debug tips/test_benchmark.md">

When encountering an error in the `gpt2_arc/tests/test_benchmark.py` test suite, it's essential to identify the most relevant files that could provide insights into the issue. Here's a prioritized list of files to examine, along with brief explanations of why they are likely to be involved:

1. **`gpt2_arc/benchmark.py`**
   - **Reason:** This is the primary module being tested. Functions like `benchmark_model` and `main` are directly imported and invoked in your test cases. Any issues with these functions (e.g., logic errors, incorrect handling of inputs/outputs) will likely manifest during testing.
   
2. **`gpt2_arc/src/models/gpt2.py`**
   - **Reason:** The `GPT2ARC` class is a core component being mocked and used in the tests. Errors related to model configuration, initialization, or methods (like `forward`) can affect the benchmark tests.
   
3. **`gpt2_arc/src/config.py`**
   - **Reason:** The `ModelConfig` dataclass is imported and potentially used within both the `benchmark.py` and model modules. Misconfigurations or incorrect parameter settings here can lead to unexpected behaviors during benchmarking.
   
4. **`gpt2_arc/src/data/arc_dataset.py`**
   - **Reason:** The dataset (`ARCDataset`) is mocked in the tests, but any underlying issues with data processing, loading, or preprocessing in the actual implementation can cause tests to fail or behave unpredictably.
   
5. **`gpt2_arc/src/utils/experiment_tracker.py`**
   - **Reason:** If `benchmark_model` or related functions utilize the `ExperimentTracker` for logging or tracking experiments, any bugs or exceptions within this utility can propagate to your tests.
   
6. **`gpt2_arc/src/utils/results_collector.py`**
   - **Reason:** Similar to `experiment_tracker.py`, if results collection is part of the benchmarking process, issues in `ResultsCollector` can affect the output and validation in your tests.
   
7. **`gpt2_arc/src/training/train.py` &amp; `gpt2_arc/src/training/trainer.py`**
   - **Reason:** While not directly invoked in the provided test code, these modules may be indirectly involved if `benchmark_model` interacts with training routines or utilizes components from these scripts.
   
8. **External Dependencies (e.g., `torch`, `pytest`, `unittest.mock`)**
   - **Reason:** Although less likely, issues with the external libraries or how they are mocked in the tests can also lead to errors. Ensure that the versions are compatible and that mocks are correctly set up.

### Steps to Diagnose the Error:

1. **Examine the Error Message:**
   - Start by looking at the exact error message and traceback provided when the test fails. This will often point directly to the problematic file and line number.

2. **Check `benchmark.py`:**
   - Since this is the main module under test, review the functions `benchmark_model` and `main` for any logical errors or incorrect handling of inputs and outputs.

3. **Validate Mocks and Fixtures:**
   - Ensure that your mocks (e.g., `mock_model`, `mock_dataset`, `mock_dataloader`) accurately represent the behavior of the real objects. Incorrect mocking can lead to misleading test results.

4. **Review Dependencies in `gpt2_arc/src/models/gpt2.py`:**
   - Look for any issues in the `GPT2ARC` class, especially in methods that are invoked during benchmarking, such as `forward`.

5. **Inspect Configuration in `gpt2_arc/src/config.py`:**
   - Verify that all necessary configurations are correctly set and that there are no mismatches between expected and actual parameters.

6. **Analyze Data Handling in `gpt2_arc/src/data/arc_dataset.py`:**
   - Ensure that data loading and preprocessing steps are functioning as intended. Errors here can lead to incorrect inputs being fed into the model during benchmarking.

7. **Evaluate Utility Modules:**
   - Check `experiment_tracker.py` and `results_collector.py` for any bugs or exceptions that might interfere with the benchmarking process.

8. **Run Isolated Tests:**
   - Consider running individual tests or components in isolation to pinpoint where the failure occurs.

9. **Check for Environment Issues:**
   - Sometimes, errors arise from the testing environment, such as incompatible library versions or insufficient resources (e.g., GPU availability). Ensure that the environment matches the expectations set in your tests.

### Additional Tips:

- **Enable Verbose Logging:**
  - Add logging statements within `benchmark.py` and related modules to trace the flow of execution and identify where things might be going wrong.

- **Use Debugging Tools:**
  - Utilize debugging tools like `pdb` to step through the test execution and inspect the state of variables at different points.

- **Review Recent Changes:**
  - If the tests were passing previously, review recent changes to the codebase that might have introduced the error.

By systematically examining these files and following the diagnostic steps, you should be able to identify and resolve the error in your test suite effectively.
</file>
<file name="debug tips/test_results_collector.md">
When debugging errors in the `test_results_collector.py` test suite, the most relevant files to examine are those directly involved in the functionality being tested. Here's a prioritized list of files that are most likely to provide information to help fix any issues:

1. **`gpt2_arc/src/utils/results_collector.py`**
   - **Reason:** This is the primary module being tested. Any errors in initialization, metric updates, or result handling are likely originating from here.
   - **Key Components to Check:**
     - `ResultsCollector` class implementation.
     - Methods like `update_train_metrics`, `update_val_metrics`, `set_test_results`, `add_task_specific_result`, and `get_summary`.
     - Initialization logic, especially how `experiment_id`, `timestamp`, and `config` are set up.

2. **`gpt2_arc/src/config.py`**
   - **Reason:** The test initializes `ResultsCollector` using configurations defined in this file. Errors related to configuration attributes (e.g., `n_embd`, `n_head`, `n_layer`, `batch_size`, etc.) may stem from issues in the configuration classes.
   - **Key Components to Check:**
     - `Config`, `ModelConfig`, and `TrainingConfig` dataclasses.
     - Any methods or default values that manipulate or validate configuration parameters.

3. **`gpt2_arc/src/utils/experiment_tracker.py`**
   - **Reason:** `ResultsCollector` may internally utilize `ExperimentTracker` for logging and tracking experiments. Issues in experiment tracking could affect the results collection process.
   - **Key Components to Check:**
     - `ExperimentTracker` class methods, especially those related to logging metrics and handling experiment IDs.
     - Initialization and any interactions with external services like WandB (if `use_wandb` is enabled).

4. **Dependencies and External Modules:**
   - **`gpt2_arc/src/utils/results_collector.py` Dependencies:**
     - Ensure that any utility functions or classes used within `ResultsCollector` are functioning correctly.
   - **Environment and Configuration Files:**
     - Check for any environment-specific configurations or dependencies that might affect the test execution.

5. **Test Environment Setup:**
   - Although the test file itself is primarily for testing, ensure that the `setUp` method correctly initializes all necessary components. Misconfigurations or incorrect setups here can lead to misleading test failures.

**Steps to Diagnose the Issue:**

1. **Identify the Error Message:**
   - Start by examining the exact error message or traceback from the failed test. This will often point directly to the problematic line of code.

2. **Trace the Source:**
   - Use the traceback to trace back to the source file and line number where the error originated. This will help you determine whether the issue is within `results_collector.py`, `config.py`, or another related module.

3. **Review Recent Changes:**
   - If the tests were previously passing, consider any recent changes made to the related modules that might have introduced the error.

4. **Check for Dependency Issues:**
   - Ensure that all dependencies are correctly installed and compatible with each other, especially if there have been updates to packages like `torch`, `pytorch_lightning`, or others used in the project.

5. **Isolate the Problem:**
   - Temporarily simplify or isolate parts of the `ResultsCollector` to identify which specific method or component is causing the failure.

By systematically reviewing these files and following the diagnostic steps, you should be able to identify and fix the error in your test suite.
</file>
</source>