{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apologize for the oversight. You're right, there is indeed a script for hyperparameter tuning. Let's use this `optimize_hyperparameters.py` script for our workflow. Here's how we can proceed with hyperparameter tuning, scaling test, and evaluation using this script in a Colab notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the Colab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yourusername/arc-neural-reasoning-model.git\n",
    "%cd arc-neural-reasoning-model\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gpt2_arc/src/optimize_hyperparameters.py --n_trials 50 --storage sqlite:///optuna_results.db --n_embd_min 64 --n_embd_max 256 --n_head_min 2 --n_head_max 8 --n_layer_min 2 --n_layer_max 6 --batch_size_min 16 --batch_size_max 64 --learning_rate_min 1e-5 --learning_rate_max 1e-3 --max_epochs_min 10 --max_epochs_max 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda44504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available studies in the database:\n",
      "- gpt2_arc_optimization\n",
      "Best hyperparameters:\n",
      "{\n",
      "  \"n_embd\": 87,\n",
      "  \"n_head\": 3,\n",
      "  \"n_layer\": 1,\n",
      "  \"batch_size\": 56,\n",
      "  \"learning_rate\": 0.0006206653045449178,\n",
      "  \"max_epochs\": 10\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import json\n",
    "\n",
    "storage_name = \"sqlite:////workspaces/arc-neural-reasoning-model/optuna_results.db\"\n",
    "study_name = \"gpt2_arc_optimization\"\n",
    "\n",
    "try:\n",
    "    # List all study names in the database\n",
    "    study_summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
    "    print(\"Available studies in the database:\")\n",
    "    for study_summary in study_summaries:\n",
    "        print(f\"- {study_summary.study_name}\")\n",
    "\n",
    "    # Attempt to load the specified study\n",
    "    study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(json.dumps(best_params, indent=2))\n",
    "except KeyError as e:\n",
    "    print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
    "    print(f\"Details: {str(e)}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ba232c",
   "metadata": {},
   "source": [
    "### 4. Use the best hyperparameters for a scaling test (longer training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a37638f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:gpt2_arc.src.training.train:Data loaded successfully using arckit\n",
      "INFO:gpt2_arc.src.training.train:Initializing model with new configuration\n",
      "DEBUG:gpt2_arc.src.models.gpt2:Initialized Attention with n_embd=96, n_head=3\n",
      "DEBUG:gpt2_arc.src.models.gpt2:Initialized FeedForward with n_embd=96\n",
      "DEBUG:gpt2_arc.src.models.gpt2:Initialized TransformerBlock with n_embd=96, n_head=3\n",
      "INFO:gpt2_arc.src.training.train:Initializing trainer with new configuration\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Starting ARCDataset initialization\n",
      "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
      "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
      "DEBUG: Processed data length: 400\n",
      "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
      "DEBUG: First train item: {'input': array([[0, 7, 7],\n",
      "       [7, 7, 7],\n",
      "       [0, 7, 7]]), 'output': array([[0, 0, 0, 0, 7, 7, 0, 7, 7],\n",
      "       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n",
      "       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n",
      "       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n",
      "       [7, 7, 7, 7, 7, 7, 7, 7, 7],\n",
      "       [0, 7, 7, 0, 7, 7, 0, 7, 7],\n",
      "       [0, 0, 0, 0, 7, 7, 0, 7, 7],\n",
      "       [0, 0, 0, 7, 7, 7, 7, 7, 7],\n",
      "       [0, 0, 0, 0, 7, 7, 0, 7, 7]])}\n",
      "Number of train samples: 1302\n",
      "Number of test samples: 416\n",
      "DEBUG: Starting ARCDataset initialization\n",
      "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
      "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
      "DEBUG: Processed data length: 400\n",
      "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
      "DEBUG: First train item: {'input': array([[8, 6],\n",
      "       [6, 4]]), 'output': array([[8, 6, 8, 6, 8, 6],\n",
      "       [6, 4, 6, 4, 6, 4],\n",
      "       [6, 8, 6, 8, 6, 8],\n",
      "       [4, 6, 4, 6, 4, 6],\n",
      "       [8, 6, 8, 6, 8, 6],\n",
      "       [6, 4, 6, 4, 6, 4]])}\n",
      "Number of train samples: 1363\n",
      "Number of test samples: 419\n",
      "ExperimentTracker initialized with config: {\n",
      "  \"model\": {\n",
      "    \"n_embd\": 96,\n",
      "    \"n_head\": 3,\n",
      "    \"n_layer\": 1,\n",
      "    \"dropout\": 0.1\n",
      "  },\n",
      "  \"training\": {\n",
      "    \"batch_size\": 56,\n",
      "    \"learning_rate\": 0.0006206653045449178,\n",
      "    \"max_epochs\": 100,\n",
      "    \"use_gpu\": true,\n",
      "    \"log_level\": \"INFO\"\n",
      "  }\n",
      "}\n",
      "Project: arc-scaling-test, Entity: None\n",
      "use_wandb: False\n",
      "Using local logging only\n",
      "Training interrupted: ARCTrainer.__init__() got an unexpected keyword argument 'results_collector'\n",
      "Logged metric locally: training_interrupted=1, step=None\n",
      "Logged metric locally: error_message=ARCTrainer.__init__() got an unexpected keyword argument 'results_collector', step=None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Namespace' object has no attribute 'no_logging'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m args\u001b[38;5;241m.\u001b[39muse_gpu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# Enable GPU if available\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Run the training\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gpt2_arc/src/training/train.py:72\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     69\u001b[0m         tracker\u001b[38;5;241m.\u001b[39mfinish()\n\u001b[1;32m     71\u001b[0m \u001b[38;5;66;03m# Create PyTorch Lightning trainer\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m tb_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_logging\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m TensorBoardLogger(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtb_logs\u001b[39m\u001b[38;5;124m\"\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marc_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     73\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args\u001b[38;5;241m.\u001b[39mno_checkpointing:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Namespace' object has no attribute 'no_logging'"
     ]
    }
   ],
   "source": [
    "from gpt2_arc.src.training.train import main\n",
    "import argparse\n",
    "\n",
    "# Create an argument parser with the best hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_embd\", type=int, default=best_params['n_embd'])\n",
    "parser.add_argument(\"--n_head\", type=int, default=best_params['n_head'])\n",
    "parser.add_argument(\"--n_layer\", type=int, default=best_params['n_layer'])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=best_params['batch_size'])\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=best_params['learning_rate'])\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=100)  # Extend for longer training\n",
    "parser.add_argument(\"--use_gpu\", action=\"store_true\")\n",
    "parser.add_argument(\"--project\", type=str, default=\"arc-scaling-test\")\n",
    "parser.add_argument(\"--log_level\", type=str, default=\"INFO\", help=\"Set the logging level\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.use_gpu = True  # Enable GPU if available\n",
    "\n",
    "# Run the training\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062c297",
   "metadata": {},
   "source": [
    "### 5. Evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "987843f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: Starting ARCDataset initialization\n",
      "DEBUG: data_source type: <class 'arckit.data.TaskSet'>\n",
      "DEBUG: data_source content: <TaskSet: 400 tasks>\n",
      "DEBUG: Processed data length: 400\n",
      "DEBUG: First item keys: dict_keys(['id', 'train', 'test'])\n",
      "DEBUG: First train item: {'input': array([[8, 6],\n",
      "       [6, 4]]), 'output': array([[8, 6, 8, 6, 8, 6],\n",
      "       [6, 4, 6, 4, 6, 4],\n",
      "       [6, 8, 6, 8, 6, 8],\n",
      "       [4, 6, 4, 6, 4, 6],\n",
      "       [8, 6, 8, 6, 8, 6],\n",
      "       [6, 4, 6, 4, 6, 4]])}\n",
      "Number of train samples: 1363\n",
      "Number of test samples: 419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/gpt2_arc/src/evaluate.py:51: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(args.model_checkpoint)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Model configuration not found in checkpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 23\u001b[0m\n\u001b[1;32m     14\u001b[0m eval_args \u001b[38;5;241m=\u001b[39m eval_parser\u001b[38;5;241m.\u001b[39mparse_args([\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--model_checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_checkpoint_path,\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--batch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m32\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--wandb_run_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaling-test-evaluation\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m ])\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Run the evaluation\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[43mevaluate_main\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/gpt2_arc/src/evaluate.py:63\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     56\u001b[0m     model_config \u001b[38;5;241m=\u001b[39m ModelConfig(\n\u001b[1;32m     57\u001b[0m         n_embd\u001b[38;5;241m=\u001b[39mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_embd\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     58\u001b[0m         n_head\u001b[38;5;241m=\u001b[39mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_head\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     59\u001b[0m         n_layer\u001b[38;5;241m=\u001b[39mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_layer\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     60\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mconfig_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel configuration not found in checkpoint\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Initialize the model with the checkpoint configuration\u001b[39;00m\n\u001b[1;32m     66\u001b[0m model \u001b[38;5;241m=\u001b[39m GPT2ARC(model_config)\n",
      "\u001b[0;31mValueError\u001b[0m: Model configuration not found in checkpoint"
     ]
    }
   ],
   "source": [
    "from gpt2_arc.src.evaluate import main as evaluate_main\n",
    "import argparse\n",
    "# Set up evaluation arguments\n",
    "eval_parser = argparse.ArgumentParser()\n",
    "eval_parser.add_argument(\"--model_checkpoint\", type=str, required=True, help=\"Path to the model checkpoint\")\n",
    "eval_parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "eval_parser.add_argument(\"--output_dir\", type=str, default=\"./evaluation_results\")\n",
    "eval_parser.add_argument(\"--wandb_project\", type=str, default=\"arc-evaluation\")\n",
    "eval_parser.add_argument(\"--wandb_run_name\", type=str, default=\"scaling-test-evaluation\")\n",
    "\n",
    "# Replace with the actual path to your trained model checkpoint\n",
    "model_checkpoint_path = \"/workspaces/arc-neural-reasoning-model/final_model_4fe9801e-c839-454f-a46c-6e94e3c04e81.pth\"\n",
    "\n",
    "eval_args = eval_parser.parse_args([\n",
    "    \"--model_checkpoint\", model_checkpoint_path,\n",
    "    \"--batch_size\", \"32\",\n",
    "    \"--output_dir\", \"./evaluation_results\",\n",
    "    \"--wandb_project\", \"arc-evaluation\",\n",
    "    \"--wandb_run_name\", \"scaling-test-evaluation\"\n",
    "])\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_main(eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and print evaluation results\n",
    "with open(\"./evaluation_results/scaling-test-evaluation_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
