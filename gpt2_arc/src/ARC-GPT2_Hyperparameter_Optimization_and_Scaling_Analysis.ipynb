{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apologize for the oversight. You're right, there is indeed a script for hyperparameter tuning. Let's use this `optimize_hyperparameters.py` script for our workflow. Here's how we can proceed with hyperparameter tuning, scaling test, and evaluation using this script in a Colab notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the Colab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yourusername/arc-neural-reasoning-model.git\n",
    "%cd arc-neural-reasoning-model\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gpt2_arc/src/optimize_hyperparameters.py --n_trials 50 --storage sqlite:///optuna_results.db --n_embd_min 64 --n_embd_max 256 --n_head_min 2 --n_head_max 8 --n_layer_min 2 --n_layer_max 6 --batch_size_min 16 --batch_size_max 64 --learning_rate_min 1e-5 --learning_rate_max 1e-3 --max_epochs_min 10 --max_epochs_max 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fda44504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\n",
      "Details: 'Record does not exist.'\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import json\n",
    "\n",
    "try:\n",
    "    study = optuna.load_study(study_name=\"gpt2_arc_optimization\", storage=\"sqlite:///optuna_results.db\")\n",
    "    best_params = study.best_params\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(json.dumps(best_params, indent=2))\n",
    "except KeyError as e:\n",
    "    print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
    "    print(f\"Details: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use the best hyperparameters for a scaling test (longer training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_arc.src.training.train import main\n",
    "import argparse\n",
    "\n",
    "# Create an argument parser with the best hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_embd\", type=int, default=best_params['n_embd'])\n",
    "parser.add_argument(\"--n_head\", type=int, default=best_params['n_head'])\n",
    "parser.add_argument(\"--n_layer\", type=int, default=best_params['n_layer'])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=best_params['batch_size'])\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=best_params['learning_rate'])\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=100)  # Extend for longer training\n",
    "parser.add_argument(\"--use_gpu\", action=\"store_true\")\n",
    "parser.add_argument(\"--project\", type=str, default=\"arc-scaling-test\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.use_gpu = True  # Enable GPU if available\n",
    "\n",
    "# Run the training\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_arc.src.evaluate import main as evaluate_main\n",
    "\n",
    "# Set up evaluation arguments\n",
    "eval_parser = argparse.ArgumentParser()\n",
    "eval_parser.add_argument(\"--model_checkpoint\", type=str, required=True, help=\"Path to the model checkpoint\")\n",
    "eval_parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "eval_parser.add_argument(\"--output_dir\", type=str, default=\"./evaluation_results\")\n",
    "eval_parser.add_argument(\"--wandb_project\", type=str, default=\"arc-evaluation\")\n",
    "eval_parser.add_argument(\"--wandb_run_name\", type=str, default=\"scaling-test-evaluation\")\n",
    "\n",
    "# Replace with the actual path to your trained model checkpoint\n",
    "model_checkpoint_path = \"path/to/your/trained/model/checkpoint.pth\"\n",
    "\n",
    "eval_args = eval_parser.parse_args([\n",
    "    \"--model_checkpoint\", model_checkpoint_path,\n",
    "    \"--batch_size\", \"32\",\n",
    "    \"--output_dir\", \"./evaluation_results\",\n",
    "    \"--wandb_project\", \"arc-evaluation\",\n",
    "    \"--wandb_run_name\", \"scaling-test-evaluation\"\n",
    "])\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_main(eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and print evaluation results\n",
    "with open(\"./evaluation_results/scaling-test-evaluation_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
