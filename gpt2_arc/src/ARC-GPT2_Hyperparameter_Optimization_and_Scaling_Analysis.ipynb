{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I apologize for the oversight. You're right, there is indeed a script for hyperparameter tuning. Let's use this `optimize_hyperparameters.py` script for our workflow. Here's how we can proceed with hyperparameter tuning, scaling test, and evaluation using this script in a Colab notebook:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Set up the Colab environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/yourusername/arc-neural-reasoning-model.git\n",
    "%cd arc-neural-reasoning-model\n",
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Run hyperparameter tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python gpt2_arc/src/optimize_hyperparameters.py --n_trials 50 --storage sqlite:///optuna_results.db --n_embd_min 64 --n_embd_max 256 --n_head_min 2 --n_head_max 8 --n_layer_min 2 --n_layer_max 6 --batch_size_min 16 --batch_size_max 64 --learning_rate_min 1e-5 --learning_rate_max 1e-3 --max_epochs_min 10 --max_epochs_max 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Get the best hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Record does not exist.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01moptuna\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[43moptuna\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_study\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt2_arc_optimization\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msqlite:///optuna_results.db\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m best_params \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/_convert_positional_args.py:83\u001b[0m, in \u001b[0;36mconvert_positional_args.<locals>.converter_decorator.<locals>.converter_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() got multiple values for arguments \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mduplicated_kwds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m     )\n\u001b[1;32m     81\u001b[0m kwargs\u001b[38;5;241m.\u001b[39mupdate(inferred_kwargs)\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/study.py:1369\u001b[0m, in \u001b[0;36mload_study\u001b[0;34m(study_name, storage, sampler, pruner)\u001b[0m\n\u001b[1;32m   1363\u001b[0m     study_name \u001b[38;5;241m=\u001b[39m study_names[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1364\u001b[0m     _logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[1;32m   1365\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStudy name was omitted but trying to load \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m because that was the only \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1366\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstudy found in the storage.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1367\u001b[0m     )\n\u001b[0;32m-> 1369\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStudy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstudy_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msampler\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msampler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpruner\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpruner\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/study/study.py:87\u001b[0m, in \u001b[0;36mStudy.__init__\u001b[0;34m(self, study_name, storage, sampler, pruner)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstudy_name \u001b[38;5;241m=\u001b[39m study_name\n\u001b[1;32m     86\u001b[0m storage \u001b[38;5;241m=\u001b[39m storages\u001b[38;5;241m.\u001b[39mget_storage(storage)\n\u001b[0;32m---> 87\u001b[0m study_id \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_study_id_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_study_id \u001b[38;5;241m=\u001b[39m study_id\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_storage \u001b[38;5;241m=\u001b[39m storage\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/storages/_cached_storage.py:107\u001b[0m, in \u001b[0;36m_CachedStorage.get_study_id_from_name\u001b[0;34m(self, study_name)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_study_id_from_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, study_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_study_id_from_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/storages/_rdb/storage.py:332\u001b[0m, in \u001b[0;36mRDBStorage.get_study_id_from_name\u001b[0;34m(self, study_name)\u001b[0m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_study_id_from_name\u001b[39m(\u001b[38;5;28mself\u001b[39m, study_name: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    331\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _create_scoped_session(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscoped_session) \u001b[38;5;28;01mas\u001b[39;00m session:\n\u001b[0;32m--> 332\u001b[0m         study \u001b[38;5;241m=\u001b[39m \u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStudyModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_or_raise_by_name\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    333\u001b[0m         study_id \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mstudy_id\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m study_id\n",
      "File \u001b[0;32m/usr/local/python/3.12.1/lib/python3.12/site-packages/optuna/storages/_rdb/models.py:88\u001b[0m, in \u001b[0;36mStudyModel.find_or_raise_by_name\u001b[0;34m(cls, study_name, session)\u001b[0m\n\u001b[1;32m     86\u001b[0m study \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mfind_by_name(study_name, session)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m study \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(NOT_FOUND_MSG)\n\u001b[1;32m     90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m study\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Record does not exist.'"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import json\n",
    "\n",
    "study = optuna.load_study(study_name=\"gpt2_arc_optimization\", storage=\"sqlite:///optuna_results.db\")\n",
    "best_params = study.best_params\n",
    "\n",
    "print(\"Best hyperparameters:\")\n",
    "print(json.dumps(best_params, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use the best hyperparameters for a scaling test (longer training):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_arc.src.training.train import main\n",
    "import argparse\n",
    "\n",
    "# Create an argument parser with the best hyperparameters\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--n_embd\", type=int, default=best_params['n_embd'])\n",
    "parser.add_argument(\"--n_head\", type=int, default=best_params['n_head'])\n",
    "parser.add_argument(\"--n_layer\", type=int, default=best_params['n_layer'])\n",
    "parser.add_argument(\"--batch_size\", type=int, default=best_params['batch_size'])\n",
    "parser.add_argument(\"--learning_rate\", type=float, default=best_params['learning_rate'])\n",
    "parser.add_argument(\"--max_epochs\", type=int, default=100)  # Extend for longer training\n",
    "parser.add_argument(\"--use_gpu\", action=\"store_true\")\n",
    "parser.add_argument(\"--project\", type=str, default=\"arc-scaling-test\")\n",
    "\n",
    "args = parser.parse_args([])\n",
    "args.use_gpu = True  # Enable GPU if available\n",
    "\n",
    "# Run the training\n",
    "main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluate the trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gpt2_arc.src.evaluate import main as evaluate_main\n",
    "\n",
    "# Set up evaluation arguments\n",
    "eval_parser = argparse.ArgumentParser()\n",
    "eval_parser.add_argument(\"--model_checkpoint\", type=str, required=True, help=\"Path to the model checkpoint\")\n",
    "eval_parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "eval_parser.add_argument(\"--output_dir\", type=str, default=\"./evaluation_results\")\n",
    "eval_parser.add_argument(\"--wandb_project\", type=str, default=\"arc-evaluation\")\n",
    "eval_parser.add_argument(\"--wandb_run_name\", type=str, default=\"scaling-test-evaluation\")\n",
    "\n",
    "# Replace with the actual path to your trained model checkpoint\n",
    "model_checkpoint_path = \"path/to/your/trained/model/checkpoint.pth\"\n",
    "\n",
    "eval_args = eval_parser.parse_args([\n",
    "    \"--model_checkpoint\", model_checkpoint_path,\n",
    "    \"--batch_size\", \"32\",\n",
    "    \"--output_dir\", \"./evaluation_results\",\n",
    "    \"--wandb_project\", \"arc-evaluation\",\n",
    "    \"--wandb_run_name\", \"scaling-test-evaluation\"\n",
    "])\n",
    "\n",
    "# Run the evaluation\n",
    "evaluate_main(eval_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Analyze the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load and print evaluation results\n",
    "with open(\"./evaluation_results/scaling-test-evaluation_results.json\", \"r\") as f:\n",
    "    results = json.load(f)\n",
    "\n",
    "print(\"Evaluation Results:\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
