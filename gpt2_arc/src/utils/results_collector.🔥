# gpt2_arc/src/utils/results_collector.py
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

struct ResultsCollector:
    fn __init__(inout self, config: Any):
        """Initialize the ResultsCollector with a given configuration."""
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = asdict(config)
        self.symbol_freq = self.config['training']['symbol_freq'] if 'symbol_freq' in self.config['training'] else {}
        logger.debug(f"Symbol frequencies set in ResultsCollector: {self.symbol_freq}")
        self.results = {
            "train": {},
            "validation": {},
            "test": {}
        }
        self.metrics = {}
        self.task_specific_results = {}
        self.tensorboard_log_path = getattr(config.training, 'tensorboard_log_path', None)
        self.used_synthetic_data = getattr(config.training, 'use_synthetic_data', False)
        self.environment = self._get_environment_info()
        self.checkpoint_path = None
        print(f"DEBUG: Initialized self.results['train'] as {type(self.results['train'])}")
        self._log_results_type("After initialization")

    fn set_tensorboard_log_path(inout self, path: str) -> None:
        self.tensorboard_log_path = path
        print(f"DEBUG: Set TensorBoard log path in ResultsCollector: {path}")

    fn _get_environment_info(inout self) -> Dict[]:
        """Retrieve environment information such as Python and PyTorch versions."""
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    fn _log_results_type(inout self, context: str):
        """Log the type of self.results['train'] for debugging."""
        logger.debug(f"{context}: self.results['train'] is of type {type(self.results['train'])}")
    
    fn update_train_metrics(inout self, epoch: Int, metrics: Dict[]):
        """Update training metrics for a specific epoch."""
        if "train" not in self.results:
            self.results["train"] = {}
        if not isinstance(self.results["train"], dict):
            raise TypeError(f"Expected self.results['train'] to be a dict, but got {type(self.results['train'])}")
        self.results["train"].setdefault(epoch, {})
        self.results["train"][epoch].update(metrics)
        logger.debug(f"Updated train metrics for epoch {epoch}: {metrics}")

    fn update_val_metrics(inout self, epoch: Int, metrics: Dict[]):
        """Update validation metrics for a specific epoch."""
        if "validation" not in self.results:
            self.results["validation"] = {}
        if not isinstance(self.results["validation"], dict):
            raise TypeError(f"Expected self.results['validation'] to be a dict, but got {type(self.results['validation'])}")
        self.results["validation"].setdefault(epoch, {})
        self.results["validation"][epoch].update(metrics)
        logger.debug(f"Updated validation metrics for epoch {epoch}: {metrics}")

    fn set_test_results(inout self, metrics: Dict[]):
        """Set the test results metrics."""
        self.results["test"] = metrics
        
    fn add_task_specific_result(inout self, task_id: str, metrics: Dict[]):
        """Add task-specific results for a given task ID."""
        if task_id == "default_task":
            logger.error("Attempted to add metrics for 'default_task'. This should be avoided.")
            raise ValueError("Cannot add metrics for 'default_task'. Ensure that task_id is correctly assigned.")
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        for key, value in metrics.items():
            if key not in self.task_specific_results[task_id]:
                self.task_specific_results[task_id][key] = []
            self.task_specific_results[task_id][key].append(value)

    fn get_task_specific_results(inout self) -> Dict[]:
        """Retrieve aggregated task-specific metrics."""
        aggregated = {}
        for task_id, metrics in self.task_specific_results.items():
            aggregated[task_id] = {}
            for metric_name, values in metrics.items():
                aggregated[task_id][metric_name] = sum(values) / len(values) if values else 0.0
        return aggregated
    
    fn set_final_metrics(inout self, metrics: Dict[]):                                                                                          
        """Set the final metrics after training."""
        self.metrics = metrics

    fn set_checkpoint_path(inout self, path: str):
        """Set the path to the model checkpoint."""
        self.checkpoint_path = path

    fn save_to_json(inout self, filepath: str):
        """Save the collected results to a JSON file."""
        try:
            self._ensure_directory_exists(os.path.dirname(filepath))
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path,
                "used_synthetic_data": self.used_synthetic_data
            }
            if self.symbol_freq:
                data["symbol_freq"] = self.symbol_freq
            else:
                data["symbol_freq"] = {}
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    fn _ensure_directory_exists(inout self, directory: str):
        """Ensure that the directory exists; create it if it does not."""
        if not os.path.exists(directory):
            os.makedirs(directory)

    fn get_summary(inout self) -> Dict[]:
        """
        Get a summary of the results.
        
        Returns:
            Dict[str, Any]: Summary of key metrics.
        """
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_loss": self.results["test"].get("avg_loss"),
            "test_acc_with_pad": self.results["test"].get("avg_acc_with_pad"),
            "test_acc_without_pad": self.results["test"].get("avg_acc_without_pad"),
            "best_val_loss": self.results.get("best_val_loss"),
            "best_val_epoch": self.results.get("best_val_epoch"),
            "learning_rate": self.config['training']['learning_rate'],
            "batch_size": self.config['training']['batch_size'],
            "training_duration": self.results.get("training_duration"),
            "config": self._serialize_config(self.config),
            "tensorboard_log_path": self.tensorboard_log_path
        }
        logger.debug(f"DEBUG: Added TensorBoard log path to results: {summary['tensorboard_log_path']}")
        return {k: self._make_serializable(v) for k, v in summary.items()}

    fn _make_serializable(inout self, obj: Any) -> Any:
        """Ensure the value is serializable, handling non-serializable objects."""
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)
