# gpt2_arc/src/utils/experiment_tracker.py
import logging
import wandb
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

struct ExperimentTracker:
    fn __init__(inout self, config: Dict[], project: str, entity: Optional[str] = None, use_wandb: bool = False):
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = config.to_dict() if hasattr(config, 'to_dict') else self._config_to_dict(config)
        self.project = project
        self.entity = entity
        self.run = None
        self.use_wandb = use_wandb
        self.metrics = {}
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False

        self.results = {
            "train": [],
            "validation": [],
            "test": {}
        }
        self.metrics = {}
        self.task_specific_results = {}
        self.environment = self._get_environment_info()
        self.checkpoint_path = None

        # Add debug logging
        logger.debug(f"ExperimentTracker initialized with config: {json.dumps(self.config, indent=2)}")
        logger.debug(f"Project: {project}, Entity: {entity}")
        logger.debug(f"use_wandb: {self.use_wandb}")

    fn _get_environment_info(inout self) -> Dict[]:
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    fn _config_to_dict(inout self, config: Any) -> Dict[]:
        if isinstance(config, dict):
            return {k: self._config_to_dict(v) for k, v in config.items()}
        elif hasattr(config, '__dict__'):
            return {k: self._config_to_dict(v) for k, v in config.__dict__.items() if not k.startswith('_')}
        else:
            return config
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False
        
        if not self.use_wandb:
            print("Using local logging only")

    fn finish(inout self):
        if self.use_wandb and self.run:
            try:
                wandb.finish()
                print("Wandb run finished")
            except Exception as e:
                print(f"Error finishing wandb run: {str(e)}")
        else:
            print("Experiment finished. Metrics:", self.metrics)

    fn log_metric(inout self, name: str, value: Float32, step: Optional[Int] = None):
        if self.use_wandb:
            try:
                wandb.log({name: value}, step=step)
                print(f"Logged metric to wandb: {name}={value}, step={step}")
            except Exception as e:
                print(f"Error logging metric to wandb: {str(e)}")
        
        # Always log locally as a fallback
        print(f"Logged metric locally: {name}={value}, step={step}")

    fn update_train_metrics(inout self, epoch: Int, metrics: Dict[]):
        if "train" not in self.results:
            self.results["train"] = []
        while len(self.results["train"]) <= epoch:
            self.results["train"].append({})
        self.results["train"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"train": metrics}, step=epoch)

    fn update_val_metrics(inout self, epoch: Int, metrics: Dict[]):
        if "validation" not in self.results:
            self.results["validation"] = []
        while len(self.results["validation"]) <= epoch:
            self.results["validation"].append({})
        self.results["validation"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"validation": metrics}, step=epoch)

    fn set_test_results(inout self, metrics: Dict[]):
        self.results["test"] = metrics
        if self.use_wandb:
            wandb.log({"test": metrics})

    fn add_task_specific_result(inout self, task_id: str, metrics: Dict[]):
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        self.task_specific_results[task_id].update(metrics)
        logger.debug(f"Added task-specific result for task_id {task_id}: {metrics}")
        if self.use_wandb:
            wandb.log({f"task_{task_id}": metrics})

    fn set_final_metrics(inout self, metrics: Dict[]):
        self.metrics = metrics
        if self.use_wandb:
            wandb.log(metrics)

    fn set_checkpoint_path(inout self, path: str):
        self.checkpoint_path = path
        if self.use_wandb:
            wandb.save(path)

    fn save_to_json(inout self, filepath: str):
        try:
            directory = os.path.dirname(filepath)
            if directory and not os.path.exists(directory):
                os.makedirs(directory)
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path
            }
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"Results saved to {filepath}")
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    fn _ensure_directory_exists(inout self, directory: str):
        if not os.path.exists(directory):
            os.makedirs(directory)

    fn get_summary(inout self) -> Dict[]:
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_loss": self.results["test"].get("avg_loss"),
            "test_acc_with_pad": self.results["test"].get("avg_acc_with_pad"),
            "test_acc_without_pad": self.results["test"].get("avg_acc_without_pad"),
            "best_val_loss": self.results.get("best_val_loss"),
            "best_val_epoch": self.results.get("best_val_epoch"),
            "learning_rate": self.config.get("training", {}).get("learning_rate"),
            "batch_size": self.config.get("training", {}).get("batch_size"),
            "training_duration": self.results.get("training_duration"),
            "config": self._serialize_config(self.config),
            "tensorboard_log_path": self.tensorboard_log_path
        }
        self.logger.debug(f"DEBUG: Added TensorBoard log path to results: {summary['tensorboard_log_path']}")
        return {k: self._make_serializable(v) for k, v in summary.items()}

    fn _make_serializable(inout self, obj: Any) -> Any:
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

    fn _serialize_config(inout self, config: Dict[]) -> Dict[]:
        return {k: self._make_serializable(v) for k, v in config.items()}

    fn log_metric(inout self, name: str, value: Float32, step: Optional[Int] = None):
        if name.startswith("default_task_"):
            logger.error("Attempted to log metric under 'default_task'. This should be avoided.")
            raise ValueError("Cannot log metrics under 'default_task'. Ensure that task_id is correctly assigned.")

# Add a simple test
if __name__ == "__main__":
    config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": True}
    tracker = ExperimentTracker(config, project="test-project")
    tracker.start()
    tracker.log_metric("accuracy", 0.85, step=1)
    tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8})
    tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75})
    tracker.set_test_results({"loss": 0.55, "accuracy": 0.82})
    tracker.add_task_specific_result("task_1", {"accuracy": 0.9})
    tracker.set_final_metrics({"best_accuracy": 0.85})
    tracker.set_checkpoint_path("model_checkpoint.pth")
    tracker.save_to_json("results.json")
    tracker.finish()
