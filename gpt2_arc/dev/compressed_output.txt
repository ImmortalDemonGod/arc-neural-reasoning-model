<?xml version='1.0' encoding='utf-8'?>
<source type="local_directory" path="/workspaces/arc-neural-reasoning-model/gpt2_arc"><file name="requirements.txt">cysimdjson absl-py==2.1.0 accelerate==0.33.0 aider-chat==0.59.1 aiohappyeyeballs==2.4.0 aiohttp==3.10.5 aiosignal==1.3.1 alembic==1.13.3 altair==5.3.0 annotated-types==0.7.0 anyio==4.6.0 arckit==0.1.0 argon2-cffi==23.1.0 argon2-cffi-bindings==21.2.0 arrow==1.3.0 asttokens==2.4.1 async-lru==2.0.4 attrs==24.2.0 babel==2.16.0 backoff==2.2.1 beartype==0.18.5 beautifulsoup4==4.12.3 bitnet==0.2.5 bitsandbytes==0.44.1 black==24.8.0 bleach==6.1.0 blinker==1.8.2 bottle==0.13.1 build==1.2.2 cachecontrol==0.14.0 cachetools==5.3.3 certifi==2024.8.30 cffi==1.17.1 chardet==5.2.0 charset-normalizer==3.3.2 cleo==2.1.0 click==8.1.7 colorama==0.4.6 colorlog==6.8.2 colt5-attention==0.10.19 comm==0.2.2 commonmark==0.9.1 configargparse==1.7 contourpy==1.3.0 coverage==7.6.1 crashtest==0.4.1 cryptography==43.0.1 cycler==0.12.1 datasets==2.14.4 debugpy==1.8.5 decorator==5.1.1 defusedxml==0.7.1 diff-match-patch==20230430 dill==0.3.7 diskcache==5.6.3 distlib==0.3.8 distro==1.9.0 docker-pycreds==0.4.0 drawsvg==2.4.0 dulwich==0.21.7 e==1.4.5 einops==0.8.0 einops-exts==0.0.4 einx==0.3.0 entrypoints==0.4 executing==2.1.0 fairscale==0.4.13 fastjsonschema==2.20.0 filelock==3.16.1 flake8==7.1.1 fonttools==4.54.0 fqdn==1.5.1 frozendict==2.4.4 frozenlist==1.4.1 fsspec==2024.9.0 gitdb==4.0.11 gitpython==3.1.43 google-ai-generativelanguage==0.6.2 google-api-core==2.19.0 google-api-python-client==2.128.0 google-auth==2.29.0 google-auth-httplib2==0.2.0 google-generativeai==0.5.2 googleapis-common-protos==1.63.0 greenlet==3.0.3 grep-ast==0.3.3 grpcio==1.63.0 grpcio-status==1.62.2 h11==0.14.0 httpcore==1.0.5 httplib2==0.22.0 httpx==0.27.2 huggingface-hub==0.25.0 hypothesis==6.115.0 idna==3.10 importlib_metadata==7.2.1 importlib_resources==6.4.5 iniconfig==2.0.0 installer==0.7.0 ipykernel==6.29.5 ipython==8.27.0 isoduration==20.11.0 isort==5.13.2 jaraco.classes==3.4.0 jedi==0.19.1 jeepney==0.8.0 jinja2==3.1.4 jiter==0.5.0 joblib==1.4.2 json5==0.9.25 jsonpointer==3.0.0 jsonschema==4.23.0 jsonschema-specifications==2023.12.1 jupyter-events==0.10.0 jupyter-lsp==2.2.5 jupyter-server-mathjax==0.2.6 jupyter_client==8.6.3 jupyter_core==5.7.2 jupyter_server==2.14.2 jupyter_server_terminals==0.5.3 jupyterlab==4.2.5 jupyterlab_git==0.50.1 jupyterlab_pygments==0.3.0 jupyterlab_server==2.27.3 keyring==24.3.1 kiwisolver==1.4.7 libcst==1.1.0 lightning==2.4.0 lightning-utilities==0.11.7 lion-pytorch==0.2.2 litellm==1.47.0 local-attention==1.9.15 loguru==0.7.2 mako==1.3.5 markdown==3.7 markdown-it-py==3.0.0 markupsafe==2.1.5 matplotlib==3.9.2 matplotlib-inline==0.1.7 mccabe==0.7.0 mdurl==0.1.2 memory-profiler==0.61.0 mistune==0.8.4 more-itertools==10.5.0 mpmath==1.3.0 msgpack==1.1.0 multidict==6.1.0 multiprocess==0.70.15 mypy==1.11.2 mypy-extensions==1.0.0 nbclient==0.10.0 nbconvert==6.5.0 nbdime==4.0.2 nbformat==5.4.0 nest-asyncio==1.6.0 networkx==3.2.1 nltk==3.7 notebook_shim==0.2.4 numpy==1.26.4 nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==9.1.0.70 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-nccl-cu12==2.20.5 nvidia-nvjitlink-cu12==12.6.77 nvidia-nvtx-cu12==12.1.105 openai==1.47.0 optuna==4.0.0 optuna-dashboard==0.16.2 optuna-integration==4.0.0 overrides==7.7.0 packaging==24.1 pandas==2.2.2 pandocfilters==1.5.1 parso==0.8.4 pathspec==0.12.1 pexpect==4.9.0 pillow==10.4.0 pkginfo==1.11.1 platformdirs==4.3.6 playwright==1.43.0 plotly==5.24.1 pluggy==1.5.0 poetry==1.8.3 poetry-core==1.9.0 poetry-plugin-export==1.8.0 prometheus_client==0.21.0 prompt_toolkit==3.0.47 proto-plus==1.23.0 protobuf==4.25.3 psutil==6.0.0 ptyprocess==0.7.0 pure_eval==0.2.3 pyarrow==16.0.0 pyasn1==0.6.0 pyasn1_modules==0.4.0 pycodestyle==2.12.1 pycparser==2.22 pydantic==2.9.2 pydantic_core==2.23.4 pydeck==0.9.0 pydub==0.25.1 pyee==11.1.0 pyflakes==3.2.0 pygments==2.18.0 pyngrok==7.2.0 pynvml==11.5.3 pypandoc==1.13 pyparsing==3.1.2 pypdf2==2.10.0 pyperclip==1.9.0 pyproject-api==1.6.1 pyproject_hooks==1.2.0 pytest==8.3.2 pytest-cov==5.0.0 pytest-mock==3.14.0 python-dateutil==2.9.0.post0 python-dotenv==1.0.1 python-json-logger==2.0.7 pytorch-lightning==2.4.0 pytz==2024.1 pyyaml==6.0.2 pyzmq==26.2.0 rapidfuzz==3.10.0 referencing==0.35.1 regex==2024.9.11 requests==2.32.3 requests-toolbelt==1.0.0 rfc3339-validator==0.1.4 rfc3986-validator==0.1.1 rich==13.8.1 rpds-py==0.20.0 rsa==4.9 ruff==0.6.9 safetensors==0.4.5 scikit-learn==1.5.2 scipy==1.13.1 seaborn==0.13.2 secretstorage==3.3.3 send2trash==1.8.3 sentencepiece==0.2.0 sentry-sdk==2.15.0 setproctitle==1.3.3 setuptools==75.1.0 shellingham==1.5.4 six==1.16.0 smmap==5.0.1 sniffio==1.3.1 sortedcontainers==2.4.0 sounddevice==0.5.0 soundfile==0.12.1 soupsieve==2.6 sqlalchemy==2.0.35 stack-data==0.6.3 streamlit==1.34.0 sympy==1.12 tenacity==8.3.0 tensorboard==2.18.0 tensorboard-data-server==0.7.2 terminado==0.18.1 threadpoolctl==3.5.0 tiktoken==0.7.0 timm==1.0.9 tinycss2==1.3.0 tokenizers==0.19.1 tokenmonster==1.1.12 toml==0.10.2 tomlkit==0.13.2 toolz==0.12.1 torch==2.4.1 torchdiffeq==0.2.4 torchfix==0.6.0 torchmetrics==1.4.2 torchsummary==1.5.1 torchvision==0.19.1 tornado==6.4 tox==4.15.1 tqdm==4.66.5 traitlets==5.14.3 transformers==4.44.2 tree-sitter==0.21.3 tree-sitter-languages==1.10.2 triton==3.0.0 trove-classifiers==2024.9.12 types-python-dateutil==2.9.0.20240906 typing==3.7.4.3 typing-inspect==0.9.0 typing_extensions==4.12.2 tzdata==2024.1 ultralytics-thop==2.0.8 uri-template==1.3.0 uritemplate==4.1.1 urllib3==2.2.3 vector-quantize-pytorch==1.12.0 virtualenv==20.26.6 wandb==0.18.3 watchdog==5.0.3 wcwidth==0.2.13 webcolors==24.8.0 webencodings==0.5.1 websocket-client==1.8.0 werkzeug==3.0.4 wget==3.2 xxhash==3.5.0 yarl==1.11.1 youtube-transcript-api==0.4.1 zetascale==0.9.1 zipp==3.20.2</file><file name="benchmark.py"># gp2_arc/benchmark.py import sys import os # add project root directory python path sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) import torch._dynamo import csv import uuid datetime import datetime import os import torch torch.utils.data import dataloader import arckit gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import modelconfig import time torch.amp import autocast import psutil import logging import argparse import statistics import numpy np scipy import stats # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) # dynamically adjustable baseline values cpu, gpu, mps baselines = { 'cpu': {'total_time': 1.6391, 'grids_per_second': 199.27}, 'cuda': {'total_time': 0.0481, 'grids_per_second': 13774.98}, 'mps': {'total_time': 0.0481, 'grids_per_second': 13774.98} # updated baselines mps } def benchmark_model(model, dataset, batch_size=1, num_batches=1, num_runs=1, device_type='cpu', precision='medium', model_checkpoint=none): print(f"starting benchmark_model parameters: batch_size={batch_size}, num_batches={num_batches}, device_type={device_type}, precision={precision}, model_checkpoint={model_checkpoint}") device_type ['cpu', 'cuda', 'mps']: raise valueerror("invalid device type") len(dataset) == 0: raise valueerror("dataset empty") checkpoint_used = false checkpoint_info = {} model_checkpoint: checkpoint = torch.load(model_checkpoint) 'state_dict' checkpoint: state_dict = checkpoint['state_dict'] else: state_dict = checkpoint model.load_state_dict(state_dict, strict=false) model.to(device_type) model.eval() checkpoint_used = true checkpoint_info = { 'state_dict_keys': list(state_dict.keys()) } run_id = str(uuid.uuid4()) current_time = datetime.now().strftime("%y-%m-%d %h:%m:%s") practical_threshold = 20.0 # define threshold practical significance total_time_runs = [] grids_per_second_runs = [] cpu_usages = [] memory_usages = [] run_results = [] # initialize run_results store run's data gpu_usages = [] # initialize gpu_usages store gpu utilization data # set float32 matmul precision torch.set_float32_matmul_precision(precision) # select device based argument (including support mps) device = torch.device("cuda" device_type == "cuda" torch.cuda.is_available() else "mps" device_type == "mps" torch.backends.mps.is_available() else "cpu") model = model.to(device) torch._dynamo.config.suppress_errors = true device.type == "cpu": compiled_model = model # use model directly cpu else: try: device.type != "mps": compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=true) else: compiled_model = model # use model directly mps except importerror e: logger.warning(f"compilation failed error: {e}. falling back eager execution.") compiled_model = model try: dataloader = dataloader(dataset, batch_size=batch_size, collate_fn=arcdataset.collate_fn) total_time = 0.0 total_grids = 0 i, batch enumerate(dataloader): &gt;= num_batches: break print(f"processing batch {i+1}/{num_batches}") logger.debug(f"batch content unpacking: {batch}") len(batch) != 3: raise valueerror(f"unexpected batch format. expected 3 items, got {len(batch)}") inputs, outputs, task_ids = batch print(f"inputs type: {type(inputs)}") hasattr(inputs, 'shape'): print(f"inputs shape: {inputs.shape}") else: print("inputs shape: n/a") print(f"outputs type: {type(outputs)}, shape: {outputs.shape torch.is_tensor(outputs) else 'n/a'}") print(f"task ids: {task_ids}") inputs none isinstance(inputs, torch.tensor): raise valueerror(f"expected inputs torch.tensor, got {type(inputs)}") inputs.numel() == 0: raise valueerror("inputs tensor empty") print(f"inputs shape: {inputs.shape}, outputs shape: {outputs.shape}, task ids: {task_ids}") inputs.dim() == 2: # inputs 2d (batch_size, sequence_length), reshape 4d height = width = int(inputs.size(1)**0.5) inputs = inputs.view(inputs.size(0), 1, height, width) elif inputs.dim() == 3: # inputs 3d (batch_size, height, width), add channel dimension inputs = inputs.unsqueeze(1) elif inputs.dim() != 4: raise valueerror(f"unexpected input dimensions: {inputs.dim()}. expected 2, 3, 4 dimensions.") attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32) inputs, attention_mask = inputs.to(device), attention_mask.to(device) # log system load system state processing batch cpu_percent = psutil.cpu_percent(interval=none) memory_info = psutil.virtual_memory() cpu_usages.append(cpu_percent) memory_usages.append(memory_info.percent) device.type == 'cuda': gpu_utilization = torch.cuda.utilization(device.index) gpu_usages.append(gpu_utilization) logger.info(f"batch {i+1}: cpu usage: {cpu_percent}%, memory usage: {memory_info.percent}%, gpu utilization: {gpu_utilization}%") else: logger.info(f"batch {i+1}: cpu usage: {cpu_percent}%, memory usage: {memory_info.percent}%") # measure time taken process batch start_time = time.time() torch.cuda.is_available(): torch.cuda.synchronize() logger.debug("invoking model inputs attention_mask") torch.no_grad(): device.type == 'cuda': autocast(device_type=device.type, dtype=torch.float16): compiled_model(inputs, attention_mask) else: compiled_model(inputs, attention_mask) torch.cuda.is_available(): torch.cuda.synchronize() end_time = time.time() batch_time = end_time - start_time print(f"batch time: {batch_time}") batch_time &lt;= 0: print(f"warning: invalid batch time: {batch_time}. skipping batch.") continue total_time += batch_time total_grids += len(inputs) except exception e: logger.error(f"an error occurred benchmarking: {e}") raise print(f"benchmark completed. total time: {total_time}, total grids: {total_grids}") # calculate average standard deviation runs num_runs = len(total_time_runs) avg_total_time = np.mean(total_time_runs) std_total_time = np.std(total_time_runs) avg_grids_per_second = np.mean(grids_per_second_runs) std_grids_per_second = np.std(grids_per_second_runs) total_time &gt; 0: grids_per_second = total_grids / total_time else: grids_per_second = 0.0 # avoid division zero logger.warning("total time zero. setting grids_per_second 0.0 avoid division zero.") logger.info(f"total time: {total_time:.4f} seconds, grids per second: {grids_per_second:.2f}") # store results run run_results.append({ 'run_id': run_id, 'datetime': current_time, 'total_time': total_time, 'grids_per_second': grids_per_second, 'cpu_usage': np.mean(cpu_usages), 'memory_usage': np.mean(memory_usages), 'gpu_usage': np.mean(gpu_usages) gpu_usages else none, 'batch_size': batch_size, 'num_batches': num_batches, 'device': device.type, 'n_embd': model.config.n_embd, 'n_head': model.config.n_head, 'n_layer': model.config.n_layer, 'precision': precision, # add precision 'checkpoint_used': checkpoint_used, 'checkpoint_info': checkpoint_info }) total_time_runs.append(total_time) grids_per_second_runs.append(grids_per_second) total_time &lt;= 0 total_grids &lt;= 0: logger.warning(f"error: invalid total time ({total_time}) total grids ({total_grids}). check benchmark implementation.") return 0.0, 0.0 # return sensible defaults instead infinity avg_total_time = total_time avg_grids_per_second = total_grids / total_time total_time &gt; 0 else 0.0 logger.info(f"total time: {avg_total_time:.4f} seconds, grids per second: {avg_grids_per_second:.2f}") # perform statistical analysis (confidence intervals, effect size, etc.) confidence_level = 0.95 z_score = stats.norm.ppf((1 + confidence_level) / 2) ci_total_time = z_score * (std_total_time / np.sqrt(num_runs)) ci_grids_per_second = z_score * (std_grids_per_second / np.sqrt(num_runs)) effect_size_time = (avg_total_time - baselines[device.type]['total_time']) / std_total_time effect_size_grids = (avg_grids_per_second - baselines[device.type]['grids_per_second']) / std_grids_per_second # calculate improvements regressions based averages time_improvement = baselines[device.type]['total_time'] - avg_total_time time_improvement_percent = (time_improvement / baselines[device.type]['total_time']) * 100 time_regression = avg_total_time - baselines[device.type]['total_time'] time_regression_percent = (time_regression / baselines[device.type]['total_time']) * 100 grids_per_second_improvement = avg_grids_per_second - baselines[device.type]['grids_per_second'] grids_per_second_improvement_percent = (grids_per_second_improvement / baselines[device.type]['grids_per_second']) * 100 grids_per_second_regression = baselines[device.type]['grids_per_second'] - avg_grids_per_second grids_per_second_regression_percent = (grids_per_second_regression / baselines[device.type]['grids_per_second']) * 100 # determine improvement improvement_time = avg_total_time &lt; baselines[device.type]['total_time'] improvement_grids = avg_grids_per_second &gt; baselines[device.type]['grids_per_second'] # log improvements regressions based averages avg_total_time &lt; baselines[device.type]['total_time']: logger.info(f"improvement average total time: -{time_improvement:.4f} seconds ({time_improvement_percent:.2f}%)") else: logger.info(f"regression average total time: +{time_regression:.4f} seconds ({time_regression_percent:.2f}%)") avg_grids_per_second &gt; baselines[device.type]['grids_per_second']: logger.info(f"improvement average grids per second: +{grids_per_second_improvement:.2f} ({grids_per_second_improvement_percent:.2f}%)") else: logger.info(f"regression average grids per second: -{grids_per_second_regression:.2f} ({grids_per_second_regression_percent:.2f}%)") # update practical significance checks practical_significance_time = time_improvement_percent &gt;= practical_threshold practical_significance_grids = grids_per_second_improvement_percent &gt;= practical_threshold # log practical significance improvement_time: practical_significance_time: logger.info("the improvement average total time practically significant.") else: logger.info("the improvement average total time practically significant.") else: practical_significance_time: logger.info("the regression average total time practically significant.") else: logger.info("the regression average total time practically significant.") improvement_grids: practical_significance_grids: logger.info("the improvement average grids per second practically significant.") else: logger.info("the improvement average grids per second practically significant.") else: practical_significance_grids: logger.info("the regression average grids per second practically significant.") else: logger.info("the regression average grids per second practically significant.") # perform one-sample t-test t_stat_time, p_value_time = stats.ttest_1samp(total_time_runs, baselines[device.type]['total_time']) t_stat_grids, p_value_grids = stats.ttest_1samp(grids_per_second_runs, baselines[device.type]['grids_per_second']) logger.info(f"t-test total time: t-statistic = {t_stat_time:.4f}, p-value = {p_value_time:.4f}") logger.info(f"t-test grids per second: t-statistic = {t_stat_grids:.4f}, p-value = {p_value_grids:.4f}") # log results including confidence intervals logger.info(f"run summary:") logger.info(f" avg total time: {avg_total_time:.4f}s (ci 95%: {ci_total_time:.4f}s)") logger.info(f" avg grids per second: {avg_grids_per_second:.2f} (ci 95%: {ci_grids_per_second:.2f})") logger.info(f" effect size (total time): {effect_size_time:.4f}, effect size (grids per second): {effect_size_grids:.4f}") # determine improvement improvement_time = avg_total_time &lt; baselines[device.type]['total_time'] improvement_grids = avg_grids_per_second &gt; baselines[device.type]['grids_per_second'] csv_file_path = 'benchmark_results.csv' file_exists = os.path.isfile(csv_file_path) open(csv_file_path, 'a', newline='') csvfile: fieldnames = [ 'run_id', 'datetime', 'run', 'total_time', 'grids_per_second', 'cpu_usage', 'memory_usage', 'batch_size', 'num_batches', 'device', 'n_embd', 'n_head', 'n_layer', 'gpu_usage', 'precision', 'checkpoint_used', 'checkpoint_info' ] writer = csv.dictwriter(csvfile, fieldnames=fieldnames) file_exists: writer.writeheader() result run_results: writer.writerow(result) # write statistical summary csv stats_csv_file_path = 'benchmark_statistics.csv' stats_file_exists = os.path.isfile(stats_csv_file_path) open(stats_csv_file_path, 'a', newline='') csvfile: fieldnames = [ 'run_id', 'datetime', 'avg_total_time', 'std_total_time', 'ci_total_time', 'avg_grids_per_second', 'std_grids_per_second', 'ci_grids_per_second', 'effect_size_time', 'effect_size_grids', 'percent_change_time', 'percent_change_grids', 't_stat_time', 'p_value_time', 't_stat_grids', 'p_value_grids', 'improvement_time', 'improvement_grids', 'practical_significance_time', 'practical_significance_grids', 'precision', 'checkpoint_used', 'checkpoint_info' ] writer = csv.dictwriter(csvfile, fieldnames=fieldnames) stats_file_exists: writer.writeheader() writer.writerow({ 'run_id': run_id, 'datetime': current_time, 'avg_total_time': avg_total_time, 'std_total_time': std_total_time, 'ci_total_time': ci_total_time, 'avg_grids_per_second': avg_grids_per_second, 'std_grids_per_second': std_grids_per_second, 'ci_grids_per_second': ci_grids_per_second, 'effect_size_time': effect_size_time, 'effect_size_grids': effect_size_grids, 'percent_change_time': time_improvement_percent improvement_time else time_regression_percent, 'percent_change_grids': grids_per_second_improvement_percent improvement_grids else grids_per_second_regression_percent, 't_stat_time': t_stat_time, 'p_value_time': p_value_time, 't_stat_grids': t_stat_grids, 'p_value_grids': p_value_grids, 'improvement_time': improvement_time, 'improvement_grids': improvement_grids, 'practical_significance_time': practical_significance_time, 'practical_significance_grids': practical_significance_grids, 'precision': precision, # add precision 'checkpoint_used': checkpoint_used, 'checkpoint_info': checkpoint_info }) print(f"benchmark completed. final results - avg_time: {avg_total_time}, avg_grids: {avg_grids_per_second}") return avg_total_time, avg_grids_per_second def main(args): print(f"starting main function args: {args}") # set float32 matmul precision torch.set_float32_matmul_precision(args.precision) train_set, _ = arckit.load_data() full_dataset = arcdataset(train_set, is_test=false) # create model configuration model_config = modelconfig( n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer, mamba_ratio=args.mamba_ratio, d_state=args.d_state, d_conv=args.d_conv ) model = gpt2arc(model_config, num_classes=args.num_classes) # run benchmark different configurations run_num range(args.num_full_runs): logger.info(f"starting full benchmark run {run_num + 1}/{args.num_full_runs}") avg_time, avg_grids = benchmark_model( model, full_dataset, batch_size=args.batch_size, num_batches=args.num_batches, num_runs=args.num_runs, device_type=args.device, precision=args.precision, model_checkpoint=args.model_checkpoint ) logger.info(f"full run {run_num + 1} - avg time: {avg_time:.4f}s, avg grids per second: {avg_grids:.2f}") __name__ == "__main__": parser = argparse.argumentparser(description="benchmark gpt2arc model.") parser.add_argument('--model_checkpoint', type=str, help='path model checkpoint') parser.add_argument('--num-runs', type=int, default=20, help='number runs configuration') parser.add_argument('--num-full-runs', type=int, default=1, help='number full configurations run') parser.add_argument('--batch-size', type=int, default=32, help='batch size run') parser.add_argument('--num-batches', type=int, default=10, help='number batches per run') parser.add_argument('--n-embd', type=int, default=64, help='number embeddings model') parser.add_argument('--n-head', type=int, default=2, help='number attention heads') parser.add_argument('--n-layer', type=int, default=1, help='number layers') parser.add_argument('--mamba-ratio', type=int, default=7, help='number mamba layers per transformer layer') parser.add_argument('--d-state', type=int, default=16, help='mamba state dimension') parser.add_argument('--d-conv', type=int, default=4, help='mamba convolution dimension') parser.add_argument('--device', choices=['cpu', 'cuda', 'mps'], default='cpu', help='device run benchmark (cpu, cuda, mps)') parser.add_argument('--precision', choices=['highest', 'high', 'medium'], default='highest', help='precision level float32 matrix multiplications') parser.add_argument('--num-classes', type=int, default=10, help='number classes model') args = parser.parse_args() main(args)</file><file name="README.md"># gpt-2 arc neural reasoning model project implements neural reasoning model based gpt-2 architecture solve tasks abstraction reasoning corpus (arc) challenge. ## features - **data handling**: utilizes custom `arcdataset` class handling preprocessing arc data. - **model architecture**: implements `gpt2arc` model leveraging pre-trained gpt-2 architecture. - **training**: includes `train.py` script training model using pytorch lightning, support logging checkpointing. - **testing**: comprehensive test suite using `pytest` ensure model data integrity. ## installation clone repository install required packages: ```bash git clone https://github.com/yourusername/arc-neural-reasoning-model.git cd arc-neural-reasoning-model pip install -e . ``` development, install extra dependencies: ```bash pip install -e ".[dev]" ``` ## usage ### training model train model, use following command: ``` python src/train.py --train_data path/to/train_data --val_data path/to/val_data --batch_size 32 --learning_rate 1e-4 --max_epochs 10 --use_gpu ``` adjust parameters needed. trained model checkpoints saved `checkpoints` directory. ### evaluating model evaluate trained model test set, use following command: ``` python src/evaluate.py --test_data path/to/test_data --model_checkpoint path/to/model_checkpoint.ckpt --batch_size 32 ``` output evaluation metrics model test dataset. ## running tests run tests, use following command: ``` pytest -v ``` run tests display results, including test coverage. ## contributing [add contribution guidelines here] ## license project licensed mit license - see [license](license) file details.</file><file name="setup.py">setuptools import setup setup()</file><file name="src/evaluate.py"># gpt2_arc/src/evaluate.py import sys import sys import os import json import typing import dict, any, list import argparse import pytorch_lightning pl import os import torch import wandb import numpy np datetime import datetime pytorch_lightning.utilities.model_summary import modelsummary torchsummary import summary pytorch_lightning.utilities.model_summary import modelsummary # define base directory arc-neural-reasoning-model arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) # add root directory project pythonpath project_root = arc_model_dir sys.path.insert(0, project_root) gpt2_arc.src.config import config, modelconfig, trainingconfig, evaluationconfig import arckit import logging gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.utils.training_helpers import get_num_workers gpt2_arc.src.utils.helpers import differential_pixel_accuracy # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) def evaluate(model, test_dataset, config, batch_size=32, args=none): trainer = arctrainer( model=model, train_dataset=none, val_dataset=none, config=config, args=args, test_dataset=test_dataset ) pl_trainer = pl.trainer(accelerator='gpu' torch.cuda.is_available() else 'cpu') results = pl_trainer.test(trainer) logger.debug(f"debug: raw results test: {results}") avg_test_loss = pl_trainer.callback_metrics.get('avg_test_loss') avg_test_accuracy = pl_trainer.callback_metrics.get('avg_test_accuracy') avg_test_diff_accuracy = pl_trainer.callback_metrics.get('avg_test_diff_accuracy') # convert tensors python floats necessary avg_test_loss none: avg_test_loss = avg_test_loss.item() avg_test_accuracy none: avg_test_accuracy = avg_test_accuracy.item() isinstance(avg_test_accuracy, torch.tensor) else avg_test_accuracy avg_test_diff_accuracy none: avg_test_diff_accuracy = avg_test_diff_accuracy.item() isinstance(avg_test_diff_accuracy, torch.tensor) else avg_test_diff_accuracy aggregated_results = { 'test_loss': avg_test_loss, 'test_accuracy': avg_test_accuracy, 'test_diff_accuracy': avg_test_diff_accuracy, } print(f"debug: logged metrics - avg test loss: {avg_test_loss}, avg test accuracy: {avg_test_accuracy}, avg diff accuracy: {avg_test_diff_accuracy}") # collect individual task metrics resultscollector individual_metrics = trainer.results_collector.get_task_specific_results() # optional: log individual_metrics debugging logger.debug(f"debug: individual metrics retrieved: {individual_metrics}") print(f"debug: individual metrics retrieved: {individual_metrics}") # compute complete task accuracy (fraction tasks perfect accuracy) num_tasks = len(individual_metrics) perfect_accuracy_threshold = config.evaluation.perfect_accuracy_threshold / 100.0 # convert percentage fraction num_complete_accuracy = 0 task_id, metrics individual_metrics.items(): test_accuracy = metrics.get('test_accuracy', 0) # determine task completely solved completely_solved = test_accuracy &gt;= perfect_accuracy_threshold metrics['completely_solved'] = completely_solved completely_solved: num_complete_accuracy += 1 complete_task_accuracy = num_complete_accuracy / num_tasks num_tasks &gt; 0 else 0.0 aggregated_results['complete_task_accuracy'] = complete_task_accuracy print(f"debug: computed complete task accuracy: {complete_task_accuracy}") return aggregated_results, individual_metrics def load_config_from_json(json_path): open(json_path, 'r') f: data = json.load(f) return data['config'] import json import def parse_model_summary(model_summary: str, model_checkpoint: str) -&gt; dict[str, any]: """ parses model summary string structured json format adds model file size. args: model_summary (str): raw model summary string. model_checkpoint (str): path model checkpoint file. returns: dict[str, any]: dictionary containing 'header', 'layers', 'summary', 'filesize'. """ # split model summary lines lines = model_summary.strip().split('\n') len(lines) &lt; 2: print("model summary contain sufficient lines.") return {"layers": [], "summary": {}, "header": []} # find header line separator line header_line = none separator_line = none idx, line enumerate(lines): 'name' line 'type' line: header_line = line separator_line = lines[idx + 1] idx + 1 &lt; len(lines) else none data_start_idx = idx + 2 # data starts header separator break header_line none separator_line none: print("header separator line found.") return {"layers": [], "summary": {}, "header": []} # use positions '|' determine column boundaries positions = [match.start() match re.finditer(r'\|', header_line)] # function parse line columns based '|' positions def parse_line(line: str, positions: list[int]) -&gt; list[str]: cols = [] range(len(positions) - 1): start = positions[i] + 1 end = positions[i + 1] col = line[start:end].strip() cols.append(col) # last column last '|' start = positions[-1] + 1 col = line[start:].strip() cols.append(col) return cols # get header columns header_columns = parse_line(header_line, positions) # initialize list hold layer details layers = [] # iterate data lines non-data line encountered line lines[data_start_idx:]: # stop reach separator line (line dashes) set(line.strip()) == {'-'}: # summary section starts line summary_start_idx = lines.index(line) + 1 break # skip empty lines line.strip(): continue # parse line columns cols = parse_line(line, positions) # ensure number columns matches header len(cols) != len(header_columns): continue # skip lines match expected format # create dictionary current layer layer_dict = dict(zip(header_columns, cols)) layers.append(layer_dict) else: # break loop, set summary_start_idx end summary_start_idx = len(lines) # extract summary metrics remaining lines summary_lines = lines[summary_start_idx:] summary_dict = {} line summary_lines: line = line.strip() line: continue # match lines like "195 trainable params" match = re.match(r"(\d+\.?\d*)\s+(.+)", line) match: value, key = match.groups() # convert numeric values float int appropriate try: '.' value: value = float(value) else: value = int(value) except valueerror: pass # keep string conversion fails summary_dict[key.strip()] = value else: # handle lines may key value reverse order match = re.match(r"(.+)\s+(\d+\.?\d*)", line) match: key, value = match.groups() try: '.' value: value = float(value) else: value = int(value) except valueerror: pass summary_dict[key.strip()] = value # get model file size try: filesize = os.path.getsize(model_checkpoint) summary_dict["model file size (bytes)"] = filesize except exception e: print(f"error getting model file size: {e}") # combine header, layers, summary final output output = { "header": header_columns, "layers": layers, "summary": summary_dict } return output def save_results(results, individual_metrics, output_dir, model_name, model_summary, model_checkpoint): """ saves evaluation results along parsed model summary json file. args: results (dict[str, any]): aggregate evaluation metrics. individual_metrics (dict[str, dict[str, any]]): per-task evaluation metrics. output_dir (str): directory save results. model_name (str): name model file naming. model_summary (str): raw model summary string. returns: str: path saved json file. """ timestamp = datetime.now().strftime("%y%m%d_%h%m%s") filename = f"{model_name}_eval_results_{timestamp}.json" output_path = os.path.join(output_dir, filename) # parse model_summary string json parsed_model_summary = parse_model_summary(model_summary, model_checkpoint) data_to_save = { "aggregate_results": results, "individual_metrics": {task_id: metrics task_id, metrics individual_metrics.items()}, "model_summary": parsed_model_summary # use parsed json } logger.debug(f"debug: data saved: {data_to_save}") os.makedirs(output_dir, exist_ok=true) open(output_path, 'w') f: json.dump(data_to_save, f, indent=2) logger.info(f"results saved {output_path}") return output_path def main(args): args.use_wandb: api_key = os.getenv("wandb_api_key") api_key: wandb.login(key=api_key) wandb.init(project=args.wandb_project, name=args.wandb_run_name) else: print("warning: wandb_api_key found environment variables.") print("weights &amp; biases logging disabled.") args.use_wandb = false else: print("weights &amp; biases logging disabled.") # load test data using arckit _, test_set = arckit.load_data() test_data = arcdataset(test_set) # compute symbol frequencies test dataset symbol_freq_array = test_data.get_symbol_frequencies() symbol_freq = {str(i): float(freq) i, freq enumerate(symbol_freq_array)} checkpoint = torch.load(args.model_checkpoint, map_location='cpu') # extract convert model configuration checkpoint 'model_config' checkpoint: model_config_dict = checkpoint['model_config'] # convert dict modelconfig object model_config = modelconfig(**model_config_dict) else: logger.error("model configuration found checkpoint. please ensure checkpoint includes 'model_config'.") raise valueerror("model configuration found checkpoint. ensure training process includes modelconfigsaver callback.") # create configuration config = config( model=model_config, training=trainingconfig(), evaluation=evaluationconfig() ) # determine number classes test dataset max_label_test = max([sample[1].max().item() sample test_data]) num_classes = int(max_label_test) + 1 # ensure num_classes integer config.training.symbol_freq = symbol_freq # initialize model complete config object symbol frequencies model = gpt2arc(config, num_classes=num_classes, symbol_freq=symbol_freq) try: # remove "model." prefix state dict keys state_dict = {k.replace('model.', ''): v k, v checkpoint['state_dict'].items()} missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=false) # check 'loss_fn.weight' missing initialize necessary "loss_fn.weight" missing_keys: logger.debug("'loss_fn.weight' found state_dict. initializing default weights.") num_classes = config.training.num_classes # ensure correctly retrieved config default_weights = torch.ones(num_classes) model.loss_fn.weight = default_weights logger.debug(f"'loss_fn.weight' initialized weights: {default_weights}") # optionally, log unexpected keys debugging unexpected_keys: logger.warning(f"unexpected keys state_dict: {unexpected_keys}") # print keys model's state dictionary print("model state_dict keys:", list(model.state_dict().keys())) except exception e: logger.error(f"error loading state_dict: {e}") logger.error(f"available keys checkpoint: {list(checkpoint.keys())}") raise model.eval() # generate model summary print("debug: attempting generate model summary") try: model_summary = str(modelsummary(model, max_depth=-1)) print("debug: model summary generated successfully") except exception e: print(f"debug: error generating model summary - {str(e)}") model_summary = "error generating model summary" print("debug: model summary:") print(model_summary) device = torch.device('cuda' torch.cuda.is_available() else 'cpu') model.to(device) logger.info(f"model moved device: {device}") # define input size based model's expected input sequence_length = 100 # example value; adjust needed input_size = (1, 1, sequence_length) # adjusted match (batch_size, channels, sequence_length) logger.info(f"defined input_size summary: {input_size}") # extract model name checkpoint path sanitize model_name = os.path.basename(args.model_checkpoint).split('.')[0] # sanitize model_name contain valid characters model_name = ''.join(c c.isalnum() c '-_.' else '_' c model_name) # debugging statements logger.debug(f"sanitized model_name: {model_name}") print(f"debug: sanitized model_name: {model_name}") # verify model_name contains allowed characters allowed_chars = set("abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz0123456789-_.") invalid_chars = set(model_name) - allowed_chars invalid_chars: logger.error(f"model name contains invalid characters sanitization: {invalid_chars}") print(f"error: model name contains invalid characters sanitization: {invalid_chars}") else: logger.debug("model name contains valid characters.") print("debug: model name contains valid characters.") # create configuration config = config( model=model_config, training=trainingconfig(), evaluation=evaluationconfig() ) # evaluate model results, individual_metrics = evaluate(model, test_data, config, args.batch_size, args) logger.debug(f"debug: evaluation results: {results}") logger.debug(f"debug: individual metrics: {individual_metrics}") logger.info("evaluation results:") metric, value results.items(): metric != 'complete_task_accuracy': print(f"{metric}: {value}") args.use_wandb: wandb.log({f"eval/{metric}": value}) # print complete_task_accuracy bottom 'complete_task_accuracy' results: print(f"complete_task_accuracy: {results['complete_task_accuracy']}") args.use_wandb: wandb.log({"eval/complete_task_accuracy": results['complete_task_accuracy']}) # log individual task metrics task_id, metrics individual_metrics.items(): # ensure metrics already floats isinstance(metrics['test_accuracy'], list): metrics['test_accuracy'] = sum(metrics['test_accuracy']) / len(metrics['test_accuracy']) isinstance(metrics['test_diff_accuracy'], list): metrics['test_diff_accuracy'] = sum(metrics['test_diff_accuracy']) / len(metrics['test_diff_accuracy']) logger.info(f"task {task_id}: accuracy = {metrics['test_accuracy']:.4f}, diff accuracy = {metrics['test_diff_accuracy']:.4f}") # save results regardless wandb usage results_path = save_results(results, individual_metrics, args.output_dir, model_name, model_summary, args.model_checkpoint) args.use_wandb: # wandb artifact creation logging logger.debug(f"creating wandb artifact name: {model_name}") print(f"debug: creating wandb artifact name: {model_name}") try: artifact = wandb.artifact(name=model_name, type='evaluation') artifact.add_file(results_path) wandb.log_artifact(artifact) logger.debug("artifact created logged successfully.") print("debug: artifact created logged successfully.") except valueerror ve: logger.error(f"failed create wandb artifact: {ve}") print(f"error: failed create wandb artifact: {ve}") raise wandb.finish() __name__ == "__main__": parser = argparse.argumentparser(description="evaluate arc neural reasoning model") parser.add_argument("--model_checkpoint", type=str, required=true, help="path model checkpoint") parser.add_argument("--batch_size", type=int, default=32, help="batch size evaluation") parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="directory save evaluation results") parser.add_argument("--log-level", type=str, default="info", help="set logging level (e.g., debug, info, warning)") parser.add_argument("--wandb_project", type=str, default="arc-evaluation", help="weights &amp; biases project name") parser.add_argument("--wandb_run_name", type=str, default=none, help="weights &amp; biases run name") parser.add_argument("--use_wandb", action='store_true', help="use weights &amp; biases logging") args = parser.parse_args() # create output directory exist os.makedirs(args.output_dir, exist_ok=true) # set logging level logging.basicconfig(level=getattr(logging, args.log_level.upper(), none)) main(args)</file><file name="src/optimize_hyperparameters.py"># gpt2_arc/src/optimize_hyperparameters.py import argparse import multiprocessing import random import optuna import logging import sys import os import torch torch.utils.data import dataloader import gc import pytorch_lightning pl import numpy np pytorch_lightning.utilities.model_summary import modelsummary optuna.pruners import percentilepruner optuna.samplers import tpesampler pytorch_lightning.callbacks import earlystopping, modelcheckpoint, callback pytorch_lightning.loggers import tensorboardlogger #from gpt2_arc.src.utils.training_helpers import get_num_workers gpt2_arc.src.training.trainer import nanlosspruningcallback functools import partial # import partial gpt2_arc.src.training.train import load_dataset, load_and_split_synthetic_data gpt2_arc.src.training.train import modelconfigsaver gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.utils.results_collector import resultscollector class bestepochtrackercallback(callback): def __init__(self): super().__init__() self.best_epoch = 0 def on_validation_end(self, trainer, pl_module): current_val_loss = trainer.callback_metrics.get("val_loss") current_val_loss none: hasattr(self, 'best_val_loss') current_val_loss &lt; self.best_val_loss: self.best_val_loss = current_val_loss self.best_epoch = trainer.current_epoch logger.debug(f"new best_val_loss: {self.best_val_loss} epoch {self.best_epoch}") gpt2_arc.src.utils.model_memory_estimator import ( calculate_params, estimate_memory_usage, get_available_memory, can_fit_model ) class custompruningcallback(pl.callback): def __init__(self, trial, monitor="val_loss"): super().__init__() self.trial = trial self.monitor = monitor def on_validation_end(self, trainer, pl_module): epoch = trainer.current_epoch current_score = trainer.callback_metrics.get(self.monitor) current_score none: return self.trial.report(current_score, step=epoch) self.trial.should_prune(): raise optuna.trialpruned() # define base directory arc-neural-reasoning-model arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) # add project root python path project_root = arc_model_dir sys.path.insert(0, project_root) gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.data.arc_dataset import arcdataset import arckit gpt2_arc.src.utils.performance_metrics import calculate_mamba_efficiency def validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout): """validate hyperparameters meet necessary constraints.""" logger.debug(f"validating hyperparameters: n_embd={n_embd}, n_head={n_head}, n_layer={n_layer}, " f"mamba_ratio={mamba_ratio}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}") assert n_embd % n_head == 0, f"n_embd ({n_embd}) must divisible n_head ({n_head})" assert n_embd &gt;= n_head, f"n_embd ({n_embd}) must greater equal n_head ({n_head})" assert n_layer &gt; 0, f"n_layer ({n_layer}) must positive" assert d_state &gt; 0, f"d_state ({d_state}) must positive" assert d_conv &gt; 0, f"d_conv ({d_conv}) must positive" assert 0.0 &lt;= dropout &lt;= 1.0, f"dropout ({dropout}) must 0.0 1.0" logger.debug("hyperparameters validated successfully") return true def objective(trial, args, all_synthetic_data): model = none trainer = none arc_trainer = none logger.info(f"starting trial {trial.number}") try: # initialize config model_config = modelconfig() training_config = trainingconfig() config = config(model=model_config, training=training_config) # use pre-loaded synthetic data training train_data = all_synthetic_data['train_dataset'] args.use_synthetic_data else load_dataset(args, config, dataset_type='train', max_samples=args.max_train_samples) # load validation test data arckit val_data = load_dataset(args, config, dataset_type='val') test_data = load_dataset(args, config, dataset_type='test') # log dataset sizes logger.debug(f"trial {trial.number}: loaded {len(train_data)} training samples.") logger.debug(f"trial {trial.number}: loaded {len(val_data)} validation samples.") logger.debug(f"trial {trial.number}: loaded {len(test_data)} test samples.") fixed_hyperparams = {} # initialize config symbol_freq_dict model_config = modelconfig() training_config = trainingconfig() config = config(model=model_config, training=training_config) symbol_freq_dict = {} # create configuration model_config = modelconfig() training_config = trainingconfig() config = config(model=model_config, training=training_config) args.fast_dev_run: # disable symbol frequency balancing fast development run symbol_freq_dict = {} balance_symbols = false balancing_method = "none" logger.debug("fast_dev_run enabled. disabling symbol frequency balancing.") else: args.enable_symbol_freq: # calculate symbol frequencies args.use_synthetic_data: logger.debug("calculating symbol frequencies synthetic training set") symbol_freq = train_data.get_symbol_frequencies() else: logger.debug("calculating symbol frequencies arc training set") symbol_freq = train_data.get_symbol_frequencies() logger.debug(f"computed symbol frequencies: {symbol_freq}") # directly copy symbol_freq symbol_freq_dict # ensure symbol_freq_dict dictionary isinstance(symbol_freq, np.ndarray): # symbol_freq numpy array, convert dictionary symbol_freq_dict = {i: float(freq) i, freq enumerate(symbol_freq)} logger.debug("converted symbol_freq numpy array dictionary.") elif isinstance(symbol_freq, dict): symbol_freq_dict = symbol_freq.copy() logger.debug("copied symbol_freq dictionary.") else: raise typeerror(f"unexpected type symbol_freq: {type(symbol_freq)}. expected dict np.ndarray.") # assert symbol_freq_dict indeed dictionary assert isinstance(symbol_freq_dict, dict), f"symbol_freq_dict must dict, got {type(symbol_freq_dict)}." # remove padding symbol symbol_freq_dict pad_symbol_idx = config.training.pad_symbol_idx symbol_freq_dict.pop(pad_symbol_idx, none) logger.debug(f"removed pad_symbol_idx ({pad_symbol_idx}) symbol_freq_dict. new length: {len(symbol_freq_dict)}") # debugging: check keys types logger.debug(f"keys symbol_freq_dict popping padding symbol: {list(symbol_freq_dict.keys())}") logger.debug(f"types keys symbol_freq_dict: {set(type(k) k symbol_freq_dict.keys())}") # ensure length symbol_freq_dict matches num_classes - 1 assert len(symbol_freq_dict) == config.training.num_classes - 1, ( f"length symbol_freq_dict ({len(symbol_freq_dict)}) match num_classes minus padding ({config.training.num_classes - 1})." ) balance_symbols = true balancing_method = "weighting" else: symbol_freq_dict = {} logger.debug("symbol frequency calculation disabled. using empty symbol_freq_dict.") balance_symbols = false balancing_method = "none" include_pad_in_loss = args.include_pad_in_loss torch.set_float32_matmul_precision(args.matmul_precision) logger.info(f"trial {trial.number}: set float32 matmul precision to: {args.matmul_precision}") args.model_checkpoint: # suggest hyperparameters fixed checkpoint # suggest n_head exponent calculate n_head n_head_exp = trial.suggest_int("n_head_exp", args.n_head_exp_min, args.n_head_exp_max) n_head = 2 ** n_head_exp logger.debug(f"suggested n_head: {n_head} (2^{n_head_exp})") # suggest n_embd multiple n_head ensure power 2 n_embd_multiplier = trial.suggest_int("n_embd_multiplier", args.n_embd_multiplier_min, args.n_embd_multiplier_max) n_embd = n_head * n_embd_multiplier n_embd = 2 ** int(np.log2(n_embd)) logger.debug(f"adjusted n_embd: {n_embd}") # suggest n_layer n_layer = trial.suggest_int("n_layer", args.n_layer_min, args.n_layer_max) logger.debug(f"suggested n_layer: {n_layer}") # suggest mamba-specific hyperparameters mamba_ratio = trial.suggest_float("mamba_ratio", args.mamba_ratio_min, args.mamba_ratio_max, step=args.mamba_ratio_step) d_state = trial.suggest_int("d_state", args.d_state_min, args.d_state_max) d_conv = trial.suggest_int("d_conv_min", args.d_conv_min, args.d_conv_max) # suggest dropout rate dropout = trial.suggest_float("dropout", args.dropout_min, args.dropout_max, step=args.dropout_step) mamba_depth = trial.suggest_int("mamba_depth", args.mamba_depth_min, args.mamba_depth_max) logger.debug(f"suggested mamba_depth: {mamba_depth}") mamba_expand = trial.suggest_int("mamba_expand", args.mamba_expand_min, args.mamba_expand_max) logger.debug(f"suggested mamba_expand: {mamba_expand}") logger.debug(f"using suggested hyperparameters: n_head={n_head}, n_embd={n_embd}, " f"n_layer={n_layer}, mamba_ratio={mamba_ratio}, d_state={d_state}, " f"d_conv={d_conv}, dropout={dropout}, mamba_depth={mamba_depth}, " f"mamba_expand={mamba_expand}") # ensure config uses fixed hyperparameters config = config(model=model_config, training=training_config) # use grokfast based command line argument use_grokfast = args.use_grokfast use_grokfast: # suggest grokfast type based command-line choices grokfast_type = trial.suggest_categorical("grokfast_type", args.grokfast_type_choices) # suggest grokfast alpha within specified range grokfast_alpha = trial.suggest_float("grokfast_alpha", args.grokfast_alpha_min, args.grokfast_alpha_max) # suggest grokfast lambda within specified range grokfast_lamb = trial.suggest_float("grokfast_lamb", args.grokfast_lamb_min, args.grokfast_lamb_max) # using 'ma', suggest window_size within specified range grokfast_type == "ma": grokfast_window_size = trial.suggest_int("grokfast_window_size", args.grokfast_window_size_min, args.grokfast_window_size_max) else: grokfast_window_size = none else: grokfast_type = none grokfast_alpha = none grokfast_lamb = none grokfast_window_size = none batch_size = trial.suggest_int("batch_size", args.batch_size_min, args.batch_size_max) learning_rate = trial.suggest_float("learning_rate", args.learning_rate_min, args.learning_rate_max, log=true) max_epochs = trial.suggest_int("max_epochs", args.max_epochs_min, args.max_epochs_max) # checkpoint used, set fixed values suggest architecture-related hyperparameters args.model_checkpoint: n_head = model_config.n_head n_embd = model_config.n_embd n_layer = model_config.n_layer mamba_ratio = model_config.mamba_ratio d_state = model_config.d_state d_conv = model_config.d_conv dropout = model_config.dropout mamba_depth = model_config.mamba_depth mamba_expand = model_config.mamba_expand # validate hyperparameters validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout) # check model fit memory # adjust total number layers include mamba layers total_mamba_layers = int(n_layer * mamba_ratio) total_layers = n_layer + total_mamba_layers # recalculate total parameters based total_layers total_params = calculate_params( n_layers=total_layers, n_heads=n_head, d_model=n_embd, mamba_ratio=mamba_ratio, d_state=d_state, d_conv=d_conv, mamba_depth=mamba_depth, mamba_expand=mamba_expand ) # improve memory estimation considering additional factors like optimizer state activation memory safety_margin = 0.1 # 10% safety margin estimated_memory = estimate_memory_usage( total_params=total_params, batch_size=batch_size, height=30, # adjust necessary based data width=30, # adjust necessary d_model=n_embd ) available_memory = get_available_memory() estimated_memory *= (1 + safety_margin) logger.debug(f"trial {trial.number}: estimated memory usage: {estimated_memory:.2f} gb") logger.debug(f"trial {trial.number}: available memory: {available_memory:.2f} gb") # prune trial estimated memory exceeds 80% available memory can_fit_model(estimated_memory, available_memory * 0.8): logger.warning(f"trial {trial.number}: model large available memory. skipping.") raise optuna.exceptions.trialpruned() logger.debug(f"suggested dropout rate: {dropout}") args.model_checkpoint: # use fixed hyperparameters checkpoint model_config = modelconfig(**fixed_hyperparams) training_config = trainingconfig( num_classes=config.model.num_classes, batch_size=batch_size, learning_rate=learning_rate, max_epochs=max_epochs, use_grokfast=use_grokfast, grokfast_type=grokfast_type, grokfast_alpha=grokfast_alpha, grokfast_lamb=grokfast_lamb, grokfast_window_size=grokfast_window_size, include_pad_in_loss=include_pad_in_loss, symbol_freq=symbol_freq_dict, balance_symbols=balance_symbols, balancing_method="weighting" balance_symbols else "none" ) else: # use suggested hyperparameters model_config = modelconfig( n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout, mamba_ratio=mamba_ratio, d_state=d_state, d_conv=d_conv, mamba_depth=mamba_depth, mamba_expand=mamba_expand ) training_config = trainingconfig( batch_size=batch_size, learning_rate=learning_rate, max_epochs=max_epochs, use_grokfast=use_grokfast, grokfast_type=grokfast_type, grokfast_alpha=grokfast_alpha, grokfast_lamb=grokfast_lamb, grokfast_window_size=grokfast_window_size, include_pad_in_loss=include_pad_in_loss, symbol_freq=symbol_freq_dict, balance_symbols=balance_symbols, balancing_method=balancing_method, num_workers=args.num_workers args.num_workers none else multiprocessing.cpu_count(), prefetch_factor=args.prefetch_factor, persistent_workers=not args.no_persistent_workers, pin_memory=not args.no_pin_memory ) config = config(model=model_config, training=training_config) config.estimated_memory = estimated_memory config.available_memory = available_memory logger.debug(f"suggested mamba parameters - mamba_ratio: {mamba_ratio}, d_state: {d_state}, d_conv: {d_conv}") trial.set_user_attr("mamba_ratio", mamba_ratio) trial.set_user_attr("d_state", d_state) trial.set_user_attr("d_conv", d_conv) trial.set_user_attr("mamba_depth", mamba_depth) trial.set_user_attr("mamba_expand", mamba_expand) logger.debug(f"full config: {config}") # instantiate modelconfigsaver callback current config model_config_saver = modelconfigsaver(config) # calculate symbol frequencies args.use_synthetic_data: logger.debug("calculating symbol frequencies synthetic training set") symbol_freq = train_data.get_symbol_frequencies() else: logger.debug("calculating symbol frequencies arc training set") symbol_freq = train_data.get_symbol_frequencies() logger.debug(f"computed symbol frequencies: {symbol_freq}") # create model trainer logger.debug("creating model trainer") num_classes = config.training.num_classes # instantiate gpt2arc model constructed config args.model_checkpoint: # load checkpoint logger.info(f"loading model checkpoint: {args.model_checkpoint}") checkpoint = torch.load(args.model_checkpoint, map_location="cpu") # extract model configuration checkpoint 'model_config' checkpoint: model_config_dict = checkpoint['model_config'] model_config = modelconfig(**model_config_dict) logger.debug("extracted model configuration checkpoint.") else: logger.error("model configuration found checkpoint. cannot proceed.") raise valueerror("model configuration found checkpoint.") # set hyperparameters extracted model_config n_head = model_config.n_head n_embd = model_config.n_embd n_layer = model_config.n_layer mamba_ratio = model_config.mamba_ratio d_state = model_config.d_state d_conv = model_config.d_conv dropout = model_config.dropout mamba_depth = model_config.mamba_depth mamba_expand = model_config.mamba_expand # validate hyperparameters validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout) # reconstruct config extracted model_config training_config config = config(model=model_config, training=training_config) # instantiate model exact configuration used training model = gpt2arc(config=config, num_classes=model_config.num_classes, symbol_freq=symbol_freq_dict, pad_symbol_idx=config.training.pad_symbol_idx) # load state_dict checkpoint 'state_dict' checkpoint: # remove "model." prefix state dict keys state_dict = {k.replace('model.', ''): v k, v checkpoint['state_dict'].items()} else: state_dict = checkpoint # adjust based saved model # load state_dict model strict=false try: missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=false) logger.debug(f"successfully loaded state_dict checkpoint: {args.model_checkpoint}") missing_keys: logger.warning(f"missing keys loading state_dict: {missing_keys}") unexpected_keys: logger.warning(f"unexpected keys loading state_dict: {unexpected_keys}") # handle specific missing keys necessary "loss_fn.weight" missing_keys: logger.debug("'loss_fn.weight' found state_dict. initializing default weights.") default_weights = torch.ones(config.model.num_classes) model.loss_fn.weight = default_weights logger.debug(f"'loss_fn.weight' initialized weights: {default_weights}") except runtimeerror e: logger.error(f"error loading state_dict: {e}") raise e else: model = gpt2arc(config=config, num_classes=num_classes, symbol_freq=symbol_freq_dict) # generate model summary print("debug: attempting generate model summary") try: model_summary = str(modelsummary(model, max_depth=-1)) print("debug: model summary generated successfully") except exception e: print(f"debug: error generating model summary - {str(e)}") model_summary = "error generating model summary" # save model summary trial user attributes print("debug: attempting save model summary trial user attributes") try: trial.set_user_attr("model_summary", model_summary) print("debug: model summary saved trial user attributes") except exception e: print(f"debug: error saving model summary trial - {str(e)}") print("debug: model summary:") print(model_summary) # calculate mamba efficiency metrics device = torch.device('cuda' torch.cuda.is_available() else 'cpu') logger.debug("calculating mamba efficiency metrics") sample_input = torch.randn(1, 1, 6, 6).to(device) model.to(device) mamba_metrics = calculate_mamba_efficiency(model, sample_input) key, value mamba_metrics.items(): trial.set_user_attr(key, value) logger.debug(f"mamba metric - {key}: {value}") # initialize resultscollector results_collector = resultscollector(config) arc_trainer = arctrainer( model=model, train_dataset=train_data, val_dataset=val_data, config=config, args=args, # add line pass args compile_model=false, # disable model compilation results_collector=results_collector # pass resultscollector arctrainer ) # set pytorch lightning trainer custom pruning callback args.no_progress_bar: logger.info("disabling progress bar.") else: logger.info("enabling progress bar.") pruning_callback = custompruningcallback(trial, monitor="val_loss") early_stop_callback = earlystopping(monitor="val_loss", min_delta=0.00, patience=3, verbose=false, mode="min") # determine accelerator parameters based --accelerator argument args.accelerator == "tpu": accelerator = 'tpu' devices = 'xla:1' # use 'xla:8' tpu v3-8 pods strategy = 'tpu_spawn' # recommended strategy tpu elif args.accelerator == "gpu": torch.cuda.is_available(): accelerator = 'gpu' devices = 1 else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' nan_loss_pruning_callback = nanlosspruningcallback() #callbacks.append(nan_loss_pruning_callback) logger.info("nanlosspruningcallback added training callbacks.") experiment_id = f"optuna_trial_{trial.number}" tb_logger = tensorboardlogger(save_dir="runs", name=f"experiment_{experiment_id}") print(f"debug: optuna trial tensorboard logger initialized. log dir: {tb_logger.log_dir}") # extract trial number trial_num = trial.number # define task_id (assuming single task; modify needed multiple tasks) task_id = "main_task" # replace dynamic task identification necessary # define iter_num (e.g., based trial.number another tracking mechanism) iter_num = 1 # initialize 1; increment needed within optimization loop # initialize checkpoint callback descriptive filename checkpoint_callback = modelcheckpoint( dirpath=f"checkpoints/trial_{trial.number}", filename=f"{'tuning-' args.model_checkpoint else ''}step_{{step}}-val_loss_{{val_loss:.4f}}", save_top_k=3, monitor="val_loss", mode="min", ) logger.info("standard modelcheckpoint callback added training callbacks.") # initialize bestepochtrackercallback best_epoch_tracker = bestepochtrackercallback() # initialize pytorch lightning trainer checkpoint callback trainer = pl.trainer( max_epochs=config.training.max_epochs, callbacks=[pruning_callback, early_stop_callback, nan_loss_pruning_callback, checkpoint_callback, model_config_saver, best_epoch_tracker], logger=tb_logger, gradient_clip_val=1.0, # add gradient clipping val_check_interval=args.val_check_interval, # added line precision=16, # enable automatic mixed precision enable_checkpointing=true, accelerator=accelerator, devices=devices, strategy=strategy, enable_progress_bar=not args.no_progress_bar ) print("debug: trainer created optuna trial tensorboard logger") logger.debug(f"trainer created config: {trainer.state}") # ensure model train mode training model.train() logger.debug("model set train mode training") # enhanced logging: log model mode training logger.info("before training:") name, module model.named_modules(): logger.debug(f"{name}: {'train' module.training else 'eval'}") logger.debug("starting training") trainer.fit(arc_trainer) # retrieve best validation loss modelcheckpoint callback checkpoint_callback.best_model_score none: best_val_loss = checkpoint_callback.best_model_score.item() logger.info(f"trial {trial.number}: best validation loss: {best_val_loss}") else: logger.warning(f"trial {trial.number}: checkpoints saved. assigning high validation loss.") best_val_loss = float('inf') logger.info(f"trial {trial.number} completed. best validation loss: {best_val_loss}") # enhanced logging: log model mode training logger.info("after training:") name, module model.named_modules(): logger.debug(f"{name}: {'train' module.training else 'eval'}") # define dataloader test data test_loader = dataloader( test_data, batch_size=config.training.batch_size, shuffle=false, num_workers=config.training.num_workers, pin_memory=config.training.pin_memory ) # evaluate model test data logger.info("evaluating model test dataset.") test_results = trainer.test(model=arc_trainer, dataloaders=test_loader) # process test results test_results: avg_test_loss = sum(result['test_loss'] result test_results) / len(test_results) avg_test_accuracy = sum(result['test_accuracy'] result test_results) / len(test_results) avg_test_diff_accuracy = sum(result['test_diff_accuracy'] result test_results) / len(test_results) logger.info(f"test results - loss: {avg_test_loss:.4f}, accuracy: {avg_test_accuracy:.4f}, diff accuracy: {avg_test_diff_accuracy:.4f}") # update final metrics actual test results arc_trainer.results_collector.set_final_metrics({ "best_val_loss": best_val_loss, "best_epoch": trainer.current_epoch, "final_test_loss": avg_test_loss, "final_test_accuracy": avg_test_accuracy, "final_test_diff_accuracy": avg_test_diff_accuracy }) return best_val_loss except runtimeerror e: 'cuda memory' str(e): logger.error(f"trial {trial.number}: cuda memory error.") logger.error("pruning trial suggesting adjust hyperparameters.") trial.set_user_attr('failed_reason', 'cuda memory') raise optuna.exceptions.trialpruned() else: logger.error(f"trial {trial.number}: runtime error occurred: {str(e)}", exc_info=true) raise runtimeerror(f"trial {trial.number}: runtime error occurred: {str(e)}") except exception e: # improved exception handling symbol frequency issues "symbol_freq" str(e): logger.error(f"trial {trial.number}: 'symbol_freq' missing invalid. ensure calculated passed correctly.", exc_info=true) raise optuna.exceptions.trialpruned(f"trial {trial.number}: 'symbol_freq' missing invalid.") else: logger.error(f"trial {trial.number}: unexpected error occurred: {str(e)}", exc_info=true) raise optuna.exceptions.trialpruned(f"trial {trial.number}: unexpected error occurred: {str(e)}") finally: # ensure proper cleanup trials logger.debug(f"cleaning trial {trial.number}") model none: del model trainer none: del trainer arc_trainer none: del arc_trainer gc.collect() torch.cuda.empty_cache() logger.debug(f"cleanup completed trial {trial.number}") gc.collect() torch.cuda.empty_cache() functools import partial def run_optimization(n_trials=100, storage_name="sqlite:///optuna_results.db", n_jobs=-1, args=none, study_name="gpt2_arc_optimization_v2"): n_trials &lt; 10: n_startup_trials = 1 else: n_startup_trials = 5 pruner = percentilepruner( percentile=25, n_startup_trials=n_startup_trials, n_warmup_steps=2, interval_steps=1 ) sampler = tpesampler(n_startup_trials=5) # initialize configuration model_config = modelconfig() training_config = trainingconfig() config = config(model=model_config, training=training_config) all_synthetic_data = load_and_split_synthetic_data(args, config) args.use_synthetic_data else none all_synthetic_data: logger.info(f"synthetic data loaded {len(all_synthetic_data['train_dataset'])} samples.") # create partial objective function includes all_synthetic_data objective_partial = partial(objective, args=args, all_synthetic_data=all_synthetic_data) study = optuna.create_study( study_name=study_name, storage=storage_name, load_if_exists=true, direction="minimize", pruner=pruner, sampler=sampler ) logger.info(f"starting optimization {n_trials} trials using {n_jobs} parallel jobs") logger.info(f"data splitting ratios - train: 80%, validation: 10%, test: 10%") args.use_gpu: available_gpus = torch.cuda.device_count() available_gpus &gt; 1: n_jobs = max(n_jobs, available_gpus) else: n_jobs = 1 # limit 1 prevent memory issues study.optimize(objective_partial, n_trials=n_trials, n_jobs=n_jobs) # use partial function logger.info("optimization completed") study.best_trial study.best_trial.state == optuna.trial.trialstate.complete: print("debug: best trial found, attempting retrieve model summary") best_model_summary = study.best_trial.user_attrs.get("model_summary") best_model_summary: print("debug: model summary retrieved successfully") logger.info("model summary best trial:") logger.info(best_model_summary) else: print("debug: model summary found best trial") else: logger.warning("no successful trials found. please check trial configurations constraints.") study.best_trial: logger.info(f"best trial: {study.best_trial.number}") logger.info(f"best value: {study.best_trial.value}") best_trial = study.best_trial best_trial.set_user_attr("mamba_ratio", best_trial.params.get("mamba_ratio")) best_trial.set_user_attr("d_state", best_trial.params.get("d_state")) best_trial.set_user_attr("d_conv", best_trial.params.get("d_conv")) logger.info("best mamba metrics:") key ['mamba_forward_pass_time', 'mamba_params', 'mamba_params_ratio']: value = study.best_trial.user_attrs.get(key) value none: logger.info(f" {key}: {value}") logger.info("best hyperparameters:") key, value study.best_trial.params.items(): logger.info(f" {key}: {value}") else: logger.info("no trials completed successfully.") __name__ == "__main__": parser = argparse.argumentparser(description="optimize hyperparameters gpt2arc model.") parser.add_argument("--max_train_samples", type=int, default=none, help="maximum number training samples load. use none load samples.") parser.add_argument("--n_trials", type=int, default=10, help="number trials optimization.") parser.add_argument("--n_jobs", type=int, default=1, help="number parallel jobs. -1 means using available cores.") parser.add_argument("--batch_size_min", type=int, default=1, help="minimum value batch_size.") parser.add_argument("--batch_size_max", type=int, default=1, help="maximum value batch_size.") parser.add_argument( "--fast_dev_run", action="store_true", help="run fast development test." ) parser.add_argument( "--use_grokfast", action="store_true", help="enable grokfast gradient filtering." ) parser.add_argument("--storage", type=str, default="sqlite:///optuna_results.db", help="storage path optuna results.") parser.add_argument("--random_seed", type=int, default=42, help="random seed reproducibility.") parser.add_argument( "--val_check_interval", type=float, default=0.01, help=( "how often perform validation. " "if float, represents fraction epoch (e.g., 0.5 halfway epoch). " "if integer, represents number training steps." ) ) parser.add_argument( "--model_checkpoint", type=str, default=none, help="path model checkpoint resume optimization from." ) parser.add_argument( "--include_pad_in_loss", type=lambda x: (str(x).lower() ['true', '1', 't', 'y', 'yes']), default=true, help="whether include padding class loss calculation. (true/false)" ) parser.add_argument( "--study_name", type=str, default="gpt2_arc_optimization_v3", help="name optuna study." ) parser.add_argument("--train_split", type=float, default=0.8, help="proportion data use training") parser.add_argument("--val_split", type=float, default=0.1, help="proportion data use validation") parser.add_argument("--test_split", type=float, default=0.1, help="proportion data use testing") parser.add_argument("--n_embd_max", type=int, default=1, help="maximum value n_embd") parser.add_argument( "--num_workers", type=int, default=none, help="number worker threads dataloader. set, uses configuration default (total cpu count)." ) parser.add_argument( "--prefetch_factor", type=int, default=2, help="number batches prefetch per worker." ) parser.add_argument( "--no_persistent_workers", action="store_true", help="disable persistent workers dataloader." ) parser.add_argument( "--no_pin_memory", action="store_true", help="disable pin_memory dataloader." ) parser.add_argument("--n_head_min", type=int, default=1, help="minimum value n_head") parser.add_argument("--n_head_max", type=int, default=1, help="maximum value n_head") parser.add_argument("--n_head_exp_min", type=int, default=1, help="minimum exponent n_head (2^x)") parser.add_argument("--n_head_exp_max", type=int, default=1, help="maximum exponent n_head (2^x)") parser.add_argument("--n_embd_multiplier_min", type=int, default=1, help="minimum multiplier n_embd") parser.add_argument("--n_embd_multiplier_max", type=int, default=1, help="maximum multiplier n_embd") parser.add_argument("--n_layer_min", type=int, default=1, help="minimum value n_layer") parser.add_argument("--n_layer_max", type=int, default=1, help="maximum value n_layer") parser.add_argument("--learning_rate_min", type=float, default=1e-5, help="minimum value learning_rate") parser.add_argument("--learning_rate_max", type=float, default=1e-2, help="maximum value learning_rate") parser.add_argument("--max_epochs_min", type=int, default=1, help="minimum value max_epochs") parser.add_argument("--max_epochs_max", type=int, default=10, help="maximum value max_epochs") parser.add_argument("--mamba_ratio_min", type=float, default=1.0, help="minimum value mamba_ratio") parser.add_argument("--mamba_ratio_max", type=float, default=8.0, help="maximum value mamba_ratio") parser.add_argument("--mamba_ratio_step", type=float, default=0.25, help="step size mamba_ratio") parser.add_argument("--d_state_min", type=int, default=1, help="minimum value d_state") parser.add_argument("--d_state_max", type=int, default=1, help="maximum value d_state") parser.add_argument("--d_conv_min", type=int, default=1, help="minimum value d_conv") parser.add_argument("--d_conv_max", type=int, default=1, help="maximum value d_conv") parser.add_argument("--dropout_min", type=float, default=0.0, help="minimum value dropout") parser.add_argument("--mamba_depth_min", type=int, default=1, help="minimum value mamba_depth") parser.add_argument("--mamba_depth_max", type=int, default=1, help="maximum value mamba_depth") parser.add_argument("--mamba_expand_min", type=int, default=2, help="minimum value mamba_expand") parser.add_argument("--mamba_expand_max", type=int, default=2, help="maximum value mamba_expand") parser.add_argument( "--enable_symbol_freq", action="store_true", help="enable calculation symbol frequencies." ) parser.set_defaults(enable_symbol_freq=false) parser.add_argument("--dropout_max", type=float, default=0.5, help="maximum value dropout") parser.add_argument("--dropout_step", type=float, default=0.1, help="step size dropout") parser.add_argument("--use_gpu", action="store_true", help="flag indicate whether use gpu training.") parser.add_argument( "--no_progress_bar", action="store_true", help="disable progress bar training." ) parser.add_argument("--use_synthetic_data", action="store_true", help="flag indicate whether use synthetic data training.") parser.add_argument( "--matmul_precision", type=str, default="medium", choices=["highest", "high", "medium"], help="set internal precision float32 matrix multiplications optimization trials. options: 'highest', 'high', 'medium'. defaults 'medium'." ) parser.add_argument("--synthetic_data_path", type=str, default="", help="path synthetic data training.") parser.add_argument("--log_level", type=str, default="info", help="logging level (e.g., debug, info, warning, error, critical).") parser.add_argument( "--accelerator", type=str, default="gpu", choices=["cpu", "gpu", "tpu"], help="accelerator use training: 'cpu', 'gpu', 'tpu'. defaults 'gpu'." ) # grokfast parameter ranges parser.add_argument("--grokfast_alpha_min", type=float, default=0.9, help="minimum value grokfast_alpha.") parser.add_argument("--grokfast_alpha_max", type=float, default=0.99, help="maximum value grokfast_alpha.") parser.add_argument("--grokfast_lamb_min", type=float, default=1.0, help="minimum value grokfast_lamb.") parser.add_argument("--grokfast_lamb_max", type=float, default=3.0, help="maximum value grokfast_lamb.") parser.add_argument("--grokfast_window_size_min", type=int, default=50, help="minimum value grokfast_window_size.") parser.add_argument("--grokfast_window_size_max", type=int, default=200, help="maximum value grokfast_window_size.") parser.add_argument("--grokfast_type_choices", type=str, nargs='+', default=["ema", "ma"], choices=["ema", "ma"], help="list grokfast types consider tuning.") args = parser.parse_args() log_level = getattr(logging, args.log_level.upper(), logging.info) logging.basicconfig( level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', handlers=[ logging.streamhandler() ] ) logger = logging.getlogger(__name__) logger.setlevel(log_level) # log parsed arguments debugging logger.debug(f"parsed arguments: {vars(args)}") logger.setlevel(log_level) # ensure storage_name correct sqlite prefix handle relative paths import os # ensure os imported top file args.storage.startswith("sqlite:///"): os.path.isabs(args.storage): args.storage = f"sqlite:////{args.storage}" else: args.storage = f"sqlite:///{os.path.abspath(args.storage)}" logger.debug(f"optuna storage url set to: {args.storage}") # validate val_check_interval args.val_check_interval &lt;= 0: logger.error("the --val_check_interval must positive number.") sys.exit(1) # set random seeds reproducibility random.seed(args.random_seed) np.random.seed(args.random_seed) torch.manual_seed(args.random_seed) torch.cuda.is_available(): torch.cuda.manual_seed_all(args.random_seed) logger.debug(f"random seed set to: {args.random_seed}") run_optimization( n_trials=args.n_trials, storage_name=args.storage, n_jobs=args.n_jobs, args=args, study_name=args.study_name )</file><file name="src/__init__.py"># file allows src directory recognized package.</file><file name="src/checkpoint_evaluator.py">#!/usr/bin/env python3 """ checkpoint_evaluator.py script monitor directory new model checkpoints evaluate using specified evaluation script. logs maintained, resource usage monitored. usage: python checkpoint_evaluator.py \ --output_dir /path/to/output \ --arc_model_dir /path/to/arc_model \ --date_folder 2024-10-07 \ --wandb_project arc-evaluation \ [optional arguments] """ import os import sys import time import subprocess import threading import shutil import logging import argparse datetime import datetime import psutil # resource monitoring watchdog.observers import observer watchdog.events import filesystemeventhandler def parse_arguments(): parser = argparse.argumentparser(description="monitor directories model checkpoints evaluate them.") parser.add_argument('--output_dir', type=str, required=true, help='directory store logs evaluation results.') parser.add_argument('--arc_model_dir', type=str, required=true, help='directory containing arc model scripts.') parser.add_argument('--date_folder', type=str, required=true, help='date folder name (e.g., 2024-10-07) organize outputs.') parser.add_argument('--wandb_project', type=str, default='arc-evaluation', help='weights &amp; biases project name.') parser.add_argument('--batch_size', type=int, default=32, help='batch size evaluation.') parser.add_argument('--log_level', type=str, default='debug', choices=['debug', 'info', 'warning', 'error', 'critical'], help='logging level.') parser.add_argument('--resource_monitor_interval', type=int, default=60, help='interval seconds resource monitoring logs.') return parser.parse_args() def setup_logging(output_dir, log_level): os.makedirs(output_dir, exist_ok=true) logging.basicconfig( level=getattr(logging, log_level), format='%(asctime)s - %(levelname)s - %(message)s', handlers=[ logging.filehandler(os.path.join(output_dir, "checkpoint_evaluator.log")), logging.streamhandler(sys.stdout) ] ) logger = logging.getlogger("checkpointevaluator") return logger def load_evaluated_models(evaluated_models_file, logger): evaluated_models = set() os.path.exists(evaluated_models_file): try: open(evaluated_models_file, "r") f: evaluated_models.update(line.strip() line f) logger.info(f"loaded evaluated models {evaluated_models_file}") except exception e: logger.error(f"error loading evaluated models {evaluated_models_file}: {e}") else: logger.info("no previously evaluated models found. starting fresh.") return evaluated_models def save_evaluated_model(evaluated_models_file, model_path, logger): try: open(evaluated_models_file, "a") f: f.write(model_path + "\n") logger.debug(f"recorded evaluation {model_path} {evaluated_models_file}") except exception e: logger.error(f"error writing evaluated models file {evaluated_models_file}: {e}") def wait_for_file_stable(file_path, wait_time=1.0, max_retries=10, logger=none): """wait file stable (not changing size)""" previous_size = -1 retries = 0 retries &lt; max_retries: os.path.exists(file_path): logger: logger.warning(f"file {file_path} exist.") return false current_size = os.path.getsize(file_path) current_size == previous_size: return true else: previous_size = current_size time.sleep(wait_time) retries += 1 logger: logger.warning(f"file {file_path} stable {max_retries} retries.") return false class checkpointhandler(filesystemeventhandler): def __init__(self, evaluated_models, temp_checkpoint_dir, evaluate_callback, logger): super().__init__() self.evaluated_models = evaluated_models self.temp_checkpoint_dir = temp_checkpoint_dir self.evaluate_callback = evaluate_callback self.logger = logger def on_created(self, event): event.is_directory: return event.src_path.endswith('.ckpt') event.src_path.endswith('.pth'): self.evaluate_model(event.src_path) def evaluate_model(self, model_path): model_file = os.path.basename(model_path) model_path self.evaluated_models: self.logger.info(f"skipping already evaluated model: {model_file}") return # skip model already evaluated # wait file stable wait_for_file_stable(model_path, logger=self.logger): self.logger.warning(f"file {model_file} stable. skipping evaluation.") return # copy checkpoint file temp_checkpoint_dir temp_model_path = os.path.join(self.temp_checkpoint_dir, model_file) try: shutil.copy2(model_path, temp_model_path) self.logger.info(f"copied {model_file} temporary directory.") except exception e: self.logger.error(f"error copying {model_file} temporary directory: {e}") return # extract epoch val_loss filename run_name try: parts = model_file.replace('.ckpt', '').replace('.pth', '').split('-') epoch = none val_loss = none part parts: part.startswith('epoch='): epoch = part.split('=')[1] elif part.startswith('val_loss='): val_loss = part.split('=')[1] epoch none val_loss none: run_name = f"evaluation-epoch{epoch}-val_loss{val_loss}" else: run_name = f"evaluation-{model_file}" self.logger.debug(f"parsed run name: {run_name}") except exception e: self.logger.error(f"error parsing run name filename {model_file}: {e}") run_name = f"evaluation-{model_file}" # define evaluation command eval_command = [ "python", os.path.join(args.arc_model_dir, "gpt2_arc/src/evaluate.py"), "--model_checkpoint", temp_model_path, "--batch_size", str(args.batch_size), "--output_dir", args.output_dir, "--wandb_project", args.wandb_project, "--wandb_run_name", run_name ] self.logger.info(f"evaluating model: {model_file} run name: {run_name}") # define log file path log_file_path = os.path.join(args.output_dir, f"{model_file}_evaluation.log") try: open(log_file_path, "w") log_file: # run evaluation command redirect stdout stderr log file subprocess.run( eval_command, check=true, stdout=log_file, stderr=subprocess.stdout, text=true # automatically decode bytes string ) self.logger.info(f"successfully evaluated model: {model_file}. logs {log_file_path}") except subprocess.calledprocesserror e: self.logger.error(f"error evaluation {model_file}. see log {log_file_path}") except exception ex: self.logger.exception(f"an unexpected error occurred evaluating {model_file}: {ex}") self.evaluated_models.add(model_path) save_evaluated_model(os.path.join(args.output_dir, "evaluated_models.txt"), model_path, self.logger) # delete temp model file try: os.remove(temp_model_path) self.logger.debug(f"deleted temporary model file: {temp_model_path}") except exception e: self.logger.error(f"error deleting temp model file {temp_model_path}: {e}") def get_all_checkpoint_files(directory): checkpoint_files = [] root, _, files os.walk(directory): checkpoint_files.extend([os.path.join(root, f) f files f.endswith('.ckpt') f.endswith('.pth')]) return checkpoint_files def start_observer(model_dir, handler, logger): # set start watchdog observer observer = observer() observer.schedule(handler, model_dir, recursive=true) observer.start() logger.info("watching new checkpoints final models subdirectories...") logger.info("this script continue running background.") try: true: time.sleep(10) # optionally, implement additional periodic checks except keyboardinterrupt: observer.stop() logger.info("observer stopped user.") except filenotfounderror fnf_error: logger.error(f"filenotfounderror: {fnf_error}") logger.error(f"please ensure directory '{model_dir}' exists.") except exception e: logger.exception(f"an error occurred observer: {e}") finally: observer.join() logger.info("checkpoint final model evaluation completed.") def monitor_resources(logger, interval=60): true: try: memory = psutil.virtual_memory() cpu = psutil.cpu_percent(interval=1) logger.debug(f"memory usage: {memory.percent}%") logger.debug(f"cpu usage: {cpu}%") time.sleep(interval) except exception e: logger.error(f"error resource monitoring: {e}") def main(args): logger = setup_logging(args.output_dir, args.log_level) logger.info("starting checkpoint evaluator script") logger.debug(f"arguments: {args}") model_dir = os.path.join(args.date_folder, "checkpoints") logger.debug(f"watching new models directory: {model_dir}") # create necessary directories os.makedirs(model_dir, exist_ok=true) temp_checkpoint_dir = os.path.join(args.output_dir, "temp_checkpoints") os.makedirs(temp_checkpoint_dir, exist_ok=true) # load previously evaluated models evaluated_models_file = os.path.join(args.output_dir, "evaluated_models.txt") evaluated_models = load_evaluated_models(evaluated_models_file, logger) # set event handler handler = checkpointhandler(evaluated_models, temp_checkpoint_dir, none, logger) # initialize watchdog event handler ability evaluate models event_handler = checkpointhandler(evaluated_models, temp_checkpoint_dir, none, logger) # start observer separate thread observer_thread = threading.thread(target=start_observer, args=(model_dir, event_handler, logger)) observer_thread.daemon = true # ensures thread exit main program exits observer_thread.start() logger.info("background checkpoint observer started.") # start resource monitor background thread resource_monitor_thread = threading.thread(target=monitor_resources, args=(logger, args.resource_monitor_interval)) resource_monitor_thread.daemon = true resource_monitor_thread.start() logger.info("background resource monitor started.") # keep main thread alive try: true: time.sleep(1) except keyboardinterrupt: logger.info("script terminated user.") except exception e: logger.exception(f"an unexpected error occurred: {e}") __name__ == "__main__": args = parse_arguments() main(args)</file><file name="src/config.py"># gpt2_arc/src/config.py typing import optional, dict dataclasses import dataclass, asdict, field import multiprocessing import logging logger = logging.getlogger(__name__) logger.setlevel(logging.debug) # set debug detailed logs @dataclass class modelconfig: n_embd: int = 256 # reduced 768 256 n_head: int = 2 # increased 1 2 n_layer: int = 2 # increased 12 2 num_classes: int = field(default=11, metadata={"description": "number output classes model."}) dropout: float = 0.1 mamba_ratio: float = 0.0 d_state: int = 4 d_conv: int = 1 mamba_depth: int = 1 mamba_expand: int = 2 def __post_init__(self): assert self.n_embd % self.n_head == 0, f"n_embd ({self.n_embd}) must divisible n_head ({self.n_head})" assert self.n_embd &gt;= self.n_head, f"n_embd ({self.n_embd}) must greater equal n_head ({self.n_head})" assert self.n_layer &gt; 0, f"n_layer ({self.n_layer}) must positive" assert self.d_state &gt;= 1, f"d_state ({self.d_state}) must least 1" assert self.d_conv &gt;= 1, f"d_conv ({self.d_conv}) must least 1" assert self.mamba_depth &gt;= 1, f"mamba_depth ({self.mamba_depth}) must least 1" assert self.mamba_expand &gt;= 2, f"mamba_expand ({self.mamba_expand}) must least 2" logger.debug("modelconfig initialized successfully") @dataclass class trainingconfig: batch_size: int = 16 # reduced 32 16 learning_rate: float = 1e-4 # keep adjust necessary max_epochs: int = 1 # already set fast dev run num_classes: int = field(default=11, metadata={"description": "number output classes model."}) num_symbols: int = 11 # ensure num_symbols set 11 num_workers: int = 1 # reduced multiprocessing.cpu_count() 1 symbol_freq: optional[dict[int, float]] = none pin_memory: bool = false # disable using gpu prefetch_factor: int = 1 # reduced 2 1 persistent_workers: bool = false # ensure workers stay alive use_gpu: bool = true log_level: str = "info" use_synthetic_data: bool = false use_grokfast: bool = false grokfast_type: optional[str] = field(default=none) # 'ema' 'ma' grokfast_alpha: float = field(default=0.98) grokfast_lamb: float = field(default=2.0) grokfast_window_size: optional[int] = field(default=100) # relevant grokfast_type == 'ma' balance_symbols: bool = true balancing_method: str = "weighting" synthetic_data_path: optional[str] = none include_pad_in_loss: bool = true # whether include padding class loss calculation include_pad_in_accuracy: bool = true # whether include padding class accuracy calculations tensorboard_log_path: optional[str] = none # default none set # new fields padding symbol pad_symbol: str = "&lt;pad&gt;" pad_symbol_idx: int = field(default=10) # add line def __post_init__(self): # dynamically set num_classes based symbol_freq self.pad_symbol_idx = 10 # set default padding index print(f"trainingconfig initialized {self.num_classes} classes pad symbol index {self.pad_symbol_idx}") print(f"include_pad_in_loss: {self.include_pad_in_loss}") # added debug statement print(f"include_pad_in_accuracy: {self.include_pad_in_accuracy}") # added debug statement @dataclass class evaluationconfig: perfect_accuracy_threshold: float = 99.9 # set 99.9 near-perfect accuracy @dataclass class config: model: modelconfig = field(default_factory=modelconfig) training: trainingconfig = field(default_factory=trainingconfig) evaluation: evaluationconfig = field(default_factory=evaluationconfig) estimated_memory: optional[float] = none available_memory: optional[float] = none def to_dict(self): return asdict(self)</file><file name="src/utils/helpers.py"># gpt2_arc/src/utils/helpers.py import torch import logging logger = logging.getlogger(__name__) def differential_pixel_accuracy(input, target, prediction, pad_symbol_idx=10): logger.debug(f"differential pixel accuracy - input shape: {input.shape}, target shape: {target.shape}, prediction shape: {prediction.shape}") assert isinstance(input, torch.tensor) isinstance(target, torch.tensor) isinstance(prediction, torch.tensor), "all inputs must torch.tensor" assert input.numel() == target.numel() == prediction.numel(), "input, target, prediction must number elements" """ compute differential pixel accuracy, excluding padding tokens. args: input (torch.tensor): input tensor model. target (torch.tensor): ground truth labels. prediction (torch.tensor): model predictions. pad_symbol_idx (int): index padding token exclude calculations. returns: tuple: tuple containing: - accuracy (float): differential pixel accuracy. - input_target_diff (torch.tensor): tensor indicating input differs target. - correct_diff_predictions (torch.tensor): tensor indicating correct predictions differing pixels. """ device = input.device target = target.to(device) prediction = prediction.to(device) prediction = prediction.view_as(target) logger.debug(f"reshaped - input: {input.shape}, target: {target.shape}, prediction: {prediction.shape}") # exclude padding tokens creating valid mask valid_mask = target != pad_symbol_idx input_target_diff = (input != target) &amp; valid_mask correct_diff_predictions = (prediction == target) &amp; input_target_diff total_diff_pixels = input_target_diff.sum().item() correct_diff_pixels = correct_diff_predictions.sum().item() logger.debug(f"total different pixels: {total_diff_pixels}") logger.debug(f"correctly predicted different pixels: {correct_diff_pixels}") total_diff_pixels &gt; 0: accuracy = correct_diff_pixels / total_diff_pixels else: accuracy = 1.0 # pixels differ, consider 100% accurate logger.debug(f"calculated accuracy: {accuracy}") return accuracy, input_target_diff, correct_diff_predictions</file><file name="src/utils/grokfast_callback.py">pytorch_lightning.callbacks import callback typing import optional, dict import torch.nn nn import logging .grokfast import gradfilter_ema, gradfilter_ma logger = logging.getlogger(__name__) class grokfastcallback(callback): def __init__( self, filter_type: str = 'ema', # 'ema' 'ma' alpha: float = 0.98, lamb: float = 2.0, window_size: int = 100, warmup: bool = true, trigger: bool = false, # ablation study. ): """ initializes grokfast callback. args: filter_type (str): type grokfast filter ('ema' 'ma'). alpha (float): momentum parameter ema. lamb (float): amplifying factor. window_size (int): window size ma. warmup (bool): whether use warmup ma. trigger (bool): ablation studies. """ super().__init__() self.filter_type = filter_type self.alpha = alpha self.lamb = lamb self.window_size = window_size self.warmup = warmup self.trigger = trigger self.grads = none # hold state across batches def on_after_backward(self, trainer, pl_module): """ called backward pass optimizer step. args: trainer: trainer instance. pl_module: lightningmodule instance. """ model = pl_module.model # adjust model nested differently self.filter_type == 'ema': self.grads = gradfilter_ema( m=model, # pass actual model grads=self.grads, alpha=self.alpha, lamb=self.lamb ) logger.debug("applied grokfast-ema filter.") elif self.filter_type == 'ma': self.grads = gradfilter_ma( m=model, # pass actual model grads=self.grads, window_size=self.window_size, lamb=self.lamb, filter_type='mean', # 'sum' based preference warmup=self.warmup, trigger=self.trigger ) logger.debug("applied grokfast-ma filter.") else: logger.warning(f"unknown grokfast filter type: {self.filter_type}. skipping gradient filtering.")</file><file name="src/utils/grokfast.py">collections import deque typing import dict, optional, literal import torch import torch.nn nn def gradfilter_ma( m: nn.module, grads: optional[dict[str, deque]] = none, window_size: int = 100, lamb: float = 5.0, filter_type: literal['mean', 'sum'] = 'mean', warmup: bool = true, trigger: bool = false, # ablation study. ) -&gt; dict[str, deque]: grads none: grads = {n: deque(maxlen=window_size) n, p m.named_parameters() p.requires_grad p.grad none} n, p m.named_parameters(): p.requires_grad p.grad none: grads[n].append(p.grad.data.detach().clone()) # modify gradients. warmup len(grads[n]) == window_size trigger: filter_type == "mean": avg = sum(grads[n]) / len(grads[n]) elif filter_type == "sum": avg = sum(grads[n]) else: raise valueerror(f"unrecognized filter_type {filter_type}") p.grad.data = p.grad.data + avg * lamb return grads def gradfilter_ema( m: nn.module, grads: optional[dict[str, torch.tensor]] = none, alpha: float = 0.98, lamb: float = 2.0, ) -&gt; dict[str, torch.tensor]: grads none: grads = {n: p.grad.data.detach().clone() n, p m.named_parameters() p.requires_grad p.grad none} n, p m.named_parameters(): p.requires_grad p.grad none: grads[n] = grads[n] * alpha + p.grad.data.detach() * (1 - alpha) p.grad.data = p.grad.data + grads[n] * lamb return grads</file><file name="src/utils/performance_metrics.py">import torch import time def calculate_mamba_efficiency(model, input_data): """ calculates performance metrics specific mamba layers model. args: model: gpt2arc model instance. input_data: sample input tensor. returns: dictionary containing mamba-specific performance metrics. """ metrics = {} model.eval() # set model evaluation mode # ensure model input data device device = next(model.parameters()).device input_data = input_data.to(device) # measure forward pass time torch.no_grad(): start_time = time.time() _ = model(input_data) total_time = time.time() - start_time metrics['mamba_forward_pass_time'] = total_time # count number parameters mamba layers mamba_params = 0 total_params = 0 name, param model.named_parameters(): param_count = param.numel() total_params += param_count 'mamba_block' name: mamba_params += param_count metrics['mamba_params'] = mamba_params metrics['total_params'] = total_params metrics['mamba_params_ratio'] = mamba_params / total_params total_params &gt; 0 else 0 return metrics</file><file name="src/utils/results_collector.py"># gpt2_arc/src/utils/results_collector.py import json import time import uuid import torch import platform import os dataclasses import asdict import logging typing import dict, logger = logging.getlogger(__name__) class resultscollector: def __init__(self, config): """initialize resultscollector given configuration.""" self.experiment_id = str(uuid.uuid4()) self.timestamp = time.strftime("%y-%m-%d %h:%m:%s") self.config = asdict(config) self.symbol_freq = self.config['training']['symbol_freq'] 'symbol_freq' self.config['training'] else {} logger.debug(f"symbol frequencies set resultscollector: {self.symbol_freq}") self.results = { "train": {}, "validation": {}, "test": {} } self.metrics = {} self.task_specific_results = {} self.tensorboard_log_path = getattr(config.training, 'tensorboard_log_path', none) self.used_synthetic_data = getattr(config.training, 'use_synthetic_data', false) self.environment = self._get_environment_info() self.checkpoint_path = none print(f"debug: initialized self.results['train'] {type(self.results['train'])}") self._log_results_type("after initialization") def set_tensorboard_log_path(self, path): self.tensorboard_log_path = path print(f"debug: set tensorboard log path resultscollector: {path}") def _get_environment_info(self) -&gt; dict[str, str]: """retrieve environment information python pytorch versions.""" return { "python_version": platform.python_version(), "torch_version": torch.__version__, "gpu_info": torch.cuda.get_device_name(0) torch.cuda.is_available() else "cpu" } def _log_results_type(self, context: str): """log type self.results['train'] debugging.""" logger.debug(f"{context}: self.results['train'] type {type(self.results['train'])}") def update_train_metrics(self, epoch: int, metrics: dict[str, float]): """update training metrics specific epoch.""" "train" self.results: self.results["train"] = {} isinstance(self.results["train"], dict): raise typeerror(f"expected self.results['train'] dict, got {type(self.results['train'])}") self.results["train"].setdefault(epoch, {}) self.results["train"][epoch].update(metrics) logger.debug(f"updated train metrics epoch {epoch}: {metrics}") def update_val_metrics(self, epoch: int, metrics: dict[str, float]): """update validation metrics specific epoch.""" "validation" self.results: self.results["validation"] = {} isinstance(self.results["validation"], dict): raise typeerror(f"expected self.results['validation'] dict, got {type(self.results['validation'])}") self.results["validation"].setdefault(epoch, {}) self.results["validation"][epoch].update(metrics) logger.debug(f"updated validation metrics epoch {epoch}: {metrics}") def set_test_results(self, metrics: dict[str, float]): """set test results metrics.""" self.results["test"] = metrics def add_task_specific_result(self, task_id: str, metrics: dict[str, float]): """add task-specific results given task id.""" task_id == "default_task": logger.error("attempted add metrics 'default_task'. avoided.") raise valueerror("cannot add metrics 'default_task'. ensure task_id correctly assigned.") task_id self.task_specific_results: self.task_specific_results[task_id] = {} key, value metrics.items(): key self.task_specific_results[task_id]: self.task_specific_results[task_id][key] = [] self.task_specific_results[task_id][key].append(value) def get_task_specific_results(self) -&gt; dict[str, dict[str, float]]: """retrieve aggregated task-specific metrics.""" aggregated = {} task_id, metrics self.task_specific_results.items(): aggregated[task_id] = {} metric_name, values metrics.items(): aggregated[task_id][metric_name] = sum(values) / len(values) values else 0.0 return aggregated def set_final_metrics(self, metrics: dict[str, float]): """set final metrics training.""" self.metrics = metrics def set_checkpoint_path(self, path: str): """set path model checkpoint.""" self.checkpoint_path = path def save_to_json(self, filepath: str): """save collected results json file.""" try: self._ensure_directory_exists(os.path.dirname(filepath)) data = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "config": self.config, "results": self.results, "metrics": self.metrics, "task_specific_results": self.task_specific_results, "environment": self.environment, "checkpoint_path": self.checkpoint_path, "used_synthetic_data": self.used_synthetic_data } self.symbol_freq: data["symbol_freq"] = self.symbol_freq else: data["symbol_freq"] = {} open(filepath, 'w') f: json.dump(data, f, indent=2) except ioerror e: print(f"error saving results {filepath}: {e}") def _ensure_directory_exists(self, directory: str): """ensure directory exists; create not.""" os.path.exists(directory): os.makedirs(directory) def get_summary(self) -&gt; dict[str, any]: """ get summary results. returns: dict[str, any]: summary key metrics. """ summary = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "final_train_loss": self.results["train"][-1]["loss"] self.results["train"] else none, "final_val_loss": self.results["validation"][-1]["loss"] self.results["validation"] else none, "test_loss": self.results["test"].get("avg_loss"), "test_acc_with_pad": self.results["test"].get("avg_acc_with_pad"), "test_acc_without_pad": self.results["test"].get("avg_acc_without_pad"), "best_val_loss": self.results.get("best_val_loss"), "best_val_epoch": self.results.get("best_val_epoch"), "learning_rate": self.config['training']['learning_rate'], "batch_size": self.config['training']['batch_size'], "training_duration": self.results.get("training_duration"), "config": self._serialize_config(self.config), "tensorboard_log_path": self.tensorboard_log_path } logger.debug(f"debug: added tensorboard log path results: {summary['tensorboard_log_path']}") return {k: self._make_serializable(v) k, v summary.items()} def _make_serializable(self, obj): """ensure value serializable, handling non-serializable objects.""" isinstance(obj, (int, float, str, bool, type(none))): return obj elif isinstance(obj, (list, tuple)): return [self._make_serializable(item) item obj] elif isinstance(obj, dict): return {k: self._make_serializable(v) k, v obj.items()} else: return str(obj)</file><file name="src/utils/experiment_tracker.py"># gpt2_arc/src/utils/experiment_tracker.py import logging import wandb import json import time import uuid import torch import platform import os dataclasses import asdict typing import dict, any, optional logger = logging.getlogger(__name__) logger.setlevel(logging.debug) class experimenttracker: def __init__(self, config: dict[str, any], project: str, entity: optional[str] = none, use_wandb: bool = false): self.experiment_id = str(uuid.uuid4()) self.timestamp = time.strftime("%y-%m-%d %h:%m:%s") self.config = config.to_dict() hasattr(config, 'to_dict') else self._config_to_dict(config) self.project = project self.entity = entity self.run = none self.use_wandb = use_wandb self.metrics = {} self.use_wandb: try: self.run = wandb.init(project=self.project, entity=self.entity, config=self.config) print(f"wandb run initialized: {self.run.id}") except exception e: print(f"error initializing wandb: {str(e)}") self.use_wandb = false self.results = { "train": [], "validation": [], "test": {} } self.metrics = {} self.task_specific_results = {} self.environment = self._get_environment_info() self.checkpoint_path = none # add debug logging logger.debug(f"experimenttracker initialized config: {json.dumps(self.config, indent=2)}") logger.debug(f"project: {project}, entity: {entity}") logger.debug(f"use_wandb: {self.use_wandb}") def _get_environment_info(self) -&gt; dict[str, str]: return { "python_version": platform.python_version(), "torch_version": torch.__version__, "gpu_info": torch.cuda.get_device_name(0) torch.cuda.is_available() else "cpu" } def _config_to_dict(self, config): isinstance(config, dict): return {k: self._config_to_dict(v) k, v config.items()} elif hasattr(config, '__dict__'): return {k: self._config_to_dict(v) k, v config.__dict__.items() k.startswith('_')} else: return config self.use_wandb: try: self.run = wandb.init(project=self.project, entity=self.entity, config=self.config) print(f"wandb run initialized: {self.run.id}") except exception e: print(f"error initializing wandb: {str(e)}") self.use_wandb = false self.use_wandb: print("using local logging only") def finish(self): self.use_wandb self.run: try: wandb.finish() print("wandb run finished") except exception e: print(f"error finishing wandb run: {str(e)}") else: print("experiment finished. metrics:", self.metrics) def log_metric(self, name: str, value: float, step: optional[int] = none): self.use_wandb: try: wandb.log({name: value}, step=step) print(f"logged metric wandb: {name}={value}, step={step}") except exception e: print(f"error logging metric wandb: {str(e)}") # always log locally fallback print(f"logged metric locally: {name}={value}, step={step}") def update_train_metrics(self, epoch: int, metrics: dict[str, float]): "train" self.results: self.results["train"] = [] len(self.results["train"]) &lt;= epoch: self.results["train"].append({}) self.results["train"][epoch] = metrics self.use_wandb: wandb.log({"train": metrics}, step=epoch) def update_val_metrics(self, epoch: int, metrics: dict[str, float]): "validation" self.results: self.results["validation"] = [] len(self.results["validation"]) &lt;= epoch: self.results["validation"].append({}) self.results["validation"][epoch] = metrics self.use_wandb: wandb.log({"validation": metrics}, step=epoch) def set_test_results(self, metrics: dict[str, float]): self.results["test"] = metrics self.use_wandb: wandb.log({"test": metrics}) def add_task_specific_result(self, task_id: str, metrics: dict[str, float]): task_id self.task_specific_results: self.task_specific_results[task_id] = {} self.task_specific_results[task_id].update(metrics) logger.debug(f"added task-specific result task_id {task_id}: {metrics}") self.use_wandb: wandb.log({f"task_{task_id}": metrics}) def set_final_metrics(self, metrics: dict[str, float]): self.metrics = metrics self.use_wandb: wandb.log(metrics) def set_checkpoint_path(self, path: str): self.checkpoint_path = path self.use_wandb: wandb.save(path) def save_to_json(self, filepath: str): try: directory = os.path.dirname(filepath) directory os.path.exists(directory): os.makedirs(directory) data = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "config": self.config, "results": self.results, "metrics": self.metrics, "task_specific_results": self.task_specific_results, "environment": self.environment, "checkpoint_path": self.checkpoint_path } open(filepath, 'w') f: json.dump(data, f, indent=2) print(f"results saved {filepath}") except ioerror e: print(f"error saving results {filepath}: {e}") def _ensure_directory_exists(self, directory: str): os.path.exists(directory): os.makedirs(directory) def get_summary(self) -&gt; dict[str, any]: summary = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "final_train_loss": self.results["train"][-1]["loss"] self.results["train"] else none, "final_val_loss": self.results["validation"][-1]["loss"] self.results["validation"] else none, "test_loss": self.results["test"].get("avg_loss"), "test_acc_with_pad": self.results["test"].get("avg_acc_with_pad"), "test_acc_without_pad": self.results["test"].get("avg_acc_without_pad"), "best_val_loss": self.results.get("best_val_loss"), "best_val_epoch": self.results.get("best_val_epoch"), "learning_rate": self.config.get("training", {}).get("learning_rate"), "batch_size": self.config.get("training", {}).get("batch_size"), "training_duration": self.results.get("training_duration"), "config": self._serialize_config(self.config), "tensorboard_log_path": self.tensorboard_log_path } self.logger.debug(f"debug: added tensorboard log path results: {summary['tensorboard_log_path']}") return {k: self._make_serializable(v) k, v summary.items()} def _make_serializable(self, obj): isinstance(obj, (int, float, str, bool, type(none))): return obj elif isinstance(obj, (list, tuple)): return [self._make_serializable(item) item obj] elif isinstance(obj, dict): return {k: self._make_serializable(v) k, v obj.items()} else: return str(obj) def _serialize_config(self, config): return {k: self._make_serializable(v) k, v config.items()} def log_metric(self, name: str, value: float, step: optional[int] = none): name.startswith("default_task_"): logger.error("attempted log metric 'default_task'. avoided.") raise valueerror("cannot log metrics 'default_task'. ensure task_id correctly assigned.") # add simple test __name__ == "__main__": config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": true} tracker = experimenttracker(config, project="test-project") tracker.start() tracker.log_metric("accuracy", 0.85, step=1) tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8}) tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75}) tracker.set_test_results({"loss": 0.55, "accuracy": 0.82}) tracker.add_task_specific_result("task_1", {"accuracy": 0.9}) tracker.set_final_metrics({"best_accuracy": 0.85}) tracker.set_checkpoint_path("model_checkpoint.pth") tracker.save_to_json("results.json") tracker.finish()</file><file name="src/utils/__init__.py">.grokfast import ( gradfilter_ma, gradfilter_ema ) .grokfast_callback import grokfastcallback</file><file name="src/utils/training_helpers.py">typing import optional gpt2_arc.src.config import trainingconfig def get_num_workers(config: trainingconfig) -&gt; int: """determine number dataloader workers based configuration.""" return config.num_workers</file><file name="src/utils/model_memory_estimator.py"># gpt2_arc/src/utils/model_memory_estimator.py import torch import math import psutil import logging logger = logging.getlogger(__name__) logger.setlevel(logging.debug) def calculate_params(n_layers, n_heads, d_model, mamba_ratio, d_state=16, d_conv=4, mamba_depth=1, mamba_expand=2): logger.debug(f"executing calculate_params mamba_ratio = {mamba_ratio}") transformer_params_per_layer = ( 12 * d_model * d_model + 13 * d_model ) # calculate number mamba layers total_mamba_layers = int(n_layers * mamba_ratio) # calculate parameters mamba layers # assuming mambablock parameters based d_state, d_conv, depth, expand mamba_params_per_layer = ( d_state * d_conv * mamba_expand * mamba_depth # example calculation ) total_mamba_params = total_mamba_layers * mamba_params_per_layer # total parameters total_params = n_layers * transformer_params_per_layer + total_mamba_params logger.debug(f"total parameters calculated: {total_params}") return total_params def estimate_memory_usage(total_params, batch_size, height, width, d_model, dtype_size=4): model_memory = total_params * dtype_size # model parameters optimizer_memory = model_memory * 2 # adam optimizer uses 2x model size input_memory = batch_size * height * width * dtype_size # input tensors conv_output_memory = batch_size * height * width * d_model * dtype_size # conv layer activations_memory = batch_size * (height * width) * d_model * dtype_size * 2 # forward &amp; backward pass total_memory = model_memory + optimizer_memory + input_memory + conv_output_memory + activations_memory return total_memory / (1024**3) # convert gb def get_available_memory(): torch.cuda.is_available(): return torch.cuda.get_device_properties(0).total_memory / (1024**3) # convert gb else: return psutil.virtual_memory().total / (1024**3) # get actual system memory cpu def get_device_info(): torch.cuda.is_available(): return { "device": "gpu", "name": torch.cuda.get_device_name(0), "compute_capability": torch.cuda.get_device_capability(0), "total_memory": torch.cuda.get_device_properties(0).total_memory / (1024**3), "cuda_version": torch.version.cuda } else: return { "device": "cpu", "name": "system cpu", "total_memory": psutil.virtual_memory().total / (1024**3), "cpu_count": psutil.cpu_count(logical=false), "cpu_freq": psutil.cpu_freq().max psutil.cpu_freq() else "n/a" } def can_fit_model(estimated_memory, available_memory, threshold=0.9): return estimated_memory &lt; available_memory * threshold def estimate_single_configuration(n_layers, n_heads, d_model, batch_size, height, width): device_info = get_device_info() available_memory = get_available_memory() print(f"device information:") key, value device_info.items(): print(f" {key}: {value}") print(f"available memory: {available_memory:.2f} gb") total_params = calculate_params(n_layers, n_heads, d_model) estimated_memory = estimate_memory_usage(total_params, batch_size, height, width, d_model) print(f"\nconfiguration:") print(f" n_layers: {n_layers}") print(f" n_heads: {n_heads}") print(f" d_model: {d_model}") print(f" batch_size: {batch_size}") print(f" input_height: {height}") print(f" input_width: {width}") print(f"total parameters: {total_params:,}") print(f"estimated memory usage: {estimated_memory:.2f} gb") can_fit_model(estimated_memory, available_memory): print(f"model fit {device_info['device']} memory.") else: print(f"warning: model may large available {device_info['device']} memory!") print(f"memory utilization: {(estimated_memory / available_memory) * 100:.2f}%")</file><file name="src/data/__init__.py" /><file name="src/data/arc_dataset.py"># gp2_arc/src/data/arc_dataset.py import os import json import random typing import union, list, dict, tuple, any, optional typing import list, dict, union, optional import logging logger = logging.getlogger(__name__) import numpy np import pickle import hashlib import torch import torch.nn.functional f torch.utils.data import dataset, subset, weightedrandomsampler import logging import ijson # import ijson streaming json parsing import cysimdjson # new import added tqdm import tqdm # import tqdm progress bars import sys # import sys handling tqdm output concurrent.futures import threadpoolexecutor, as_completed # parallel processing concurrent.futures import threadpoolexecutor, as_completed # parallel processing import multiprocessing # determine cpu count threading import lock jsonschema import validate, validationerror torch.utils.data import get_worker_info try: arckit.data import taskset, task except importerror: taskset = none logger = logging.getlogger(__name__) # create handler writes stderr handler = logging.streamhandler() formatter = logging.formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') handler.setformatter(formatter) # add handler logger logger.addhandler(handler) # determine debug mode debug_mode = os.getenv("debug_mode", "false") == "true" # set logging levels based debug_mode debug_mode: logger.setlevel(logging.debug) handler.setlevel(logging.debug) else: logger.setlevel(logging.info) handler.setlevel(logging.info) # define json schema task validation task_schema = { "type": "object", "properties": { "id": {"type": "string"}, "input": { "type": "array", "items": {"type": "number"} }, "output": { "type": "array", "items": {"type": "number"} } }, "required": ["input", "output"], "additionalproperties": false } def set_debug_mode(debug=false): debug: logger.setlevel(logging.debug) handler.setlevel(logging.debug) else: logger.setlevel(logging.error) handler.setlevel(logging.error) class arcdataset(dataset): def __init__( self, data_source: union[str, list[dict], 'taskset', tuple[union[list, 'taskset'], str]], is_test: bool = false, max_samples: optional[int] = none, # add parameter num_symbols: int = 11, test_split: float = 0.2, pad_symbol_idx: int = 10, symbol_freq: optional[dict[int, float]] = none, debug: bool = false, ): self.is_test = is_test self.num_symbols = num_symbols self.test_split = test_split self.pad_symbol_idx = pad_symbol_idx self.symbol_freq = symbol_freq symbol_freq none else {} logger.debug(f"initialized arcdataset pad_symbol_idx: {self.pad_symbol_idx}") self.symbol_freq: logger.debug("symbol frequencies provided; initializing weightedrandomsampler.") else: logger.debug("no symbol frequencies provided.") self.data_files = [] self.data_source = data_source self.num_samples = 0 # initialize cysimdjson jsonparser self.json_parser = cysimdjson.jsonparser() logger.debug("initialized cysimdjson.jsonparser instance") self.cache_path = self._generate_cache_path( data_source=self.data_source, num_symbols=self.num_symbols, is_test=self.is_test, test_split=self.test_split ) self._load_cache(self.cache_path): # load data loaded cache try: logger.debug("loading data data source cache found failed.") self.data = self._load_data(data_source) self.data: logger.error("no valid samples loaded. ensure samples 'input' 'output' keys.") raise valueerror("no valid samples loaded. ensure samples 'input' 'output' keys.") logger.debug(f"data loaded successfully data source {len(self.data)} samples.") except exception e: logger.error(f"failed load data: {e}", exc_info=true) raise # apply sample limit specified max_samples none: self.data = self.data[:max_samples] logger.debug(f"limited dataset {max_samples} samples.") self.num_samples = len(self.data) self._compute_and_cache_statistics() self._save_cache(self.cache_path) self.symbol_freq: # calculate weights sample based symbol frequencies weights = [] sample self.data: input_freq = torch.tensor([self.symbol_freq.get(symbol.item(), 0.0) symbol sample["input"].flatten()]) output_freq = torch.tensor([self.symbol_freq.get(symbol.item(), 0.0) symbol sample["output"].flatten()]) sample_freq = torch.cat((input_freq, output_freq)).mean() weights.append(1.0 / (sample_freq + 1e-8)) # add epsilon avoid division zero self.sample_weights = torch.tensor(weights, dtype=torch.float) self.sampler = weightedrandomsampler(self.sample_weights, num_samples=len(self.sample_weights), replacement=true) logger.debug("weightedrandomsampler initialized based symbol frequencies.") else: self.sampler = none logger.debug("no symbol frequencies provided; sampler initialized.") debug: logger.setlevel(logging.debug) handler.setlevel(logging.debug) logger.debug("debug mode enabled arcdataset.") else: logger.setlevel(logging.info) handler.setlevel(logging.info) logger.debug("arcdataset initialization completed.") def _process_single_task(self, task: union[dict, list], task_id: str) -&gt; list[dict]: """ processes single task dictionary list returns list samples. args: task (union[dict, list]): task data containing 'input' 'output', list dictionaries. task_id (str): identifier task. returns: list[dict]: list processed sample dictionaries. """ samples = [] try: isinstance(task, dict): # existing processing dictionary tasks ex task.get('train', []): logger.debug(f"processing training example keys: {ex.keys()}") logger.debug(f"processing example keys: {ex.keys()}") input_tensor = self._preprocess_grid(ex['input']) output_tensor = self._preprocess_grid(ex['output']) samples.append({ "input": input_tensor, "output": output_tensor, "task_id": task_id }) ex task.get('test', []): input_tensor = self._preprocess_grid(ex['input']) output_tensor = self._preprocess_grid(ex['output']) samples.append({ "input": input_tensor, "output": output_tensor, "task_id": task_id }) elif isinstance(task, list): # new processing list-type tasks ex task: input_tensor = self._preprocess_grid(ex['input']) output_tensor = self._preprocess_grid(ex['output']) samples.append({ "input": input_tensor, "output": output_tensor, "task_id": task_id }) else: raise valueerror(f"unsupported task type: {type(task)}") except exception e: logger.error(f"error processing task {task_id}: {e}", exc_info=true) return samples def _process_single_file_streaming(self, file_path: str) -&gt; list[dict]: """ processes single json file using streaming parsing returns list samples. args: file_path (str): path json file. returns: list[dict]: list processed samples file. """ samples = [] samples = [] sample_count = 0 # initialize sample counter missing_id_logged = false # flag track warning logged file # skip empty files early os.path.getsize(file_path) == 0: logger.warning(f"empty json file detected: {file_path}. skipping.") return samples try: open(file_path, 'rb') f: # open binary mode # use cysimdjson efficient parsing parsed_json = self.json_parser.parse(f.read()) # convert parsed_json native python structures parsed_py = self._cysimdjson_to_native(parsed_json) isinstance(parsed_py, list): ex parsed_py: 'input' ex 'output' ex: try: input_tensor = self._preprocess_grid(ex['input']) output_tensor = self._preprocess_grid(ex['output']) task_id = ex.get('id', f"default_task_{sample_count}") isinstance(task_id, str) task_id: missing_id_logged: task_id = f"default_task_{sample_count}" logger.warning(f"sample missing valid 'id'. assigned task_id: {task_id}") missing_id_logged = true else: task_id = f"default_task_{sample_count}" samples.append({ "input": input_tensor, "output": output_tensor, "task_id": task_id }) sample_count += 1 except exception e: logger.error( f"error preprocessing sample {sample_count} (task id: {task_id}) file {file_path}: {e}", exc_info=true ) else: logger.warning(f"sample missing 'input' 'output' keys file {file_path}. skipping.") elif isinstance(parsed_py, dict): # existing processing dictionary tasks ex parsed_py.get('train', []): try: logger.debug(f"processing training example keys: {ex.keys()}") input_tensor = self._preprocess_grid(ex['input']) output_tensor = self._preprocess_grid(ex['output']) task_id = parsed_py.get('id', f"default_task_{sample_count}") isinstance(task_id, str) task_id: missing_id_logged: task_id = f"default_task_{sample_count}" logger.warning(f"task missing valid 'id'. assigned task_id: {task_id}") missing_id_logged = true else: task_id = f"default_task_{sample_count}" samples.append({ "input": input_tensor, "output": output_tensor, "task_id": task_id }) sample_count += 1 except exception e: logger.error( f"error preprocessing training sample {sample_count} (task id: {task_id}) file {file_path}: {e}", exc_info=true ) ex parsed_py.get('test', []): try: input_tensor = self._preprocess_grid(ex['input']) output_tensor = self._preprocess_grid(ex['output']) task_id = parsed_py.get('id', f"default_task_{sample_count}") isinstance(task_id, str) task_id: missing_id_logged: task_id = f"default_task_{sample_count}" logger.warning(f"task missing valid 'id'. assigned task_id: {task_id}") missing_id_logged = true else: task_id = f"default_task_{sample_count}" samples.append({ "input": input_tensor, "output": output_tensor, "task_id": task_id }) sample_count += 1 except exception e: logger.error( f"error preprocessing testing sample {sample_count} (task id: {task_id}) file {file_path}: {e}", exc_info=true ) else: logger.warning(f"unexpected json structure file {file_path}. skipping.") logger.info(f"finished processing synthetic data file: {file_path}. extracted {len(samples)} samples.") except exception e: # catch exceptions related parsing logger.error(f"cysimdjson failed parse file {file_path}: {e}. skipping.") return samples def _cysimdjson_to_native(self, parsed_json): """ recursively converts cysimdjson parsed objects native python lists dicts. args: parsed_json (cysimdjson.cysimdjson.jsonvalue): parsed json object. returns: union[dict, list, primitive]: native python data structure. """ isinstance(parsed_json, cysimdjson.jsonobject): return {k: self._cysimdjson_to_native(v) k, v parsed_json.items()} elif isinstance(parsed_json, cysimdjson.jsonarray): return [self._cysimdjson_to_native(item) item parsed_json] elif isinstance(parsed_json, (int, float, str, bool)): return parsed_json elif parsed_json none: return none else: logger.warning(f"unknown json type encountered: {type(parsed_json)}") return none """ wrapper method process single file parallel. args: file_path (str): path json file. returns: list[dict]: list processed samples file. """ """ wrapper method process single file parallel. args: file_path (str): path json file. returns: list[dict]: list processed samples file. """ return self._process_single_file_streaming def _save_cache(self, cache_path: str, data_only=false): """ saves dataset statistics specified cache path using pickle. args: cache_path (str): file path cache saved. data_only (bool): true, save data without statistics. """ logger.debug(f"attempting save {'data ' data_only else ''}cache {cache_path}") try: data_only: cache_data = { "data": self.data } else: cache_data = { "data": self.data, "statistics": self.statistics } open(cache_path, 'wb') f: pickle.dump(cache_data, f) logger.info(f"successfully saved {'data ' data_only else ''}cache {cache_path}") except exception e: logger.error(f"failed save cache {cache_path}: {e}", exc_info=true) # add data validation self._validate_data() def _validate_data(self): """ validates dataset ensure sample contains required keys correct data types. raises: valueerror: sample missing required keys incorrect types. """ required_keys = {"input", "output", "task_id"} idx, sample enumerate(self.data): # check required keys required_keys.issubset(sample.keys()): missing = required_keys - sample.keys() raise keyerror(f"sample index {idx} missing keys: {missing}") # validate 'input' 'output' types key ["input", "output"]: isinstance(sample[key], torch.tensor): raise typeerror(f"sample index {idx} '{key}' type {type(sample[key])}, expected torch.tensor.") sample[key].ndimension() != 3 sample[key].shape[0] != 1: raise valueerror(f"sample index {idx} '{key}' shape {sample[key].shape}, expected shape (1, h, w).") # validate symbols within allowed range based num_symbols max_symbol_allowed = self.num_symbols - 1 max_symbol = sample[key].max() max_symbol &gt; max_symbol_allowed: logger.error( f"sample index {idx} symbol {max_symbol.item()} exceeding allowed maximum ({max_symbol_allowed})." ) raise valueerror( f"sample index {idx} symbol {max_symbol.item()} exceeding allowed maximum ({max_symbol_allowed})." ) # validate 'task_id' type isinstance(sample["task_id"], str): raise typeerror(f"sample index {idx} 'task_id' type {type(sample['task_id'])}, expected str.") logger.debug("all samples passed validation.") def __len__(self): return len(self.data) def get_num_samples(self): return self.num_samples def __getitem__(self, idx): sample = self.data[idx] task_id = sample["task_id"] assert task_id != "default_task", f"sample index {idx} 'default_task' task_id." input_tensor = sample["input"] # already padded output_tensor = sample["output"] # already padded idx &lt; 5: # log first 5 samples avoid clutter logger.debug(f"sample {idx} - task id: {task_id}") return input_tensor, output_tensor, task_id @staticmethod def _generate_cache_path(data_source: union[str, list[dict], 'taskset', tuple[union[list, 'taskset'], str]], num_symbols: int, is_test: bool, test_split: float) -&gt; str: dataset_version = "v1" # create stable representation data_source based type isinstance(data_source, str): data_source_str = os.path.abspath(data_source) # use absolute path consistency elif isinstance(data_source, taskset): data_source_str = f"taskset:{len(data_source.tasks)}" # use number tasks identifier elif isinstance(data_source, list): data_source_str = f"list:{len(data_source)}" # use length list else: data_source_str = str(data_source) # fallback string representation # create json string stable identifiers hash_input = json.dumps({ 'version': dataset_version, 'data_source': data_source_str, 'num_symbols': num_symbols, 'is_test': is_test, 'test_split': test_split }, sort_keys=true).encode('utf-8') # generate md5 hash cache filename hash_digest = hashlib.md5(hash_input).hexdigest() cache_filename = f"arc_dataset_cache_{hash_digest}.pkl" # define cache directory relative current file cache_dir = os.path.join(os.path.dirname(__file__), 'cache') os.makedirs(cache_dir, exist_ok=true) return os.path.join(cache_dir, cache_filename) def _load_cache(self, cache_path: str) -&gt; bool: logger.debug(f"attempting load cache from: {cache_path}") os.path.exists(cache_path): try: open(cache_path, 'rb') f: cache_data = pickle.load(f) self.data = cache_data.get("data", []) self.statistics = cache_data.get("statistics", {}) self.num_samples = len(self.data) logger.info(f"successfully loaded cache {cache_path} {self.num_samples} samples.") return true except exception e: logger.error(f"failed load cache {cache_path}: {e}", exc_info=true) else: logger.warning(f"cache file exist at: {cache_path}. proceeding without cache.") return false def _compute_and_cache_statistics(self): """ computes dataset statistics caches alongside dataset cache. """ logger.info("starting computation dataset statistics.") try: grid_size_stats = self._compute_grid_size_stats() logger.debug(f"computed grid size statistics: {grid_size_stats}") symbol_frequencies = self._compute_symbol_frequencies() logger.debug(f"computed symbol frequencies: {symbol_frequencies}") statistics = { "grid_size_stats": grid_size_stats, "symbol_frequencies": symbol_frequencies } # update cache dictionary statistics self.statistics = statistics logger.info("completed computation dataset statistics.") self._save_cache(self.cache_path) logger.info("dataset statistics cached successfully.") self.symbol_freq: logger.debug(f"sampling weights - min: {self.sample_weights.min().item()}, " f"max: {self.sample_weights.max().item()}, " f"mean: {self.sample_weights.mean().item()}") except exception e: logger.error(f"failed compute cache dataset statistics: {e}", exc_info=true) raise def _process_list_data(self, data_list: list[dict], task_id: optional[str] = none) -&gt; list[dict]: processed_data = [] logger.debug(f"processing list data {len(data_list)} items") idx, example enumerate(data_list): 'input' example 'output' example: input_grid = self._preprocess_grid(example['input']) output_grid = self._preprocess_grid(example['output']) task_id_sample = task_id task_id else example.get('task_id') task_id_sample task_id_sample == "default_task": task_id_sample = f"default_task_{idx}" logger.warning(f"sample index {idx} invalid 'task_id'. assigning new task_id: {task_id_sample}") processed_data.append({ "input": input_grid, "output": output_grid, "task_id": task_id_sample }) else: logger.warning(f"example index {idx} missing 'input' 'output' keys. skipping.") logger.debug(f"processed {len(processed_data)} samples") return processed_data def _combine_data(self, official_data, synthetic_data_path): official_processed = self._process_arckit_data(official_data) taskset none isinstance(official_data, taskset) else official_data synthetic_processed = self._process_synthetic_data(synthetic_data_path) return official_processed + synthetic_processed def _process_synthetic_data(self, directory: str): self.data_files = [] filename os.listdir(directory): filename.endswith('.json'): file_path = os.path.join(directory, filename) self.data_files.append(file_path) logger.debug(f"processing file: {file_path}") open(file_path, 'r') f: try: task_data = json.load(f) # assign task_id filename task_id = os.path.splitext(filename)[0] processed_samples = self._process_single_task(task_data, task_id=task_id) self.data.extend(processed_samples) except json.jsondecodeerror e: logger.error(f"error decoding json file {file_path}: {e}") def _process_arckit_data(self, taskset: 'taskset') -&gt; list[dict]: """ processes data arckit taskset returns list samples. args: taskset (taskset): taskset object containing tasks. returns: list[dict]: list processed sample dictionaries. """ processed_data = [] logger.debug(f"processing taskset {len(taskset.tasks)} tasks") task taskset.tasks: logger.debug(f"processing task: {task.id}") logger.debug(f"train samples: {len(task.train)}, test samples: {len(task.test)}") # process training samples ex task.train: try: input_tensor = self._preprocess_grid(ex[0]) output_tensor = self._preprocess_grid(ex[1]) processed_data.append({ "input": input_tensor, "output": output_tensor, "task_id": task.id }) except exception e: logger.error(f"error processing training example task {task.id}: {e}", exc_info=true) # process testing samples ex task.test: try: input_tensor = self._preprocess_grid(ex[0]) output_tensor = self._preprocess_grid(ex[1]) processed_data.append({ "input": input_tensor, "output": output_tensor, "task_id": task.id }) except exception e: logger.error(f"error processing testing example task {task.id}: {e}", exc_info=true) logger.debug(f"processed task {task.id}: total samples added: {len(task.train) + len(task.test)}") logger.debug(f"total samples processed taskset: {len(processed_data)}") return processed_data def get_grid_size_stats(self) -&gt; dict[str, any]: """ returns precomputed grid size statistics. returns: dict[str, any]: dictionary containing grid size statistics. """ hasattr(self, 'statistics') 'grid_size_stats' self.statistics: return self.statistics['grid_size_stats'] else: logger.warning("grid size statistics available.") return {} def get_symbol_frequencies(self) -&gt; dict[int, float]: """ returns precomputed symbol frequencies. returns: dict[int, float]: dictionary mapping symbols frequencies. """ hasattr(self, 'statistics') 'symbol_frequencies' self.statistics: return self.statistics['symbol_frequencies'] else: logger.warning("symbol frequencies available.") return {} def _compute_grid_size_stats(self): max_height, max_width = 0, 0 sample self.data: # assuming sample["input"] sample["output"] shape [c, h, w] max_height = max(max_height, sample["input"].shape[1], sample["output"].shape[1]) max_width = max(max_width, sample["input"].shape[2], sample["output"].shape[2]) grid_size_stats = {"max_height": max_height, "max_width": max_width} self.max_grid_size = (max_height, max_width) return grid_size_stats def _compute_symbol_frequencies(self): symbol_counts = np.zeros(self.num_symbols, dtype=int) max_symbol_in_data = 0 sample self.data: input_symbols = sample["input"].flatten().numpy().astype(int) output_symbols = sample["output"].flatten().numpy().astype(int) input_symbols.size &gt; 0: max_symbol_in_data = max(max_symbol_in_data, input_symbols.max()) output_symbols.size &gt; 0: max_symbol_in_data = max(max_symbol_in_data, output_symbols.max()) symbol_counts += np.bincount(input_symbols, minlength=self.num_symbols) symbol_counts += np.bincount(output_symbols, minlength=self.num_symbols) logger.debug(f"maximum symbol index data: {max_symbol_in_data}") logger.debug(f"symbol counts length: {len(symbol_counts)}") max_symbol_in_data &gt;= self.num_symbols: logger.error(f"found symbol index {max_symbol_in_data} exceeding num_symbols - 1 ({self.num_symbols - 1}).") raise valueerror(f"symbol index {max_symbol_in_data} exceeds allowed range.") total_symbols = symbol_counts.sum() total_symbols == 0: logger.warning("total de smbolos es 0. evitando la divisin por cero.") symbol_freq = np.zeros_like(symbol_counts, dtype=float) else: symbol_freq = symbol_counts / total_symbols return symbol_freq def _preprocess_grid(self, grid: union[dict, list, np.ndarray, torch.tensor], pad_value: int = 0) -&gt; torch.tensor: logger.debug(f"preprocessing grid initial type: {type(grid)}") # convert grid torch.tensor list numpy array isinstance(grid, list): grid_tensor = torch.as_tensor(grid, dtype=torch.float32) logger.debug(f"converted list tensor shape: {grid_tensor.shape}") elif isinstance(grid, np.ndarray): grid_tensor = torch.as_tensor(grid, dtype=torch.float32) logger.debug(f"converted numpy array tensor shape: {grid_tensor.shape}") elif isinstance(grid, torch.tensor): grid_tensor = grid.float() logger.debug(f"using existing tensor shape: {grid_tensor.shape}") elif isinstance(grid, int): logger.debug("grid type int. converting 1x1 grid.") grid = [[grid]] grid_tensor = torch.as_tensor(grid, dtype=torch.float32) logger.debug(f"converted int tensor shape: {grid_tensor.shape}") else: raise valueerror(f"unexpected grid type: {type(grid)}") # ensure grid_tensor three dimensions [c, h, w] grid_tensor.ndim == 2: logger.debug("grid tensor 2d. adding channel dimension.") grid_tensor = grid_tensor.unsqueeze(0) # add channel dimension logger.debug(f"grid tensor shape unsqueeze: {grid_tensor.shape}") elif grid_tensor.ndim != 3: raise valueerror(f"unexpected grid tensor dimensions: {grid_tensor.ndim}. expected 2d 3d tensor.") logger.debug(f"grid shape padding: {grid_tensor.shape}") # apply padding using pytorch's built-in functions correct pad_value padded_grid = self._pad_grid_torch(grid_tensor, height=30, width=30, pad_value=self.pad_symbol_idx) logger.debug(f"grid shape padding: {padded_grid.shape}") logger.debug(f"padded grid pad_symbol_idx: {self.pad_symbol_idx}, resulting shape: {padded_grid.shape}") return padded_grid def kronecker_scale(self, x, target_height=30, target_width=30): logger.debug(f"kronecker scaling input shape: {x.shape}") h, w = x.shape scale_h = target_height / h scale_w = target_width / w = int(np.floor(min(scale_h, scale_w))) x_scaled = np.kron(x, np.ones((d, d))) logger.debug(f"kronecker scaled output shape: {x_scaled.shape}") return x_scaled def reverse_scaling(self, x_orig, x_pred): logger.debug(f"reverse scaling - original shape: {x_orig.shape}, prediction shape: {x_pred.shape}") h, w = x_orig.shape # reshape x_pred 2d 1d x_pred.ndim == 1: x_pred = x_pred.reshape((int(np.sqrt(x_pred.size)), -1)) x_pred_cropped = x_pred[:h, :w] # crop original size h == x_pred.shape[0] w == x_pred.shape[1]: logger.debug("no rescaling needed") return x_pred_cropped # calculate downscale factor d_h = x_pred_cropped.shape[0] // h d_w = x_pred_cropped.shape[1] // w # ensure dimensions compatible reshaping d_h &gt; 0 d_w &gt; 0: try: x_rev = x_pred_cropped.reshape(h, d_h, w, d_w).mean(axis=(1, 3)) except valueerror e: logger.error(f"error reshaping: {e}") logger.debug(f"x_pred_cropped shape: {x_pred_cropped.shape}, h: {h}, w: {w}, d_h: {d_h}, d_w: {d_w}") raise else: logger.warning(f"invalid downscale factors: d_h={d_h}, d_w={d_w}") raise valueerror("invalid dimensions reverse scaling") # resize result match original target shape result = np.resize(x_rev.round().astype(int), x_orig.shape) logger.debug(f"reverse scaled output shape: {result.shape}") return result def _scale_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray: return grid # scaling, preserve original size def _pad_grid_torch(self, grid: torch.tensor, height: int, width: int, pad_value: int = 0) -&gt; torch.tensor: """ pads input grid tensor specified height width using pytorch's functional padding. args: grid (torch.tensor): input grid tensor shape [c, h, w]. height (int): target height padding. width (int): target width padding. returns: torch.tensor: padded grid tensor. """ _, h, w = grid.shape pad_h = max((height - h) // 2, 0) pad_w = max((width - w) // 2, 0) # calculate padding top, bottom, left, right padding = (pad_w, width - w - pad_w, pad_h, height - h - pad_h) # (left, right, top, bottom) logger.debug(f"padding applied: left={pad_w}, right={width - w - pad_w}, top={pad_h}, bottom={height - h - pad_h}") # apply padding using pytorch's functional pad padded_grid = f.pad(grid, padding, mode='constant', value=pad_value) return padded_grid @staticmethod def collate_fn(batch): # debugging: check batch size logger.debug(f"collating batch size: {len(batch)}") batch: logger.warning("empty batch received") return torch.tensor([]), torch.tensor([]), [] inputs, outputs, task_ids = zip(*batch) # since samples already padded 30x30, additional padding required here. # however, ensure consistency, verify shapes. padded_inputs = torch.stack(inputs) padded_outputs = torch.stack(outputs) # debugging: verify shapes stacking #print(f"padded inputs shape: {padded_inputs.shape}") #print(f"padded outputs shape: {padded_outputs.shape}") return padded_inputs, padded_outputs, list(task_ids) def _load_data(self, data_source): logger.debug(f"loading data source type: {type(data_source)}") isinstance(data_source, list): return self._process_list_data(data_source) elif isinstance(data_source, taskset): return self._process_arckit_data(data_source) elif isinstance(data_source, str): os.path.isdir(data_source): logger.debug(f"loading data directory: {data_source}") return self._load_directory(data_source) elif os.path.isfile(data_source): logger.debug(f"loading data file: {data_source}") # since synthetic data preloaded passed via 'all_synthetic_data', avoid reloading # instead, assume 'all_synthetic_data' contains necessary datasets samples = self._process_single_file_parallel(data_source) return samples else: raise valueerror(f"invalid data source path: {data_source}") else: raise valueerror(f"unsupported data_source type: {type(data_source)}") def _load_directory(self, directory_path: str) -&gt; list[dict]: """ loads json files specified directory processes them. args: directory_path (str): path directory containing json files. returns: list[dict]: list processed samples json files directory. """ all_samples = [] file_paths = [] # collect json file paths root, _, files os.walk(directory_path): file files: file.endswith('.json'): file_path = os.path.join(root, file) file_paths.append(file_path) logger.debug(f"queued file processing: {file_path}") # define number threads (adjust based system's resources) max_workers = min(32, os.cpu_count() + 4) # example adjustment threadpoolexecutor(max_workers=max_workers) executor: # submit file processing tasks executor future_to_file = {executor.submit(self._process_single_file_streaming, fp): fp fp file_paths} tqdm(total=len(file_paths), desc="loading synthetic data", unit="file") pbar: future as_completed(future_to_file): fp = future_to_file[future] try: samples = future.result() all_samples.extend(samples) logger.debug(f"completed processing file: {fp} {len(samples)} samples") except exception e: logger.error(f"error processing file {fp}: {e}", exc_info=true) pbar.update(1) logger.debug(f"loaded {len(all_samples)} samples directory {directory_path}") return all_samples def _load_single_file(self, file_path: str) -&gt; list[dict]: """ loads processes single json file. args: file_path (str): path json file. returns: list[dict]: list processed samples json file. """ try: return self._process_single_file_streaming(file_path) except exception e: logger.error(f"error processing file {file_path}: {e}", exc_info=true) return []</file><file name="src/training/trainer.py"># gpt2_arc/src/training/trainer.py import pytorch_lightning pl import torch import logging torch import nn, optim import time typing import any, dict, optional collections import deque torch.optim.lr_scheduler import lambdalr ..config import config gpt2_arc.src.utils.training_helpers import get_num_workers gpt2_arc.src.utils.helpers import differential_pixel_accuracy ..utils.results_collector import resultscollector torch.utils.data import dataloader gpt2_arc.src.data.arc_dataset import arcdataset logger = logging.getlogger(__name__) torch.utils.tensorboard import summarywriter pytorch_lightning.loggers import tensorboardlogger gpt2_arc.src.utils.training_helpers import get_num_workers import os optuna.exceptions import trialpruned pytorch_lightning.callbacks import callback import torch logger = logging.getlogger(__name__) class nanlosspruningcallback(callback): def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx): # extract loss outputs loss = outputs.get('loss') isinstance(outputs, dict) else outputs loss none: torch.isnan(loss) torch.isinf(loss): logger.warning(f"invalid loss detected epoch {trainer.current_epoch}, batch {batch_idx}: {loss.item()}") raise trialpruned("invalid loss encountered, pruning trial.") class arctrainer(pl.lightningmodule): def __init__(self, model, train_dataset, val_dataset, config: config, args, compile_model: bool = true, results_collector=none, test_dataset=none): logger.debug("initializing arctrainer") super().__init__() logger.debug(f"arctrainer received args.accelerator: {args.accelerator}") self.model = model # determine device type based model's parameters device = next(self.model.parameters()).device logger.debug(f"arctrainer initialization device: {device}") compile_model args.fast_dev_run device.type != "cpu": logger.info("compiling model torch.compile improved performance.") self.model = torch.compile(self.model, mode="reduce-overhead") else: logger.info("torch.compile applied (compile_model=false, fast_dev_run=true, using cpu).") logger.debug(f"model device: {device}") self.train_dataset = train_dataset self.val_dataset = val_dataset self.config = config self.test_dataset = test_dataset # add line self.batch_size = config.training.batch_size self.lr = config.training.learning_rate self.train_losses = [] self.logged_metrics = {} self.test_outputs = [] # initialize empty list store test outputs self.test_results = [] # initialize test results storing test outcomes self.best_val_loss = float('inf') self.best_epoch = 0 self.results_collector = results_collector results_collector else resultscollector(config) self.writer = summarywriter(f"runs/experiment_{self.results_collector.experiment_id}") self.args = args # add line assign args def train_dataloader(self): logger.info("creating training dataloader centralized num_workers") self.config.training.balance_symbols: self.config.training.balancing_method == "weighting": # compute class weights (inverse frequencies) class_weights = 1.0 / torch.tensor( list(self.config.training.symbol_freq.values()), dtype=torch.float ) train_loader = dataloader( self.train_dataset, batch_size=self.config.training.batch_size, num_workers=get_num_workers(self.config.training), sampler=self.train_dataset.sampler, # use sampler instead shuffle pin_memory=true self.args.use_gpu else false, prefetch_factor=self.config.training.prefetch_factor, persistent_workers=self.config.training.persistent_workers, collate_fn=self.train_dataset.dataset.collate_fn isinstance(self.train_dataset, torch.utils.data.subset) else arcdataset.collate_fn ) elif self.config.training.balancing_method == "oversampling": # placeholder oversampling implementation logger.info("oversampling method selected, yet implemented.") train_loader = dataloader( self.train_dataset, batch_size=self.config.training.batch_size, num_workers=get_num_workers(self.config.training), shuffle=true, # enable shuffle using sampler pin_memory=true self.args.use_gpu else false, prefetch_factor=self.config.training.prefetch_factor, persistent_workers=self.config.training.persistent_workers, collate_fn=self.train_dataset.dataset.collate_fn isinstance(self.train_dataset, torch.utils.data.subset) else self.train_dataset.collate_fn ) else: logger.warning(f"unknown balancing method: {self.config.training.balancing_method}. skipping balancing.") train_loader = dataloader( self.train_dataset, batch_size=self.config.training.batch_size, num_workers=get_num_workers(self.config.training), shuffle=true, # enable shuffle pin_memory=true self.args.use_gpu else false, prefetch_factor=self.config.training.prefetch_factor, persistent_workers=self.config.training.persistent_workers, collate_fn=self.train_dataset.dataset.collate_fn isinstance(self.train_dataset, torch.utils.data.subset) else self.train_dataset.collate_fn ) else: train_loader = dataloader( self.train_dataset, batch_size=self.config.training.batch_size, num_workers=get_num_workers(self.config.training), shuffle=true, # enable shuffle pin_memory=self.config.training.pin_memory, prefetch_factor=self.config.training.prefetch_factor, persistent_workers=self.config.training.persistent_workers, collate_fn=self.train_dataset.dataset.collate_fn isinstance(self.train_dataset, torch.utils.data.subset) else self.train_dataset.collate_fn ) logger.debug(f"training dataloader created num_workers={get_num_workers(self.config.training)}") return train_loader def val_dataloader(self): logger.debug("entering arctrainer.val_dataloader") collate_fn = self.val_dataset.dataset.collate_fn isinstance(self.val_dataset, torch.utils.data.subset) else arcdataset.collate_fn dataloader = dataloader( self.val_dataset, batch_size=self.config.training.batch_size, num_workers=self.config.training.num_workers, # updated num_workers pin_memory=self.config.training.pin_memory, # updated pin_memory prefetch_factor=self.config.training.prefetch_factor, persistent_workers=self.config.training.persistent_workers, collate_fn=collate_fn ) logger.debug("exiting arctrainer.val_dataloader") return dataloader def test_dataloader(self): self.test_dataset none: logger.error("test dataset provided. please ensure test dataset correctly loaded.") raise valueerror("test dataset provided.") collate_fn = self.test_dataset.dataset.collate_fn isinstance(self.test_dataset, torch.utils.data.subset) else arcdataset.collate_fn return dataloader( self.test_dataset, batch_size=self.config.training.batch_size, num_workers=get_num_workers(self.config.training), shuffle=false, pin_memory=self.config.training.pin_memory, prefetch_factor=self.config.training.prefetch_factor, persistent_workers=self.config.training.persistent_workers, collate_fn=collate_fn ) def get_tensorboard_logger(self): logger self.trainer.loggers: isinstance(logger, tensorboardlogger): return logger.experiment logger.debug("debug: tensorboardlogger found trainer.loggers") return none def training_step(self, batch, batch_idx): logger.debug(f"starting training step {batch_idx}") inputs, targets, _ = batch logger.debug(f" inputs shape: {inputs.shape}, dtype: {inputs.dtype}") logger.debug(f" targets shape: {targets.shape}, dtype: {targets.dtype}") outputs = self(inputs) logger.debug(f" outputs shape: {outputs.shape}, dtype: {outputs.dtype}") loss = self.compute_loss(outputs, targets) logger.debug(f"loss computed: {loss.item()}") preds = torch.argmax(outputs, dim=-1) logger.debug(f" preds shape: {preds.shape}, dtype: {preds.dtype}") # reshape preds match targets necessary try: preds = preds.view(targets.shape) logger.debug(f" reshaped preds match targets shape: {preds.shape}") except runtimeerror e: logger.error(f" failed reshape preds: {e}") raise e accuracy = (preds == targets).float().mean() logger.debug(f" training accuracy: {accuracy.item()}") self.log('train_loss', loss, on_step=true, on_epoch=true, prog_bar=true, logger=true) self.log('train_accuracy', accuracy, on_step=true, on_epoch=true, prog_bar=true, logger=true) return loss def validation_step(self, batch, batch_idx): logger.debug(f"starting validation step {batch_idx}") inputs, targets, _ = batch logger.debug(f"validation inputs shape: {inputs.shape}, targets shape: {targets.shape}, targets dtype: {targets.dtype}") logger.debug(f"validation batch input shape: {batch[0].shape}, validation batch target shape: {batch[1].shape}") targets = targets.long() # ensure targets type long logger.debug(f"targets dtype casting: {targets.dtype}") outputs = self(inputs) logger.debug(f"validation outputs shape: {outputs.shape}") loss = self.compute_loss(outputs, targets) logger.debug(f"validation loss computed: {loss.item()}") preds = torch.argmax(outputs, dim=-1) logger.debug(f" preds shape reshape: {preds.shape}, dtype: {preds.dtype}") # reshape preds match targets shape using view_as safety try: preds = preds.view_as(targets) logger.debug(f" reshaped preds match targets shape: {preds.shape}, dtype: {preds.dtype}") except runtimeerror e: logger.error(f" failed reshape preds: {e}") raise e accuracy = (preds == targets).float().mean() logger.debug(f" validation accuracy: {accuracy.item()}") self.log('val_loss', loss, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log('val_accuracy', accuracy, on_step=false, on_epoch=true, prog_bar=true, logger=true) return loss def on_test_epoch_start(self): self.test_outputs = [] # clear previous test outputs def test_step(self, batch, batch_idx): logger.debug(f"debug: test_step input - batch: {batch}, batch_idx: {batch_idx}") logger.debug(f"test batch input shape: {batch[0].shape}, test batch target shape: {batch[1].shape}") logger.debug(f"debug: test step - batch type: {type(batch)}, length: {len(batch)}") # unpack batch len(batch) == 3: inputs, outputs, task_ids = batch elif len(batch) == 2: inputs, outputs = batch logger.error("batch contain 'task_ids'. ensure dataset provides 'task_ids'.") raise valueerror("batch contain 'task_ids'. ensure dataset provides 'task_ids'.") else: raise valueerror(f"unexpected batch format length {len(batch)}") logger.debug(f"received task_ids: {task_ids}") inputs = inputs.float() outputs = outputs.long() # ensure outputs type long attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32, device=inputs.device) model_outputs = self(inputs, attention_mask) loss = self.compute_loss(model_outputs, outputs) accuracies = [] diff_accuracies = [] # compute batch-wise accuracy accuracy = self.compute_accuracy(model_outputs, outputs) diff_accuracy = self.compute_diff_accuracy(inputs, outputs, model_outputs) # append batch metrics accuracies.append(accuracy) diff_accuracies.append(diff_accuracy) logger.debug(f"debug: batch accuracy: {accuracy}, batch diff_accuracy: {diff_accuracy}") result = { 'test_loss': loss.item(), 'task_ids': task_ids, 'test_accuracy': sum(accuracies) / len(accuracies) accuracies else 0, 'test_diff_accuracy': sum(diff_accuracies) / len(diff_accuracies) diff_accuracies else 0, } logger.debug(f"debug: test loss: {result['test_loss']}, avg accuracy: {result['test_accuracy']}, avg diff accuracy: {result['test_diff_accuracy']}") # log task-specific metrics task_ids available task_ids none: # aggregate task-specific metrics across batch task_id set(task_ids): # remove .tolist() task_id == "default_task": logger.error(f"'default_task' detected task_id: {task_id}") raise valueerror(f"'default_task' detected task_id: {task_id}") try: self.writer.add_scalar('test/loss', result['test_loss'], self.current_epoch) self.writer.add_scalar('test/avg_accuracy', result['test_accuracy'], self.current_epoch) self.writer.add_scalar('test/diff_accuracy', result['test_diff_accuracy'], self.current_epoch) logger.debug(f"debug: logged test metrics epoch {self.current_epoch}: loss={result['test_loss']}, avg_accuracy={result['test_accuracy']}, diff_accuracy={result['test_diff_accuracy']}") except exception e: logger.error(f"debug: error logging test step: {str(e)}") logger.debug(f"debug: test_step output - result: {result}") logger.debug(f"debug: test step result: {result}") # add per-task metrics resultscollector i, task_id enumerate(task_ids): task_accuracy = self.compute_accuracy(model_outputs[i], outputs[i]) task_diff_accuracy = self.compute_diff_accuracy(inputs[i], outputs[i], model_outputs[i]) self.results_collector.add_task_specific_result(task_id, { "test_accuracy": task_accuracy, "test_diff_accuracy": task_diff_accuracy }) # append result self.test_outputs self.test_outputs.append(result) return result def on_test_epoch_end(self): total_loss = torch.stack([torch.tensor(x['test_loss']) x self.test_outputs]).mean() all_accuracies = [] all_diff_accuracies = [] per_task_accuracy = {} per_task_diff_accuracy = {} output self.test_outputs: 'test_accuracy' output: all_accuracies.append(output['test_accuracy']) 'test_diff_accuracy' output: all_diff_accuracies.append(output['test_diff_accuracy']) # collect per-task metrics key, value output.items(): key.endswith('_test_accuracy'): per_task_accuracy[key] = value elif key.endswith('_test_diff_accuracy'): per_task_diff_accuracy[key] = value avg_accuracy = sum(all_accuracies) / len(all_accuracies) all_accuracies else 0 avg_diff_accuracy = sum(all_diff_accuracies) / len(all_diff_accuracies) all_diff_accuracies else 0 self.log('avg_test_loss', total_loss, prog_bar=true) self.log('avg_test_accuracy', avg_accuracy, prog_bar=true) self.log('avg_test_diff_accuracy', avg_diff_accuracy, prog_bar=true) print(f"debug: test epoch end - avg loss: {total_loss}, avg accuracy: {avg_accuracy}, avg diff accuracy: {avg_diff_accuracy}") # prepare final metrics including per-task metrics final_metrics = { "best_val_loss": self.best_val_loss, "best_epoch": self.best_epoch, "final_test_loss": total_loss.item(), "final_test_accuracy": avg_accuracy, "final_test_diff_accuracy": avg_diff_accuracy } # retrieve per-task metrics resultscollector include final_metrics task_id, metrics self.results_collector.task_specific_results.items(): final_metrics.update({ f"{task_id}_test_accuracy": metrics.get("test_accuracy", 0.0), f"{task_id}_test_diff_accuracy": metrics.get("test_diff_accuracy", 0.0) }) logger.debug(f"debug: final metrics including per-task metrics: {final_metrics}") self.results_collector.set_final_metrics(final_metrics) def compute_accuracy(self, outputs, targets): predictions = outputs.argmax(dim=-1) # reshape predictions match target shape predictions = predictions.view(targets.size()) # calculate accuracy elements accuracy = (predictions == targets).float().mean() logger.debug(f"debug: compute_accuracy - accuracy: {accuracy.item()}") return accuracy.item() def compute_diff_accuracy(self, inputs, targets, outputs): pad_symbol_idx = self.config.training.pad_symbol_idx # retrieve pad_symbol_idx config predictions = outputs.argmax(dim=-1) diff_accuracy, _, _ = differential_pixel_accuracy(inputs, targets, predictions, pad_symbol_idx=pad_symbol_idx) logger.debug(f"computed differential pixel accuracy (excluding padding tokens): {diff_accuracy}") return diff_accuracy def on_validation_epoch_end(self): # compute average validation loss val_loss = self.trainer.callback_metrics.get('val_loss') val_loss none: avg_val_loss = val_loss.item() else: avg_val_loss = float('inf') # default high value val_loss available # update best_val_loss best_epoch avg_val_loss &lt; self.best_val_loss: self.best_val_loss = avg_val_loss self.best_epoch = self.current_epoch # log validation metrics self.log('val_loss', avg_val_loss) self.log('best_val_loss', self.best_val_loss) self.log('best_epoch', self.best_epoch) # update results collector self.results_collector.update_val_metrics(self.current_epoch, { "avg_loss": avg_val_loss, "best_val_loss": self.best_val_loss, "best_epoch": self.best_epoch }) # log additional information self.log('epoch', self.current_epoch) def configure_optimizers(self): optimizer = optim.adam(self.model.parameters(), lr=self.lr) lr_scheduler = { 'scheduler': optim.lr_scheduler.steplr(optimizer, step_size=1, gamma=0.95), 'name': 'learning_rate', } return [optimizer], [lr_scheduler] def on_fit_end(self): try: self.writer.close() print("debug: tensorboard writer closed successfully.") except exception e: print(f"debug: error closing tensorboard writer: {str(e)}") logger.debug("debug: results saved tensorboard writer closed.") def compute_loss(self, outputs, labels): labels = labels.long() # ensure labels type long loss = self.model.loss_fn( outputs.view(-1, outputs.size(-1)), labels.view(-1) ) logger.debug(f"using model's loss function ignore_index={self.model.loss_fn.ignore_index}") logger.debug(f"computed loss: {loss.item()}") return loss def forward(self, input_ids, attention_mask=none): return self.model(input_ids, attention_mask) def log_hyperparameters(self): hparams = { 'learning_rate': self.config.training.learning_rate, 'batch_size': self.config.training.batch_size, 'n_embd': self.config.model.n_embd, 'n_head': self.config.model.n_head, 'n_layer': self.config.model.n_layer, } metric_dict = { 'train_loss': 0, 'val_loss': 0, 'test_accuracy': 0, } try: self.writer.add_hparams(hparams, metric_dict) print(f"debug: successfully logged hyperparameters: {hparams}") except exception e: print(f"debug: error logging hyperparameters: {str(e)}")</file><file name="src/training/__init__.py" /><file name="src/training/train.py"># gpt2_arc/src/training/train.py import argparse import logging logging.basicconfig( level=logging.debug, # set logging level debug format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' ) logger = logging.getlogger(__name__) typing import optional import multiprocessing import sys import logging import os import json import datetime unittest.mock import magicmock, patch import optuna import arckit import numpy np import torch concurrent.futures import threadpoolexecutor lightning.pytorch.profilers import pytorchprofiler pytorch_lightning.callbacks import callback torch.profiler import profileractivity torch.utils.data import dataloader, weightedrandomsampler, subset import concurrent.futures import random tqdm import tqdm # define base directory arc-neural-reasoning-model arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")) # add root directory project pythonpath project_root = arc_model_dir sys.path.insert(0, project_root) import pytorch_lightning pl #import torch.autograd.profiler profiler import torch pytorch_lightning.callbacks import modelcheckpoint pytorch_lightning.loggers import tensorboardlogger gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.training.trainer import arctrainer, get_num_workers logger = logging.getlogger(__name__) logger.debug(f"imported get_num_workers training_helpers: {get_num_workers}") gpt2_arc.src.utils.experiment_tracker import experimenttracker gpt2_arc.src.utils.results_collector import resultscollector gpt2_arc.src.utils import grokfastcallback logger = logging.getlogger(__name__) class configsavingmodelcheckpoint(modelcheckpoint): def __init__(self, config, trial_num='na', task_id='na', iter_num='na', *args, **kwargs): super().__init__(*args, **kwargs) self.config = config self.trial_num = trial_num self.task_id = task_id self.iter_num = iter_num self.timestamp = datetime.datetime.now().strftime("%y%m%dt%h%m%s") # e.g., 20240308t153045 def on_save_checkpoint(self, trainer, pl_module, checkpoint): # add custom metadata checkpoint checkpoint['model_config'] = self.config.model.__dict__ checkpoint['trial_num'] = self.trial_num checkpoint['task_id'] = self.task_id checkpoint['iter_num'] = self.iter_num checkpoint['timestamp'] = self.timestamp # add current epoch checkpoint checkpoint['epoch'] = trainer.current_epoch super().on_save_checkpoint(trainer, pl_module, checkpoint) def format_checkpoint_name(self, metrics): """ override method include custom placeholders filename. """ return self.filename.format( trial_num=self.trial_num, task_id=self.task_id, iter_num=self.iter_num, val_loss=metrics.get("val_loss", 0.0), epoch=metrics.get("epoch", 0), timestamp=self.timestamp ) class modelconfigsaver(callback): def __init__(self, config): """ initialize modelconfigsaver callback current configuration. args: config (config): configuration object containing model parameters. """ super().__init__() self.config = config def on_save_checkpoint(self, trainer, pl_module, checkpoint): """ override checkpoint saving include model configuration. args: trainer (pl.trainer): trainer instance. pl_module (pl.lightningmodule): lightningmodule trained. checkpoint (dict): checkpoint dictionary modified. """ checkpoint['model_config'] = self.config.model.__dict__ def prepare_val_or_test_data(eval_set, args, is_validation=true): """ prepare validation test data arckit evaluation set. args: eval_set: evaluation taskset arckit.load_data(). args: parsed command-line arguments. is_validation: boolean indicating whether validation data. returns: list dictionaries keys 'input', 'output', 'task_id'. """ logger.debug(f"preparing {'validation' is_validation else 'test'} data arckit evaluation set") samples = [] task tqdm(eval_set.tasks, desc=f"processing tasks {'validation' is_validation else 'test'} dataset"): ex task.train is_validation else task.test: sample = {'input': ex[0], 'output': ex[1], 'task_id': task.id} samples.append(sample) logger.debug(f"prepared {len(samples)} samples {'validation' is_validation else 'test'} dataset") return samples def load_dataset(args, config, dataset_type='train', all_synthetic_data=none): logger.debug(f"load_dataset called dataset_type='{dataset_type}', args.use_synthetic_data={args.use_synthetic_data}") dataset_type.lower() == 'train': dataset = all_synthetic_data['train_dataset'] args.use_synthetic_data else arcdataset( data_source=arckit.load_data()[0], is_test=false, max_samples=args.max_train_samples, num_symbols=config.training.num_symbols, pad_symbol_idx=config.training.pad_symbol_idx, symbol_freq=config.training.symbol_freq args.enable_symbol_freq else none ) else: _, eval_set = arckit.load_data() data_source = prepare_val_or_test_data(eval_set, args, is_validation=(dataset_type.lower() == 'val')) dataset = arcdataset( data_source=data_source, is_test=(dataset_type.lower() == 'test'), num_symbols=config.training.num_symbols, pad_symbol_idx=config.training.pad_symbol_idx, symbol_freq=config.training.symbol_freq args.enable_symbol_freq else none ) logger.debug(f"{dataset_type.capitalize()} dataset loaded {len(dataset)} samples") return dataset def load_and_split_synthetic_data(args, config): """ load synthetic data using arcdataset. returns dictionary containing 'train_dataset'. args: args: parsed command-line arguments. config: configuration object. returns: dict: {'train_dataset': synthetic_train_dataset} """ logger.debug("entering load_and_split_synthetic_data function") synthetic_dataset = arcdataset( data_source=args.synthetic_data_path, is_test=false, max_samples=args.max_train_samples, # pass new argument num_symbols=config.training.num_symbols, pad_symbol_idx=config.training.pad_symbol_idx, symbol_freq=config.training.symbol_freq args.enable_symbol_freq else none ) total_samples = len(synthetic_dataset) logger.debug(f"total synthetic samples loaded: {total_samples}") return {'train_dataset': synthetic_dataset} def main(args): args.use_synthetic_data args.synthetic_data_path: raise valueerror("--synthetic_data_path must provided using synthetic data.") total_split = args.train_split + args.val_split + args.test_split abs(total_split - 1.0) &lt; 1e-6: raise valueerror("train, validation, test splits must sum 1.0") torch.set_float32_matmul_precision(args.matmul_precision) logger.info(f"set float32 matmul precision to: {args.matmul_precision}") logger.debug(f"command line arguments: {args}") log_level = getattr(logging, args.log_level.upper() hasattr(args, 'log_level') else 'debug', logging.debug) logging.basicconfig( level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' ) profiler = pytorchprofiler( dirpath=args.profiler_dirpath, filename=args.profiler_filename, activities=[profileractivity.cpu, profileractivity.cuda], # include cuda activities record_shapes=true, with_stack=true # enable stack tracing ) args.use_profiler else none logger.setlevel(logging.debug) # ensure logger set debug logger.info("starting main function") logger.debug("initializing pytorch lightning trainer") logger.debug(f"command line arguments: {args}") trainer = none # initialize trainer none try: args.use_optuna: logger.info("loading best hyperparameters optuna study") study_name = args.optuna_study_name study_name none: # retrieve study summaries storage study_summaries = optuna.get_all_study_summaries(storage=args.optuna_storage) study_names = [summary.study_name summary study_summaries] len(study_names) == 1: study_name = study_names[0] logger.info(f"automatically selected available study: {study_name}") elif len(study_names) == 0: logger.error("no studies found specified optuna storage.") sys.exit(1) else: logger.error("multiple studies found specified optuna storage. please specify study name using --optuna-study-name.") sys.exit(1) study = optuna.load_study(study_name=study_name, storage=args.optuna_storage) best_params = study.best_params logger.debug(f"loaded best parameters: {best_params}") n_head = 2 ** best_params['n_head_exp'] n_embd = n_head * best_params['n_embd_multiplier'] n_embd = 2 ** int(np.log2(n_embd)) model_config = modelconfig( n_embd=n_embd, n_head=n_head, n_layer=best_params['n_layer'], dropout=best_params['dropout'], num_workers=args.num_workers args.num_workers none else multiprocessing.cpu_count(), prefetch_factor=args.prefetch_factor, persistent_workers=not args.no_persistent_workers, pin_memory=not args.no_pin_memory, ) training_config = trainingconfig( batch_size=best_params['batch_size'], learning_rate=best_params['learning_rate'], max_epochs=args.max_epochs, use_gpu=args.use_gpu, log_level=args.log_level, use_synthetic_data=args.use_synthetic_data, synthetic_data_path=args.synthetic_data_path, include_pad_in_loss=args.include_pad_in_loss, include_pad_in_accuracy=args.include_pad_in_accuracy, num_workers=args.num_workers, prefetch_factor=args.prefetch_factor, persistent_workers=not args.no_persistent_workers, pin_memory=args.pin_memory ) training_config = trainingconfig( batch_size=best_params['batch_size'], learning_rate=best_params['learning_rate'], max_epochs=args.max_epochs, # always use user-provided max_epochs ) else: logger.info("using provided default hyperparameters") model_config = modelconfig( n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer, dropout=args.dropout, mamba_ratio=args.mamba_ratio, d_state=args.d_state, d_conv=args.d_conv, mamba_depth=args.mamba_depth, mamba_expand=args.mamba_expand, ) training_config = trainingconfig( batch_size=args.batch_size, learning_rate=args.learning_rate, max_epochs=args.max_epochs, use_gpu=args.use_gpu, log_level=args.log_level, use_synthetic_data=args.use_synthetic_data, synthetic_data_path=args.synthetic_data_path, use_grokfast=args.use_grokfast, grokfast_type=args.grokfast_type, grokfast_alpha=args.grokfast_alpha, grokfast_lamb=args.grokfast_lamb, grokfast_window_size=args.grokfast_window_size, ) config = config(model=model_config, training=training_config) logger.debug(f"configuration: {config}") # validate split ratios sum 1.0 total_split = args.train_split + args.val_split + args.test_split abs(total_split - 1.0) &lt; 1e-6: raise valueerror("train, validation, test splits must sum 1.0") args.use_synthetic_data: # load split synthetic data all_synthetic_data = load_and_split_synthetic_data(args, config) else: all_synthetic_data = none # sequentially load datasets avoid memory allocation issues logger.info("loading datasets sequentially avoid memory allocation issues") try: train_data = load_dataset(args, config, dataset_type='train', all_synthetic_data=all_synthetic_data) val_data = load_dataset(args, config, dataset_type='val') # removed all_synthetic_data test_data = load_dataset(args, config, dataset_type='test') # removed all_synthetic_data except exception e: logger.error(f"error loading datasets: {e}") raise # log source dataset logger.info(f"training dataset source: {'synthetic data' args.use_synthetic_data else 'official arc data'}") logger.info(f"validation dataset source: official arc data") logger.info(f"test dataset source: official arc data") # log number samples dataset logger.debug(f"number training samples: {len(train_data)}") logger.debug(f"number validation samples: {len(val_data)}") logger.debug(f"number test samples: {len(test_data)}") args.enable_symbol_freq: logger.debug("calculating symbol frequencies enabled.") args.use_synthetic_data: logger.debug("calculating symbol frequencies synthetic training set") symbol_freq = train_data.get_symbol_frequencies() else: logger.debug("calculating symbol frequencies arc training set") symbol_freq = train_data.get_symbol_frequencies() symbol_freq_dict = {i: float(freq) i, freq enumerate(symbol_freq)} pad_symbol_idx = config.training.pad_symbol_idx symbol_freq_dict.pop(pad_symbol_idx, none) logger.debug(f"removed pad_symbol_idx ({pad_symbol_idx}) symbol_freq_dict. new length: {len(symbol_freq_dict)}") assert len(symbol_freq_dict) == config.training.num_classes - 1, ( f"length symbol_freq_dict ({len(symbol_freq_dict)}) match num_classes minus padding ({config.training.num_classes - 1})." ) balance_symbols = true balancing_method = "weighting" else: symbol_freq_dict = {} logger.debug("symbol frequency calculation disabled. using empty symbol_freq_dict.") balance_symbols = false balancing_method = "none" training_config = trainingconfig( batch_size=args.batch_size, learning_rate=args.learning_rate, max_epochs=args.max_epochs, use_grokfast=args.use_grokfast, grokfast_type=args.grokfast_type, grokfast_alpha=args.grokfast_alpha, grokfast_lamb=args.grokfast_lamb, grokfast_window_size=args.grokfast_window_size, include_pad_in_loss=args.include_pad_in_loss, symbol_freq=symbol_freq_dict, balance_symbols=balance_symbols, balancing_method=balancing_method, num_workers=args.num_workers, prefetch_factor=args.prefetch_factor, persistent_workers=not args.no_persistent_workers, pin_memory=args.pin_memory ) config = config(model=model_config, training=training_config) # calculate symbol frequencies enabled args.enable_symbol_freq: args.use_synthetic_data: logger.debug("calculating symbol frequencies synthetic training set") train_symbol_freq = train_data.get_symbol_frequencies() else: logger.debug("calculating symbol frequencies arc training set") train_symbol_freq = train_data.get_symbol_frequencies() else: train_symbol_freq = {} # set number classes based trainingconfig num_classes = config.training.num_classes logger.info(f"number classes set to: {num_classes}") logger.info("creating dataloader instances") torch.utils.data import subset # ensure test_data none assert test_data none, "test dataset none loading." # initialize model logger.info("initializing model") model = gpt2arc( config=config, num_classes=config.training.num_classes, # use num_classes config symbol_freq=symbol_freq_dict, pad_symbol_idx=config.training.pad_symbol_idx ) logger.debug(f"model initialized config: {model_config}") # load checkpoint specified args.model_checkpoint: logger.info(f"loading model checkpoint: {args.model_checkpoint}") checkpoint = torch.load(args.model_checkpoint) 'model_config' checkpoint 'training_config' checkpoint: model_config = modelconfig(**checkpoint['model_config']) training_config = trainingconfig(**checkpoint['training_config']) config = config(model=model_config, training=training_config) num_classes = config.training.num_classes symbol_freq_dict = config.training.symbol_freq model = gpt2arc(config=config, num_classes=num_classes, symbol_freq=symbol_freq_dict, pad_symbol_idx=config.training.pad_symbol_idx) logger.debug(f"loaded trainingconfig num_classes={num_classes} checkpoint") else: logger.error("checkpoint missing 'model_config' 'training_config'.") raise keyerror("checkpoint must contain 'model_config' 'training_config'.") model.load_state_dict(checkpoint['state_dict']) # log dataset source information args.use_synthetic_data: logger.info("using synthetic data training, validation, testing.") else: logger.info("using official arc datasets training, validation, testing.") results_collector = resultscollector(config) # initialize experiment tracker tracker = experimenttracker(config, project=args.project) logger.info("initializing arctrainer") trainer = arctrainer( model=model, train_dataset=train_data, val_dataset=val_data, config=config, args=args, results_collector=results_collector, test_dataset=test_data ) trainer.log_hyperparameters() # determine accelerator parameters based --accelerator argument args.accelerator == "tpu": accelerator = 'tpu' devices = 'xla:1' # use 'xla:8' tpu v3-8 pods strategy = 'tpu_spawn' # recommended strategy tpu elif args.accelerator == "gpu": torch.cuda.is_available(): accelerator = 'gpu' devices = 1 else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' # initialize callbacks list callbacks = [] # initialize grokfastcallback enabled config.training.use_grokfast: grokfast_callback = grokfastcallback( filter_type=config.training.grokfast_type, # 'ema' 'ma' alpha=config.training.grokfast_alpha, lamb=config.training.grokfast_lamb, window_size=config.training.grokfast_window_size config.training.grokfast_type == 'ma' else 100, # default warmup=true, trigger=false ) callbacks.append(grokfast_callback) logger.info("grokfastcallback added training callbacks.") else: logger.info("grokfast disabled; callback added.") # add standard modelcheckpoint callback args.no_checkpointing: checkpoint_filename = f"{'resume-' args.model_checkpoint else ''}checkpoint-step_{{step}}-val_loss_{{val_loss:.4f}}" checkpoint_callback = modelcheckpoint( dirpath="checkpoints", filename=checkpoint_filename, save_top_k=3, monitor="val_loss", mode="min", ) callbacks.append(checkpoint_callback) # instantiate add modelconfigsaver callback model_config_saver = modelconfigsaver(config) callbacks.append(model_config_saver) logger.info("modelconfigsaver callback added training callbacks.") logger.info("setting pytorch lightning trainer") # define trial_num, task_id, iter_num trial_num = 0 # initialize 0 another appropriate default task_id = "default_task" # replace dynamic task identification necessary iter_num = 1 # initialize 1; increment needed within training loop # removed custom configsavingmodelcheckpoint needed args.no_logging: tb_logger = tensorboardlogger( save_dir="runs", name=f"experiment_{trainer.results_collector.experiment_id}" ) logger.debug(f"tensorboard logger initialized. log dir: {tb_logger.log_dir}") else: tb_logger = false logger.debug("logging disabled") pl_trainer = pl.trainer( max_epochs=config.training.max_epochs, logger=tb_logger, callbacks=callbacks callbacks else none, # includes modelcheckpoint enable_checkpointing=not args.no_checkpointing, enable_progress_bar=not args.no_progress_bar, fast_dev_run=args.fast_dev_run, # use command-line argument gradient_clip_val=1.0, # add gradient clipping precision=16, # enable automatic mixed precision accelerator=accelerator, devices=devices, strategy=strategy, profiler=profiler, val_check_interval=args.val_check_interval # added line ) tb_logger: trainer.results_collector.set_tensorboard_log_path(tb_logger.log_dir) logger.debug(f"tensorboard log path set results collector: {tb_logger.log_dir}") # log initial memory usage args.use_gpu torch.cuda.is_available(): logger.info(f"initial cuda memory allocated: {torch.cuda.memory_allocated()} bytes") logger.info(f"initial cuda memory reserved: {torch.cuda.memory_reserved()} bytes") logger.info("starting model training") logger.debug("training parameters: ") logger.debug(f"batch size: {config.training.batch_size}, learning rate: {config.training.learning_rate}, max epochs: {config.training.max_epochs}") # train model logger.info("starting model training") # update fit call exclude dataloaders pl_trainer.fit(trainer) # log memory usage training args.use_gpu torch.cuda.is_available(): logger.info(f"cuda memory allocated training: {torch.cuda.memory_allocated()} bytes") logger.info(f"cuda memory reserved training: {torch.cuda.memory_reserved()} bytes") logger.info("model training completed successfully.") # define dataloader test data test_loader = dataloader( test_data, batch_size=config.training.batch_size, shuffle=false, num_workers=config.training.num_workers, pin_memory=config.training.pin_memory ) # training, run test logger.info("starting model evaluation test dataset.") logger.info("running model evaluation") logger.debug("preparing run trainer.test()") test_results = pl_trainer.test(model=trainer, dataloaders=test_loader) test_results: avg_test_loss = sum(result['avg_test_loss'] result test_results) / len(test_results) avg_test_accuracy = sum(result['avg_test_accuracy'] result test_results) / len(test_results) avg_test_diff_accuracy = sum(result['avg_test_diff_accuracy'] result test_results) / len(test_results) logger.info(f"test results - loss: {avg_test_loss:.4f}, accuracy: {avg_test_accuracy:.4f}, diff accuracy: {avg_test_diff_accuracy:.4f}") results = { "avg_test_loss": avg_test_loss, "avg_test_accuracy": avg_test_accuracy, "avg_test_diff_accuracy": avg_test_diff_accuracy, } # add task-specific results result test_results: key, value result.items(): key.endswith('_test_accuracy') key.endswith('_test_diff_accuracy'): results[key] = value trainer.results_collector.set_test_results(results) trainer.results_collector.set_final_metrics({ "best_val_loss": trainer.best_val_loss, "best_epoch": trainer.best_epoch, "final_test_loss": avg_test_loss, "final_test_accuracy": avg_test_accuracy, "final_test_diff_accuracy": avg_test_diff_accuracy }) # save final model configuration logger.info("saving final model configuration") model_path = f"final_model_{trainer.results_collector.experiment_id}.pth" os.makedirs("checkpoints", exist_ok=true) torch.save({ 'state_dict': trainer.model.state_dict(), 'model_config': trainer.config.model.__dict__, 'training_config': trainer.config.training.__dict__, 'pad_symbol_idx': trainer.config.training.pad_symbol_idx, 'symbol_freq': trainer.config.training.symbol_freq }, model_path) trainer.results_collector.set_checkpoint_path(model_path) logger.debug(f"model configuration saved to: {model_path}") # save results logger.info("saving experiment results") os.makedirs("results", exist_ok=true) results_path = f"results/experiment_{trainer.results_collector.experiment_id}.json" trainer.results_collector.save_to_json(results_path) logger.debug(f"results saved to: {results_path}") except runtimeerror e: 'cuda memory' str(e): logger.error("cuda memory error occurred.") logger.error("consider reducing batch size model complexity.") raise runtimeerror("cuda memory error occurred.") else: logger.error(f"a runtime error occurred: {str(e)}", exc_info=true) raise runtimeerror(f"a runtime error occurred: {str(e)}") except exception e: logger.error(f"an unexpected error occurred: {str(e)}", exc_info=true) sys.exit(1) # exit program logging error finally: 'tracker' locals(): tracker.finish() trainer none: # ... proceed training ... pass else: logger.error("trainer initialized. exiting training loop.") __name__ == "__main__": parser = argparse.argumentparser(description="train arc neural reasoning model") group = parser.add_mutually_exclusive_group() group.add_argument("--use_profiler", action="store_true", help="enable custom profiler") group.add_argument("--fast_dev_run", action="store_true", help="run fast development test") parser.add_argument( "--optuna_study_name", type=str, default=none, help="name optuna study load. provided one study exists storage, used automatically." ) parser.add_argument("--optuna_storage", type=str, default="sqlite:///optuna_results.db", help="storage url optuna study") parser.add_argument( "--max_train_samples", type=int, default=none, help="maximum number training samples load. use none load samples." ) parser.add_argument( "--num_workers", type=int, default=4, # increased none/1 4 help="number worker threads dataloader. increasing speed data loading." ) parser.add_argument( "--prefetch_factor", type=int, default=2, help="number batches prefetch per worker." ) parser.add_argument( "--no_persistent_workers", action="store_true", help="disable persistent workers dataloader." ) parser.add_argument( "--pin_memory", action="store_true", help="enable pin_memory dataloader faster gpu data transfer." ) parser.set_defaults(pin_memory=true) # enable default using gpu parser.add_argument("--n_embd", type=int, default=12, help="embedding size model.") parser.add_argument("--n_head", type=int, default=1, help="number attention heads profiling") parser.add_argument("--n_layer", type=int, default=1, help="number transformer layers profiling") parser.add_argument("--batch_size", type=int, default=16, help="batch size profiling") # increased 1 16 parser.add_argument("--learning_rate", type=float, default=1e-4, help="learning rate") parser.add_argument("--max_epochs", type=int, required=true, help="maximum number epochs") parser.add_argument("--mamba_ratio", type=float, default=0.0, help="mamba ratio (float value)") parser.add_argument("--dropout", type=float, default=0.05, help="dropout rate") parser.add_argument("--d_state", type=int, default=4, help="mamba state dimension") parser.add_argument("--d_conv", type=int, default=1, help="mamba convolution dimension") parser.add_argument("--mamba_depth", type=int, default=1, help="depth mamba layer") parser.add_argument("--mamba_expand", type=int, default=2, help="expand factor mamba layer") parser.add_argument("--use_gpu", action="store_true", help="use gpu training available") parser.add_argument("--use_grokfast", action="store_true", help="enable grokfast gradient filtering.") parser.add_argument( "--include_pad_in_loss", dest="include_pad_in_loss", action="store_true", help="include padding class loss calculation." ) parser.add_argument( "--no_include_pad_in_loss", dest="include_pad_in_loss", action="store_false", help="exclude padding class loss calculation." ) parser.set_defaults(include_pad_in_loss=true) parser.add_argument( "--grokfast_type", type=str, default="ema", choices=["ema", "ma"], help="type grokfast filter use: 'ema' 'ma'." ) parser.add_argument( "--grokfast_alpha", type=float, default=0.98, help="alpha parameter grokfast-ema." ) parser.add_argument( "--grokfast_lamb", type=float, default=2.0, help="lambda parameter grokfast filters." ) parser.add_argument( "--grokfast_window_size", type=int, default=100, help="window size grokfast-ma." ) parser.add_argument("--no_logging", action="store_true", help="disable logging") parser.add_argument("--no_checkpointing", action="store_true", help="disable checkpointing") parser.add_argument("--no_progress-bar", action="store_true", help="disable progress bar") parser.add_argument("--model_checkpoint", type=str, help="path model checkpoint resume training") parser.add_argument("--project", type=str, default="gpt2-arc", help="w&amp;b project name") parser.add_argument( "--val_check_interval", type=float, default=0.01, help=( "how often perform validation. " "if float, represents fraction epoch (e.g., 0.5 halfway epoch). " "if integer, represents number training steps." ) ) parser.add_argument( "--enable_symbol_freq", action="store_true", help="enable calculation symbol frequencies." ) parser.set_defaults(enable_symbol_freq=false) parser.add_argument("--results_dir", type=str, default="./results", help="directory save results") parser.add_argument("--run_name", type=str, default="default_run", help="name run saving results") parser.add_argument("--use_synthetic_data", action="store_true", help="use synthetic data training") parser.add_argument("--train_split", type=float, default=0.8, help="proportion data use training") parser.add_argument("--val_split", type=float, default=0.1, help="proportion data use validation") parser.add_argument("--test_split", type=float, default=0.1, help="proportion data use testing") parser.add_argument( "--matmul_precision", type=str, default="medium", choices=["highest", "high", "medium"], help="set internal precision float32 matrix multiplications. options: 'highest', 'high', 'medium'. defaults 'medium'." ) parser.add_argument("--synthetic_data_path", type=str, help="path synthetic data directory") parser.add_argument("--log_level", type=str, default="info", help="logging level") parser.add_argument("--use_optuna", action="store_true", help="use best hyperparameters optuna study") parser.add_argument( "--accelerator", type=str, default="gpu", choices=["cpu", "gpu", "tpu"], help="accelerator use training: 'cpu', 'gpu', 'tpu'. defaults 'gpu'." ) parser.add_argument( "--profiler_dirpath", type=str, default="./profiler_logs", help="directory path profiler output files." ) parser.add_argument( "--profiler_filename", type=str, default="profile", help="filename profiler output." ) parser.add_argument( "--include_pad_in_accuracy", type=lambda x: (str(x).lower() ['true', '1', 't', 'y', 'yes']), default=true, help="whether include padding class accuracy calculations. (true/false)" ) args = parser.parse_args() # validate mamba_ratio args.mamba_ratio &lt; 0.0: logger.error("invalid value --mamba_ratio: must non-negative.") sys.exit(1) # validate val_check_interval args.val_check_interval &lt;= 0: logger.error("the --val_check_interval must positive number.") sys.exit(1) main(args)</file><file name="src/models/mamba_block_internal.py">__future__ import annotations import math import torch import torch.nn.functional f einops import einsum, rearrange, repeat torch import tensor, nn bitnet import bitlinearnew zeta.nn.modules.rms_norm import rmsnorm zeta.utils import exists class mambablock(nn.module): """ initialize single mamba block. args: dim (int): input dimension. dim_inner (optional[int]): inner dimension. provided, set dim * expand. depth (int): depth mamba block. d_state (int): state dimension. default 16. expand (int): expansion factor. default 2. dt_rank (union[int, str]): rank temporal difference () tensor. default "auto". d_conv (int): dimension convolutional kernel. default 4. conv_bias (bool): whether include bias convolutional layer. default true. bias (bool): whether include bias linear layers. default false. examples: &gt;&gt;&gt; import torch &gt;&gt;&gt; zeta.nn.modules.simple_mamba import mambablock &gt;&gt;&gt; block = mambablock(dim=64, depth=1) &gt;&gt;&gt; x = torch.randn(1, 10, 64) &gt;&gt;&gt; = block(x) &gt;&gt;&gt; y.shape torch.size([1, 10, 64]) """ def __init__( self, dim: int = none, depth: int = 5, d_state: int = 16, expand: int = 2, d_conv: int = 4, conv_bias: bool = true, bias: bool = false, ): """a single mamba block, described figure 3 section 3.4 mamba paper [1].""" super().__init__() self.dim = dim self.depth = depth self.d_state = d_state self.expand = expand self.d_conv = d_conv self.conv_bias = conv_bias self.bias = bias # dt_rank provided, set ceil(dim / d_state) dt_rank = math.ceil(self.dim / 16) self.dt_rank = dt_rank # dim_inner provided, set dim * expand dim_inner = dim * expand self.dim_inner = dim_inner # dim_inner provided, set dim * expand self.in_proj = bitlinearnew(dim, dim_inner * 2, bias=bias) self.conv1d = nn.conv1d( in_channels=dim_inner, out_channels=dim_inner, bias=conv_bias, kernel_size=d_conv, groups=dim_inner, padding=d_conv - 1, ) # x_proj takes `x` outputs input-specific , b, c self.x_proj = bitlinearnew( dim_inner, dt_rank + self.d_state * 2, bias=false ) # dt_proj projects dt_rank d_in self.dt_proj = bitlinearnew(dt_rank, dim_inner, bias=true) = repeat(torch.arange(1, self.d_state + 1), "n -&gt; n", d=dim_inner) self.a_log = nn.parameter(torch.log(a)) self.d = nn.parameter(torch.ones(dim_inner)) self.out_proj = bitlinearnew(dim_inner, dim, bias=bias) def forward(self, x: tensor): """mamba block forward. looks figure 3 section 3.4 mamba paper [1]. args: x: shape (b, l, d) (see glossary top definitions b, l, d_in, n...) returns: output: shape (b, l, d) official implementation: class mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#l119 mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#l311 """ (b, l, d) = x.shape x_and_res = self.in_proj(x) # shape (b, l, 2 * d_in) x_and_res = rearrange(x_and_res, "b l x -&gt; b x l") (x, res) = x_and_res.split( split_size=[self.dim_inner, self.dim_inner], dim=1 ) x = self.conv1d(x)[:, :, :l] x = f.silu(x) = self.ssm(x) = * f.silu(res) output = self.out_proj(rearrange(y, "b dim l -&gt; b l dim")) return output def ssm(self, x: tensor): """runs ssm. see: - algorithm 2 section 3.2 mamba paper [1] - run_ssm(a, b, c, u) annotated s4 [2] args: x: shape (b, d_in, l) (see glossary top definitions b, l, d_in, n...) returns: output: shape (b, d_in, l) official implementation: mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#l311 """ (d_in, n) = self.a_log.shape # compute b c d, state space parameters. # a, input independent # , b, c input-dependent (this key difference mamba linear time invariant s4) = -torch.exp(self.a_log.float()) # shape (d_in, n) = self.d.float() x_dbl = rearrange(x, "b l -&gt; b l d") x_dbl = self.x_proj(x_dbl) # (b, l, dt_rank + 2*n) (delta, b, c) = x_dbl.split( split_size=[self.dt_rank, n, n], dim=-1 ) # delta: (b, l, dt_rank). b, c: (b, l, n) delta = f.softplus(self.dt_proj(delta)) # (b, l, d_in) = self.selective_scan( x, delta, a, b, c, ) # similar run_ssm(a, b, c, u) annotated s4 [2] return def selective_scan(self, u, delta, a, b, c, d): """does selective scan algorithm. see: - section 2 state space models mamba paper [1] - algorithm 2 section 3.2 mamba paper [1] - run_ssm(a, b, c, u) annotated s4 [2] classic discrete state space formula: x(t + 1) = ax(t) + bu(t) y(t) = cx(t) + du(t) except b c (and step size delta, used discretization) dependent input x(t). args: u: shape (b, d_in, l) (see glossary top definitions b, l, d_in, n...) delta: shape (b, l, d_in) a: shape (d_in, n) b: shape (b, l, n) c: shape (b, l, n) d: shape (d_in,) returns: output: shape (b, d_in, l) official implementation: selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#l86 note: refactored parts `selective_scan_ref` out, functionality match exactly. """ (b, d_in, l) = u.shape n = a.shape[1] # discretize continuous parameters (, a, b) (see section 2 equation 4 mamba paper [1]) # note b parameterized directly deltaa = torch.exp(einsum(delta, a, "b l d_in, d_in n -&gt; b d_in l n")) deltab_u = einsum( delta, b, u, "b l d_in, b l n, b d_in l -&gt; b d_in l n" ) # perform selective scan (see scan_ssm() annotated s4 [2]) x = torch.zeros((b, d_in, n), device=next(self.parameters()).device) ys = [] range(l): x = deltaa[:, :, i] * x + deltab_u[:, :, i] = einsum(x, c[:, i, :], "b d_in n , b n -&gt; b d_in") ys.append(y) = torch.stack(ys, dim=2) # (b d_in l) none: = + u * rearrange(d, "d_in -&gt; d_in 1") return class mamba(nn.module): """mamba model. args: vocab_size (int): size vocabulary. dim (int): input dimension. depth (int): depth mamba block. d_state (int): state dimension. default 16. expand (int): expansion factor. default 2. dt_rank (union[int, str]): rank temporal difference () tensor. default "auto". d_conv (int): dimension convolutional kernel. default 4. examples: x = torch.randint(0, 16, (1, 64)) model = mamba(16, 64, 5, 16) = model(x) print(out) """ def __init__( self, vocab_size: int = none, dim: int = none, depth: int = 5, d_state: int = 16, img_dim: int = 64, *args, **kwargs, ): """full mamba model.""" super().__init__() self.embedding = nn.embedding(vocab_size, dim) self.norm_f = rmsnorm(dim) self.lm_head = bitlinearnew(dim, vocab_size, bias=false) self.lm_head.weight = self.embedding.weight self.mamba_layers = nn.modulelist( [ mambablock( dim=dim, depth=depth, d_state=d_state, *args, **kwargs ) _ range(depth) ] ) # projection img self.img_proj = bitlinearnew(img_dim, dim) def forward( self, x: tensor, context: tensor = none, ): """ args: x (long tensor): shape (b, l) (see glossary top definitions b, l, d_in, n...) returns: logits: shape (b, l, vocab_size) official implementation: class mambalmheadmodel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#l173 """ x = self.embedding(x) exists(context): # project image projected_img = self.img_proj(context) # concatenate image text x = torch.cat([x, projected_img], dim=1) layer self.mamba_layers: x = layer(self.norm_f(x)) + x x = self.norm_f(x) logits = self.lm_head(x) return logits</file><file name="src/models/__init__.py" /><file name="src/models/gpt2.py"># gpt2_arc/src/models/gpt2.py import logging import torch import torch.nn.functional f import pytorch_lightning pl torch import nn typing import dict, optional import torch.nn.init init bitnet import bitlinearnew torch.utils.data import dataloader logger = logging.getlogger(__name__) gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.config import config #from zeta.nn import mambablock gpt2_arc.src.models.mamba_block_internal import mambablock class attention(nn.module): def __init__(self, n_embd, n_head, dropout): super().__init__() self.n_head = n_head self.n_embd = n_embd self.key = bitlinearnew(n_embd, n_embd) self.query = bitlinearnew(n_embd, n_embd) self.value = bitlinearnew(n_embd, n_embd) self.proj = bitlinearnew(n_embd, n_embd) self.dropout = nn.dropout(dropout) # add line logger.debug(f"initialized attention n_embd={n_embd}, n_head={n_head}") def forward(self, x, mask=none): b, t, c = x.size() logger.debug(f"model input shape: {x.shape}") torch._dynamo.is_compiling(): logger.debug(f"attention input shape: {x.shape}") k = self.key(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) q = self.query(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) v = self.value(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))) mask none: att = att.masked_fill(mask[:, none, none, :] == 0, float("-inf")) att = f.softmax(att, dim=-1) # apply dropout attention probabilities att = self.dropout(att) # add line = att @ v = y.transpose(1, 2).contiguous().view(b, t, c) output = self.proj(y) torch._dynamo.is_compiling(): logger.debug(f"attention output shape: {output.shape}") return output class feedforward(nn.module): def __init__(self, n_embd, dropout): super().__init__() self.net = nn.sequential( bitlinearnew(n_embd, 4 * n_embd), nn.relu(), nn.dropout(dropout), bitlinearnew(4 * n_embd, n_embd) ) logger.debug(f"initialized feedforward n_embd={n_embd}") def forward(self, x): torch._dynamo.is_compiling(): logger.debug(f"feedforward input shape: {x.shape}") output = self.net(x) torch._dynamo.is_compiling(): logger.debug(f"feedforward output shape: {output.shape}") return output class transformerblock(nn.module): def __init__(self, n_embd, n_head, dropout): super().__init__() self.attention = attention(n_embd, n_head, dropout) self.feed_forward = feedforward(n_embd, dropout) self.ln1 = nn.layernorm(n_embd) self.ln2 = nn.layernorm(n_embd) self.dropout = nn.dropout(dropout) logger.debug( f"initialized transformerblock n_embd={n_embd}, n_head={n_head}" ) def forward(self, x, mask=none): torch._dynamo.is_compiling(): logger.debug(f"transformerblock input shape: {x.shape}") # attention sublayer residual dropout attn_output = self.attention(self.ln1(x), mask) attn_output = self.dropout(attn_output) # apply dropout x = x + attn_output # feed-forward sublayer residual dropout ff_output = self.feed_forward(self.ln2(x)) ff_output = self.dropout(ff_output) # apply dropout x = x + ff_output torch._dynamo.is_compiling(): logger.debug(f"transformerblock output shape: {x.shape}") logger.debug(f"final output shape: {x.shape}") return x class mambalayer(nn.module): def __init__(self, n_embd, d_state, d_conv, dropout, depth, expand): super().__init__() self.dropout = nn.dropout(dropout) self.mamba_block = mambablock( dim=n_embd, depth=depth, # use depth config d_state=d_state, d_conv=d_conv, expand=expand, # use expand config conv_bias=true, # default value bias=false # default value ) self.layer_norm = nn.layernorm(n_embd) logger.debug( f"initialized mambalayer n_embd={n_embd}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}" ) def forward(self, x): torch._dynamo.is_compiling(): logger.debug(f"mambalayer input shape: {x.shape}") x_norm = self.layer_norm(x) x_mamba = self.mamba_block(x_norm) x_mamba = self.dropout(x_mamba) output = x + x_mamba torch._dynamo.is_compiling(): logger.debug(f"mambalayer output shape: {output.shape}") return output class gpt2arc(pl.lightningmodule): def __init__(self, config: config, num_classes: int, symbol_freq: optional[dict[int, float]] = none, pad_symbol_idx: int = 10): super().__init__() self.example_input_array = torch.zeros(1, 1, 6, 6) # adjust dimensions needed self.config = config self.symbol_freq = symbol_freq symbol_freq none else {} self.pad_symbol_idx = pad_symbol_idx # add line self.include_pad_in_loss = self.config.training.include_pad_in_loss # reintroduced self.conv1 = nn.conv2d( in_channels=1, out_channels=self.config.model.n_embd, # accessing 'model' attribute within config kernel_size=3, padding=1 ).to(torch.float32) # initialize blocks interleaved transformerblocks mambalayer(s) self.blocks = nn.modulelist() num_transformer_blocks = self.config.model.n_layer total_mamba_layers = int(num_transformer_blocks * self.config.model.mamba_ratio) logger.debug(f"total transformerblocks: {num_transformer_blocks}") logger.debug(f"total mambalayers add: {total_mamba_layers}") # distribute mambalayers across transformerblocks mamba_layer_positions = [] total_mamba_layers &gt; 0: step = num_transformer_blocks / total_mamba_layers mamba_layer_positions = [int(i * step) range(total_mamba_layers)] current_mamba_index = 0 layer_idx range(num_transformer_blocks): # add transformerblock self.blocks.append(transformerblock(self.config.model.n_embd, self.config.model.n_head, self.config.model.dropout)) logger.debug(f"layer {len(self.blocks)}: added transformerblock") # check add mambalayer transformerblock current_mamba_index &lt; len(mamba_layer_positions) layer_idx == mamba_layer_positions[current_mamba_index]: # add mambalayer self.blocks.append( mambalayer( n_embd=self.config.model.n_embd, d_state=self.config.model.d_state, d_conv=self.config.model.d_conv, dropout=self.config.model.dropout, depth=self.config.model.mamba_depth, expand=self.config.model.mamba_expand ) ) logger.debug(f"layer {len(self.blocks)}: added mambalayer transformerblock {layer_idx + 1}") current_mamba_index += 1 self.ln_f = nn.layernorm(self.config.model.n_embd) assert isinstance(self.config.model.n_embd, int), "model.n_embd must integer" assert isinstance(num_classes, int), "num_classes must integer" self.fc_out = bitlinearnew(int(self.config.model.n_embd), num_classes) # use num_classes directly # initialize loss function class weights needed self.symbol_freq: # create tensor class weights based symbol frequencies class_weights = torch.tensor([self.symbol_freq.get(i, 1.0) range(num_classes)], dtype=torch.float32) class_weights = class_weights.to(self.device) # ensure weights correct device self.loss_fn = nn.crossentropyloss(weight=class_weights, ignore_index=self.pad_symbol_idx) else: self.loss_fn = nn.crossentropyloss(ignore_index=self.pad_symbol_idx) # initialize weights self.apply(self._init_weights) def _init_weights(self, module): isinstance(module, nn.conv2d): # calculate fan_in conv2d fan_in = module.in_channels * module.kernel_size[0] * module.kernel_size[1] std = 1.0 / fan_in**0.5 init.normal_(module.weight, mean=0.0, std=std) module.bias none: init.zeros_(module.bias) elif isinstance(module, bitlinearnew): fan_in = module.in_features std = 1.0 / fan_in**0.5 init.normal_(module.weight, mean=0.0, std=std) module.bias none: init.zeros_(module.bias) # initialization nn.layernorm, using default def training_step(self, batch, batch_idx): inputs, targets, _ = batch outputs = self(inputs) loss = self.loss_fn(outputs.view(-1, self.config.training.num_classes), targets.view(-1)) preds = torch.argmax(outputs, dim=-1) # accuracy including padding correct_with_pad = (preds == targets).float() total_with_pad = torch.numel(targets) acc_with_pad = correct_with_pad.sum() / total_with_pad total_with_pad &gt; 0 else torch.tensor(0.0) # accuracy excluding padding mask = targets != self.pad_symbol_idx correct_without_pad = (preds == targets).float() * mask total_without_pad = mask.sum() acc_without_pad = correct_without_pad.sum() / total_without_pad total_without_pad &gt; 0 else torch.tensor(0.0) # log accuracies: padding without padding self.log('train_loss', loss, on_step=true, on_epoch=true, prog_bar=true, logger=true) self.log('train_acc_with_pad', acc_with_pad, on_step=true, on_epoch=true, prog_bar=true, logger=true) self.log('train_acc_without_pad', acc_without_pad, on_step=true, on_epoch=true, prog_bar=true, logger=true) return loss def test_dataloader(self): # initialize test dataset test_dataset = arcdataset( data_source=self.config.test_data_path, # ensure path correctly set configuration is_test=true, num_symbols=self.config.training.num_symbols, pad_symbol_idx=self.config.training.pad_symbol_idx, symbol_freq=self.config.training.symbol_freq, debug=self.config.debug ) # create return dataloader logger.debug("entering gpt2arc.test_dataloader") dataloader = dataloader( test_dataset, batch_size=self.config.training.batch_size, num_workers=self.config.training.num_workers, shuffle=false, # typically, shuffling needed test data pin_memory=self.config.training.use_gpu # optimize memory usage based gpu availability ) def validation_step(self, batch, batch_idx): inputs, targets, _ = batch outputs = self(inputs) loss = self.loss_fn(outputs.view(-1, self.config.training.num_classes), targets.view(-1)) preds = torch.argmax(outputs, dim=-1) # accuracy including padding correct_with_pad = (preds == targets).float() total_with_pad = torch.numel(targets) acc_with_pad = correct_with_pad.sum() / total_with_pad total_with_pad &gt; 0 else torch.tensor(0.0) # accuracy excluding padding mask = targets != self.pad_symbol_idx correct_without_pad = (preds == targets).float() * mask total_without_pad = mask.sum() acc_without_pad = correct_without_pad.sum() / total_without_pad total_without_pad &gt; 0 else torch.tensor(0.0) # log accuracies self.log('val_loss', loss, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log('val_acc_with_pad', acc_with_pad, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log('val_acc_without_pad', acc_without_pad, on_step=false, on_epoch=true, prog_bar=true, logger=true) return loss def test_step(self, batch, batch_idx): inputs, targets, _ = batch outputs = self(inputs) loss = self.loss_fn(outputs.view(-1, self.config.training.num_classes), targets.view(-1)) preds = torch.argmax(outputs, dim=-1) # accuracy including padding correct_with_pad = (preds == targets).float() total_with_pad = torch.numel(targets) acc_with_pad = correct_with_pad.sum() / total_with_pad total_with_pad &gt; 0 else torch.tensor(0.0) # accuracy excluding padding mask = targets != self.pad_symbol_idx correct_without_pad = (preds == targets).float() * mask total_without_pad = mask.sum() acc_without_pad = correct_without_pad.sum() / total_without_pad total_without_pad &gt; 0 else torch.tensor(0.0) # log accuracies self.log('test_loss', loss, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log('test_acc_with_pad', acc_with_pad, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log('test_acc_without_pad', acc_without_pad, on_step=false, on_epoch=true, prog_bar=true, logger=true) return loss def forward(self, input_ids, attention_mask=none): logger.debug(f"gpt2arc forward - input shape: {input_ids.shape}, dtype: {input_ids.dtype}") input_ids.dim() == 4: x = input_ids.float() else: batch_size = input_ids.size(0) seq_length = input_ids.size(1) height = width = int(seq_length ** 0.5) x = input_ids.float().view(batch_size, 1, height, width) x = self.conv1(x) logger.debug(f"after conv1 shape: {x.shape}") b, c, h, w = x.size() x = x.view(b, c, h * w) x = x.permute(0, 2, 1) logger.debug(f"reshaped transformer blocks: {x.shape}") i, block enumerate(self.blocks): isinstance(block, transformerblock): x = block(x, attention_mask) logger.debug(f"after transformerblock {i + 1}: shape {x.shape}") else: x = block(x) logger.debug(f"after mambalayer {i + 1}: shape {x.shape}") x = self.ln_f(x) x = self.fc_out(x) logger.debug(f"gpt2arc forward - final output shape: {x.shape}, dtype: {x.dtype}") return x</file></source>