<?xml version='1.0' encoding='utf-8'?>
<source type="local_directory" path="/workspaces/arc-neural-reasoning-model/gpt2_arc"><file name="requirements.txt">absl-py==2.1.0 accelerate==0.33.0 aider-chat==0.59.1 aiohappyeyeballs==2.4.0 aiohttp==3.10.5 aiosignal==1.3.1 alembic==1.13.3 altair==5.3.0 annotated-types==0.7.0 anyio==4.6.0 arckit==0.1.0 argon2-cffi==23.1.0 argon2-cffi-bindings==21.2.0 arrow==1.3.0 asttokens==2.4.1 async-lru==2.0.4 attrs==24.2.0 babel==2.16.0 backoff==2.2.1 beartype==0.18.5 beautifulsoup4==4.12.3 bitnet==0.2.5 bitsandbytes==0.44.1 black==24.8.0 bleach==6.1.0 blinker==1.8.2 bottle==0.13.1 build==1.2.2 cachecontrol==0.14.0 cachetools==5.3.3 certifi==2024.8.30 cffi==1.17.1 chardet==5.2.0 charset-normalizer==3.3.2 cleo==2.1.0 click==8.1.7 colorama==0.4.6 colorlog==6.8.2 colt5-attention==0.10.19 comm==0.2.2 commonmark==0.9.1 configargparse==1.7 contourpy==1.3.0 coverage==7.6.1 crashtest==0.4.1 cryptography==43.0.1 cycler==0.12.1 datasets==2.14.4 debugpy==1.8.5 decorator==5.1.1 defusedxml==0.7.1 diff-match-patch==20230430 dill==0.3.7 diskcache==5.6.3 distlib==0.3.8 distro==1.9.0 docker-pycreds==0.4.0 drawsvg==2.4.0 dulwich==0.21.7 e==1.4.5 einops==0.8.0 einops-exts==0.0.4 einx==0.3.0 entrypoints==0.4 executing==2.1.0 fairscale==0.4.13 fastjsonschema==2.20.0 filelock==3.16.1 flake8==7.1.1 fonttools==4.54.0 fqdn==1.5.1 frozendict==2.4.4 frozenlist==1.4.1 fsspec==2024.9.0 gitdb==4.0.11 gitpython==3.1.43 google-ai-generativelanguage==0.6.2 google-api-core==2.19.0 google-api-python-client==2.128.0 google-auth==2.29.0 google-auth-httplib2==0.2.0 google-generativeai==0.5.2 googleapis-common-protos==1.63.0 greenlet==3.0.3 grep-ast==0.3.3 grpcio==1.63.0 grpcio-status==1.62.2 h11==0.14.0 httpcore==1.0.5 httplib2==0.22.0 httpx==0.27.2 huggingface-hub==0.25.0 hypothesis==6.115.0 idna==3.10 importlib_metadata==7.2.1 importlib_resources==6.4.5 iniconfig==2.0.0 installer==0.7.0 ipykernel==6.29.5 ipython==8.27.0 isoduration==20.11.0 isort==5.13.2 jaraco.classes==3.4.0 jedi==0.19.1 jeepney==0.8.0 jinja2==3.1.4 jiter==0.5.0 joblib==1.4.2 json5==0.9.25 jsonpointer==3.0.0 jsonschema==4.23.0 jsonschema-specifications==2023.12.1 jupyter-events==0.10.0 jupyter-lsp==2.2.5 jupyter-server-mathjax==0.2.6 jupyter_client==8.6.3 jupyter_core==5.7.2 jupyter_server==2.14.2 jupyter_server_terminals==0.5.3 jupyterlab==4.2.5 jupyterlab_git==0.50.1 jupyterlab_pygments==0.3.0 jupyterlab_server==2.27.3 keyring==24.3.1 kiwisolver==1.4.7 libcst==1.1.0 lightning==2.4.0 lightning-utilities==0.11.7 lion-pytorch==0.2.2 litellm==1.47.0 local-attention==1.9.15 loguru==0.7.2 mako==1.3.5 markdown==3.7 markdown-it-py==3.0.0 markupsafe==2.1.5 matplotlib==3.9.2 matplotlib-inline==0.1.7 mccabe==0.7.0 mdurl==0.1.2 memory-profiler==0.61.0 mistune==0.8.4 more-itertools==10.5.0 mpmath==1.3.0 msgpack==1.1.0 multidict==6.1.0 multiprocess==0.70.15 mypy==1.11.2 mypy-extensions==1.0.0 nbclient==0.10.0 nbconvert==6.5.0 nbdime==4.0.2 nbformat==5.4.0 nest-asyncio==1.6.0 networkx==3.2.1 nltk==3.7 notebook_shim==0.2.4 numpy==1.26.4 nvidia-cublas-cu12==12.1.3.1 nvidia-cuda-cupti-cu12==12.1.105 nvidia-cuda-nvrtc-cu12==12.1.105 nvidia-cuda-runtime-cu12==12.1.105 nvidia-cudnn-cu12==9.1.0.70 nvidia-cufft-cu12==11.0.2.54 nvidia-curand-cu12==10.3.2.106 nvidia-cusolver-cu12==11.4.5.107 nvidia-cusparse-cu12==12.1.0.106 nvidia-nccl-cu12==2.20.5 nvidia-nvjitlink-cu12==12.6.77 nvidia-nvtx-cu12==12.1.105 openai==1.47.0 optuna==4.0.0 optuna-dashboard==0.16.2 optuna-integration==4.0.0 overrides==7.7.0 packaging==24.1 pandas==2.2.2 pandocfilters==1.5.1 parso==0.8.4 pathspec==0.12.1 pexpect==4.9.0 pillow==10.4.0 pkginfo==1.11.1 platformdirs==4.3.6 playwright==1.43.0 plotly==5.24.1 pluggy==1.5.0 poetry==1.8.3 poetry-core==1.9.0 poetry-plugin-export==1.8.0 prometheus_client==0.21.0 prompt_toolkit==3.0.47 proto-plus==1.23.0 protobuf==4.25.3 psutil==6.0.0 ptyprocess==0.7.0 pure_eval==0.2.3 pyarrow==16.0.0 pyasn1==0.6.0 pyasn1_modules==0.4.0 pycodestyle==2.12.1 pycparser==2.22 pydantic==2.9.2 pydantic_core==2.23.4 pydeck==0.9.0 pydub==0.25.1 pyee==11.1.0 pyflakes==3.2.0 pygments==2.18.0 pyngrok==7.2.0 pynvml==11.5.3 pypandoc==1.13 pyparsing==3.1.2 pypdf2==2.10.0 pyperclip==1.9.0 pyproject-api==1.6.1 pyproject_hooks==1.2.0 pytest==8.3.2 pytest-cov==5.0.0 pytest-mock==3.14.0 python-dateutil==2.9.0.post0 python-dotenv==1.0.1 python-json-logger==2.0.7 pytorch-lightning==2.4.0 pytz==2024.1 pyyaml==6.0.2 pyzmq==26.2.0 rapidfuzz==3.10.0 referencing==0.35.1 regex==2024.9.11 requests==2.32.3 requests-toolbelt==1.0.0 rfc3339-validator==0.1.4 rfc3986-validator==0.1.1 rich==13.8.1 rpds-py==0.20.0 rsa==4.9 ruff==0.6.9 safetensors==0.4.5 scikit-learn==1.5.2 scipy==1.13.1 seaborn==0.13.2 secretstorage==3.3.3 send2trash==1.8.3 sentencepiece==0.2.0 sentry-sdk==2.15.0 setproctitle==1.3.3 setuptools==75.1.0 shellingham==1.5.4 six==1.16.0 smmap==5.0.1 sniffio==1.3.1 sortedcontainers==2.4.0 sounddevice==0.5.0 soundfile==0.12.1 soupsieve==2.6 sqlalchemy==2.0.35 stack-data==0.6.3 streamlit==1.34.0 sympy==1.12 tenacity==8.3.0 tensorboard==2.18.0 tensorboard-data-server==0.7.2 terminado==0.18.1 threadpoolctl==3.5.0 tiktoken==0.7.0 timm==1.0.9 tinycss2==1.3.0 tokenizers==0.19.1 tokenmonster==1.1.12 toml==0.10.2 tomlkit==0.13.2 toolz==0.12.1 torch==2.4.1 torchdiffeq==0.2.4 torchfix==0.6.0 torchmetrics==1.4.2 torchsummary==1.5.1 torchvision==0.19.1 tornado==6.4 tox==4.15.1 tqdm==4.66.5 traitlets==5.14.3 transformers==4.44.2 tree-sitter==0.21.3 tree-sitter-languages==1.10.2 triton==3.0.0 trove-classifiers==2024.9.12 types-python-dateutil==2.9.0.20240906 typing==3.7.4.3 typing-inspect==0.9.0 typing_extensions==4.12.2 tzdata==2024.1 ultralytics-thop==2.0.8 uri-template==1.3.0 uritemplate==4.1.1 urllib3==2.2.3 vector-quantize-pytorch==1.12.0 virtualenv==20.26.6 wandb==0.18.3 watchdog==5.0.3 wcwidth==0.2.13 webcolors==24.8.0 webencodings==0.5.1 websocket-client==1.8.0 werkzeug==3.0.4 wget==3.2 xxhash==3.5.0 yarl==1.11.1 youtube-transcript-api==0.4.1 zetascale==0.9.1 zipp==3.20.2</file><file name="benchmark.py"># gp2_arc/benchmark.py import sys import os # add project root directory python path sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))) import torch._dynamo import csv import uuid datetime import datetime import os import torch torch.utils.data import dataloader import arckit gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import modelconfig import time torch.amp import autocast import psutil import logging import argparse import statistics import numpy np scipy import stats # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) # dynamically adjustable baseline values cpu, gpu, mps baselines = { 'cpu': {'total_time': 1.6391, 'grids_per_second': 199.27}, 'cuda': {'total_time': 0.0481, 'grids_per_second': 13774.98}, 'mps': {'total_time': 0.0481, 'grids_per_second': 13774.98} # updated baselines mps } def benchmark_model(model, dataset, batch_size=1, num_batches=1, num_runs=1, device_type='cpu', precision='medium', model_checkpoint=none): print(f"starting benchmark_model parameters: batch_size={batch_size}, num_batches={num_batches}, device_type={device_type}, precision={precision}, model_checkpoint={model_checkpoint}") device_type ['cpu', 'cuda', 'mps']: raise valueerror("invalid device type") len(dataset) == 0: raise valueerror("dataset empty") checkpoint_used = false checkpoint_info = {} model_checkpoint: checkpoint = torch.load(model_checkpoint) 'state_dict' checkpoint: state_dict = checkpoint['state_dict'] else: state_dict = checkpoint model.load_state_dict(state_dict, strict=false) model.to(device_type) model.eval() checkpoint_used = true checkpoint_info = { 'state_dict_keys': list(state_dict.keys()) } run_id = str(uuid.uuid4()) current_time = datetime.now().strftime("%y-%m-%d %h:%m:%s") practical_threshold = 20.0 # define threshold practical significance total_time_runs = [] grids_per_second_runs = [] cpu_usages = [] memory_usages = [] run_results = [] # initialize run_results store run's data gpu_usages = [] # initialize gpu_usages store gpu utilization data # set float32 matmul precision torch.set_float32_matmul_precision(precision) # select device based argument (including support mps) device = torch.device("cuda" device_type == "cuda" torch.cuda.is_available() else "mps" device_type == "mps" torch.backends.mps.is_available() else "cpu") model = model.to(device) torch._dynamo.config.suppress_errors = true device.type == "cpu": compiled_model = model # use model directly cpu else: try: device.type != "mps": compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=true) else: compiled_model = model # use model directly mps except importerror e: logger.warning(f"compilation failed error: {e}. falling back eager execution.") compiled_model = model try: dataloader = dataloader(dataset, batch_size=batch_size, collate_fn=arcdataset.collate_fn) total_time = 0.0 total_grids = 0 i, batch enumerate(dataloader): &gt;= num_batches: break print(f"processing batch {i+1}/{num_batches}") logger.debug(f"batch content unpacking: {batch}") len(batch) != 3: raise valueerror(f"unexpected batch format. expected 3 items, got {len(batch)}") inputs, outputs, task_ids = batch print(f"inputs type: {type(inputs)}") hasattr(inputs, 'shape'): print(f"inputs shape: {inputs.shape}") else: print("inputs shape: n/a") print(f"outputs type: {type(outputs)}, shape: {outputs.shape torch.is_tensor(outputs) else 'n/a'}") print(f"task ids: {task_ids}") inputs none isinstance(inputs, torch.tensor): raise valueerror(f"expected inputs torch.tensor, got {type(inputs)}") inputs.numel() == 0: raise valueerror("inputs tensor empty") print(f"inputs shape: {inputs.shape}, outputs shape: {outputs.shape}, task ids: {task_ids}") inputs.dim() == 2: # inputs 2d (batch_size, sequence_length), reshape 4d height = width = int(inputs.size(1)**0.5) inputs = inputs.view(inputs.size(0), 1, height, width) elif inputs.dim() == 3: # inputs 3d (batch_size, height, width), add channel dimension inputs = inputs.unsqueeze(1) elif inputs.dim() != 4: raise valueerror(f"unexpected input dimensions: {inputs.dim()}. expected 2, 3, 4 dimensions.") attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32) inputs, attention_mask = inputs.to(device), attention_mask.to(device) # log system load system state processing batch cpu_percent = psutil.cpu_percent(interval=none) memory_info = psutil.virtual_memory() cpu_usages.append(cpu_percent) memory_usages.append(memory_info.percent) device.type == 'cuda': gpu_utilization = torch.cuda.utilization(device.index) gpu_usages.append(gpu_utilization) logger.info(f"batch {i+1}: cpu usage: {cpu_percent}%, memory usage: {memory_info.percent}%, gpu utilization: {gpu_utilization}%") else: logger.info(f"batch {i+1}: cpu usage: {cpu_percent}%, memory usage: {memory_info.percent}%") # measure time taken process batch start_time = time.time() torch.cuda.is_available(): torch.cuda.synchronize() logger.debug("invoking model inputs attention_mask") torch.no_grad(): device.type == 'cuda': autocast(device_type=device.type, dtype=torch.float16): compiled_model(inputs, attention_mask) else: compiled_model(inputs, attention_mask) torch.cuda.is_available(): torch.cuda.synchronize() end_time = time.time() batch_time = end_time - start_time print(f"batch time: {batch_time}") batch_time &lt;= 0: print(f"warning: invalid batch time: {batch_time}. skipping batch.") continue total_time += batch_time total_grids += len(inputs) except exception e: logger.error(f"an error occurred benchmarking: {e}") raise print(f"benchmark completed. total time: {total_time}, total grids: {total_grids}") # calculate average standard deviation runs num_runs = len(total_time_runs) avg_total_time = np.mean(total_time_runs) std_total_time = np.std(total_time_runs) avg_grids_per_second = np.mean(grids_per_second_runs) std_grids_per_second = np.std(grids_per_second_runs) total_time &gt; 0: grids_per_second = total_grids / total_time else: grids_per_second = 0.0 # avoid division zero logger.warning("total time zero. setting grids_per_second 0.0 avoid division zero.") logger.info(f"total time: {total_time:.4f} seconds, grids per second: {grids_per_second:.2f}") # store results run run_results.append({ 'run_id': run_id, 'datetime': current_time, 'total_time': total_time, 'grids_per_second': grids_per_second, 'cpu_usage': np.mean(cpu_usages), 'memory_usage': np.mean(memory_usages), 'gpu_usage': np.mean(gpu_usages) gpu_usages else none, 'batch_size': batch_size, 'num_batches': num_batches, 'device': device.type, 'n_embd': model.config.n_embd, 'n_head': model.config.n_head, 'n_layer': model.config.n_layer, 'precision': precision, # add precision 'checkpoint_used': checkpoint_used, 'checkpoint_info': checkpoint_info }) total_time_runs.append(total_time) grids_per_second_runs.append(grids_per_second) total_time &lt;= 0 total_grids &lt;= 0: logger.warning(f"error: invalid total time ({total_time}) total grids ({total_grids}). check benchmark implementation.") return 0.0, 0.0 # return sensible defaults instead infinity avg_total_time = total_time avg_grids_per_second = total_grids / total_time total_time &gt; 0 else 0.0 logger.info(f"total time: {avg_total_time:.4f} seconds, grids per second: {avg_grids_per_second:.2f}") # perform statistical analysis (confidence intervals, effect size, etc.) confidence_level = 0.95 z_score = stats.norm.ppf((1 + confidence_level) / 2) ci_total_time = z_score * (std_total_time / np.sqrt(num_runs)) ci_grids_per_second = z_score * (std_grids_per_second / np.sqrt(num_runs)) effect_size_time = (avg_total_time - baselines[device.type]['total_time']) / std_total_time effect_size_grids = (avg_grids_per_second - baselines[device.type]['grids_per_second']) / std_grids_per_second # calculate improvements regressions based averages time_improvement = baselines[device.type]['total_time'] - avg_total_time time_improvement_percent = (time_improvement / baselines[device.type]['total_time']) * 100 time_regression = avg_total_time - baselines[device.type]['total_time'] time_regression_percent = (time_regression / baselines[device.type]['total_time']) * 100 grids_per_second_improvement = avg_grids_per_second - baselines[device.type]['grids_per_second'] grids_per_second_improvement_percent = (grids_per_second_improvement / baselines[device.type]['grids_per_second']) * 100 grids_per_second_regression = baselines[device.type]['grids_per_second'] - avg_grids_per_second grids_per_second_regression_percent = (grids_per_second_regression / baselines[device.type]['grids_per_second']) * 100 # determine improvement improvement_time = avg_total_time &lt; baselines[device.type]['total_time'] improvement_grids = avg_grids_per_second &gt; baselines[device.type]['grids_per_second'] # log improvements regressions based averages avg_total_time &lt; baselines[device.type]['total_time']: logger.info(f"improvement average total time: -{time_improvement:.4f} seconds ({time_improvement_percent:.2f}%)") else: logger.info(f"regression average total time: +{time_regression:.4f} seconds ({time_regression_percent:.2f}%)") avg_grids_per_second &gt; baselines[device.type]['grids_per_second']: logger.info(f"improvement average grids per second: +{grids_per_second_improvement:.2f} ({grids_per_second_improvement_percent:.2f}%)") else: logger.info(f"regression average grids per second: -{grids_per_second_regression:.2f} ({grids_per_second_regression_percent:.2f}%)") # update practical significance checks practical_significance_time = time_improvement_percent &gt;= practical_threshold practical_significance_grids = grids_per_second_improvement_percent &gt;= practical_threshold # log practical significance improvement_time: practical_significance_time: logger.info("the improvement average total time practically significant.") else: logger.info("the improvement average total time practically significant.") else: practical_significance_time: logger.info("the regression average total time practically significant.") else: logger.info("the regression average total time practically significant.") improvement_grids: practical_significance_grids: logger.info("the improvement average grids per second practically significant.") else: logger.info("the improvement average grids per second practically significant.") else: practical_significance_grids: logger.info("the regression average grids per second practically significant.") else: logger.info("the regression average grids per second practically significant.") # perform one-sample t-test t_stat_time, p_value_time = stats.ttest_1samp(total_time_runs, baselines[device.type]['total_time']) t_stat_grids, p_value_grids = stats.ttest_1samp(grids_per_second_runs, baselines[device.type]['grids_per_second']) logger.info(f"t-test total time: t-statistic = {t_stat_time:.4f}, p-value = {p_value_time:.4f}") logger.info(f"t-test grids per second: t-statistic = {t_stat_grids:.4f}, p-value = {p_value_grids:.4f}") # log results including confidence intervals logger.info(f"run summary:") logger.info(f" avg total time: {avg_total_time:.4f}s (ci 95%: {ci_total_time:.4f}s)") logger.info(f" avg grids per second: {avg_grids_per_second:.2f} (ci 95%: {ci_grids_per_second:.2f})") logger.info(f" effect size (total time): {effect_size_time:.4f}, effect size (grids per second): {effect_size_grids:.4f}") # determine improvement improvement_time = avg_total_time &lt; baselines[device.type]['total_time'] improvement_grids = avg_grids_per_second &gt; baselines[device.type]['grids_per_second'] csv_file_path = 'benchmark_results.csv' file_exists = os.path.isfile(csv_file_path) open(csv_file_path, 'a', newline='') csvfile: fieldnames = [ 'run_id', 'datetime', 'run', 'total_time', 'grids_per_second', 'cpu_usage', 'memory_usage', 'batch_size', 'num_batches', 'device', 'n_embd', 'n_head', 'n_layer', 'gpu_usage', 'precision', 'checkpoint_used', 'checkpoint_info' ] writer = csv.dictwriter(csvfile, fieldnames=fieldnames) file_exists: writer.writeheader() result run_results: writer.writerow(result) # write statistical summary csv stats_csv_file_path = 'benchmark_statistics.csv' stats_file_exists = os.path.isfile(stats_csv_file_path) open(stats_csv_file_path, 'a', newline='') csvfile: fieldnames = [ 'run_id', 'datetime', 'avg_total_time', 'std_total_time', 'ci_total_time', 'avg_grids_per_second', 'std_grids_per_second', 'ci_grids_per_second', 'effect_size_time', 'effect_size_grids', 'percent_change_time', 'percent_change_grids', 't_stat_time', 'p_value_time', 't_stat_grids', 'p_value_grids', 'improvement_time', 'improvement_grids', 'practical_significance_time', 'practical_significance_grids', 'precision', 'checkpoint_used', 'checkpoint_info' ] writer = csv.dictwriter(csvfile, fieldnames=fieldnames) stats_file_exists: writer.writeheader() writer.writerow({ 'run_id': run_id, 'datetime': current_time, 'avg_total_time': avg_total_time, 'std_total_time': std_total_time, 'ci_total_time': ci_total_time, 'avg_grids_per_second': avg_grids_per_second, 'std_grids_per_second': std_grids_per_second, 'ci_grids_per_second': ci_grids_per_second, 'effect_size_time': effect_size_time, 'effect_size_grids': effect_size_grids, 'percent_change_time': time_improvement_percent improvement_time else time_regression_percent, 'percent_change_grids': grids_per_second_improvement_percent improvement_grids else grids_per_second_regression_percent, 't_stat_time': t_stat_time, 'p_value_time': p_value_time, 't_stat_grids': t_stat_grids, 'p_value_grids': p_value_grids, 'improvement_time': improvement_time, 'improvement_grids': improvement_grids, 'practical_significance_time': practical_significance_time, 'practical_significance_grids': practical_significance_grids, 'precision': precision, # add precision 'checkpoint_used': checkpoint_used, 'checkpoint_info': checkpoint_info }) print(f"benchmark completed. final results - avg_time: {avg_total_time}, avg_grids: {avg_grids_per_second}") return avg_total_time, avg_grids_per_second def main(args): print(f"starting main function args: {args}") # set float32 matmul precision torch.set_float32_matmul_precision(args.precision) train_set, _ = arckit.load_data() full_dataset = arcdataset(train_set, is_test=false) # create model configuration model_config = modelconfig( n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer, mamba_ratio=args.mamba_ratio, d_state=args.d_state, d_conv=args.d_conv ) model = gpt2arc(model_config, num_classes=args.num_classes) # run benchmark different configurations run_num range(args.num_full_runs): logger.info(f"starting full benchmark run {run_num + 1}/{args.num_full_runs}") avg_time, avg_grids = benchmark_model( model, full_dataset, batch_size=args.batch_size, num_batches=args.num_batches, num_runs=args.num_runs, device_type=args.device, precision=args.precision, model_checkpoint=args.model_checkpoint ) logger.info(f"full run {run_num + 1} - avg time: {avg_time:.4f}s, avg grids per second: {avg_grids:.2f}") __name__ == "__main__": parser = argparse.argumentparser(description="benchmark gpt2arc model.") parser.add_argument('--model_checkpoint', type=str, help='path model checkpoint') parser.add_argument('--num-runs', type=int, default=20, help='number runs configuration') parser.add_argument('--num-full-runs', type=int, default=1, help='number full configurations run') parser.add_argument('--batch-size', type=int, default=32, help='batch size run') parser.add_argument('--num-batches', type=int, default=10, help='number batches per run') parser.add_argument('--n-embd', type=int, default=64, help='number embeddings model') parser.add_argument('--n-head', type=int, default=2, help='number attention heads') parser.add_argument('--n-layer', type=int, default=1, help='number layers') parser.add_argument('--mamba-ratio', type=int, default=7, help='number mamba layers per transformer layer') parser.add_argument('--d-state', type=int, default=16, help='mamba state dimension') parser.add_argument('--d-conv', type=int, default=4, help='mamba convolution dimension') parser.add_argument('--device', choices=['cpu', 'cuda', 'mps'], default='cpu', help='device run benchmark (cpu, cuda, mps)') parser.add_argument('--precision', choices=['highest', 'high', 'medium'], default='highest', help='precision level float32 matrix multiplications') parser.add_argument('--num-classes', type=int, default=10, help='number classes model') args = parser.parse_args() main(args)</file><file name="README.md"># gpt-2 arc neural reasoning model project implements neural reasoning model based gpt-2 architecture solve tasks abstraction reasoning corpus (arc) challenge. ## features - **data handling**: utilizes custom `arcdataset` class handling preprocessing arc data. - **model architecture**: implements `gpt2arc` model leveraging pre-trained gpt-2 architecture. - **training**: includes `train.py` script training model using pytorch lightning, support logging checkpointing. - **testing**: comprehensive test suite using `pytest` ensure model data integrity. ## installation clone repository install required packages: ```bash git clone https://github.com/yourusername/arc-neural-reasoning-model.git cd arc-neural-reasoning-model pip install -e . ``` development, install extra dependencies: ```bash pip install -e ".[dev]" ``` ## usage ### training model train model, use following command: ``` python src/train.py --train_data path/to/train_data --val_data path/to/val_data --batch_size 32 --learning_rate 1e-4 --max_epochs 10 --use_gpu ``` adjust parameters needed. trained model checkpoints saved `checkpoints` directory. ### evaluating model evaluate trained model test set, use following command: ``` python src/evaluate.py --test_data path/to/test_data --model_checkpoint path/to/model_checkpoint.ckpt --batch_size 32 ``` output evaluation metrics model test dataset. ## running tests run tests, use following command: ``` pytest -v ``` run tests display results, including test coverage. ## contributing [add contribution guidelines here] ## license project licensed mit license - see [license](license) file details.</file><file name="setup.py">setuptools import setup setup()</file><file name="scripts/memory_test.py">import sys import os # determine absolute path project root project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')) # add project root python path sys.path.insert(0, project_root) import torch torch.utils.data import dataloader gpt2_arc.src.data.arc_dataset import arcdataset import psutil arckit import load_data import gc memory_profiler import profile import json def get_system_memory_usage(): process = psutil.process(os.getpid()) mem_bytes = process.memory_info().rss # resident set size mem_mb = mem_bytes / (1024 ** 2) # convert mb return mem_mb @profile def initialize_dataset(): # load data using arckit.load_data() training.py train_set, eval_set = load_data() # initialize arcdataset training set dataset = arcdataset( data_source=train_set, is_test=false, num_symbols=10, test_split=0.2, debug=true # enable debug mode needed ) return dataset @profile def main(): # perform garbage collection ensure accurate measurements gc.collect() # measure memory loading dataset mem_before = get_system_memory_usage() print(f"memory loading dataset: {mem_before:.2f} mb") # initialize dataset dataset = initialize_dataset() # measure memory loading dataset mem_after = get_system_memory_usage() print(f"memory loading dataset: {mem_after:.2f} mb") # calculate memory used dataset mem_used = mem_after - mem_before print(f"total memory used dataset: {mem_used:.2f} mb") # calculate per-example memory usage total_samples = len(dataset) total_samples == 0: print("dataset empty.") return per_example_mem = mem_used / total_samples print(f"estimated memory usage per example: {per_example_mem:.2f} kb") # define range batch sizes test batch_sizes = [1, 2, 4, 8, 16, 32] # extend needed all_memory_data = [] batch_size batch_sizes: print(f"\ntesting batch size: {batch_size}") # perform garbage collection batch test gc.collect() # measure memory processing batch mem_before_batch = get_system_memory_usage() # initialize dataloader dataloader = dataloader( dataset, batch_size=batch_size, shuffle=false, num_workers=0, # ensure main process data loading pin_memory=false # disable pin_memory cpu ) # iterate subset batches measure memory num_batches = 10 i, batch enumerate(dataloader): &gt;= num_batches: break inputs, outputs, task_ids = batch # unpack batch # optionally, validate tensor shapes print(f"batch {i+1}: inputs shape: {inputs.shape}, outputs shape: {outputs.shape}") # measure memory processing batch mem_after_batch = get_system_memory_usage() mem_used_batch = mem_after_batch - mem_before_batch memory_records = { "batch_size": batch_size, "memory_used_mb": mem_used_batch } all_memory_data.append(memory_records) print(f"batch size: {batch_size} | memory used: {mem_used_batch:.2f} mb") # save memory usage data json file output_path = os.path.join("gpt2_arc", "memory_usage_results.json") open(output_path, 'w') f: json.dump(all_memory_data, f, indent=4) print(f"\nmemory usage data saved {output_path}") __name__ == "__main__": main()</file><file name="tests/test_gpt2.py"># gpt2_arc/tests/test_gpt2.py import logging import pytest import torch src.config import modelconfig src.models.gpt2 import gpt2arc, attention, feedforward, transformerblock logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def model(): config = modelconfig() return gpt2arc(config) def test_gpt2arc_initialization(model): assert isinstance(model, gpt2arc) assert hasattr(model, "conv1") # check conv1 instead token_embedding assert hasattr(model, "blocks") assert hasattr(model, "ln_f") assert hasattr(model, "config") def test_gpt2arc_forward_pass(model): batch_size = 2 height = 30 width = 30 input_ids = torch.randn(batch_size, 1, height, width) # simulate image-like input attention_mask = torch.ones((batch_size, height * width)) output = model(input_ids, attention_mask) assert isinstance(output, torch.tensor) assert output.shape == (batch_size, height * width, model.config.n_embd) logger.debug(f"output shape: {output.shape}") def test_gpt2arc_output_values(model): logger.debug("testing gpt2arc output values") batch_size = 1 channels = 1 height = 30 width = 30 input_ids = torch.randn(batch_size, channels, height, width) # simulate image-like input attention_mask = torch.ones((batch_size, height * width)) output = model(input_ids, attention_mask) assert torch.isnan(output).any(), "output contains nan values" def test_gpt2arc_forward_pass(model): batch_size = 2 channels = 1 height = 30 width = 30 input_ids = torch.randn(batch_size, channels, height, width) # simulate image-like input attention_mask = torch.ones((batch_size, height * width)) output_with_mask = model(input_ids, attention_mask) output_without_mask = model(input_ids) logger.debug( f"difference outputs: {(output_with_mask - output_without_mask).abs().mean()}" ) def test_attention_module(): logger.debug("testing attention module") attention = attention(n_embd=768, n_head=12) x = torch.randn(2, 10, 768) output = attention(x) assert output.shape == x.shape logger.debug(f"attention input shape: {x.shape}, output shape: {output.shape}") def test_feedforward_module(): logger.debug("testing feedforward module") ff = feedforward(n_embd=768) x = torch.randn(2, 10, 768) output = ff(x) assert output.shape == x.shape logger.debug(f"feedforward input shape: {x.shape}, output shape: {output.shape}") def test_transformer_block(): logger.debug("testing transformerblock") block = transformerblock(n_embd=768, n_head=12) x = torch.randn(2, 10, 768) output = block(x) assert output.shape == x.shape logger.debug( f"transformerblock input shape: {x.shape}, output shape: {output.shape}" )</file><file name="tests/test_dataset.py">import unittest import torch gpt2_arc.src.data.arc_dataset import arcdataset class testarcdataset(unittest.testcase): def setup(self): # initialize dataset mock data source self.dataset = arcdataset(data_source="path/to/mock_data") def test_task_ids_loaded_from_filenames(self): # mock os.listdir return predefined filenames synthetic_filenames = ['task_alpha.json', 'task_beta.json'] patch('os.listdir', return_value=synthetic_filenames): # mock open function return empty json data mock_data = json.dumps({'train': [], 'test': []}) patch('builtins.open', mock_open(read_data=mock_data)): dataset = arcdataset(data_source='path/to/synthetic_data', debug=true) expected_task_ids = ['task_alpha', 'task_beta'] actual_task_ids = [sample['task_id'] sample dataset.data] self.assertequal(actual_task_ids, expected_task_ids, "task ids match filenames.") # initialize dataset mock directory dataset = arcdataset(data_source="path/to/mock_directory") self.assertgreater(len(dataset), 0, "dataset contain samples loaded directory.") def test_dataset_preprocessing(self): # initialize dataset mock data source dataset = arcdataset(data_source="path/to/mock_data") input_tensor, output_tensor, task_id = dataset[0] self.assertequal(input_tensor.shape, (1, 30, 30), "input tensor padded (1, 30, 30).") self.assertequal(output_tensor.shape, (1, 30, 30), "output tensor padded (1, 30, 30).") def test_symbol_frequencies(self): # test symbol frequency calculation frequencies = self.dataset.get_symbol_frequencies() self.assertisinstance(frequencies, dict, "symbol frequencies dictionary.") def test_dataset_length_and_item_retrieval(self): # test dataset length item retrieval self.assertequal(len(self.dataset), self.dataset.get_num_samples(), "dataset length match number samples.") input_tensor, output_tensor, task_id = self.dataset[0] self.assertisinstance(input_tensor, torch.tensor, "input torch.tensor.") self.assertisinstance(output_tensor, torch.tensor, "output torch.tensor.") self.assertisinstance(task_id, str, "task id string.") __name__ == '__main__': unittest.main()</file><file name="tests/test_train.py"># gpt2_arc/tests/test_train.py import os import sys sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))) import os import sys import pytest import logging logger = logging.getlogger(__name__) def set_logging_level(level=logging.error): logger = logging.getlogger() logger.setlevel(level) # add project root pythonpath sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../"))) import argparse unittest.mock import any, magicmock, patch import pytorch_lightning pl import torch gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.train import main gpt2_arc.src.training.trainer import arctrainer @pytest.fixture def mock_args(): args = argparse.namespace() args.train_data = "mock_train_data.json" args.val_data = "mock_val_data.json" args.batch_size = 32 args.learning_rate = 1e-4 args.max_epochs = 10 args.use_gpu = false args.no_logging = false args.no_checkpointing = false args.no_progress_bar = false args.log_level = "info" # add log_level attribute args.fast_dev_run = false # add fast_dev_run attribute args.project = "test_project" # add project attribute mock_args return args @pytest.fixture def mock_dataset(): dataset = magicmock(spec=arcdataset) dataset.data = [{"input": "mock input", "output": "mock output"}] dataset.__len__.return_value = 100 return dataset src.config import config, modelconfig, trainingconfig @pytest.fixture def model(): config = config(model=modelconfig(), training=trainingconfig()) return gpt2arc(config.model) @pytest.fixture def trainer(): model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) return arctrainer(model, none, none, config) @pytest.fixture def mock_pl_trainer(): return magicmock(spec=pl.trainer) # existing gpt2arc model tests def test_gpt2arc_initialization(model): assert isinstance(model, gpt2arc) assert hasattr(model, "conv1") # check conv1 instead token_embedding assert hasattr(model, "blocks") assert hasattr(model, "ln_f") assert hasattr(model, "config") def test_gpt2arc_forward_pass(model): batch_size = 2 height = width = 30 seq_length = height * width input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output_with_mask = model(input_ids, attention_mask) output_without_mask = model(input_ids) assert isinstance(output_with_mask, torch.tensor) assert output_with_mask.shape == (batch_size, seq_length, model.config.n_embd) assert isinstance(output_without_mask, torch.tensor) assert output_without_mask.shape == (batch_size, seq_length, model.config.n_embd) logger.debug(f"difference outputs: {(output_with_mask - output_without_mask).abs().mean()}") def test_gpt2arc_output_values(model): logger.debug("testing gpt2arc output values") batch_size = 1 height = width = 30 seq_length = height * width input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output = model(input_ids, attention_mask) assert torch.isnan(output).any(), "output contains nan values" assert torch.isinf(output).any(), "output contains infinity values" def test_gpt2arc_attention_mask(model): batch_size = 2 channels = 1 height = 30 width = 30 input_ids = torch.randint(0, 2, (batch_size, channels, height, width)) attention_mask = torch.zeros((batch_size, height * width)) attention_mask[:, :450] = 1 # attend first half pixels output_with_mask = model(input_ids, attention_mask) output_without_mask = model(input_ids) assert torch.allclose(output_with_mask, output_without_mask), "attention mask affect output" # new tests train.py def test_logging(mock_args, mock_dataset, model, mock_pl_trainer): print("entering test_logging") patch( "gpt2_arc.src.training.train.arcdataset", return_value=mock_dataset ), patch("gpt2_arc.src.training.train.gpt2arc", return_value=model), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( ) mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer", return_value=mock_pl_trainer ), patch("gpt2_arc.src.training.train.tensorboardlogger") mock_logger, patch( "gpt2_arc.src.training.train.modelcheckpoint" ), patch("torch.utils.data.dataloader") mock_dataloader: mock_dataloader.return_value = magicmock() # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "1234", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } mock_trainer_instance.results_collector = mock_results_collector # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_logger.assert_called_once_with("tb_logs", name="arc_model") def test_fit_call(mock_args, mock_dataset, model): mock_pl_trainer = magicmock() mock_pl_trainer.fit = magicmock() print("entering test_fit_call") patch( "gpt2_arc.src.training.train.arcdataset", return_value=mock_dataset ), patch("gpt2_arc.src.training.train.gpt2arc", return_value=model), patch( "gpt2_arc.src.training.train.arctrainer" ) mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer", return_value=mock_pl_trainer ), patch("gpt2_arc.src.training.train.tensorboardlogger"), patch( "gpt2_arc.src.training.train.modelcheckpoint" ), patch("torch.utils.data.dataloader", new_callable=magicmock) mock_dataloader: mock_dataloader.return_value = magicmock() # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_pl_trainer.fit.assert_called_once_with(mock_trainer_instance) def test_data_loading(mock_args): patch( "gpt2_arc.src.data.arc_dataset.arcdataset.__init__", return_value=none ) mock_init: arcdataset(mock_args.train_data) mock_init.assert_called_once_with(mock_args.train_data) def test_trainer_initialization(model, mock_dataset): config = config(model=modelconfig(), training=trainingconfig()) trainer = arctrainer( model=model, train_dataset=mock_dataset, val_dataset=mock_dataset, config=config ) assert isinstance(trainer, arctrainer) assert trainer.model == model assert trainer.train_dataset == mock_dataset assert trainer.val_dataset == mock_dataset assert trainer.batch_size == 32 assert trainer.lr == 1e-4 @pytest.mark.parametrize("batch_size", [1, 1000000]) def test_batch_size_extremes(mock_args, batch_size): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=batch_size, learning_rate=5e-4, max_epochs=10)) mock_args.batch_size = batch_size mock_args.no_logging = true mock_args.no_checkpointing = true mock_args.no_progress_bar = true mock_args.use_gpu = false patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch( "gpt2_arc.src.training.train.arctrainer" ), patch("gpt2_arc.src.training.trainer.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ) mock_trainer, patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) main(mock_args) mock_trainer.assert_called_with( max_epochs=config.training.max_epochs, logger=false, callbacks=none, enable_checkpointing=false, enable_progress_bar=false, fast_dev_run=false, # include fast_dev_run expected call gradient_clip_val=1.0, accelerator='cpu' ) @pytest.mark.parametrize("learning_rate", [1e-10, 1000]) def test_learning_rate_extremes(mock_args, learning_rate): set_logging_level(logging.warning) # suppress info debug messages mock_args.learning_rate = learning_rate logger.debug(f"testing learning_rate: {learning_rate}") patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ), patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "1234", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } main(mock_args) # raise exception def test_non_existent_train_data(mock_args): mock_args.train_data = "non_existent_path.json" pytest.raises(filenotfounderror): os.path.exists(mock_args.train_data): raise filenotfounderror(f"file found: {mock_args.train_data}") main(mock_args) def test_gpu_not_available(mock_args): mock_args.use_gpu = true mock_args.no_logging = false mock_args.no_checkpointing = false mock_args.no_progress_bar = false patch("torch.cuda.is_available", return_value=false), patch( "gpt2_arc.src.training.train.arcdataset" ), patch("gpt2_arc.src.training.train.gpt2arc"), patch( "gpt2_arc.src.training.train.arctrainer" "gpt2_arc.src.training.train.arctrainer" ), patch("gpt2_arc.src.training.train.pl.trainer") mock_trainer, \ patch("gpt2_arc.src.utils.results_collector.resultscollector.get_summary") mock_get_summary: # mock get_summary method return serializable dictionary mock_get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # use simple function instead magicmock main def simple_main(args): pass simple_main(mock_args) mock_trainer.assert_called_with( max_epochs=mock_args.max_epochs, logger=any, callbacks=any, enable_checkpointing=true, enable_progress_bar=true, fast_dev_run=false, gradient_clip_val=1.0, accelerator='cpu' ) hypothesis import healthcheck, given, settings hypothesis import strategies st @settings(suppress_health_check=[healthcheck.function_scoped_fixture], deadline=none) @given(batch_size=st.integers(min_value=1, max_value=1024)) def test_valid_batch_sizes(mock_args, batch_size): mock_args.batch_size = batch_size patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ), patch("gpt2_arc.src.training.train.resultscollector.get_summary", return_value={ "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} }), patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) main(mock_args) # raise exception @settings(suppress_health_check=[healthcheck.function_scoped_fixture], deadline=none) @given( learning_rate=st.floats( min_value=1e-6, max_value=1.0, allow_nan=false, allow_infinity=false ) ) def test_valid_learning_rates(mock_args, learning_rate): mock_args.learning_rate = learning_rate import glob import os patch("gpt2_arc.src.training.train.arcdataset"), patch( "gpt2_arc.src.training.train.gpt2arc" ), patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, patch( "gpt2_arc.src.training.train.pl.trainer" ) mock_trainer, patch( "torch.utils.data.dataloader" ) mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) try: # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } mock_results_collector.config = {"model": {}, "training": {}} # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) # raise exception finally: # ensure cleanup generated files file glob.glob("results/summary_*.json"): os.remove(file) def test_end_to_end_training(mock_args, tmp_path): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=5e-4, max_epochs=2)) checkpoint_dir = tmp_path / "checkpoints" checkpoint_dir.mkdir() mock_args.checkpoint_dir = str(checkpoint_dir) patch("gpt2_arc.src.training.train.arcdataset"), \ patch("gpt2_arc.src.training.train.gpt2arc"), \ patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, \ patch("gpt2_arc.src.training.train.pl.trainer") mock_trainer, \ patch("gpt2_arc.src.training.train.modelcheckpoint") mock_checkpoint, \ patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_trainer.return_value.fit.assert_called_once() mock_checkpoint.assert_called_once() def test_tensorboard_logging(mock_args, tmp_path): log_dir = tmp_path / "tb_logs" log_dir.mkdir() patch("gpt2_arc.src.training.train.arcdataset"), \ patch("gpt2_arc.src.training.train.gpt2arc"), \ patch("gpt2_arc.src.training.train.arctrainer") mock_arctrainer, \ patch("gpt2_arc.src.training.train.pl.trainer"), \ patch("gpt2_arc.src.training.train.tensorboardlogger") mock_logger, \ patch("torch.utils.data.dataloader") mock_dataloader: # directly return mock dataloader instance mock_dataloader.return_value = magicmock(spec=torch.utils.data.dataloader) # set arctrainer mock instance mock_trainer_instance = mock_arctrainer.return_value # create mock resultscollector real get_summary() method mock_results_collector = magicmock() mock_results_collector.get_summary.return_value = { "experiment_id": "test_id", "timestamp": "2023-10-01 12:00:00", "final_train_loss": 0.1, "final_val_loss": 0.2, "test_accuracy": 0.95, "config": {"model": {}, "training": {}} } # assign mock resultscollector trainer instance mock_trainer_instance.results_collector = mock_results_collector main(mock_args) mock_logger.assert_called_once_with("tb_logs", name="arc_model") # additional test gpt2arc model training context def test_arctrainer_forward_pass(trainer): batch_size = 2 seq_length = 900 # 30x30 grid input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output = trainer(input_ids, attention_mask) assert isinstance(output, torch.tensor) assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd) def test_arctrainer_training_step(trainer): batch_size = 2 height = width = 30 # 30x30 grid seq_length = height * width vocab_size = 10 # use small vocab size testing batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), # inputs torch.ones((batch_size, seq_length)).float(), # labels torch.randint(0, vocab_size, (batch_size, seq_length)).long() # task_ids ) pl_trainer = magicmock() pl_trainer.validate = magicmock() pl_trainer.validate(trainer, dataloaders=[batch]) @pytest.mark.parametrize("batch_format", ["tuple", "dict"]) def test_arctrainer_batch_format(trainer, batch_format): batch_size = 2 height = width = 30 # 30x30 grid seq_length = height * width vocab_size = 10 # use small vocab size testing batch_format == "tuple": batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), torch.ones((batch_size, seq_length)).float(), torch.randint(0, vocab_size, (batch_size, seq_length)).long(), ) else: batch = { "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), "attention_mask": torch.ones((batch_size, seq_length)).float(), "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), } loss = trainer.training_step(batch, 0) assert isinstance(loss, torch.tensor) assert loss.shape == torch.size([]) # loss scalar assert torch.isnan(loss).any(), "loss contains nan values" assert torch.isinf(loss).any(), "loss contains infinity values"</file><file name="tests/test_differential_pixel_accuracy.py"># gpt2_arc/tests/test_differential_pixel_accuracy.py import sys import os # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) sys.path.insert(0, project_root) import torch gpt2_arc.src.utils.helpers import differential_pixel_accuracy gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import modelconfig gpt2_arc.src.data.arc_dataset import arcdataset import arckit def test_identical_inputs_and_targets(): input_tensor = torch.tensor([[1, 2], [3, 4]]) target_tensor = torch.tensor([[1, 2], [3, 4]]) prediction_tensor = torch.tensor([[1, 2], [3, 4]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 identical input target" def test_completely_different_inputs_and_targets(): input_tensor = torch.tensor([[1, 1], [1, 1]]) target_tensor = torch.tensor([[0, 0], [0, 0]]) prediction_tensor = torch.tensor([[0, 0], [0, 0]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 correct prediction differing pixels" def test_partial_differences(): input_tensor = torch.tensor([[1, 2], [3, 4]]) target_tensor = torch.tensor([[1, 0], [3, 0]]) prediction_tensor = torch.tensor([[1, 0], [3, 4]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 0.5, "expected accuracy 0.5 partial correct predictions" def test_empty_tensors(): input_tensor = torch.tensor([]) target_tensor = torch.tensor([]) prediction_tensor = torch.tensor([]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 empty tensors" def test_single_pixel_difference(): input_tensor = torch.tensor([[1]]) target_tensor = torch.tensor([[0]]) prediction_tensor = torch.tensor([[0]]) accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) assert accuracy == 1.0, "expected accuracy 1.0 single pixel difference" def test_differential_pixel_accuracy_with_arckit_data(): print("starting test_differential_pixel_accuracy_with_arckit_data") task_id = "007bbfb7" task_data = arckit.load_single(task_id) print(f"loaded task data: {task_data}") print(f"debug: task_data type: {type(task_data)}") print(f"debug: task_data attributes: {dir(task_data)}") dataset = arcdataset([task_data]) # wrap list simulate multiple tasks input_tensor, target_tensor, _ = dataset[0] print(f"dataset input tensor shape: {input_tensor.shape}") print(f"dataset target tensor shape: {target_tensor.shape}") model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) model = gpt2arc(model_config) model.eval() print("model initialized set eval mode") torch.no_grad(): prediction_tensor = model(input_tensor.unsqueeze(0)) print(f"model prediction tensor shape: {prediction_tensor.shape}") # reverse scaling evaluation original_input = task_data.train[0][0] original_target = task_data.train[0][1] print(f"original input shape: {original_input.shape}") print(f"original target shape: {original_target.shape}") prediction_np = prediction_tensor.squeeze().argmax(dim=0).numpy() print(f"prediction numpy array shape: {prediction_np.shape}") reversed_prediction = dataset.reverse_scaling(original_input, prediction_np) print(f"reversed prediction shape: {reversed_prediction.shape}") # convert back tensors differential_pixel_accuracy # ensure tensors shape input_tensor = torch.tensor(original_input, dtype=torch.float32).resize_(original_target.shape) target_tensor = torch.tensor(original_target, dtype=torch.float32) prediction_tensor = torch.tensor(reversed_prediction, dtype=torch.float32).resize_(original_target.shape) print(f"final input tensor shape: {input_tensor.shape}") print(f"final target tensor shape: {target_tensor.shape}") print(f"final prediction tensor shape: {prediction_tensor.shape}") accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor) print(f"differential pixel accuracy task {task_id}: {accuracy}") assert 0 &lt;= accuracy &lt;= 1, f"accuracy 0 1, got {accuracy}" # run tests __name__ == "__main__": test_identical_inputs_and_targets() test_completely_different_inputs_and_targets() test_partial_differences() test_empty_tensors() test_single_pixel_difference() print("all tests passed!")</file><file name="tests/test_models.py">import unittest import torch gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig class testgpt2arc(unittest.testcase): def setup(self): # define model training configurations model_config = modelconfig( n_embd=16, n_head=2, n_layer=2, mamba_ratio=1, d_state=4, d_conv=1, dropout=0.05 ) training_config = trainingconfig( batch_size=2, learning_rate=0.001, max_epochs=10, use_gpu=false, log_level="debug", use_synthetic_data=false, balance_symbols=true, balancing_method="weighting", synthetic_data_path=none, symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05} ) self.config = config(model=model_config, training=training_config) self.model = gpt2arc(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq) def test_model_initialization_with_class_weights(self): expected_weights = torch.tensor([2.0, 5.0, 10.0, 10.0, 20.0, 20.0]) self.asserttrue(torch.allclose(self.model.loss_fn.weight, expected_weights), "class weights loss function match expected values.") def test_model_forward_pass(self): dummy_input = torch.zeros(1, 1, 6, 6) output = self.model(dummy_input) self.assertequal(output.shape, (1, 1, 6), "model output shape mismatch.") __name__ == '__main__': unittest.main()</file><file name="tests/test_benchmark.py"># gpt2_arc/tests/test_benchmark.py import pytest import torch import numpy np unittest.mock import magicmock, patch benchmark import benchmark_model, main, baselines src.config import modelconfig src.models.gpt2 import gpt2arc # mock classes fixtures @pytest.fixture def mock_model(): model = magicmock(spec=gpt2arc) model.forward = magicmock() # ensure forward mock return model @pytest.fixture def mock_dataset(): dataset = magicmock() dataset.__getitem__.return_value = ( torch.randn(1, 30, 30), # inputs torch.randint(0, 10, (1, 30, 30)), # outputs "task_1" # task_id ) dataset.__len__.return_value = 100 return dataset @pytest.fixture def mock_dataloader(): dataloader = magicmock() dataloader.__iter__.return_value = iter([ ( torch.randn(32, 1, 30, 30), # inputs torch.randn(32, 1, 30, 30), # outputs f"task_{i}" # task_ids ) range(10) ]) return dataloader # tests benchmark_model function def test_benchmark_model_basic(mock_model, mock_dataset, mock_dataloader): patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader), \ patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=false), \ patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=false): avg_time, avg_grids = benchmark_model(mock_model, mock_dataset) assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) assert avg_time &gt; 0 assert avg_grids &gt; 0 @pytest.mark.parametrize("batch_size,num_batches,num_runs", [ (16, 5, 10), (64, 20, 5), (128, 2, 3) ]) def test_benchmark_model_parameters(mock_model, mock_dataset, mock_dataloader, batch_size, num_batches, num_runs): patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader), \ patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=false), \ patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=false): avg_time, avg_grids = benchmark_model( mock_model, mock_dataset, batch_size=batch_size, num_batches=num_batches, num_runs=num_runs ) assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) def test_benchmark_model_cuda(mock_model, mock_dataset, mock_dataloader): patch('benchmark.torch.cuda.is_available', return_value=true), \ patch('benchmark.torch.cuda.synchronize'), \ patch('benchmark.dataloader', return_value=mock_dataloader), \ patch('benchmark.torch.compile', return_value=mock_model): torch.cuda.is_available(): pytest.skip("cuda available system") try: avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='cuda') except assertionerror e: "torch compiled cuda enabled" str(e): pytest.skip("pytorch compiled cuda support") else: raise assert isinstance(avg_time, float) assert isinstance(avg_grids, float) assert avg_time &gt;= 0 assert avg_grids &gt;= 0 def test_benchmark_model_mps(mock_model, mock_dataset, mock_dataloader): patch('benchmark.torch.backends.mps.is_available', return_value=true), \ patch('benchmark.dataloader', return_value=mock_dataloader): avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='mps') assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) def test_benchmark_model_error_handling(mock_model, mock_dataset): pytest.raises(valueerror, match="invalid device type"): benchmark_model(mock_model, mock_dataset, device_type='invalid_device') # tests main function @pytest.fixture def mock_argparse(): patch('benchmark.argparse.argumentparser') mock_argparse: mock_args = magicmock() mock_args.num_runs = 5 mock_args.num_full_runs = 1 mock_args.batch_size = 32 mock_args.num_batches = 10 mock_args.n_embd = 64 mock_args.n_head = 2 mock_args.n_layer = 1 mock_args.device = 'cpu' mock_args.precision = 'highest' mock_argparse.return_value.parse_args.return_value = mock_args yield mock_argparse def test_main_function(mock_argparse, mock_dataset, mock_model): patch('benchmark.arckit.load_data', return_value=(mock_dataset, none)), \ patch('benchmark.arcdataset', return_value=mock_dataset), \ patch('benchmark.gpt2arc', return_value=mock_model), \ patch('benchmark.benchmark_model', return_value=(1.0, 100.0)): main(mock_argparse.return_value.parse_args()) # performance tests @pytest.mark.benchmark(group="benchmark_model") def test_benchmark_model_performance(benchmark, mock_model): # create mock dataset one item mock_dataset = magicmock() mock_dataset.__len__.return_value = 1 mock_dataset.__getitem__.return_value = ( torch.randn(1, 30, 30), # input torch.randint(0, 10, (1, 30, 30)), # output "task_1" # task_id ) # create mock dataloader returns mock dataset item mock_dataloader = magicmock() mock_dataloader.__iter__.return_value = iter([mock_dataset.__getitem__()]) patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader), \ patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=false), \ patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=false): result = benchmark( benchmark_model, mock_model, mock_dataset, batch_size=1, num_batches=1, device_type='cpu', precision='medium', model_checkpoint=none ) assert isinstance(result, tuple), f"expected tuple, got {type(result)}" assert len(result) == 2, f"expected tuple length 2, got length {len(result)}" avg_time, grids_per_second = result print(f"benchmark result - average time: {avg_time}, grids per second: {grids_per_second}") assert isinstance(avg_time, float), f"expected float avg_time, got {type(avg_time)}" assert isinstance(grids_per_second, float), f"expected float grids_per_second, got {type(grids_per_second)}" assert avg_time &gt;= 0, f"average time non-negative, got {avg_time}" assert grids_per_second &gt;= 0, f"grids per second non-negative, got {grids_per_second}" avg_time &gt; 0: assert grids_per_second &gt; 0, f"grids per second positive avg_time &gt; 0, got {grids_per_second}" # edge case tests def test_benchmark_model_empty_dataset(mock_model): empty_dataset = magicmock() empty_dataset.__len__.return_value = 0 pytest.raises(valueerror, match="dataset empty"): benchmark_model(mock_model, empty_dataset) def test_benchmark_model_single_item_dataset(mock_model): single_item_dataset = magicmock() single_item_dataset.__len__.return_value = 1 mock_dataloader = magicmock() mock_dataloader.__iter__.return_value = iter([ (torch.randn(1, 1, 30, 30), torch.randn(1, 1, 30, 30), "task_1") ]) patch('benchmark.dataloader', return_value=mock_dataloader): avg_time, avg_grids = benchmark_model(mock_model, single_item_dataset, batch_size=1, num_batches=1) assert isinstance(avg_time, (float, int)) assert isinstance(avg_grids, (float, int)) # error handling tests def test_benchmark_model_with_correct_data(mock_model, mock_dataset, mock_dataloader): patch('benchmark.dataloader', return_value=mock_dataloader): avg_time, avg_grids = benchmark_model(mock_model, mock_dataset) assert isinstance(avg_time, float), "avg_time float" assert isinstance(avg_grids, float), "avg_grids float" assert avg_time &gt; 0, "avg_time positive" assert avg_grids &gt; 0, "avg_grids positive" def test_benchmark_model_model_error(mock_model, mock_dataset, mock_dataloader): # mock model's forward method raise runtimeerror execution mock_model.forward = magicmock(side_effect=runtimeerror("model execution failed")) patch('gpt2_arc.benchmark.dataloader', return_value=mock_dataloader): pytest.raises(runtimeerror, match="model execution failed"): print("debug: invoking benchmark_model") benchmark_model(mock_model, mock_dataset, device_type='cpu') # ensure forward method called assert mock_model.forward.call_count &gt; 0, "debug: forward method called" print(f"debug: forward method call count: {mock_model.forward.call_count}") #skip @pytest.mark.skip(reason="i dont want crash computer") def test_benchmark_model_out_of_memory(mock_model, mock_dataset, mock_dataloader): mock_model.side_effect = torch.cuda.outofmemoryerror("cuda memory") patch('benchmark.dataloader', return_value=mock_dataloader), \ patch('benchmark.torch.cuda.is_available', return_value=true), \ pytest.raises(torch.cuda.outofmemoryerror, match="cuda memory"): benchmark_model(mock_model, mock_dataset, device_type='cuda') # precision tests @pytest.fixture def mock_torch(): return magicmock() @pytest.mark.parametrize("precision", ['highest', 'high', 'medium']) def test_benchmark_model_precision(mock_model, mock_dataset, mock_torch, precision): patch('gpt2_arc.benchmark.dataloader') mock_dataloader_class: mock_dataloader = magicmock() mock_dataloader.__iter__.return_value = iter([ ( torch.randn(1, 1, 30, 30), # inputs torch.randint(0, 10, (1, 30, 30)), # outputs "task_1" # task_id ) ]) mock_dataloader_class.return_value = mock_dataloader patch('gpt2_arc.benchmark.torch.set_float32_matmul_precision') mock_set_precision: benchmark_model(mock_model, mock_dataset, precision=precision) mock_set_precision.assert_called_once_with(precision) # csv output tests def test_csv_output(mock_model, mock_dataset, mock_dataloader, tmp_path): csv_file = tmp_path / "benchmark_results.csv" stats_csv_file = tmp_path / "benchmark_statistics.csv" patch('benchmark.dataloader', return_value=mock_dataloader), \ patch('benchmark.csv.writer') mock_csv_writer: benchmark_model(mock_model, mock_dataset) assert mock_csv_writer.call_count == 2 # one results, one statistics # test suite execution __name__ == '__main__': pytest.main(['-v', '--cov=benchmark', '--cov-report=term-missing'])</file><file name="tests/test_synthetic_data.py">import json import tempfile import os pathlib import path import pytest unittest.mock import patch, magicmock gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.training.train import main import torch @pytest.fixture def synthetic_data(): tempfile.temporarydirectory() tmpdir: data_path = path(tmpdir) / "synthetic_data" data_path.mkdir() open(data_path / "task1.json", "w") f: json.dump({ "train": [{"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}], "test": [{"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]}] }, f) yield str(data_path) def test_synthetic_data_loading(synthetic_data): dataset = arcdataset(synthetic_data) assert len(dataset) &gt; 0 sample = dataset[0] assert isinstance(sample, tuple) assert len(sample) == 3 # input, output, task_id assert sample[0].shape == (1, 2, 2) # assuming 2x2 grid assert sample[1].shape == (1, 2, 2) def test_short_training_run(): args = magicmock() args.use_synthetic_data = true args.synthetic_data_path = "gpt2_arc/src/data/syntheticarc/tasks" args.max_epochs = 1 args.fast_dev_run = true args.use_gpu = torch.cuda.is_available() args.no_logging = true args.no_checkpointing = true args.no_progress_bar = true args.project = "test_project" args.log_level = "debug" args.batch_size = 1 args.learning_rate = 1e-4 args.n_embd = 32 args.n_head = 2 args.n_layer = 2 patch("gpt2_arc.src.training.train.pl.trainer") mock_pl_trainer, \ patch("gpt2_arc.src.training.train.arctrainer") mock_arc_trainer, \ patch("gpt2_arc.src.training.train.arcdataset") mock_dataset: # mock dataset return single sample mock_dataset.return_value = [ (torch.rand(1, 30, 30), torch.rand(1, 30, 30), 0) ] main(args) mock_pl_trainer.assert_called_once() mock_arc_trainer.assert_called_once() @pytest.mark.parametrize("use_synthetic", [true, false]) def test_main_with_synthetic_data(synthetic_data, use_synthetic): args = magicmock() args.use_synthetic_data = use_synthetic args.synthetic_data_path = synthetic_data use_synthetic else none args.max_epochs = 1 args.fast_dev_run = true args.use_gpu = false args.no_logging = true args.no_checkpointing = true args.no_progress_bar = true args.project = "test_project" args.log_level = "debug" args.batch_size = 1 # set batch_size positive integer args.learning_rate = 1e-4 # set default learning rate print(f"debug: args.batch_size = {args.batch_size}") print(f"debug: args.learning_rate = {args.learning_rate}") patch("gpt2_arc.src.training.train.pl.trainer") mock_pl_trainer, \ patch("gpt2_arc.src.training.train.arcdataset") mock_dataset, \ patch("gpt2_arc.src.training.train.gpt2arc") mock_model, \ patch("gpt2_arc.src.training.train.arctrainer") mock_arc_trainer: print("debug: inside test_main_with_synthetic_data") print(f"debug: mock_pl_trainer = {mock_pl_trainer}") print(f"debug: mock_arc_trainer = {mock_arc_trainer}") main(args) mock_pl_trainer.assert_called_once() def test_synthetic_data_argument_parsing(): import argparse parser = argparse.argumentparser() parser.add_argument("--use-synthetic-data", action="store_true") parser.add_argument("--synthetic-data-path", type=str) # test synthetic data args = parser.parse_args(["--use-synthetic-data", "--synthetic-data-path", "/path/to/data"]) assert args.use_synthetic_data assert args.synthetic_data_path == "/path/to/data" # test without synthetic data args = parser.parse_args([]) assert args.use_synthetic_data assert args.synthetic_data_path none</file><file name="tests/test_evaluate.py">import unittest import torch gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.data.arc_dataset import arcdataset class testevaluationmetrics(unittest.testcase): def setup(self): # define model training configurations model_config = modelconfig( n_embd=16, n_head=2, n_layer=2, mamba_ratio=1, d_state=4, d_conv=1, dropout=0.05 ) training_config = trainingconfig( batch_size=2, learning_rate=0.001, max_epochs=10, use_gpu=false, log_level="debug", use_synthetic_data=false, balance_symbols=true, balancing_method="weighting", synthetic_data_path=none, symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05} ) self.config = config(model=model_config, training=training_config) self.model = gpt2arc(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq) self.train_dataset = arcdataset(data_source="path/to/mock_data") self.val_dataset = arcdataset(data_source="path/to/mock_data") def test_evaluation_metrics_computation(self): trainer = arctrainer(model=self.model, train_dataset=self.train_dataset, val_dataset=self.val_dataset, config=self.config) # create dummy outputs labels outputs = torch.tensor([[0.1, 0.6, 0.3, 0.0, 0.0, 0.0], [0.3, 0.3, 0.2, 0.1, 0.05, 0.05]], requires_grad=true) labels = torch.tensor([1, 2]) loss = trainer.compute_loss(outputs, labels) self.assertgreater(loss.item(), 0, "loss positive.") accuracy = trainer.compute_accuracy(outputs, labels) self.assertgreaterequal(accuracy.item(), 0, "accuracy non-negative.") self.assertlessequal(accuracy.item(), 1, "accuracy exceed 1.") __name__ == '__main__': unittest.main()</file><file name="tests/test_end_to_end.py"># gpt2_arc/tests/test_end_to_end.py import pytest import torch import numpy np src.data.arc_dataset import arcdataset src.models.gpt2 import gpt2arc src.training.trainer import arctrainer src.config import config, modelconfig, trainingconfig import pytorch_lightning pl import time import logging import os thop import profile, clever_format # import thop pytest import approx # set logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def arc_data_path(): # adjust path location arc dataset json file return "/volumes/totallynotaharddrive/arc-neural-reasoning-model/syntheticarc/tasks/1c786137.json" import arckit def test_end_to_end(): logger.debug("starting end-to-end test") try: # load data using arckit logger.debug("loading data using arckit") train_set, eval_set = arckit.load_data() # create datasets using arcdataset logger.debug("creating train validation datasets") full_dataset = arcdataset(train_set, is_test=false) # use smaller subset dataset subset_size = int(0.1 * len(full_dataset)) # use 10% dataset train_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size]) val_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size]) logger.debug(f"train dataset size: {len(train_dataset)}, validation dataset size: {len(val_dataset)}") # create custom collate function handle data format def collate_fn(batch): inputs = [item[0].to(torch.float32) item batch] # convert float32 outputs = [item[1].to(torch.float32) item batch] # convert float32 logger.debug(f"batch input dtypes stack: {[item[0].dtype item batch]}") logger.debug(f"batch output dtypes stack: {[item[1].dtype item batch]}") # inputs outputs already tensors, need stack input_stack = torch.stack(inputs) output_stack = torch.stack(outputs) # log data types stacking logger.debug(f"collate function input_stack dtype: {input_stack.dtype}") logger.debug(f"collate function output_stack dtype: {output_stack.dtype}") # create dummy attention mask (all ones) attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32) logger.debug(f"collate function attention_mask dtype: {attention_mask.dtype}") # generate dummy task_ids item batch task_ids = [f"task_{i}" range(len(batch))] return input_stack, attention_mask, output_stack, task_ids logger.debug(f"batch output dtypes stack: {[item[1].dtype item batch]}") # inputs outputs already tensors, need stack input_stack = torch.stack(inputs) output_stack = torch.stack(outputs) # create dummy attention mask (all ones) attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32) logger.debug(f"collate function input dtype: {input_stack.dtype}") return input_stack, attention_mask, output_stack # initialize model logger.debug("initializing model") model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) # use smaller model configuration model = gpt2arc(model_config).to(torch.float32) logger.debug(f"model initialized config: {model_config}") # # thop profiling - commented due typeerror mps tensors # logger.debug("profiling model thop") # dummy_input = torch.randn(1, 1, 28, 28, dtype=torch.float32) # example input shape # macs, params = profile(model, inputs=(dummy_input,)) # macs, params = clever_format([macs, params], "%.3f") # logger.info(f"macs: {macs}, parameters: {params}") # initialize trainer logger.debug("initializing trainer") config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) # reduce epochs 2 trainer = arctrainer(model, train_dataset, val_dataset, config) trainer.train_dataloader = lambda: torch.utils.data.dataloader(train_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0) trainer.val_dataloader = lambda: torch.utils.data.dataloader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0) trainer.test_dataloader = lambda: torch.utils.data.dataloader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0) logger.debug(f"trainer initialized config: {config}") # create pytorch lightning trainer logger.debug("creating pytorch lightning trainer") # measure training time start_time = time.time() pl_trainer = pl.trainer( max_epochs=config.training.max_epochs, logger=false, enable_checkpointing=false, enable_progress_bar=false ) logger.debug("pytorch lightning trainer created") # evaluate model training get initial accuracy logger.info("evaluating model training") initial_val_results = pl_trainer.test(trainer, verbose=false) logger.debug(f"initial validation results: {initial_val_results}") initial_accuracy = initial_val_results[0].get('test_accuracy') initial_loss = initial_val_results[0].get('test_loss') print(f"initial validation results: {initial_val_results}") assert initial_accuracy none, "initial validation results missing 'test_accuracy'" assert initial_loss none, "initial validation results missing 'test_loss'" logger.info(f"initial validation accuracy: {initial_accuracy}, initial loss: {initial_loss}") print(f"initial validation accuracy: {initial_accuracy}, initial loss: {initial_loss}") logger.debug("starting model training") pl_trainer.fit(trainer) end_time = time.time() training_time = end_time - start_time logger.info(f"total training time: {training_time:.2f} seconds") logger.debug("model training completed") # check loss decreased train_losses = trainer.train_losses logger.info(f"training losses: {train_losses}") assert train_losses[-1] &lt; train_losses[0], f"training loss decrease. initial loss: {train_losses[0]}, final loss: {train_losses[-1]}" # check final loss lower initial loss assert train_losses[-1] &lt; train_losses[0], "final training loss lower initial loss" # check average loss per epoch decreases epoch_losses = [sum(train_losses[i:i+33])/33 range(0, len(train_losses), 33)] assert all(epoch_losses[i] &gt; epoch_losses[i+1] range(len(epoch_losses)-1)), "average training loss per epoch consistently decrease" # evaluate model training logger.debug("evaluating model training") final_val_results = pl_trainer.test(trainer, verbose=false) final_accuracy = final_val_results[0]['test_accuracy'] final_loss = final_val_results[0]['test_loss'] logger.info(f"final validation accuracy: {final_accuracy}, final loss: {final_loss}") print(f"final validation accuracy: {final_accuracy}, final loss: {final_loss}") # check validation accuracy improved assert final_accuracy &gt; initial_accuracy, f"validation accuracy improve. initial accuracy: {initial_accuracy}, final accuracy: {final_accuracy}" logger.info(f"final training loss: {train_losses[-1]:.4f}") logger.info(f"validation accuracy: {final_accuracy:.4f}") # check model parameters total_params = sum(p.numel() p model.parameters()) trainable_params = sum(p.numel() p model.parameters() p.requires_grad) assert total_params &gt; 0, "model parameters" assert trainable_params &gt; 0, "model trainable parameters" assert trainable_params == total_params, "not parameters trainable" logger.debug(f"total parameters: {total_params}") logger.debug(f"trainable parameters: {trainable_params}") logger.debug("end-to-end test completed successfully") except exception e: logger.error(f"end-to-end test failed error: {str(e)}") raise def test_evaluation_process_with_arckit_data(): logger.debug("starting evaluation process test arckit data") # load data using arckit _ , evaluation_data = arckit.load_data() # log structure evaluation data logger.debug(f"evaluation data structure: {evaluation_data}") logger.debug(f"evaluation data structure: {evaluation_data}") test_dataset = arcdataset(evaluation_data, is_test=true) # initialize model trainer model_configuration = modelconfig(n_embd=96, n_head=3, n_layer=1) model = gpt2arc(model_configuration) training_configuration = config(model=model_configuration, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) trainer = arctrainer(model, none, test_dataset, training_configuration) # run evaluation lightning_trainer = pl.trainer(logger=false, enable_checkpointing=false, enable_progress_bar=false) evaluation_results = lightning_trainer.test(trainer) # access test results trainer evaluation_results = trainer.test_results # log evaluation results logger.debug(f"evaluation results: {evaluation_results}") result evaluation_results: task_ids = result.get('task_ids', []) task_ids: logger.error(f"missing task_ids result: {result}") else: task_id task_ids: logger.info(f"task {task_id}: loss={result['test_loss']}, accuracy={result['test_accuracy']}") # check duplicate metrics unique_task_ids = set(task_id result evaluation_results task_id result.get('task_ids', [])) print("all task ids:", [task_id result evaluation_results task_id result.get('task_ids', [])]) print("unique task ids:", unique_task_ids) print(f"number evaluation results: {len(evaluation_results)}") print(f"number unique task ids: {len(unique_task_ids)}") len(unique_task_ids) != len(evaluation_results): print("warning: number unique task ids match number evaluation results") duplicate_tasks = [task_id task_id unique_task_ids sum(task_id result.get('task_ids', []) result evaluation_results) &gt; 1] print(f"duplicate task ids: {duplicate_tasks}") task_id duplicate_tasks: print(f"results task {task_id}:") result evaluation_results: task_id result.get('task_ids', []): print(result) print(f"unique task ids: {unique_task_ids}") print(f"evaluation results: {evaluation_results}") assert len(unique_task_ids) &gt; 0, "no tasks evaluated" logger.debug("completed evaluation process test arckit data")</file><file name="tests/test_mamba_integration.py">import unittest import torch import inspect gpt2_arc.src.models.gpt2 import mambalayer, gpt2arc zeta.nn import mambablock gpt2_arc.src.config import modelconfig class testmambalayer(unittest.testcase): def test_mamba_layer_forward(self): # print __init__ method signature mambablock print(f"mambablock.__init__ signature: {inspect.signature(mambablock.__init__)}") n_embd = 64 d_state = 16 d_conv = 4 dropout = 0.1 mamba_layer = mambalayer(n_embd, d_state, d_conv, dropout) # create sample input tensor batch_size = 2 seq_len = 10 x = torch.randn(batch_size, seq_len, n_embd) # forward pass output = mamba_layer(x) # assert output shape correct self.assertequal(output.shape, x.shape) # optional: check nans infinite values self.asserttrue(torch.all(torch.isfinite(output))) class testgpt2arcwithmamba(unittest.testcase): def test_gpt2arc_with_mamba_forward(self): # define model configuration mamba parameters model_config = modelconfig( n_embd=64, n_head=4, n_layer=2, mamba_ratio=1, d_state=16, d_conv=4, dropout=0.1 ) num_classes = 10 # adjust based dataset model = gpt2arc(config=model_config, num_classes=num_classes) # create sample input tensor (e.g., batch grids) batch_size = 2 height = width = 6 x = torch.randint(0, num_classes, (batch_size, 1, height, width), dtype=torch.long) # forward pass output = model(x) # assert output shape correct expected_output_shape = (batch_size, height * width, num_classes) self.assertequal(output.shape, expected_output_shape) # optional: check nans infinite values self.asserttrue(torch.all(torch.isfinite(output))) __name__ == '__main__': unittest.main()</file><file name="tests/test_experiment_tracker.py">import unittest import json gpt2_arc.src.utils.experiment_tracker import experimenttracker gpt2_arc.src.config import config, modelconfig, trainingconfig class testexperimenttracker(unittest.testcase): def setup(self): # define model training configurations model_config = modelconfig( n_embd=16, n_head=2, n_layer=2, mamba_ratio=1, d_state=4, d_conv=1, dropout=0.05 ) training_config = trainingconfig( batch_size=2, learning_rate=0.001, max_epochs=10, use_gpu=false, log_level="debug", use_synthetic_data=false, balance_symbols=true, balancing_method="weighting", synthetic_data_path=none, symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05} ) self.config = config(model=model_config, training=training_config) def test_experiment_tracker_logging(self): tracker = experimenttracker(config=self.config, project="test_project") tracker.log_metric("test_metric", 0.95) self.assertin("test_metric", tracker.metrics, "metric logged tracker.metrics.") self.assertequal(tracker.metrics["test_metric"], 0.95, "logged metric value mismatch.") def test_experiment_tracker_save_to_json(self): tracker = experimenttracker(config=self.config, project="test_project") tracker.log_metric("test_metric", 0.95) tracker.save_to_json("test_results.json") open("test_results.json", 'r') f: data = json.load(f) self.assertin("test_metric", data, "metric present saved json.") self.assertequal(data["test_metric"], 0.95, "saved metric value mismatch.") __name__ == '__main__': unittest.main()</file><file name="tests/test_synthetic_arc_dataset.py"># gpt2_arc/tests/test_synthetic_arc_dataset.py import os import pytest import logging gpt2_arc.src.data.arc_dataset import arcdataset synthetic_data_path = "/workspaces/arc-neural-reasoning-model/gpt2_arc/src/data/syntheticarc/small_tasks" # set logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def synthetic_dataset(): logger.debug("creating synthetic dataset") return arcdataset(synthetic_data_path) def test_synthetic_data_loading(synthetic_dataset): logger.debug("testing synthetic data loading") assert len(synthetic_dataset) &gt; 0, "synthetic dataset empty" def test_synthetic_data_structure(synthetic_dataset): logger.debug("testing synthetic data structure") sample = synthetic_dataset[0] assert isinstance(sample, tuple), "sample tuple" assert len(sample) == 3, "sample contain input, output, task_id" input_grid, output_grid, task_id = sample assert input_grid.dim() == 3, "input grid 3-dimensional (channel, height, width)" assert output_grid.dim() == 3, "output grid 3-dimensional (channel, height, width)" assert isinstance(task_id, (int, str)), "task id integer string" def test_all_synthetic_files_loaded(): logger.debug("testing synthetic files loaded") file_count = len([f f os.listdir(synthetic_data_path) f.endswith('.json')]) dataset = arcdataset(synthetic_data_path) assert len(dataset.data) == file_count, f"number loaded tasks ({len(dataset.data)}) match number json files ({file_count})" def test_synthetic_data_content(synthetic_dataset): logger.debug("testing synthetic data content") range(len(synthetic_dataset)): input_grid, output_grid, _ = synthetic_dataset[i] assert input_grid.min() &gt;= 0 input_grid.max() &lt;= 9, f"input grid values 0 9 (sample {i})" assert output_grid.min() &gt;= 0 output_grid.max() &lt;= 9, f"output grid values 0 9 (sample {i})" __name__ == "__main__": pytest.main([__file__])</file><file name="tests/test_pytest_error_fixer.py"># gpt2_arc/tests/test_pytest_error_fixer.py import os import json import pytest unittest.mock import patch, magicmock pytest_error_fixer import pytesterrorfixer # reusable fixtures test setup @pytest.fixture def error_fixer(tmp_path): # initialize pytesterrorfixer temporary directory progress log fixer = pytesterrorfixer("test_project_dir") fixer.progress_log = tmp_path / "test_progress_log.json" fixer.error_log = tmp_path / "test_error_log.json" return fixer @pytest.fixture def sample_errors(): # sample errors testing return { "gpt2_arc/test_file.py": [ "test_function assertionerror: assert 1 == 2", "test_another_function typeerror: unsupported operand type(s) +: 'int' 'str'" ] } # 1. test progress log initialization def test_init_progress_log(error_fixer): error_fixer.init_progress_log() assert os.path.exists(error_fixer.progress_log) open(error_fixer.progress_log, 'r') f: assert json.load(f) == [] # ensure log empty upon initialization # 2. test logging progress progress log def test_log_progress(error_fixer): error_fixer.init_progress_log() error_fixer.log_progress("fixed", "test error", "test_file.py") open(error_fixer.progress_log, 'r') f: log = json.load(f) assert len(log) == 1 assert log[0] == {"error": "test error", "file": "test_file.py", "status": "fixed"} # 3. test running full test suite capturing output @patch('subprocess.run') def test_run_full_test(mock_run, error_fixer): # mock output subprocess.run simulate pytest execution mock_run.return_value = magicmock(stdout="test output", stderr="test error") stdout, stderr = error_fixer.run_full_test() # assert stdout stderr captured correctly assert stdout == "test output" assert stderr == "test error" mock_run.assert_called_once() # 4. test parsing errors pytest output def test_parse_errors(error_fixer): # simulate pytest output multiple errors sample_output = """ gpt2_arc/test_file.py::test_function failed gpt2_arc/another_file.py::test_another_function failed """ errors = error_fixer.parse_errors(sample_output) # verify errors correctly parsed associated right test files assert "gpt2_arc/test_file.py" errors assert "gpt2_arc/another_file.py" errors assert "test_function failed" errors["gpt2_arc/test_file.py"] assert "test_another_function failed" errors["gpt2_arc/another_file.py"] # 5. test saving loading errors to/from json file def test_save_and_load_errors(error_fixer, sample_errors): # save errors file error_fixer.save_errors(sample_errors) # load errors back verify match original data loaded_errors = error_fixer.load_errors() assert loaded_errors == sample_errors # 6. test predicting relevant files using aider's output @patch.object(pytesterrorfixer, 'coder') def test_predict_relevant_files(mock_coder, error_fixer): # mock aider's file prediction output mock_coder.run.return_value = "the files likely involved gpt2_arc/file1.py gpt2_arc/file2.py" # predict files test error files = error_fixer.predict_relevant_files("test error") # assert correct files predicted assert files == ["gpt2_arc/file1.py", "gpt2_arc/file2.py"] mock_coder.run.assert_called_once() # 7. test fixing errors retrying needed @patch('subprocess.run') @patch.object(pytesterrorfixer, 'coder') def test_fix_error(mock_coder, mock_run, error_fixer): # simulate failed successful pytest runs mock_run.side_effect = [ magicmock(stdout="test failed", stderr="error occurred"), magicmock(stdout="test passed", stderr="") ] # simulate aider suggesting fixes mock_coder.run.return_value = "suggested fix" # run fix_error method verify retries eventually succeeds result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function") # assert error eventually fixed assert result == true assert mock_run.call_count == 2 mock_coder.run.assert_called_once() # 8. edge case: test handling invalid error output (additional coverage) def test_parse_errors_invalid_format(error_fixer): invalid_output = "this valid pytest output" errors = error_fixer.parse_errors(invalid_output) assert errors == {} # 9. edge case: test retry exhaustion errors remain unfixed @patch('subprocess.run') @patch.object(pytesterrorfixer, 'coder') def test_retry_exhaustion(mock_coder, mock_run, error_fixer): # simulate constant failure pytest runs mock_run.side_effect = [ magicmock(stdout="test failed", stderr="error occurred") ] * 3 # retry maximum number times mock_coder.run.return_value = "suggested fix" # run fix_error method ensure retries max limit result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function") # assert retries exhausted assert result == false assert mock_run.call_count == 3 # ensure retry mechanism works mock_coder.run.assert_called_once()</file><file name="tests/test_trainer.py"># gpt2_arc/tests/test_trainer.py import pytest import torch src.config import config, modelconfig, trainingconfig src.data.arc_dataset import arcdataset src.models.gpt2 import gpt2arc src.training.trainer import arctrainer @pytest.fixture def sample_data(): return [ { "train": [ {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]} ], "test": [ {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]} ] } ] @pytest.fixture def model(): config = modelconfig() return gpt2arc(config) @pytest.fixture def trainer(model, sample_data): config = config(model=modelconfig(), training=trainingconfig()) train_dataset = arcdataset(sample_data) val_dataset = arcdataset(sample_data) trainer = arctrainer(model, train_dataset, val_dataset, config) trainer.logged_metrics = {} trainer.config.training.log_level = "info" # add line trainer.log = lambda name, value, on_step=none, on_epoch=none, prog_bar=none, logger=none: trainer.logged_metrics.update({name: value}) return trainer def test_arctrainer_initialization(trainer): assert isinstance(trainer, arctrainer) assert hasattr(trainer, "model") assert hasattr(trainer, "train_dataset") assert hasattr(trainer, "val_dataset") def test_arctrainer_forward_pass(trainer): batch_size = 2 seq_length = 900 # 30x30 grid input_ids = torch.randint(0, 2, (batch_size, seq_length)) attention_mask = torch.ones((batch_size, seq_length)) output = trainer(input_ids, attention_mask) assert isinstance(output, torch.tensor) assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd) @pytest.mark.parametrize("batch_format", ["tuple", "dict"]) def test_arctrainer_training_step(trainer, batch_format): batch_size = 2 seq_length = 900 # 30x30 grid vocab_size = 10 # use small vocab size testing batch_format == "tuple": batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), torch.ones((batch_size, seq_length)).float(), torch.randint(0, vocab_size, (batch_size, seq_length)).long(), ) else: batch = { "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), "attention_mask": torch.ones((batch_size, seq_length)).float(), "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), } loss = trainer.training_step(batch, 0) assert isinstance(loss, torch.tensor) assert loss.shape == torch.size([]) assert torch.isnan(loss).any(), "loss contains nan values" assert torch.isinf(loss).any(), "loss contains infinity values" def test_training_step_with_list_input(): model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=2, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) trainer = arctrainer(model, none, none, config) batch_size = 2 vocab_size = 10 # create batch tuple length 3 (input_ids, attention_mask, labels) inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float() inputs_flat = inputs.view(batch_size, -1) attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float() labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long() labels_flat = labels.view(batch_size, -1) batch = ( inputs_flat, # input_ids attention_mask, # attention_mask labels_flat # labels ) loss = trainer.training_step(batch, 0) assert isinstance(loss, torch.tensor), "loss torch.tensor" assert loss.shape == torch.size([]), "loss scalar" assert torch.isnan(loss).any(), "loss nan" assert torch.isinf(loss).any(), "loss infinity" def test_validation_step_with_list_input(): model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=2, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) trainer = arctrainer(model, none, none, config) batch_size = 2 vocab_size = 10 # create inputs labels inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float() inputs_flat = inputs.view(batch_size, -1) # flatten inputs labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long() labels_flat = labels.view(batch_size, -1) # flatten labels # create attention mask attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float() # create batch tuple length 3 batch = ( inputs_flat, # input_ids attention_mask, # attention_mask labels_flat # labels ) trainer.validation_step(batch, 0) assert "val_loss" trainer.logged_metrics, "validation loss logged" assert isinstance(trainer.logged_metrics["val_loss"], float), "logged validation loss float" @pytest.mark.parametrize("batch_format", ["tuple", "dict"]) def test_arctrainer_validation_step(trainer, batch_format): batch_size = 2 seq_length = 900 # 30x30 grid vocab_size = 10 # use small vocab size testing batch_format == "tuple": batch = ( torch.randint(0, vocab_size, (batch_size, seq_length)).long(), torch.ones((batch_size, seq_length)).float(), torch.randint(0, vocab_size, (batch_size, seq_length)).long(), ) else: batch = { "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), "attention_mask": torch.ones((batch_size, seq_length)).float(), "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(), } trainer.validation_step(batch, 0) # check val_loss logged assert "val_loss" trainer.logged_metrics def test_arctrainer_configure_optimizers(trainer): optimizers, schedulers = trainer.configure_optimizers() assert any(isinstance(opt, torch.optim.adam) opt optimizers), "expected adam optimizer" assert any(sch['scheduler'].__class__.__name__ == 'steplr' sch schedulers), "expected steplr scheduler" def test_arctrainer_train_dataloader(trainer): dataloader = trainer.train_dataloader() assert isinstance(dataloader, torch.utils.data.dataloader) assert len(dataloader.dataset) == len(trainer.train_dataset) def test_arctrainer_val_dataloader(trainer): dataloader = trainer.val_dataloader() assert isinstance(dataloader, torch.utils.data.dataloader) assert len(dataloader.dataset) == len(trainer.val_dataset) def test_arctrainer_test_step_with_task_ids(trainer): batch_size = 2 height = width = 30 num_symbols = 10 # create mock batch inputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).float() outputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).long() task_ids = ['task1', 'task2'] batch = (inputs, outputs, task_ids) # run test step result = trainer.test_step(batch, 0) # check result contains expected keys assert 'test_loss' result assert 'test_accuracy' result assert 'task_ids' result # check 'test_loss', 'test_accuracy', 'test_diff_accuracy' logged assert 'test_loss' trainer.logged_metrics assert 'test_accuracy' trainer.logged_metrics assert 'test_diff_accuracy' trainer.logged_metrics # check task-specific metrics logged task_id task_ids: assert f'{task_id}_test_loss' trainer.logged_metrics assert f'{task_id}_test_accuracy' trainer.logged_metrics assert f'{task_id}_test_diff_accuracy' trainer.logged_metrics</file><file name="tests/__init__.py"># file empty, include initialization code necessary</file><file name="tests/test_results_collector.py"># gpt2_arc/tests/test_results_collector.py import unittest gpt2_arc.src.utils.results_collector import resultscollector gpt2_arc.src.config import config, modelconfig, trainingconfig class testresultscollector(unittest.testcase): def setup(self): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) training_config = trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=10) config = config(model=model_config, training=training_config) self.results_collector = resultscollector(config) def test_initialization(self): self.assertisnotnone(self.results_collector.experiment_id) self.assertisnotnone(self.results_collector.timestamp) self.assertequal(self.results_collector.config['model']['n_embd'], 96) def test_update_train_metrics(self): self.results_collector.update_train_metrics(1, {"loss": 0.5}) self.assertin(1, self.results_collector.results["train"]) self.assertequal(self.results_collector.results["train"][1]["loss"], 0.5) def test_update_val_metrics(self): self.results_collector.update_val_metrics(1, {"loss": 0.3}) self.assertin(1, self.results_collector.results["validation"]) self.assertequal(self.results_collector.results["validation"][1]["loss"], 0.3) def test_set_test_results(self): self.results_collector.set_test_results({"accuracy": 0.8}) self.assertequal(self.results_collector.results["test"]["accuracy"], 0.8) def test_add_task_specific_result(self): self.results_collector.add_task_specific_result("task_1", {"accuracy": 0.9}) self.assertin("task_1", self.results_collector.task_specific_results) self.assertequal(self.results_collector.task_specific_results["task_1"]["accuracy"], 0.9) def test_get_summary(self): summary = self.results_collector.get_summary() self.assertequal(summary["experiment_id"], self.results_collector.experiment_id) __name__ == '__main__': unittest.main()</file><file name="tests/test_pytorch_lightning_integration.py">import unittest import torch torch.utils.data import dataloader import pytorch_lightning pl pytorch_lightning.loggers import tensorboardlogger pytorch_lightning.callbacks import modelcheckpoint gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.data.arc_dataset import arcdataset class testpytorchlightningintegration(unittest.testcase): def setup(self): # define model training configurations model_config = modelconfig( n_embd=16, n_head=2, n_layer=2, mamba_ratio=1, d_state=4, d_conv=1, dropout=0.05 ) training_config = trainingconfig( batch_size=2, learning_rate=0.001, max_epochs=10, use_gpu=false, log_level="debug", use_synthetic_data=false, balance_symbols=true, balancing_method="weighting", synthetic_data_path=none, symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05} ) self.config = config(model=model_config, training=training_config) self.model = gpt2arc(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq) self.train_dataset = arcdataset(data_source="path/to/mock_data") self.val_dataset = arcdataset(data_source="path/to/mock_data") self.train_loader = dataloader(self.train_dataset, batch_size=2) self.val_loader = dataloader(self.val_dataset, batch_size=2) def test_pytorch_lightning_trainer_initialization(self): trainer = pl.trainer( max_epochs=10, logger=tensorboardlogger(save_dir="logs"), callbacks=[modelcheckpoint(monitor="val_loss")], accelerator='cpu', devices=1 ) self.assertequal(trainer.max_epochs, 10, "trainer max_epochs mismatch.") self.assertisinstance(trainer.logger, tensorboardlogger, "logger tensorboardlogger.") self.assertequal(len(trainer.callbacks), 1, "unexpected number callbacks initialized.") def test_training_loop_with_mock_data(self): trainer = pl.trainer(max_epochs=1, logger=false, enable_checkpointing=false, accelerator='cpu', devices=1) trainer.fit(self.model, train_dataloaders=self.train_loader, val_dataloaders=self.val_loader) # assert training completed without errors self.asserttrue(true, "training loop executed successfully.") __name__ == '__main__': unittest.main()</file><file name="tests/test_integration_experiment.py"># gpt2_arc/tests/test_integration_experiment.py import sys import os # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) sys.path.insert(0, project_root) import pytest import torch gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.utils.results_collector import resultscollector pytorch_lightning import trainer pytorch_lightning.callbacks import modelcheckpoint pytorch_lightning.loggers import tensorboardlogger import arckit @pytest.fixture def setup_experiment(): # load sample task arckit task_id = "007bbfb7" # example task id task_data = arckit.load_single(task_id) train_data = task_data.train val_data = task_data.test print(f"debug: task_data type: {type(task_data)}") print(f"debug: task_data attributes: {dir(task_data)}") print(f"debug: train_data type: {type(train_data)}") print(f"debug: train_data content: {train_data}") print(f"debug: val_data type: {type(val_data)}") print(f"debug: val_data content: {val_data}") train_dataset = arcdataset([{"train": train_data, "test": val_data}]) val_dataset = arcdataset([{"train": train_data, "test": val_data}]) # model config setup model_config = modelconfig(n_embd=64, n_head=2, n_layer=1) training_config = trainingconfig(batch_size=1, learning_rate=1e-4, max_epochs=1) config = config(model=model_config, training=training_config) model = gpt2arc(config=model_config) # trainer setup trainer = arctrainer(model=model, train_dataset=train_dataset, val_dataset=val_dataset, config=config) return trainer, config def test_full_experiment_run(setup_experiment): trainer, config = setup_experiment # pytorch lightning trainer pl_trainer = trainer( max_epochs=config.training.max_epochs, logger=tensorboardlogger("tb_logs", name="arc_model_test"), callbacks=[modelcheckpoint(dirpath="checkpoints", save_top_k=1, monitor="val_loss")], enable_checkpointing=true, enable_progress_bar=false, fast_dev_run=true ) # run training pl_trainer.fit(trainer) # verify results results_summary = trainer.results_collector.get_summary() assert results_summary["experiment_id"] none assert "final_train_loss" results_summary assert "final_val_loss" results_summary @pytest.mark.parametrize("invalid_data", [ ({"input": [[0] * 30 _ range(30)]}), # missing output ({"output": [[0] * 30 _ range(30)]}), # missing input ]) def test_invalid_data_handling(invalid_data): pytest.raises(valueerror): arcdataset([invalid_data]) def test_model_convergence_issue(setup_experiment): trainer, config = setup_experiment trainer.config.training.learning_rate = 1e-10 # set inappropriate learning rate # pytorch lightning trainer pl_trainer = trainer( max_epochs=config.training.max_epochs, logger=false, enable_checkpointing=false, enable_progress_bar=false, fast_dev_run=true ) # run training pl_trainer.fit(trainer) # verify model converge results_summary = trainer.results_collector.get_summary() assert results_summary["final_train_loss"] none assert results_summary["final_train_loss"] &gt; 1.0 # assuming high loss indicates non-convergence</file><file name="tests/test_model_evaluation.py"># gpt2_arc/tests/test_model_evaluation.py import sys import os # add root directory project pythonpath project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")) sys.path.insert(0, project_root) print("current pythonpath:", sys.path) import sys import os import json import pytest import torch pytest_mock import mocker src.models.gpt2 import gpt2arc src.config import config, modelconfig, trainingconfig torch.utils.data import dataloader src.utils.helpers import differential_pixel_accuracy src.training.trainer import arctrainer # add project root python path project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..')) sys.path.insert(0, project_root) print(f"updated python path: {sys.path}") @pytest.fixture def trainer(): model_config = modelconfig(n_embd=96, n_head=3, n_layer=1) config = config(model=model_config, training=trainingconfig(batch_size=32, learning_rate=1e-4, max_epochs=2)) model = gpt2arc(config.model) return arctrainer(model, none, none, config) import logging unittest.mock import mock # set logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) @pytest.fixture def model(mocker): mock_model = mocker.mock() mock_model.eval = mocker.mock() mock_model.side_effect = lambda inputs, attention_mask=none: torch.randn(1, 4, 2, 2) logger.debug(f"created mock model") return mock_model @pytest.fixture def inputs(): # use predetermined input inputs = torch.tensor([[[[1.0, 0.0], [0.0, 1.0]]]]) logger.debug(f"input shape: {inputs.shape}, dtype: {inputs.dtype}") return inputs @pytest.fixture def targets(): # use predetermined target targets = torch.tensor([[[1, 0], [0, 1]]]) logger.debug(f"targets shape: {targets.shape}, dtype: {targets.dtype}") return targets @pytest.fixture def attention_mask(): mask = torch.ones(1, 4) logger.debug(f"attention mask shape: {mask.shape}, dtype: {mask.dtype}") return mask @pytest.fixture def dataloader(inputs, targets, attention_mask): dataset = list(zip(inputs, targets, attention_mask)) loader = dataloader(dataset, batch_size=1) logger.debug(f"dataloader created {len(loader)} batches") return loader def test_no_grad_calculation(model, inputs, attention_mask): logger.debug("starting test_no_grad_calculation") torch.no_grad(): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"output shape: {outputs.shape}, requires_grad: {outputs.requires_grad}") assert outputs.requires_grad, "gradients tracked evaluation mode." def test_data_loop_for_evaluation(model, dataloader): logger.debug("starting test_data_loop_for_evaluation") model.eval() batch_idx, (inputs, targets, attention_mask) enumerate(dataloader): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"batch {batch_idx}: input shape: {inputs.shape}, output shape: {outputs.shape}") assert outputs none, f"model returned none batch {batch_idx}" assert outputs.shape == (1, 4, 2, 2), f"expected output shape (1, 4, 2, 2), got {outputs.shape}" def test_model_predictions(model, inputs, attention_mask): logger.debug("starting test_model_predictions") outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"model output shape: {outputs.shape}, dtype: {outputs.dtype}") initial_output = model(inputs, attention_mask=attention_mask) logger.debug(f"initial output shape: {initial_output.shape}") # change input check output changes modified_inputs = inputs + 1 modified_output = model(modified_inputs, attention_mask=attention_mask) logger.debug(f"modified output shape: {modified_output.shape}") assert torch.allclose(initial_output, modified_output), "output change input changes" @pytest.mark.skip(reason="needs checked known value arc data") def test_standard_pixel_accuracy(model, inputs, targets): logger.debug("starting test_standard_pixel_accuracy") outputs = model(inputs) logger.debug(f"outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) predicted = outputs.argmax(dim=1) accuracy = (predicted == targets).float().mean().item() logger.debug(f"calculated accuracy: {accuracy}") assert 0.0 &lt;= accuracy &lt;= 1.0, f"accuracy 0 1, got {accuracy}" # test known values known_outputs = torch.floattensor([[[[0.9, 0.1], [0.1, 0.9]]]]) known_targets = torch.tensor([[[0, 1], [1, 0]]]) known_accuracy = (known_outputs.argmax(dim=1) == known_targets).float().mean().item() logger.debug(f"known accuracy: {known_accuracy}") assert known_accuracy == 1.0, f"expected known accuracy 1.0, got {known_accuracy}" def test_differential_pixel_accuracy(model, inputs, targets): logger.debug("starting test_differential_pixel_accuracy") outputs = model(inputs) logger.debug(f"outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) predicted = outputs.argmax(dim=1) diff_accuracy, _, _ = differential_pixel_accuracy(inputs, targets, predicted) logger.debug(f"calculated differential accuracy: {diff_accuracy}") assert 0.0 &lt;= diff_accuracy &lt;= 1.0, f"differential pixel accuracy 0 1, got {diff_accuracy}" # test known values known_inputs = torch.tensor([[[[1, 0], [0, 1]]]]) known_targets = torch.tensor([[[0, 1], [1, 0]]]) known_predicted = torch.tensor([[[0, 1], [1, 0]]]) known_diff_accuracy, known_total_diff, known_correct_diff = differential_pixel_accuracy(known_inputs, known_targets, known_predicted) logger.debug(f"known differential accuracy: {known_diff_accuracy}, total diff: {known_total_diff}, correct diff: {known_correct_diff}") assert known_diff_accuracy == 1.0, f"expected known differential accuracy 1.0, got {known_diff_accuracy}" def test_task_accuracies_tracking(model, dataloader, is_training=false): logger.debug("starting test_task_accuracies_tracking") task_accuracies = {} model.eval() batch_idx, (inputs, targets, attention_mask) enumerate(dataloader): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"batch {batch_idx}: outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) accuracy = (outputs.argmax(dim=1) == targets).float().mean().item() task_id = getattr(dataloader, 'task_id', 'default_task') task_id task_accuracies: task_accuracies[task_id] = {'train': [], 'test': []} task_accuracies[task_id]['train' is_training else 'test'].append(accuracy) logger.debug(f"task accuracies batch {batch_idx}: {task_accuracies}") assert task_accuracies, "task accuracies dictionary empty" assert 'default_task' task_accuracies, "default task logged task accuracies" assert 'test' task_accuracies['default_task'], "test accuracies logged default task" def test_final_metric_calculation(model, dataloader, attention_mask): logger.debug("starting test_final_metric_calculation") model.eval() total_loss, total_accuracy = 0, 0 num_batches = 0 batch_idx, (inputs, targets, attention_mask) enumerate(dataloader): outputs = model(inputs, attention_mask=attention_mask) logger.debug(f"batch {batch_idx}: outputs shape: {outputs.shape}, targets shape: {targets.shape}") outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2]) loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(1)), targets.view(-1)) total_loss += loss.item() accuracy = (outputs.argmax(dim=1) == targets).float().mean().item() logger.debug(f"batch {batch_idx}: loss: {loss.item()}, accuracy: {accuracy}") total_accuracy += accuracy num_batches += 1 avg_loss = total_loss / num_batches avg_accuracy = total_accuracy / num_batches logger.debug(f"final metrics - average loss: {avg_loss}, average accuracy: {avg_accuracy}") assert avg_loss &gt;= 0, f"average loss non-negative, got {avg_loss}" assert 0.0 &lt;= avg_accuracy &lt;= 1.0, f"average accuracy 0 1, got {avg_accuracy}" def test_return_of_evaluation_results(model, dataloader, mocker): logger.debug("starting test_return_of_evaluation_results") # simulate simple evaluation result model.evaluate = lambda dataloader: {'loss': 0.5, 'accuracy': 0.75} results = model.evaluate(dataloader) logger.debug(f"evaluation results: {results}") assert "loss" results "accuracy" results, "evaluation results return loss accuracy." assert isinstance(results["loss"], float), f"loss float, got {type(results['loss'])}" assert 0.0 &lt;= results["accuracy"] &lt;= 1.0, f"accuracy 0 1, got {results['accuracy']}" def test_validation_step_with_incorrect_batch_format(trainer): """test validation_step raises valueerror incorrect batch format.""" # create batch incorrect format (e.g., list) incorrect_batch = [ torch.randint(0, 10, (2, 900)), # random input data # labels missing ] logger.debug(f"testing incorrect batch format: {type(incorrect_batch)}") pytest.raises(valueerror, match="batch must contain inputs labels."): trainer.validation_step(incorrect_batch, 0) def test_model_loading_from_checkpoint(mocker): logger.debug("starting test_model_loading_from_checkpoint") # load model checkpoint checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt" logger.debug(f"attempting load checkpoint from: {checkpoint_path}") # check checkpoint file exists os.path.isfile(checkpoint_path): pytest.skip(f"checkpoint file found: {checkpoint_path}") try: checkpoint = torch.load(checkpoint_path) logger.debug(f"checkpoint loaded successfully. keys: {checkpoint.keys()}") except filenotfounderror e: logger.error(f"failed load checkpoint: {str(e)}") pytest.fail(f"failed load checkpoint: {str(e)}") except exception e: logger.error(f"unexpected error: {str(e)}") pytest.fail(f"unexpected error: {str(e)}") # extract print config checkpoint 'config' checkpoint: config_dict = checkpoint['config'] logger.debug(f"config found checkpoint: {json.dumps(config_dict, indent=2)}") else: logger.error("config found checkpoint") pytest.fail("config found checkpoint") # reconstruct modelconfig try: model_config = modelconfig( n_embd=config_dict['n_embd'], n_head=config_dict['n_head'], n_layer=config_dict['n_layer'], dropout=config_dict['dropout'] ) logger.debug(f"modelconfig reconstructed: {model_config}") except keyerror e: logger.error(f"missing key config_dict: {str(e)}") pytest.fail(f"failed reconstruct modelconfig: {str(e)}") # initialize model try: model = gpt2arc(model_config) logger.debug("model initialized successfully") except exception e: logger.error(f"failed initialize model: {str(e)}") pytest.fail(f"failed initialize model: {str(e)}") # load state dict try: state_dict = {k.replace("model.", ""): v k, v checkpoint['state_dict'].items()} model.load_state_dict(state_dict) logger.debug("state dict loaded successfully") except exception e: logger.error(f"failed load state dict: {str(e)}") pytest.fail(f"failed load state dict: {str(e)}") # ensure model evaluation mode model.eval() assert model.training, "model evaluation mode calling eval()" logger.debug("completed test_model_loading_from_checkpoint") def test_checkpoint_contains_model_config(): checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt" logger.debug(f"checking checkpoint file at: {checkpoint_path}") os.path.isfile(checkpoint_path): logger.warning(f"checkpoint file found: {checkpoint_path}") pytest.skip(f"checkpoint file found: {checkpoint_path}") try: checkpoint = torch.load(checkpoint_path) # log keys checkpoint logger.debug(f"checkpoint keys: {checkpoint.keys()}") except filenotfounderror e: logger.error(f"filenotfounderror: {str(e)}") pytest.fail(f"filenotfounderror: {str(e)}") except exception e: logger.error(f"unexpected error: {str(e)}") pytest.fail(f"unexpected error: {str(e)}") # check model configuration assert 'config' checkpoint, "model configuration found checkpoint." model_config = checkpoint['config'] logger.debug(f"model configuration found checkpoint: {model_config}") print("model configuration found checkpoint:", model_config)</file><file name="tests/test_checkpoint_loading.py">import torch import pytest def test_actual_checkpoint_loading(): # path actual checkpoint file checkpoint_path = 'final_model_4fe9801e-c839-454f-a46c-6e94e3c04e81.pth' # load checkpoint checkpoint = torch.load(checkpoint_path) # print keys debugging purposes print("checkpoint keys:", checkpoint.keys()) # check checkpoint contains expected keys expected_keys = ['conv1.weight', 'conv1.bias', 'blocks.0.attention.key.weight'] key expected_keys: assert key checkpoint, f"checkpoint contain expected key: {key}" __name__ == "__main__": pytest.main([__file__])</file><file name="tests/test_hyperparameter_optimization.py">import unittest unittest.mock import patch import sys import os # adjust import path necessary sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'src'))) gpt2_arc.src.optimize_hyperparameters import run_optimization class testhyperparameteroptimization(unittest.testcase): @patch('optuna.create_study') def test_run_optimization(self, mock_create_study): # mock study object methods mock_study = mock_create_study.return_value mock_study.optimize.return_value = none mock_study.best_trial = none # simulate successful trials # call function minimal parameters try: run_optimization(n_trials=1, n_jobs=1) success = true except exception e: success = false print(f"optimization failed exception: {e}") self.asserttrue(success, "hyperparameter optimization run without errors") __name__ == '__main__': unittest.main()</file><file name="tests/test_optimize_hyperparameters.py">import unittest unittest.mock import patch import optuna gpt2_arc.src.training.train import main class testhyperparameteroptimization(unittest.testcase): def test_load_best_hyperparameters(self): # mock optuna study predefined best_params best_params = {'n_head_exp': 2, 'n_embd_multiplier': 4, 'n_layer': 3, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.001} patch('optuna.load_study') mocked_load_study: mocked_load_study.return_value.best_params = best_params # assuming function get_best_hyperparameters exists best_hyperparams = main.get_best_hyperparameters(study_name="test_study", storage="sqlite:///test.db") self.assertequal(best_hyperparams, best_params, "loaded best hyperparameters match expected values.") __name__ == '__main__': unittest.main()</file><file name="tests/test_class_weights.py">import unittest import torch gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.models.gpt2 import gpt2arc class testclassweights(unittest.testcase): def setup(self): # define sample symbol frequency dictionary self.symbol_freq = { "0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05 } # define model training configurations model_config = modelconfig( n_embd=16, n_head=2, n_layer=2, mamba_ratio=1, d_state=4, d_conv=1, dropout=0.05 ) training_config = trainingconfig( batch_size=2, learning_rate=0.001, max_epochs=10, use_gpu=false, log_level="debug", use_synthetic_data=false, balance_symbols=true, balancing_method="weighting", synthetic_data_path=none, symbol_freq=self.symbol_freq ) # create config object self.config = config(model=model_config, training=training_config) # initialize gpt2arc model symbol_freq self.model = gpt2arc(config=self.config, num_classes=len(self.symbol_freq), symbol_freq=self.symbol_freq) def test_class_weights_correctness(self): # calculate expected class weights expected_weights = {k: 1.0 / v k, v self.symbol_freq.items()} expected_weights_tensor = torch.tensor([expected_weights[str(i)] range(len(self.symbol_freq))]) # retrieve class weights model's loss function hasattr(self.model, 'loss_fn') hasattr(self.model.loss_fn, 'weight'): actual_weights = self.model.loss_fn.weight # assert actual_weights matches expected_weights_tensor self.asserttrue(torch.allclose(actual_weights, expected_weights_tensor), f"actual class weights {actual_weights} match expected {expected_weights_tensor}") else: self.fail("the model's loss function 'weight' attribute.") def test_class_weights_application(self): # create dummy outputs labels outputs = torch.tensor([[0.1, 0.6, 0.3, 0.0, 0.0, 0.0], [0.3, 0.3, 0.2, 0.1, 0.05, 0.05]], requires_grad=true) labels = torch.tensor([1, 2]) # compute loss loss = self.model.loss_fn(outputs, labels) # manually calculate expected loss expected_loss = (-torch.log(torch.tensor(0.6)) / self.symbol_freq["1"]) + (-torch.log(torch.tensor(0.2)) / self.symbol_freq["2"]) expected_loss = expected_loss / 2 # average batch # check calculated loss matches expected loss self.assertalmostequal(loss.item(), expected_loss.item(), places=4, msg=f"computed loss {loss.item()} match expected {expected_loss.item()}") __name__ == '__main__': unittest.main()</file><file name="tests/test_arc_dataset.py"># gpt2_arc/tests/test_arc_dataset.py import os import numpy np import pytest import torch import random import logging import arckit torch.utils.data import dataloader src.data.arc_dataset import arcdataset, set_debug_mode src.utils.experiment_tracker import experimenttracker # set logging tests logger = logging.getlogger(__name__) logger.setlevel(logging.error) @pytest.fixture(scope="module") def debug_mode(): set_debug_mode(true) yield set_debug_mode(false) unittest.mock import mock arckit.data import taskset @pytest.fixture def sample_data(): return [ {'input': [[1, 0], [0, 1]], 'output': [[0, 1], [1, 0]]}, {'input': [[0, 1], [1, 0]], 'output': [[1, 0], [0, 1]]} ] @pytest.fixture def mock_taskset(): mock_task = mock() mock_task.id = "mock_task_1" mock_task.train = [ (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])), (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]])) ] mock_task.test = [ (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]])) ] mock_taskset = mock(spec=taskset) mock_taskset.tasks = [mock_task] return mock_taskset def test_dataset_statistics_computation(sample_data): dataset = arcdataset(sample_data, debug=true) # retrieve statistics grid_size_stats = dataset.get_grid_size_stats() symbol_frequencies = dataset.get_symbol_frequencies() # assertions grid size statistics assert "max_height" grid_size_stats, "grid size stats include 'max_height'" assert "max_width" grid_size_stats, "grid size stats include 'max_width'" assert grid_size_stats["max_height"] == 2, "max height 2 sample data" assert grid_size_stats["max_width"] == 2, "max width 2 sample data" # assertions symbol frequencies expected_frequencies = { 0: 4 / 8, # 4 occurrences symbol '0' 1: 4 / 8, # 4 occurrences symbol '1' # assuming num_symbols=10, remaining symbols frequency 0 2: 0.0, 3: 0.0, 4: 0.0, 5: 0.0, 6: 0.0, 7: 0.0, 8: 0.0, 9: 0.0 } symbol, freq expected_frequencies.items(): assert symbol_frequencies.get(symbol, 0.0) == freq, f"frequency symbol {symbol} {freq}" def test_dataset_statistics_caching(sample_data): dataset = arcdataset(sample_data, debug=true) # ensure statistics cached assert hasattr(dataset, 'statistics'), "dataset 'statistics' attribute caching" assert "grid_size_stats" dataset.statistics, "statistics include 'grid_size_stats'" assert "symbol_frequencies" dataset.statistics, "statistics include 'symbol_frequencies'" # reload dataset ensure statistics loaded cache dataset_reloaded = arcdataset(sample_data, debug=true) grid_size_stats = dataset_reloaded.get_grid_size_stats() symbol_frequencies = dataset_reloaded.get_symbol_frequencies() # assertions verify consistency assert grid_size_stats == dataset.statistics["grid_size_stats"], "grid size stats match reloading cache" assert symbol_frequencies == dataset.statistics["symbol_frequencies"], "symbol frequencies match reloading cache" dataset = arcdataset(sample_data, debug=true) logger.debug(f"dataset length: {len(dataset)}, expected: {len(sample_data)}") assert len(dataset) == len(sample_data), "dataset length mismatch" input_grid, output_grid, *_ = dataset[0] logger.debug(f"input grid shape: {input_grid.shape}, expected: (1, 30, 30)") logger.debug(f"output grid shape: {output_grid.shape}, expected: (1, 30, 30)") assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" # update shape check match new preprocessing logic assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # verify original data preserved center padded grid center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] logger.debug(f"center input:\n{center_input}") logger.debug(f"center output:\n{center_output}") assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" dataset = arcdataset(sample_data) assert len(dataset) == 2, "dataset 2 samples" input_grid, output_grid, *_ = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" # update shape check match new preprocessing logic assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # verify original data preserved center padded grid center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" #skip @pytest.mark.skip(reason="skipping test synthetic data test problematic") def test_arc_dataset_synthetic_data(debug_mode): synthetic_data_path = "/volumes/totallynotaharddrive/arc-neural-reasoning-model/syntheticarc/tasks" assert os.path.isdir(synthetic_data_path), f"directory exist: {synthetic_data_path}" train_dataset = arcdataset(synthetic_data_path, is_test=false, debug=true) test_dataset = arcdataset(synthetic_data_path, is_test=true, debug=true) assert len(train_dataset) &gt; 0, "synthetic train dataset empty" assert len(test_dataset) &gt; 0, "synthetic test dataset empty" logger.debug(f"loaded {len(train_dataset.data)} synthetic tasks") logger.debug(f"total train dataset length: {len(train_dataset)}") logger.debug(f"total test dataset length: {len(test_dataset)}") total_train = sum(len(task['train']) task train_dataset.data) total_test = sum(len(task['test']) task test_dataset.data) logger.debug(f"total train samples: {total_train}") logger.debug(f"total test samples: {total_test}") i, task enumerate(train_dataset.data): print(f"task {i} - train samples: {len(task['train'])}, test samples: {len(task['test'])}") assert len(train_dataset) == total_train, f"train dataset length ({len(train_dataset)}) match total train samples ({total_train})" assert len(test_dataset) == total_test, f"test dataset length ({len(test_dataset)}) match total test samples ({total_test})" len(train_dataset) == 0: pytest.skip("train dataset empty; skipping random sample tests.") print(f"train dataset size: {len(train_dataset)}") print(f"test dataset size: {len(test_dataset)}") len(train_dataset) &lt; 3: pytest.skip("not enough data train dataset random sampling tests.") # test random samples train dataset range(3): idx = random.choice(range(len(train_dataset))) try: print(f"\ntrain sample {i + 1}:") print(f"generated index: {idx}") input_grid, output_grid = train_dataset[idx] print(f"input grid shape: {input_grid.shape}") print(f"output grid shape: {output_grid.shape}") except indexerror e: print(f"error: attempted access index {idx} range. train dataset size {len(train_dataset)}.") pytest.fail(f"generated index {idx} range train dataset size {len(train_dataset)}: {str(e)}") # verify grid sizes max_h, max_w = train_dataset.max_grid_size assert max_h &gt; 0 max_w &gt; 0, "grid size positive" print(f"maximum grid size: {train_dataset.max_grid_size}") # verify access train test splits assert len(train_dataset.data) &gt; 0, "dataset contain least one task" assert 'train' train_dataset.data[0], "each task 'train' split" assert 'test' train_dataset.data[0], "each task 'test' split" print(f"train dataset length: {len(train_dataset)}") print(f"test dataset length: {len(test_dataset)}") def test_data_loader_parameters(sample_data): dataset = arcdataset(sample_data) num_workers = 4 # example value prefetch_factor = 2 persistent_workers = true train_loader = dataloader( dataset, batch_size=2, num_workers=num_workers, prefetch_factor=prefetch_factor, persistent_workers=persistent_workers ) assert train_loader.num_workers == num_workers, "num_workers set correctly" assert train_loader.prefetch_factor == prefetch_factor, "prefetch_factor set correctly" assert train_loader.persistent_workers == persistent_workers, "persistent_workers set correctly" def test_dataloader_loading(sample_data): dataset = arcdataset(sample_data) data_loader = dataloader( dataset, batch_size=1, num_workers=2, prefetch_factor=1, persistent_workers=true ) batch data_loader: inputs, outputs, task_ids = batch assert inputs.shape == (1, 1, 30, 30), "input shape mismatch" assert outputs.shape == (1, 1, 30, 30), "output shape mismatch" dataset = arcdataset(sample_data) input_grid, output_grid, *_ = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # check original data preserved center center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" def test_arc_dataset_len(sample_data): print("debugging: entering test_arc_dataset_len") print(f"debugging: sample_data = {sample_data}") dataset = arcdataset(sample_data) print(f"debugging: len(dataset) = {len(dataset)}, len(sample_data) = {len(sample_data)}") assert len(dataset) == len(sample_data), "dataset length match input data length" print("debugging: exiting test_arc_dataset_len") def test_arc_dataset_invalid_data(sample_data): invalid_data = [{"input": [1, 0], "output": [[0, 1], [1, 0]]}] pytest.raises(valueerror): arcdataset(invalid_data) invalid_data = [{"input": [[1, 0], [0, 1]], "output": "not list"}] pytest.raises(valueerror): arcdataset(invalid_data) def test_arc_dataset_preprocess_grid(sample_data): dataset = arcdataset(sample_data, num_symbols=10) input_grid, output_grid, *_ = dataset[0] print(f"input grid shape: {input_grid.shape}") print(f"output grid shape: {output_grid.shape}") print(f"input grid content:\n{input_grid}") print(f"output grid content:\n{output_grid}") # check grids indeed 3d assert input_grid.ndim == 3, f"expected 3d input grid, got {input_grid.ndim}d" assert output_grid.ndim == 3, f"expected 3d output grid, got {output_grid.ndim}d" # check shape (1, 30, 30) assert input_grid.shape == (1, 30, 30), f"preprocessed grid shape (1, 30, 30), got {input_grid.shape}" assert output_grid.shape == (1, 30, 30), f"preprocessed grid shape (1, 30, 30), got {output_grid.shape}" # check original data preserved center expected_input = torch.zeros((1, 30, 30)) expected_input[0, 14:16, 14:16] = torch.tensor([[1., 0.], [0., 1.]]) expected_output = torch.zeros((1, 30, 30)) expected_output[0, 14:16, 14:16] = torch.tensor([[0., 1.], [1., 0.]]) print(f"expected input:\n{expected_input}") print(f"expected output:\n{expected_output}") assert torch.allclose(input_grid, expected_input), "input grid data mismatch" assert torch.allclose(output_grid, expected_output), "output grid data mismatch" @pytest.fixture def mock_taskset(): mock_task = mock() mock_task.id = "mock_task_1" mock_task.train = [ (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])), (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]])) ] mock_task.test = [ (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]])) ] mock_taskset = mock(spec=taskset) mock_taskset.tasks = [mock_task] return mock_taskset def test_experiment_tracker_logs_dataset_statistics(sample_data): # initialize arcdataset dataset = arcdataset(sample_data, debug=true) # initialize experimenttracker mock config mock_config = {"dummy_key": "dummy_value"} tracker = experimenttracker(config=mock_config, project="test_project") # retrieve dataset statistics train_grid_stats = dataset.get_grid_size_stats() train_symbol_freq = dataset.get_symbol_frequencies() val_grid_stats = dataset.get_grid_size_stats() # assuming dataset simplicity val_symbol_freq = dataset.get_symbol_frequencies() # log metrics tracker.log_metric("train_max_grid_height", train_grid_stats.get("max_height", 0)) tracker.log_metric("train_max_grid_width", train_grid_stats.get("max_width", 0)) tracker.log_metric("train_symbol_frequencies", train_symbol_freq) tracker.log_metric("val_max_grid_height", val_grid_stats.get("max_height", 0)) tracker.log_metric("val_max_grid_width", val_grid_stats.get("max_width", 0)) tracker.log_metric("val_symbol_frequencies", val_symbol_freq) # assertions verify metrics logged correctly assert tracker.metrics["train_max_grid_height"] == train_grid_stats.get("max_height", 0) assert tracker.metrics["train_max_grid_width"] == train_grid_stats.get("max_width", 0) assert tracker.metrics["train_symbol_frequencies"] == train_symbol_freq assert tracker.metrics["val_max_grid_height"] == val_grid_stats.get("max_height", 0) assert tracker.metrics["val_max_grid_width"] == val_grid_stats.get("max_width", 0) assert tracker.metrics["val_symbol_frequencies"] == val_symbol_freq def test_collate_fn_output(): sample_data = [ {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}, {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]}, ] dataset = arcdataset(sample_data) dataloader = dataloader(dataset, batch_size=2, collate_fn=arcdataset.collate_fn) batch = next(iter(dataloader)) assert isinstance(batch, list), "collate function return list" assert len(batch) == 3, "collate function return list 3 elements" assert isinstance(batch[0], torch.tensor), "first element tensor (inputs)" assert isinstance(batch[1], torch.tensor), "second element tensor (outputs)" assert batch[0].shape == (2, 1, 30, 30), "input tensor shape (batch_size, 1, 30, 30)" assert batch[1].shape == (2, 1, 30, 30), "output tensor shape (batch_size, 1, 30, 30)" assert batch[0].dtype == torch.float32, "input tensor type float32" assert batch[1].dtype == torch.float32, "output tensor type float32" def test_getitem_output(): sample_data = [ {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}, ] dataset = arcdataset(sample_data) input_grid, output_grid, *_ = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" assert input_grid.dtype == torch.float32, "input grid float32" assert output_grid.dtype == torch.float32, "output grid float32" #skip @pytest.mark.skip(reason="skipping test problematic") def test_arc_dataset_taskset_initialization(mock_taskset): import logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) logger.debug(f"mock taskset: {mock_taskset}") logger.debug(f"mock taskset attributes: {dir(mock_taskset)}") print(f"mock task train data: {mock_taskset.tasks[0].train}") print(f"mock task test data: {mock_taskset.tasks[0].test}") dataset = arcdataset(mock_taskset) logger.debug(f"dataset length: {len(dataset)}") print(f"dataset length: {len(dataset)}, expected: 3") assert len(dataset) == 3, "dataset 3 samples (2 train + 1 test)" input_grid, output_grid, *_ = dataset[0] print(f"input grid shape: {input_grid.shape}, expected: (1, 30, 30)") print(f"output grid shape: {output_grid.shape}, expected: (1, 30, 30)") assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # check original data preserved center center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] print(f"center input: {center_input}") print(f"center output: {center_output}") assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" import logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) logger.debug(f"mock taskset: {mock_taskset}") logger.debug(f"mock taskset attributes: {dir(mock_taskset)}") dataset = arcdataset(mock_taskset) logger.debug(f"dataset length: {len(dataset)}") assert len(dataset) == 3, "dataset 3 samples (2 train + 1 test)" input_grid, output_grid = dataset[0] assert isinstance(input_grid, torch.tensor), "input torch.tensor" assert isinstance(output_grid, torch.tensor), "output torch.tensor" assert input_grid.shape == (1, 30, 30), "input grid shape (1, 30, 30)" assert output_grid.shape == (1, 30, 30), "output grid shape (1, 30, 30)" # check original data preserved center center_input = input_grid[0, 14:16, 14:16] center_output = output_grid[0, 14:16, 14:16] assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "input data preserved correctly" assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "output data preserved correctly" torch.utils.data import dataloader def test_arc_dataset_collate_fn(sample_data): logger.debug("starting test_arc_dataset_collate_fn") dataset = arcdataset(sample_data) dataloader = dataloader(dataset, batch_size=2, collate_fn=arcdataset.collate_fn) batch = next(iter(dataloader)) input_batch, output_batch, *_ = batch logger.debug(f"collated batch shapes - inputs: {input_batch.shape}, outputs: {output_batch.shape}") assert input_batch.shape == (2, 1, 30, 30), "batched input shape (2, 1, 30, 30)" assert output_batch.shape == (2, 1, 30, 30), "batched output shape (2, 1, 30, 30)" logger.debug("completed test_arc_dataset_collate_fn") def test_arc_dataset_variable_size_grids(sample_data): logger.debug("starting test_arc_dataset_variable_size_grids") variable_data = sample_data + [{"input": [[1, 0, 2], [0, 2, 1], [2, 1, 0]], "output": [[2, 1, 0], [1, 0, 2], [0, 2, 1]]}] dataset = arcdataset(variable_data) # check first sample (2x2) input_grid_1, output_grid_1, *_ = dataset[0] assert input_grid_1.shape == (1, 30, 30), "first sample shape (1, 30, 30)" assert output_grid_1.shape == (1, 30, 30), "first sample shape (1, 30, 30)" # check center first sample (2x2) center_input_1 = input_grid_1[0, 14:16, 14:16] center_output_1 = output_grid_1[0, 14:16, 14:16] assert torch.allclose(center_input_1, torch.tensor([[1., 0.], [0., 1.]])), "first sample input data preserved correctly" assert torch.allclose(center_output_1, torch.tensor([[0., 1.], [1., 0.]])), "first sample output data preserved correctly" # check third sample (3x3) input_grid_2, output_grid_2, *_ = dataset[2] assert input_grid_2.shape == (1, 30, 30), "third sample shape (1, 30, 30)" assert output_grid_2.shape == (1, 30, 30), "third sample shape (1, 30, 30)" # check center third sample (3x3) center_input_2 = input_grid_2[0, 13:16, 13:16] center_output_2 = output_grid_2[0, 13:16, 13:16] assert torch.allclose(center_input_2, torch.tensor([[1., 0., 2.], [0., 2., 1.], [2., 1., 0.]])), f"third sample input data preserved correctly. got:\n{center_input_2}" assert torch.allclose(center_output_2, torch.tensor([[2., 1., 0.], [1., 0., 2.], [0., 2., 1.]])), f"third sample output data preserved correctly. got:\n{center_output_2}" logger.debug("completed test_arc_dataset_variable_size_grids") def test_arc_dataset_with_arckit_data_get_task_id(): # load data using arckit train_set, _ = arckit.load_data() # initialize dataset dataset = arcdataset(train_set, is_test=false) # check __getitem__ returns correct structure input_grid, output_grid, task_id = dataset[0] assert isinstance(input_grid, torch.tensor), "input grid torch.tensor" assert isinstance(output_grid, torch.tensor), "output grid torch.tensor" assert isinstance(task_id, str), "task id string" # test collate_fn batch = [dataset[i] range(2)] # create batch two samples collated_inputs, collated_outputs, collated_task_ids = arcdataset.collate_fn(batch) assert len(collated_task_ids) == 2, "batch size 2" assert collated_inputs.shape[0] == 2, "batch size 2" assert collated_outputs.shape[0] == 2, "batch size 2"</file><file name="tests/test_arc_trainer.py">import unittest import torch torch.utils.data import dataloader gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.data.arc_dataset import arcdataset class testarctrainer(unittest.testcase): def setup(self): # define model training configurations model_config = modelconfig( n_embd=16, n_head=2, n_layer=2, mamba_ratio=1, d_state=4, d_conv=1, dropout=0.05 ) training_config = trainingconfig( batch_size=2, learning_rate=0.001, max_epochs=10, use_gpu=false, log_level="debug", use_synthetic_data=false, balance_symbols=true, balancing_method="weighting", synthetic_data_path=none, symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05} ) self.config = config(model=model_config, training=training_config) self.model = gpt2arc(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq) self.train_dataset = arcdataset(data_source="path/to/mock_data") self.val_dataset = arcdataset(data_source="path/to/mock_data") def test_training_step_computes_loss(self): trainer = arctrainer(model=self.model, train_dataset=self.train_dataset, val_dataset=self.val_dataset, config=self.config) batch = next(iter(dataloader(self.train_dataset, batch_size=2))) loss = trainer.training_step(batch, batch_idx=0) self.assertisinstance(loss, torch.tensor, "loss torch.tensor instance.") self.assertgreater(loss.item(), 0, "loss positive.") def test_configure_optimizers(self): trainer = arctrainer(model=self.model, train_dataset=self.train_dataset, val_dataset=self.val_dataset, config=self.config) optimizers, schedulers = trainer.configure_optimizers() self.assertisinstance(optimizers[0], torch.optim.adam, "optimizer adam.") self.assertisinstance(schedulers[0]['scheduler'], torch.optim.lr_scheduler.steplr, "scheduler steplr.") __name__ == '__main__': unittest.main()</file><file name="tests/test_metrics.py"># gpt2_arc/tests/test_metrics.py import pytest import torch gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, trainingconfig, modelconfig def generate_test_data(num_samples=100): """ generates synthetic predictions targets testing. returns: preds (torch.tensor): predicted class indices. targets (torch.tensor): ground truth class indices. """ preds = torch.randint(0, 3, (num_samples,)) targets = torch.randint(0, 3, (num_samples,)) return preds, targets @pytest.fixture def model(): config = config( model=modelconfig(n_embd=4, n_head=2, n_layer=3, dropout=0.1, mamba_ratio=1.0, d_state=2, d_conv=4), training=trainingconfig(balance_symbols=true, balancing_method="weighting"), evaluation=none ) symbol_freq_dict = {'0': 0.5, '1': 0.3, '2': 0.2} return gpt2arc(config=config, num_classes=3, symbol_freq=symbol_freq_dict) def test_training_metrics(model): preds, targets = generate_test_data() # mock batch batch = (torch.tensor([0]*len(preds)), targets, ['task']*len(preds)) # perform training step loss = model.training_step(batch, batch_idx=0) # check metrics computed logged assert 'train_loss' model.logged_metrics assert 'train_precision' model.logged_metrics assert 'train_recall' model.logged_metrics assert 'train_f1' model.logged_metrics # verify loss scalar tensor assert isinstance(loss, torch.tensor) assert loss.dim() == 0, "loss scalar tensor" def test_validation_metrics(model): preds, targets = generate_test_data() # mock batch batch = (torch.tensor([0]*len(preds)), targets, ['task']*len(preds)) # perform validation step model.validation_step(batch, batch_idx=0) # check metrics computed logged assert 'val_loss' model.logged_metrics assert 'val_precision' model.logged_metrics assert 'val_recall' model.logged_metrics assert 'val_f1' model.logged_metrics</file><file name="tests/test_grokfast.py">import pytest collections import deque gpt2_arc.src.utils.grokfast import gradfilter_ma, gradfilter_ema gpt2_arc.src.utils.grokfast_callback import grokfastcallback import torch.nn nn import torch unittest.mock import magicmock def test_gradfilter_ma(): # initialize simple model model = nn.linear(10, 10) # create dummy gradients initialize grads dictionary initial_grad = torch.ones(10, 10) model.weight.grad = initial_grad.clone() grads = {name: deque(maxlen=5) name, param model.named_parameters() param.requires_grad} # apply gradfilter_ma updated_grads = gradfilter_ma( m=model, grads=grads, window_size=5, lamb=2.0, filter_type='mean', warmup=false, trigger=false ) # assertions verify gradients updated correctly assert "weight" updated_grads assert len(updated_grads["weight"]) == 1 expected_grad = initial_grad + (initial_grad * 2.0) assert torch.allclose(model.weight.grad, expected_grad) def test_gradfilter_ema(): # initialize simple model model = nn.linear(10, 10) # create dummy gradients initialize grads dictionary initial_grad = torch.ones(10, 10) model.weight.grad = initial_grad.clone() grads = none # ema, grads start none # apply gradfilter_ema first time updated_grads = gradfilter_ema( m=model, grads=grads, alpha=0.9, lamb=2.0 ) # expected gradient first update expected_grad = initial_grad * 2.0 assert "weight" updated_grads assert torch.allclose(model.weight.grad, expected_grad) # apply gradfilter_ema new gradients new_grad = torch.ones(10, 10) * 2.0 model.weight.grad = new_grad.clone() updated_grads = gradfilter_ema( m=model, grads=updated_grads, alpha=0.9, lamb=2.0 ) # expected gradient second update expected_grad = new_grad + (updated_grads["weight"] * 2.0) assert torch.allclose(model.weight.grad, expected_grad) class testgrokfastcallback: def test_grokfast_callback_ema(self): # initialize simple model model = nn.linear(10, 10) # initialize callback ema settings callback = grokfastcallback( filter_type='ema', alpha=0.9, lamb=2.0, window_size=100, warmup=true, trigger=false ) # mock trainer pl_module trainer = magicmock() pl_module = magicmock() pl_module.model = model # set initial gradients initial_grad = torch.ones(10, 10) model.weight.grad = initial_grad.clone() # call callback backward callback.on_after_backward(trainer, pl_module) # expected gradient update expected_grad = initial_grad + (initial_grad * 2.0) assert torch.allclose(model.weight.grad, expected_grad) def test_grokfast_callback_ma(self): # initialize simple model model = nn.linear(10, 10) # initialize callback settings callback = grokfastcallback( filter_type='ma', alpha=0.9, # used required parameter lamb=2.0, window_size=5, warmup=false, trigger=false ) # mock trainer pl_module trainer = magicmock() pl_module = magicmock() pl_module.model = model # set initial gradients initial_grad = torch.ones(10, 10) model.weight.grad = initial_grad.clone() # call callback backward callback.on_after_backward(trainer, pl_module) # expected gradient update expected_grad = initial_grad + (initial_grad * 2.0) assert torch.allclose(model.weight.grad, expected_grad) # apply test window functionality new_grad = torch.ones(10, 10) * 2.0 model.weight.grad = new_grad.clone() callback.on_after_backward(trainer, pl_module) # expected gradient second update expected_grad = new_grad + (initial_grad * 2.0) assert torch.allclose(model.weight.grad, expected_grad)</file><file name="src/evaluate.py"># gpt2_arc/src/evaluate.py import sys import sys import os import json import argparse import pytorch_lightning pl import os import torch import wandb import numpy np datetime import datetime pytorch_lightning.utilities.model_summary import modelsummary torchsummary import summary pytorch_lightning.utilities.model_summary import modelsummary # define base directory arc-neural-reasoning-model arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) # add root directory project pythonpath project_root = arc_model_dir sys.path.insert(0, project_root) gpt2_arc.src.config import config, modelconfig, trainingconfig, evaluationconfig import arckit import logging gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.utils.helpers import differential_pixel_accuracy # set logging logging.basicconfig(level=logging.info) logger = logging.getlogger(__name__) def evaluate(model, test_dataset, config, batch_size=32): trainer = arctrainer(model, none, test_dataset, config=config) pl_trainer = pl.trainer(accelerator='gpu' torch.cuda.is_available() else 'cpu') results = pl_trainer.test(trainer) logger.debug(f"debug: raw results test: {results}") avg_test_loss = pl_trainer.callback_metrics.get('avg_test_loss') avg_test_accuracy = pl_trainer.callback_metrics.get('avg_test_accuracy') avg_test_diff_accuracy = pl_trainer.callback_metrics.get('avg_test_diff_accuracy') # convert tensors python floats necessary avg_test_loss none: avg_test_loss = avg_test_loss.item() avg_test_accuracy none: avg_test_accuracy = avg_test_accuracy.item() avg_test_diff_accuracy none: avg_test_diff_accuracy = avg_test_diff_accuracy.item() aggregated_results = { 'test_loss': avg_test_loss, 'test_accuracy': avg_test_accuracy, 'test_diff_accuracy': avg_test_diff_accuracy, } print(f"debug: logged metrics - avg test loss: {avg_test_loss}, avg test accuracy: {avg_test_accuracy}, avg diff accuracy: {avg_test_diff_accuracy}") # collect individual task metrics individual_metrics = {} key, value pl_trainer.callback_metrics.items(): '_test_accuracy' key '_test_diff_accuracy' key: isinstance(value, torch.tensor): value = value.item() # key format: 'taskid_test_accuracy' 'taskid_test_diff_accuracy' task_id, metric_name = key.split('_test_') task_id individual_metrics: individual_metrics[task_id] = {} individual_metrics[task_id][f'test_{metric_name}'] = value # compute complete task accuracy (fraction tasks perfect accuracy) num_tasks = len(individual_metrics) perfect_accuracy_threshold = config.evaluation.perfect_accuracy_threshold / 100.0 # convert percentage fraction num_complete_accuracy = 0 task_id, metrics individual_metrics.items(): test_accuracy = metrics.get('test_accuracy', 0) # determine task completely solved completely_solved = test_accuracy &gt;= perfect_accuracy_threshold metrics['completely_solved'] = completely_solved completely_solved: num_complete_accuracy += 1 complete_task_accuracy = num_complete_accuracy / num_tasks num_tasks &gt; 0 else 0.0 aggregated_results['complete_task_accuracy'] = complete_task_accuracy print(f"debug: computed complete task accuracy: {complete_task_accuracy}") return aggregated_results, individual_metrics def load_config_from_json(json_path): open(json_path, 'r') f: data = json.load(f) return data['config'] def save_results(results, individual_metrics, output_dir, model_name, model_summary): timestamp = datetime.now().strftime("%y%m%d_%h%m%s") filename = f"{model_name}_eval_results_{timestamp}.json" output_path = os.path.join(output_dir, filename) data_to_save = { "aggregate_results": results, "individual_metrics": {task_id: metrics task_id, metrics individual_metrics.items()}, "model_summary": str(model_summary) # convert modelsummary string } logger.debug(f"debug: data saved: {data_to_save}") os.makedirs(output_dir, exist_ok=true) open(output_path, 'w') f: json.dump(data_to_save, f, indent=2) logger.info(f"results saved {output_path}") return output_path def main(args): args.use_wandb: api_key = os.getenv("wandb_api_key") api_key: wandb.login(key=api_key) wandb.init(project=args.wandb_project, name=args.wandb_run_name) else: print("warning: wandb_api_key found environment variables.") print("weights &amp; biases logging disabled.") args.use_wandb = false else: print("weights &amp; biases logging disabled.") # load test data using arckit _, test_set = arckit.load_data() test_data = arcdataset(test_set) # compute symbol frequencies test dataset symbol_freq_array = test_data.get_symbol_frequencies() symbol_freq = {str(i): float(freq) i, freq enumerate(symbol_freq_array)} checkpoint = torch.load(args.model_checkpoint, map_location='cpu') # extract convert model configuration checkpoint 'model_config' checkpoint: model_config_dict = checkpoint['model_config'] # convert dict modelconfig object model_config = modelconfig(**model_config_dict) else: logger.error("model configuration found checkpoint. please ensure checkpoint includes 'model_config'.") raise valueerror("model configuration found checkpoint. ensure training process includes modelconfigsaver callback.") # create configuration config = config( model=model_config, training=trainingconfig(), evaluation=evaluationconfig() ) # determine number classes test dataset max_label_test = max([sample[1].max().item() sample test_data]) num_classes = int(max_label_test) + 1 # ensure num_classes integer config.training.symbol_freq = symbol_freq # initialize model complete config object symbol frequencies model = gpt2arc(config, num_classes=num_classes, symbol_freq=symbol_freq) try: # remove "model." prefix state dict keys state_dict = {k.replace('model.', ''): v k, v checkpoint['state_dict'].items()} model.load_state_dict(state_dict) except exception e: logger.error(f"error loading state_dict: {e}") logger.error(f"available keys checkpoint: {list(checkpoint.keys())}") raise model.eval() # generate model summary print("debug: attempting generate model summary") try: model_summary = str(modelsummary(model, max_depth=-1)) print("debug: model summary generated successfully") except exception e: print(f"debug: error generating model summary - {str(e)}") model_summary = "error generating model summary" print("debug: model summary:") print(model_summary) device = torch.device('cuda' torch.cuda.is_available() else 'cpu') model.to(device) logger.info(f"model moved device: {device}") # define input size based model's expected input sequence_length = 100 # example value; adjust needed input_size = (1, 1, sequence_length) # adjusted match (batch_size, channels, sequence_length) logger.info(f"defined input_size summary: {input_size}") # extract model name checkpoint path sanitize model_name = os.path.basename(args.model_checkpoint).split('.')[0] # sanitize model_name contain valid characters model_name = ''.join(c c.isalnum() c '-_.' else '_' c model_name) # debugging statements logger.debug(f"sanitized model_name: {model_name}") print(f"debug: sanitized model_name: {model_name}") # verify model_name contains allowed characters allowed_chars = set("abcdefghijklmnopqrstuvwxyzabcdefghijklmnopqrstuvwxyz0123456789-_.") invalid_chars = set(model_name) - allowed_chars invalid_chars: logger.error(f"model name contains invalid characters sanitization: {invalid_chars}") print(f"error: model name contains invalid characters sanitization: {invalid_chars}") else: logger.debug("model name contains valid characters.") print("debug: model name contains valid characters.") # create configuration config = config( model=model_config, training=trainingconfig(), evaluation=evaluationconfig() ) # evaluate model results, individual_metrics = evaluate(model, test_data, config, args.batch_size) logger.debug(f"debug: evaluation results: {results}") logger.debug(f"debug: individual metrics: {individual_metrics}") logger.info("evaluation results:") metric, value results.items(): metric != 'complete_task_accuracy': print(f"{metric}: {value}") args.use_wandb: wandb.log({f"eval/{metric}": value}) # print complete_task_accuracy bottom 'complete_task_accuracy' results: print(f"complete_task_accuracy: {results['complete_task_accuracy']}") args.use_wandb: wandb.log({"eval/complete_task_accuracy": results['complete_task_accuracy']}) # log individual task metrics task_id, metrics individual_metrics.items(): # ensure metrics already floats isinstance(metrics['test_accuracy'], list): metrics['test_accuracy'] = sum(metrics['test_accuracy']) / len(metrics['test_accuracy']) isinstance(metrics['test_diff_accuracy'], list): metrics['test_diff_accuracy'] = sum(metrics['test_diff_accuracy']) / len(metrics['test_diff_accuracy']) logger.info(f"task {task_id}: accuracy = {metrics['test_accuracy']:.4f}, diff accuracy = {metrics['test_diff_accuracy']:.4f}") # save results regardless wandb usage results_path = save_results(results, individual_metrics, args.output_dir, model_name, model_summary) args.use_wandb: # wandb artifact creation logging logger.debug(f"creating wandb artifact name: {model_name}") print(f"debug: creating wandb artifact name: {model_name}") try: artifact = wandb.artifact(name=model_name, type='evaluation') artifact.add_file(results_path) wandb.log_artifact(artifact) logger.debug("artifact created logged successfully.") print("debug: artifact created logged successfully.") except valueerror ve: logger.error(f"failed create wandb artifact: {ve}") print(f"error: failed create wandb artifact: {ve}") raise wandb.finish() __name__ == "__main__": parser = argparse.argumentparser(description="evaluate arc neural reasoning model") parser.add_argument("--model_checkpoint", type=str, required=true, help="path model checkpoint") parser.add_argument("--batch_size", type=int, default=32, help="batch size evaluation") parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="directory save evaluation results") parser.add_argument("--log-level", type=str, default="info", help="set logging level (e.g., debug, info, warning)") parser.add_argument("--wandb_project", type=str, default="arc-evaluation", help="weights &amp; biases project name") parser.add_argument("--wandb_run_name", type=str, default=none, help="weights &amp; biases run name") parser.add_argument("--use_wandb", action='store_true', help="use weights &amp; biases logging") args = parser.parse_args() # create output directory exist os.makedirs(args.output_dir, exist_ok=true) # set logging level logging.basicconfig(level=getattr(logging, args.log_level.upper(), none)) main(args)</file><file name="src/optimize_hyperparameters.py"># gpt2_arc/src/optimize_hyperparameters.py import argparse import optuna import logging import sys import os import torch import gc import pytorch_lightning pl import numpy np pytorch_lightning.utilities.model_summary import modelsummary optuna.pruners import percentilepruner optuna.samplers import tpesampler pytorch_lightning.callbacks import earlystopping, modelcheckpoint pytorch_lightning.loggers import tensorboardlogger gpt2_arc.src.training.trainer import nanlosspruningcallback gpt2_arc.src.training.train import modelconfigsaver gpt2_arc.src.utils.model_memory_estimator import ( calculate_params, estimate_memory_usage, get_available_memory, can_fit_model ) class custompruningcallback(pl.callback): def __init__(self, trial, monitor="val_loss"): super().__init__() self.trial = trial self.monitor = monitor def on_validation_end(self, trainer, pl_module): epoch = trainer.current_epoch current_score = trainer.callback_metrics.get(self.monitor) current_score none: return self.trial.report(current_score, step=epoch) self.trial.should_prune(): raise optuna.trialpruned() # define base directory arc-neural-reasoning-model arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../..")) # add project root python path project_root = arc_model_dir sys.path.insert(0, project_root) gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.data.arc_dataset import arcdataset import arckit gpt2_arc.src.utils.performance_metrics import calculate_mamba_efficiency # set logging logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) def validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout): """validate hyperparameters meet necessary constraints.""" logger.debug(f"validating hyperparameters: n_embd={n_embd}, n_head={n_head}, n_layer={n_layer}, " f"mamba_ratio={mamba_ratio}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}") assert n_embd % n_head == 0, f"n_embd ({n_embd}) must divisible n_head ({n_head})" assert n_embd &gt;= n_head, f"n_embd ({n_embd}) must greater equal n_head ({n_head})" assert n_layer &gt; 0, f"n_layer ({n_layer}) must positive" assert d_state &gt; 0, f"d_state ({d_state}) must positive" assert d_conv &gt; 0, f"d_conv ({d_conv}) must positive" assert 0.0 &lt;= dropout &lt;= 1.0, f"dropout ({dropout}) must 0.0 1.0" logger.debug("hyperparameters validated successfully") return true def calculate_symbol_freq(dataset): """calculate frequency symbol dataset.""" symbol_counts = {} total_symbols = 0 input_tensor, output_tensor, task_id dataset: # assuming symbols represented integers tensors input_symbols = input_tensor.flatten().tolist() output_symbols = output_tensor.flatten().tolist() symbols = input_symbols + output_symbols symbol symbols: symbol_counts[symbol] = symbol_counts.get(symbol, 0) + 1 total_symbols += 1 total_symbols == 0: raise valueerror("the dataset contains symbols calculate frequencies.") # calculate normalized frequencies symbol_freq = {symbol: count / total_symbols symbol, count symbol_counts.items()} return symbol_freq def objective(trial, args): model = none trainer = none arc_trainer = none logger.info(f"starting trial {trial.number}") try: # set float32 matrix multiplication precision torch.set_float32_matmul_precision(args.matmul_precision) logger.info(f"trial {trial.number}: set float32 matmul precision to: {args.matmul_precision}") n_head_exp = trial.suggest_int("n_head_exp", args.n_head_exp_min, args.n_head_exp_max) n_head = 2 ** n_head_exp logger.debug(f"suggested n_head: {n_head} (2^{n_head_exp})") # suggest n_embd multiple n_head ensure power 2 n_embd_multiplier = trial.suggest_int("n_embd_multiplier", args.n_embd_multiplier_min, args.n_embd_multiplier_max) n_embd = n_head * n_embd_multiplier n_embd = 2 ** int(np.log2(n_embd)) logger.debug(f"adjusted n_embd: {n_embd}") # suggest n_layer n_layer = trial.suggest_int("n_layer", args.n_layer_min, args.n_layer_max) logger.debug(f"suggested n_layer: {n_layer}") # suggest mamba-specific hyperparameters mamba_ratio = trial.suggest_float("mamba_ratio", args.mamba_ratio_min, args.mamba_ratio_max, step=args.mamba_ratio_step) d_state = trial.suggest_int("d_state", args.d_state_min, args.d_state_max) d_conv = trial.suggest_int("d_conv_min", args.d_conv_min, args.d_conv_max) # suggest dropout rate dropout = trial.suggest_float("dropout", args.dropout_min, args.dropout_max, step=args.dropout_step) mamba_depth = trial.suggest_int("mamba_depth", args.mamba_depth_min, args.mamba_depth_max) logger.debug(f"suggested mamba_depth: {mamba_depth}") mamba_expand = trial.suggest_int("mamba_expand", args.mamba_expand_min, args.mamba_expand_max) logger.debug(f"suggested mamba_expand: {mamba_expand}") validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout) # suggest whether use grokfast use_grokfast = trial.suggest_categorical("use_grokfast", [true, false]) use_grokfast: # suggest grokfast type based command-line choices grokfast_type = trial.suggest_categorical("grokfast_type", args.grokfast_type_choices) # suggest grokfast alpha within specified range grokfast_alpha = trial.suggest_float("grokfast_alpha", args.grokfast_alpha_min, args.grokfast_alpha_max) # suggest grokfast lambda within specified range grokfast_lamb = trial.suggest_float("grokfast_lamb", args.grokfast_lamb_min, args.grokfast_lamb_max) # using 'ma', suggest window_size within specified range grokfast_type == "ma": grokfast_window_size = trial.suggest_int("grokfast_window_size", args.grokfast_window_size_min, args.grokfast_window_size_max) else: grokfast_window_size = none else: grokfast_type = none grokfast_alpha = none grokfast_lamb = none grokfast_window_size = none batch_size = trial.suggest_int("batch_size", args.batch_size_min, args.batch_size_max) learning_rate = trial.suggest_float("learning_rate", args.learning_rate_min, args.learning_rate_max, log=true) max_epochs = trial.suggest_int("max_epochs", args.max_epochs_min, args.max_epochs_max) # ensure hyperparameters within new limits n_head = min(n_head, 2 ** args.n_head_exp_max) n_embd = min(n_embd, 2 ** int(np.log2(args.n_embd_multiplier_max * n_head))) n_layer = min(n_layer, args.n_layer_max) mamba_ratio = min(mamba_ratio, args.mamba_ratio_max) d_state = min(d_state, args.d_state_max) d_conv = min(d_conv, args.d_conv_max) batch_size = min(batch_size, args.batch_size_max) # optionally, log clamped values logger.debug(f"clamped hyperparameters: n_head={n_head}, n_embd={n_embd}, n_layer={n_layer}, \ mamba_ratio={mamba_ratio}, d_state={d_state}, d_conv={d_conv}, batch_size={batch_size}") # check model fit memory # adjust total number layers include mamba layers total_mamba_layers = int(n_layer * mamba_ratio) total_layers = n_layer + total_mamba_layers # recalculate total parameters based total_layers total_params = calculate_params( n_layers=total_layers, n_heads=n_head, d_model=n_embd, mamba_ratio=mamba_ratio, d_state=d_state, d_conv=d_conv, mamba_depth=mamba_depth, mamba_expand=mamba_expand ) estimated_memory = estimate_memory_usage( total_params=total_params, batch_size=batch_size, height=30, # adjust necessary based data width=30, # adjust necessary d_model=n_embd ) available_memory = get_available_memory() logger.debug(f"trial {trial.number}: estimated memory usage: {estimated_memory:.2f} gb") logger.debug(f"trial {trial.number}: available memory: {available_memory:.2f} gb") # prune trial estimated memory exceeds 80% available memory can_fit_model(estimated_memory, available_memory * 0.8): logger.warning(f"trial {trial.number}: model large available memory. skipping.") raise optuna.exceptions.trialpruned() logger.debug(f"suggested dropout rate: {dropout}") model_config = modelconfig( n_embd=n_embd, n_head=n_head, n_layer=n_layer, dropout=dropout, mamba_ratio=mamba_ratio, d_state=d_state, d_conv=d_conv ) logger.debug(f"model config: {model_config}") # create trainingconfig grokfast parameters args training_config = trainingconfig( batch_size=batch_size, learning_rate=learning_rate, max_epochs=max_epochs, use_grokfast=use_grokfast, grokfast_type=grokfast_type, grokfast_alpha=grokfast_alpha, grokfast_lamb=grokfast_lamb, grokfast_window_size=grokfast_window_size ) config = config(model=model_config, training=training_config) config.estimated_memory = estimated_memory config.available_memory = available_memory logger.debug(f"suggested mamba parameters - mamba_ratio: {mamba_ratio}, d_state: {d_state}, d_conv: {d_conv}") trial.set_user_attr("mamba_ratio", mamba_ratio) trial.set_user_attr("d_state", d_state) trial.set_user_attr("d_conv", d_conv) trial.set_user_attr("mamba_depth", mamba_depth) trial.set_user_attr("mamba_expand", mamba_expand) logger.debug(f"full config: {config}") # instantiate modelconfigsaver callback current config model_config_saver = modelconfigsaver(config) # load data args.use_synthetic_data: args.synthetic_data_path: raise valueerror("synthetic data path provided") logger.info(f"loading synthetic data {args.synthetic_data_path}") train_data = arcdataset( data_source=args.synthetic_data_path, is_test=false, num_symbols=config.model.n_embd ) val_data = arcdataset( data_source=args.synthetic_data_path, is_test=true, num_symbols=config.model.n_embd ) logger.debug(f"synthetic training data size: {len(train_data)}") logger.debug(f"synthetic validation data size: {len(val_data)}") else: logger.info("loading arc dataset") train_set, eval_set = arckit.load_data() train_data = arcdataset(train_set) val_data = arcdataset(eval_set) logger.debug(f"arc training data size: {len(train_data)}") logger.debug(f"arc validation data size: {len(val_data)}") # calculate symbol frequencies args.use_synthetic_data: logger.debug("calculating symbol frequencies synthetic training set") symbol_freq = train_data.get_symbol_frequencies() else: logger.debug("calculating symbol frequencies arc training set") symbol_freq = train_data.get_symbol_frequencies() logger.debug(f"computed symbol frequencies: {symbol_freq}") # convert symbol_freq numpy array dictionary symbol_freq_dict = {str(i): float(freq) i, freq enumerate(symbol_freq)} logger.debug(f"converted symbol frequencies dictionary: {symbol_freq_dict}") # assign converted symbol_freq training configuration config.training.symbol_freq = symbol_freq_dict # validate symbol_freq_dict empty symbol_freq_dict: logger.error("symbol_freq_dict empty. cannot proceed balance_symbols=true balancing_method='weighting'.") raise valueerror("symbol_freq must provided non-empty balance_symbols true balancing_method 'weighting'.") # create model trainer logger.debug("creating model trainer") num_classes = 10 # set appropriate number classes task model = gpt2arc(config, num_classes=num_classes, symbol_freq=symbol_freq_dict) # generate model summary print("debug: attempting generate model summary") try: model_summary = str(modelsummary(model, max_depth=-1)) print("debug: model summary generated successfully") except exception e: print(f"debug: error generating model summary - {str(e)}") model_summary = "error generating model summary" # save model summary trial user attributes print("debug: attempting save model summary trial user attributes") try: trial.set_user_attr("model_summary", model_summary) print("debug: model summary saved trial user attributes") except exception e: print(f"debug: error saving model summary trial - {str(e)}") print("debug: model summary:") print(model_summary) # calculate mamba efficiency metrics device = torch.device('cuda' torch.cuda.is_available() else 'cpu') logger.debug("calculating mamba efficiency metrics") sample_input = torch.randn(1, 1, 6, 6).to(device) model.to(device) mamba_metrics = calculate_mamba_efficiency(model, sample_input) key, value mamba_metrics.items(): trial.set_user_attr(key, value) logger.debug(f"mamba metric - {key}: {value}") arc_trainer = arctrainer(model, train_data, val_data, config) # set pytorch lightning trainer custom pruning callback pruning_callback = custompruningcallback(trial, monitor="val_loss") early_stop_callback = earlystopping(monitor="val_loss", min_delta=0.00, patience=3, verbose=false, mode="min") # determine accelerator parameters based --accelerator argument args.accelerator == "tpu": accelerator = 'tpu' devices = 'xla:1' # use 'xla:8' tpu v3-8 pods strategy = 'tpu_spawn' # recommended strategy tpu elif args.accelerator == "gpu": torch.cuda.is_available(): accelerator = 'gpu' devices = 1 else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' nan_loss_pruning_callback = nanlosspruningcallback() #callbacks.append(nan_loss_pruning_callback) logger.info("nanlosspruningcallback added training callbacks.") experiment_id = f"optuna_trial_{trial.number}" tb_logger = tensorboardlogger(save_dir="runs", name=f"experiment_{experiment_id}") print(f"debug: optuna trial tensorboard logger initialized. log dir: {tb_logger.log_dir}") # extract trial number trial_num = trial.number # define task_id (assuming single task; modify needed multiple tasks) task_id = "main_task" # replace dynamic task identification necessary # define iter_num (e.g., based trial.number another tracking mechanism) iter_num = 1 # initialize 1; increment needed within optimization loop # initialize checkpoint callback descriptive filename checkpoint_callback = modelcheckpoint( dirpath="checkpoints", filename="trial_{trial_num}-epoch_{epoch:02d}-val_loss_{val_loss:.4f}", save_top_k=3, monitor="val_loss", mode="min", ) logger.info("standard modelcheckpoint callback added training callbacks.") # initialize pytorch lightning trainer checkpoint callback trainer = pl.trainer( max_epochs=config.training.max_epochs, callbacks=[pruning_callback, early_stop_callback, nan_loss_pruning_callback, checkpoint_callback, model_config_saver], logger=tb_logger, gradient_clip_val=1.0, # add gradient clipping precision=16, # enable automatic mixed precision enable_checkpointing=true, accelerator=accelerator, devices=devices, strategy=strategy, ) print("debug: trainer created optuna trial tensorboard logger") logger.debug(f"trainer created config: {trainer.state}") # ensure model train mode training model.train() logger.debug("model set train mode training") # enhanced logging: log model mode training logger.info("before training:") name, module model.named_modules(): logger.debug(f"{name}: {'train' module.training else 'eval'}") # train evaluate logger.debug("starting training") trainer.fit(arc_trainer) # enhanced logging: log model mode training logger.info("after training:") name, module model.named_modules(): logger.debug(f"{name}: {'train' module.training else 'eval'}") # update iter_num needed (e.g., multiple iterations per trial) iter_num += 1 # retrieve best validation loss optimization best_val_loss = trainer.callback_metrics.get("val_loss").item() logger.info(f"trial {trial.number} completed. best validation loss: {best_val_loss}") return best_val_loss except runtimeerror e: 'cuda memory' str(e): logger.error(f"trial {trial.number}: cuda memory error.") logger.error("pruning trial suggesting adjust hyperparameters.") trial.set_user_attr('failed_reason', 'cuda memory') raise optuna.exceptions.trialpruned() else: logger.error(f"trial {trial.number}: runtime error occurred: {str(e)}", exc_info=true) raise runtimeerror(f"trial {trial.number}: runtime error occurred: {str(e)}") except exception e: "symbol_freq" str(e): logger.error(f"trial {trial.number}: 'symbol_freq' missing. ensure calculated passed correctly.", exc_info=true) else: logger.error(f"trial {trial.number}: unexpected error occurred: {str(e)}", exc_info=true) raise optuna.exceptions.trialpruned(f"trial {trial.number}: unexpected error occurred: {str(e)}") finally: # ensure proper cleanup trials logger.debug(f"cleaning trial {trial.number}") model none: del model trainer none: del trainer arc_trainer none: del arc_trainer gc.collect() torch.cuda.empty_cache() logger.debug(f"cleanup completed trial {trial.number}") functools import partial def run_optimization(n_trials=100, storage_name="sqlite:///optuna_results.db", n_jobs=-1, args=none, study_name="gpt2_arc_optimization_v2"): pruner = percentilepruner(percentile=25, n_startup_trials=5, n_warmup_steps=2, interval_steps=1) sampler = tpesampler(n_startup_trials=5) study = optuna.create_study( study_name=study_name, storage=storage_name, load_if_exists=true, direction="minimize", pruner=pruner, sampler=sampler ) logger.info(f"starting optimization {n_trials} trials using {n_jobs} parallel jobs") study.optimize(partial(objective, args=args), n_trials=n_trials, n_jobs=n_jobs) logger.info("optimization completed") study.best_trial: print("debug: best trial found, attempting retrieve model summary") best_model_summary = study.best_trial.user_attrs.get("model_summary") best_model_summary: print("debug: model summary retrieved successfully") logger.info("model summary best trial:") logger.info(best_model_summary) else: print("debug: model summary found best trial") else: logger.warning("no successful trials found. please check trial configurations constraints.") logger.info(f"best trial: {study.best_trial.number}") logger.info(f"best value: {study.best_value}") best_trial = study.best_trial best_trial.set_user_attr("mamba_ratio", best_trial.params.get("mamba_ratio")) best_trial.set_user_attr("d_state", best_trial.params.get("d_state")) best_trial.set_user_attr("d_conv", best_trial.params.get("d_conv")) logger.info("best mamba metrics:") key ['mamba_forward_pass_time', 'mamba_params', 'mamba_params_ratio']: value = study.best_trial.user_attrs.get(key) value none: logger.info(f" {key}: {value}") logger.info("best hyperparameters:") key, value study.best_params.items(): logger.info(f" {key}: {value}") __name__ == "__main__": parser = argparse.argumentparser(description="optimize hyperparameters gpt2arc model.") parser.add_argument("--n_trials", type=int, default=10, help="number trials optimization.") parser.add_argument("--storage", type=str, default="sqlite:///optuna_results.db", help="storage path optuna results.") parser.add_argument("--n_jobs", type=int, default=1, help="number parallel jobs. -1 means using available cores.") parser.add_argument( "--study-name", type=str, default="gpt2_arc_optimization_v2", help="name optuna study." ) parser.add_argument("--n_embd_min", type=int, default=4, help="minimum value n_embd") parser.add_argument("--n_embd_max", type=int, default=8, help="maximum value n_embd") parser.add_argument("--n_head_min", type=int, default=2, help="minimum value n_head") parser.add_argument("--n_head_max", type=int, default=16, help="maximum value n_head") parser.add_argument("--n_head_exp_min", type=int, default=1, help="minimum exponent n_head (2^x)") parser.add_argument("--n_head_exp_max", type=int, default=3, help="maximum exponent n_head (2^x)") parser.add_argument("--n_embd_multiplier_min", type=int, default=1, help="minimum multiplier n_embd") parser.add_argument("--n_embd_multiplier_max", type=int, default=2, help="maximum multiplier n_embd") parser.add_argument("--n_layer_min", type=int, default=4, help="minimum value n_layer") parser.add_argument("--n_layer_max", type=int, default=8, help="maximum value n_layer") parser.add_argument("--batch_size_min", type=int, default=32, help="minimum value batch_size") parser.add_argument("--batch_size_max", type=int, default=128, help="maximum value batch_size") parser.add_argument("--learning_rate_min", type=float, default=1e-5, help="minimum value learning_rate") parser.add_argument("--learning_rate_max", type=float, default=1e-2, help="maximum value learning_rate") parser.add_argument("--max_epochs_min", type=int, default=1, help="minimum value max_epochs") parser.add_argument("--max_epochs_max", type=int, default=20, help="maximum value max_epochs") parser.add_argument("--mamba_ratio_min", type=float, default=0.0, help="minimum value mamba_ratio") parser.add_argument("--mamba_ratio_max", type=float, default=8.0, help="maximum value mamba_ratio") parser.add_argument("--mamba_ratio_step", type=float, default=0.25, help="step size mamba_ratio") parser.add_argument("--d_state_min", type=int, default=1, help="minimum value d_state") parser.add_argument("--d_state_max", type=int, default=2, help="maximum value d_state") parser.add_argument("--d_conv_min", type=int, default=1, help="minimum value d_conv") parser.add_argument("--d_conv_max", type=int, default=2, help="maximum value d_conv") parser.add_argument("--dropout_min", type=float, default=0.0, help="minimum value dropout") parser.add_argument("--mamba_depth_min", type=int, default=1, help="minimum value mamba_depth") parser.add_argument("--mamba_depth_max", type=int, default=3, help="maximum value mamba_depth") parser.add_argument("--mamba_expand_min", type=int, default=2, help="minimum value mamba_expand") parser.add_argument("--mamba_expand_max", type=int, default=4, help="maximum value mamba_expand") parser.add_argument("--dropout_max", type=float, default=0.5, help="maximum value dropout") parser.add_argument("--dropout_step", type=float, default=0.1, help="step size dropout") parser.add_argument("--use_gpu", action="store_true", help="flag indicate whether use gpu training.") parser.add_argument("--use_synthetic_data", action="store_true", help="flag indicate whether use synthetic data training.") parser.add_argument( "--matmul-precision", type=str, default="medium", choices=["highest", "high", "medium"], help="set internal precision float32 matrix multiplications optimization trials. options: 'highest', 'high', 'medium'. defaults 'medium'." ) parser.add_argument("--synthetic_data_path", type=str, default="", help="path synthetic data training.") parser.add_argument("--log_level", type=str, default="info", help="logging level (e.g., debug, info, warning, error, critical).") parser.add_argument( "--accelerator", type=str, default="gpu", choices=["cpu", "gpu", "tpu"], help="accelerator use training: 'cpu', 'gpu', 'tpu'. defaults 'gpu'." ) # grokfast parameter ranges parser.add_argument("--grokfast_alpha_min", type=float, default=0.9, help="minimum value grokfast_alpha.") parser.add_argument("--grokfast_alpha_max", type=float, default=0.99, help="maximum value grokfast_alpha.") parser.add_argument("--grokfast_lamb_min", type=float, default=1.0, help="minimum value grokfast_lamb.") parser.add_argument("--grokfast_lamb_max", type=float, default=3.0, help="maximum value grokfast_lamb.") parser.add_argument("--grokfast_window_size_min", type=int, default=50, help="minimum value grokfast_window_size.") parser.add_argument("--grokfast_window_size_max", type=int, default=200, help="maximum value grokfast_window_size.") parser.add_argument("--grokfast_type_choices", type=str, nargs='+', default=["ema", "ma"], choices=["ema", "ma"], help="list grokfast types consider tuning.") args = parser.parse_args() # ensure storage_name correct sqlite prefix handle relative paths import os # ensure os imported top file args.storage.startswith("sqlite:///"): os.path.isabs(args.storage): args.storage = f"sqlite:////{args.storage}" else: args.storage = f"sqlite:///{os.path.abspath(args.storage)}" logger.debug(f"optuna storage url set to: {args.storage}") run_optimization( n_trials=args.n_trials, storage_name=args.storage, n_jobs=args.n_jobs, args=args, study_name=args.study_name )</file><file name="src/__init__.py"># file allows src directory recognized package.</file><file name="src/checkpoint_evaluator.py">#!/usr/bin/env python3 """ checkpoint_evaluator.py script monitor directory new model checkpoints evaluate using specified evaluation script. logs maintained, resource usage monitored. usage: python checkpoint_evaluator.py \ --output_dir /path/to/output \ --arc_model_dir /path/to/arc_model \ --date_folder 2024-10-07 \ --wandb_project arc-evaluation \ [optional arguments] """ import os import sys import time import subprocess import threading import shutil import logging import argparse datetime import datetime import psutil # resource monitoring watchdog.observers import observer watchdog.events import filesystemeventhandler def parse_arguments(): parser = argparse.argumentparser(description="monitor directories model checkpoints evaluate them.") parser.add_argument('--output_dir', type=str, required=true, help='directory store logs evaluation results.') parser.add_argument('--arc_model_dir', type=str, required=true, help='directory containing arc model scripts.') parser.add_argument('--date_folder', type=str, required=true, help='date folder name (e.g., 2024-10-07) organize outputs.') parser.add_argument('--wandb_project', type=str, default='arc-evaluation', help='weights &amp; biases project name.') parser.add_argument('--batch_size', type=int, default=32, help='batch size evaluation.') parser.add_argument('--log_level', type=str, default='debug', choices=['debug', 'info', 'warning', 'error', 'critical'], help='logging level.') parser.add_argument('--resource_monitor_interval', type=int, default=60, help='interval seconds resource monitoring logs.') return parser.parse_args() def setup_logging(output_dir, log_level): os.makedirs(output_dir, exist_ok=true) logging.basicconfig( level=getattr(logging, log_level), format='%(asctime)s - %(levelname)s - %(message)s', handlers=[ logging.filehandler(os.path.join(output_dir, "checkpoint_evaluator.log")), logging.streamhandler(sys.stdout) ] ) logger = logging.getlogger("checkpointevaluator") return logger def load_evaluated_models(evaluated_models_file, logger): evaluated_models = set() os.path.exists(evaluated_models_file): try: open(evaluated_models_file, "r") f: evaluated_models.update(line.strip() line f) logger.info(f"loaded evaluated models {evaluated_models_file}") except exception e: logger.error(f"error loading evaluated models {evaluated_models_file}: {e}") else: logger.info("no previously evaluated models found. starting fresh.") return evaluated_models def save_evaluated_model(evaluated_models_file, model_path, logger): try: open(evaluated_models_file, "a") f: f.write(model_path + "\n") logger.debug(f"recorded evaluation {model_path} {evaluated_models_file}") except exception e: logger.error(f"error writing evaluated models file {evaluated_models_file}: {e}") def wait_for_file_stable(file_path, wait_time=1.0, max_retries=10, logger=none): """wait file stable (not changing size)""" previous_size = -1 retries = 0 retries &lt; max_retries: os.path.exists(file_path): logger: logger.warning(f"file {file_path} exist.") return false current_size = os.path.getsize(file_path) current_size == previous_size: return true else: previous_size = current_size time.sleep(wait_time) retries += 1 logger: logger.warning(f"file {file_path} stable {max_retries} retries.") return false class checkpointhandler(filesystemeventhandler): def __init__(self, evaluated_models, temp_checkpoint_dir, evaluate_callback, logger): super().__init__() self.evaluated_models = evaluated_models self.temp_checkpoint_dir = temp_checkpoint_dir self.evaluate_callback = evaluate_callback self.logger = logger def on_created(self, event): event.is_directory: return event.src_path.endswith('.ckpt') event.src_path.endswith('.pth'): self.evaluate_model(event.src_path) def evaluate_model(self, model_path): model_file = os.path.basename(model_path) model_path self.evaluated_models: self.logger.info(f"skipping already evaluated model: {model_file}") return # skip model already evaluated # wait file stable wait_for_file_stable(model_path, logger=self.logger): self.logger.warning(f"file {model_file} stable. skipping evaluation.") return # copy checkpoint file temp_checkpoint_dir temp_model_path = os.path.join(self.temp_checkpoint_dir, model_file) try: shutil.copy2(model_path, temp_model_path) self.logger.info(f"copied {model_file} temporary directory.") except exception e: self.logger.error(f"error copying {model_file} temporary directory: {e}") return # extract epoch val_loss filename run_name try: parts = model_file.replace('.ckpt', '').replace('.pth', '').split('-') epoch = none val_loss = none part parts: part.startswith('epoch='): epoch = part.split('=')[1] elif part.startswith('val_loss='): val_loss = part.split('=')[1] epoch none val_loss none: run_name = f"evaluation-epoch{epoch}-val_loss{val_loss}" else: run_name = f"evaluation-{model_file}" self.logger.debug(f"parsed run name: {run_name}") except exception e: self.logger.error(f"error parsing run name filename {model_file}: {e}") run_name = f"evaluation-{model_file}" # define evaluation command eval_command = [ "python", os.path.join(args.arc_model_dir, "gpt2_arc/src/evaluate.py"), "--model_checkpoint", temp_model_path, "--batch_size", str(args.batch_size), "--output_dir", args.output_dir, "--wandb_project", args.wandb_project, "--wandb_run_name", run_name ] self.logger.info(f"evaluating model: {model_file} run name: {run_name}") # define log file path log_file_path = os.path.join(args.output_dir, f"{model_file}_evaluation.log") try: open(log_file_path, "w") log_file: # run evaluation command redirect stdout stderr log file subprocess.run( eval_command, check=true, stdout=log_file, stderr=subprocess.stdout, text=true # automatically decode bytes string ) self.logger.info(f"successfully evaluated model: {model_file}. logs {log_file_path}") except subprocess.calledprocesserror e: self.logger.error(f"error evaluation {model_file}. see log {log_file_path}") except exception ex: self.logger.exception(f"an unexpected error occurred evaluating {model_file}: {ex}") self.evaluated_models.add(model_path) save_evaluated_model(os.path.join(args.output_dir, "evaluated_models.txt"), model_path, self.logger) # delete temp model file try: os.remove(temp_model_path) self.logger.debug(f"deleted temporary model file: {temp_model_path}") except exception e: self.logger.error(f"error deleting temp model file {temp_model_path}: {e}") def get_all_checkpoint_files(directory): checkpoint_files = [] root, _, files os.walk(directory): checkpoint_files.extend([os.path.join(root, f) f files f.endswith('.ckpt') f.endswith('.pth')]) return checkpoint_files def start_observer(model_dir, handler, logger): # set start watchdog observer observer = observer() observer.schedule(handler, model_dir, recursive=true) observer.start() logger.info("watching new checkpoints final models subdirectories...") logger.info("this script continue running background.") try: true: time.sleep(10) # optionally, implement additional periodic checks except keyboardinterrupt: observer.stop() logger.info("observer stopped user.") except filenotfounderror fnf_error: logger.error(f"filenotfounderror: {fnf_error}") logger.error(f"please ensure directory '{model_dir}' exists.") except exception e: logger.exception(f"an error occurred observer: {e}") finally: observer.join() logger.info("checkpoint final model evaluation completed.") def monitor_resources(logger, interval=60): true: try: memory = psutil.virtual_memory() cpu = psutil.cpu_percent(interval=1) logger.debug(f"memory usage: {memory.percent}%") logger.debug(f"cpu usage: {cpu}%") time.sleep(interval) except exception e: logger.error(f"error resource monitoring: {e}") def main(args): logger = setup_logging(args.output_dir, args.log_level) logger.info("starting checkpoint evaluator script") logger.debug(f"arguments: {args}") model_dir = os.path.join(args.date_folder, "checkpoints") logger.debug(f"watching new models directory: {model_dir}") # create necessary directories os.makedirs(model_dir, exist_ok=true) temp_checkpoint_dir = os.path.join(args.output_dir, "temp_checkpoints") os.makedirs(temp_checkpoint_dir, exist_ok=true) # load previously evaluated models evaluated_models_file = os.path.join(args.output_dir, "evaluated_models.txt") evaluated_models = load_evaluated_models(evaluated_models_file, logger) # set event handler handler = checkpointhandler(evaluated_models, temp_checkpoint_dir, none, logger) # initialize watchdog event handler ability evaluate models event_handler = checkpointhandler(evaluated_models, temp_checkpoint_dir, none, logger) # start observer separate thread observer_thread = threading.thread(target=start_observer, args=(model_dir, event_handler, logger)) observer_thread.daemon = true # ensures thread exit main program exits observer_thread.start() logger.info("background checkpoint observer started.") # start resource monitor background thread resource_monitor_thread = threading.thread(target=monitor_resources, args=(logger, args.resource_monitor_interval)) resource_monitor_thread.daemon = true resource_monitor_thread.start() logger.info("background resource monitor started.") # keep main thread alive try: true: time.sleep(1) except keyboardinterrupt: logger.info("script terminated user.") except exception e: logger.exception(f"an unexpected error occurred: {e}") __name__ == "__main__": args = parse_arguments() main(args)</file><file name="src/config.py">typing import optional # gpt2_arc/src/config.py dataclasses import dataclass, asdict, field typing import optional import multiprocessing @dataclass class modelconfig: n_embd: int = 768 n_head: int = 12 n_layer: int = 12 dropout: float = 0.1 mamba_ratio: float = 1.0 # number mamba layers per transformer layer d_state: int = 16 # mamba state dimension d_conv: int = 4 # mamba convolution dimension mamba_depth: int = 1 # depth mamba layer mamba_expand: int = 2 # expand factor mamba layer def __post_init__(self): assert self.n_embd % self.n_head == 0, f"n_embd ({self.n_embd}) must divisible n_head ({self.n_head})" assert self.n_embd &gt;= self.n_head, f"n_embd ({self.n_embd}) must greater equal n_head ({self.n_head})" assert self.n_layer &gt; 0, f"n_layer ({self.n_layer}) must positive" assert self.d_state &gt;= 1, f"d_state ({self.d_state}) must least 1" assert self.d_conv &gt;= 1, f"d_conv ({self.d_conv}) must least 1" assert self.mamba_depth &gt;= 1, f"mamba_depth ({self.mamba_depth}) must least 1" assert self.mamba_expand &gt;= 2, f"mamba_expand ({self.mamba_expand}) must least 2" dataclasses import dataclass, field import multiprocessing @dataclass class trainingconfig: # grokfast-specific parameters use_grokfast: bool = false grokfast_type: optional[str] = field(default=none) # 'ema' 'ma' grokfast_alpha: float = field(default=0.98) grokfast_lamb: float = field(default=2.0) grokfast_window_size: optional[int] = field(default=100) # relevant grokfast_type == 'ma' batch_size: int = 32 learning_rate: float = 1e-4 max_epochs: int = 10 num_workers: int = multiprocessing.cpu_count() // 2 multiprocessing.cpu_count() else 4 symbol_freq: optional[dict] = none prefetch_factor: int = 2 persistent_workers: bool = true use_gpu: bool = true log_level: str = "info" use_synthetic_data: bool = false balance_symbols: bool = true # enable balancing balancing_method: str = "weighting" # options: "weighting", "oversampling" synthetic_data_path: optional[str] = none @dataclass class evaluationconfig: perfect_accuracy_threshold: float = 99.9 # set 99.9 near-perfect accuracy @dataclass class config: model: modelconfig = field(default_factory=modelconfig) training: trainingconfig = field(default_factory=trainingconfig) evaluation: evaluationconfig = field(default_factory=evaluationconfig) estimated_memory: optional[float] = none available_memory: optional[float] = none def to_dict(self): return asdict(self)</file><file name="src/utils/helpers.py"># gpt2_arc/src/utils/helpers.py import torch def differential_pixel_accuracy(input, target, prediction): print(f"differential pixel accuracy - input shape: {input.shape}, target shape: {target.shape}, prediction shape: {prediction.shape}") assert isinstance(input, torch.tensor) isinstance(target, torch.tensor) isinstance(prediction, torch.tensor), "all inputs must torch.tensor" assert input.numel() == target.numel() == prediction.numel(), "input, target, prediction must number elements" input = input.view_as(target) prediction = prediction.view_as(target) print(f"reshaped - input: {input.shape}, target: {target.shape}, prediction: {prediction.shape}") input_target_diff = input != target correct_diff_predictions = (prediction == target) &amp; input_target_diff total_diff_pixels = input_target_diff.sum().item() correct_diff_pixels = correct_diff_predictions.sum().item() print(f"total different pixels: {total_diff_pixels}") print(f"correctly predicted different pixels: {correct_diff_pixels}") total_diff_pixels &gt; 0: accuracy = correct_diff_pixels / total_diff_pixels else: accuracy = 1.0 # pixels differ, consider 100% accurate print(f"calculated accuracy: {accuracy}") return accuracy, input_target_diff, correct_diff_predictions</file><file name="src/utils/grokfast_callback.py">pytorch_lightning.callbacks import callback typing import optional, dict import torch.nn nn import logging .grokfast import gradfilter_ema, gradfilter_ma logger = logging.getlogger(__name__) class grokfastcallback(callback): def __init__( self, filter_type: str = 'ema', # 'ema' 'ma' alpha: float = 0.98, lamb: float = 2.0, window_size: int = 100, warmup: bool = true, trigger: bool = false, # ablation study. ): """ initializes grokfast callback. args: filter_type (str): type grokfast filter ('ema' 'ma'). alpha (float): momentum parameter ema. lamb (float): amplifying factor. window_size (int): window size ma. warmup (bool): whether use warmup ma. trigger (bool): ablation studies. """ super().__init__() self.filter_type = filter_type self.alpha = alpha self.lamb = lamb self.window_size = window_size self.warmup = warmup self.trigger = trigger self.grads = none # hold state across batches def on_after_backward(self, trainer, pl_module): """ called backward pass optimizer step. args: trainer: trainer instance. pl_module: lightningmodule instance. """ model = pl_module.model # adjust model nested differently self.filter_type == 'ema': self.grads = gradfilter_ema( m=model, # pass actual model grads=self.grads, alpha=self.alpha, lamb=self.lamb ) logger.debug("applied grokfast-ema filter.") elif self.filter_type == 'ma': self.grads = gradfilter_ma( m=model, # pass actual model grads=self.grads, window_size=self.window_size, lamb=self.lamb, filter_type='mean', # 'sum' based preference warmup=self.warmup, trigger=self.trigger ) logger.debug("applied grokfast-ma filter.") else: logger.warning(f"unknown grokfast filter type: {self.filter_type}. skipping gradient filtering.")</file><file name="src/utils/grokfast.py">collections import deque typing import dict, optional, literal import torch import torch.nn nn def gradfilter_ma( m: nn.module, grads: optional[dict[str, deque]] = none, window_size: int = 100, lamb: float = 5.0, filter_type: literal['mean', 'sum'] = 'mean', warmup: bool = true, trigger: bool = false, # ablation study. ) -&gt; dict[str, deque]: grads none: grads = {n: deque(maxlen=window_size) n, p m.named_parameters() p.requires_grad p.grad none} n, p m.named_parameters(): p.requires_grad p.grad none: grads[n].append(p.grad.data.detach().clone()) # modify gradients. warmup len(grads[n]) == window_size trigger: filter_type == "mean": avg = sum(grads[n]) / len(grads[n]) elif filter_type == "sum": avg = sum(grads[n]) else: raise valueerror(f"unrecognized filter_type {filter_type}") p.grad.data = p.grad.data + avg * lamb return grads def gradfilter_ema( m: nn.module, grads: optional[dict[str, torch.tensor]] = none, alpha: float = 0.98, lamb: float = 2.0, ) -&gt; dict[str, torch.tensor]: grads none: grads = {n: p.grad.data.detach().clone() n, p m.named_parameters() p.requires_grad p.grad none} n, p m.named_parameters(): p.requires_grad p.grad none: grads[n] = grads[n] * alpha + p.grad.data.detach() * (1 - alpha) p.grad.data = p.grad.data + grads[n] * lamb return grads</file><file name="src/utils/performance_metrics.py">import torch import time def calculate_mamba_efficiency(model, input_data): """ calculates performance metrics specific mamba layers model. args: model: gpt2arc model instance. input_data: sample input tensor. returns: dictionary containing mamba-specific performance metrics. """ metrics = {} model.eval() # set model evaluation mode # ensure model input data device device = next(model.parameters()).device input_data = input_data.to(device) # measure forward pass time torch.no_grad(): start_time = time.time() _ = model(input_data) total_time = time.time() - start_time metrics['mamba_forward_pass_time'] = total_time # count number parameters mamba layers mamba_params = 0 total_params = 0 name, param model.named_parameters(): param_count = param.numel() total_params += param_count 'mamba_block' name: mamba_params += param_count metrics['mamba_params'] = mamba_params metrics['total_params'] = total_params metrics['mamba_params_ratio'] = mamba_params / total_params total_params &gt; 0 else 0 return metrics</file><file name="src/utils/results_collector.py"># gpt2_arc/src/utils/results_collector.py import json import time import uuid import torch import platform import os dataclasses import asdict typing import dict, class resultscollector: def __init__(self, config): """initialize resultscollector given configuration.""" self.experiment_id = str(uuid.uuid4()) self.timestamp = time.strftime("%y-%m-%d %h:%m:%s") self.config = asdict(config) self.results = { "train": {}, "validation": {}, "test": {} } self.used_synthetic_data = config.training.use_synthetic_data print(f"debug: initialized self.results['train'] {type(self.results['train'])}") self._log_results_type("after initialization") self.metrics = {} self.task_specific_results = {} self.environment = self._get_environment_info() self.checkpoint_path = none self.tensorboard_log_path = none def set_tensorboard_log_path(self, path): self.tensorboard_log_path = path print(f"debug: set tensorboard log path resultscollector: {path}") def _get_environment_info(self) -&gt; dict[str, str]: """retrieve environment information python pytorch versions.""" return { "python_version": platform.python_version(), "torch_version": torch.__version__, "gpu_info": torch.cuda.get_device_name(0) torch.cuda.is_available() else "cpu" } def _log_results_type(self, context: str): """log type self.results['train'] debugging.""" def update_train_metrics(self, epoch: int, metrics: dict[str, float]): # print(f"debug: self.results['train'] type {type(self.results['train'])}") """update training metrics specific epoch.""" self._log_results_type("before checking 'train' results") "train" self.results: self.results["train"] = {} self._log_results_type("before type check") isinstance(self.results["train"], dict): raise typeerror(f"expected self.results['train'] dict, got {type(self.results['train'])}") self._log_results_type("before setting default") # print(f"debug: setting default, self.results['train'] type {type(self.results['train'])}") self.results["train"].setdefault(epoch, {}) self._log_results_type("after setting default") # print(f"debug: setting default, self.results['train'] type {type(self.results['train'])}") self.results["train"][epoch].update(metrics) def update_val_metrics(self, epoch: int, metrics: dict[str, float]): """update validation metrics specific epoch.""" "validation" self.results: self.results["validation"] = {} self.results["validation"][epoch] = metrics def set_test_results(self, metrics: dict[str, float]): """set test results metrics.""" self.results["test"] = metrics def add_task_specific_result(self, task_id: str, metrics: dict[str, float]): """add task-specific results given task id.""" task_id self.task_specific_results: self.task_specific_results[task_id] = {} self.task_specific_results[task_id].update(metrics) def set_final_metrics(self, metrics: dict[str, float]): """set final metrics training.""" self.metrics = metrics def set_checkpoint_path(self, path: str): """set path model checkpoint.""" self.checkpoint_path = path def save_to_json(self, filepath: str): """save collected results json file.""" try: self._ensure_directory_exists(os.path.dirname(filepath)) data = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "config": self.config, "results": self.results, "metrics": self.metrics, "task_specific_results": self.task_specific_results, "environment": self.environment, "checkpoint_path": self.checkpoint_path, "used_synthetic_data": self.used_synthetic_data } open(filepath, 'w') f: json.dump(data, f, indent=2) except ioerror e: print(f"error saving results {filepath}: {e}") def _ensure_directory_exists(self, directory: str): """ensure directory exists; create not.""" os.path.exists(directory): os.makedirs(directory) def get_summary(self) -&gt; dict[str, any]: """get summary results.""" summary = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "final_train_loss": self.results["train"][-1]["loss"] self.results["train"] else none, "final_val_loss": self.results["validation"][-1]["loss"] self.results["validation"] else none, "test_accuracy": self.results["test"].get("accuracy"), "config": self._serialize_config(self.config), "tensorboard_log_path": self.tensorboard_log_path } print(f"debug: added tensorboard log path results: {summary['tensorboard_log_path']}") return {k: self._make_serializable(v) k, v summary.items()} def _make_serializable(self, obj): isinstance(obj, (int, float, str, bool, type(none))): return obj elif isinstance(obj, (list, tuple)): return [self._make_serializable(item) item obj] elif isinstance(obj, dict): return {k: self._make_serializable(v) k, v obj.items()} else: return str(obj) def _serialize_config(self, config): return {k: self._make_serializable(v) k, v config.items()}</file><file name="src/utils/experiment_tracker.py"># gpt2_arc/src/utils/experiment_tracker.py import wandb import json import time import uuid import torch import platform import os dataclasses import asdict typing import dict, any, optional class experimenttracker: def __init__(self, config: dict[str, any], project: str, entity: optional[str] = none, use_wandb: bool = false): self.experiment_id = str(uuid.uuid4()) self.timestamp = time.strftime("%y-%m-%d %h:%m:%s") self.config = config.to_dict() hasattr(config, 'to_dict') else self._config_to_dict(config) self.project = project self.entity = entity self.run = none self.use_wandb = use_wandb self.metrics = {} self.use_wandb: try: self.run = wandb.init(project=self.project, entity=self.entity, config=self.config) print(f"wandb run initialized: {self.run.id}") except exception e: print(f"error initializing wandb: {str(e)}") self.use_wandb = false self.results = { "train": [], "validation": [], "test": {} } self.metrics = {} self.task_specific_results = {} self.environment = self._get_environment_info() self.checkpoint_path = none # add debug logging print(f"experimenttracker initialized config: {json.dumps(self.config, indent=2)}") print(f"project: {project}, entity: {entity}") print(f"use_wandb: {self.use_wandb}") def _get_environment_info(self) -&gt; dict[str, str]: return { "python_version": platform.python_version(), "torch_version": torch.__version__, "gpu_info": torch.cuda.get_device_name(0) torch.cuda.is_available() else "cpu" } def _config_to_dict(self, config): isinstance(config, dict): return {k: self._config_to_dict(v) k, v config.items()} elif hasattr(config, '__dict__'): return {k: self._config_to_dict(v) k, v config.__dict__.items() k.startswith('_')} else: return config self.use_wandb: try: self.run = wandb.init(project=self.project, entity=self.entity, config=self.config) print(f"wandb run initialized: {self.run.id}") except exception e: print(f"error initializing wandb: {str(e)}") self.use_wandb = false self.use_wandb: print("using local logging only") def finish(self): self.use_wandb self.run: try: wandb.finish() print("wandb run finished") except exception e: print(f"error finishing wandb run: {str(e)}") else: print("experiment finished. metrics:", self.metrics) def log_metric(self, name: str, value: float, step: optional[int] = none): self.use_wandb: try: wandb.log({name: value}, step=step) print(f"logged metric wandb: {name}={value}, step={step}") except exception e: print(f"error logging metric wandb: {str(e)}") # always log locally fallback print(f"logged metric locally: {name}={value}, step={step}") def update_train_metrics(self, epoch: int, metrics: dict[str, float]): "train" self.results: self.results["train"] = [] len(self.results["train"]) &lt;= epoch: self.results["train"].append({}) self.results["train"][epoch] = metrics self.use_wandb: wandb.log({"train": metrics}, step=epoch) def update_val_metrics(self, epoch: int, metrics: dict[str, float]): "validation" self.results: self.results["validation"] = [] len(self.results["validation"]) &lt;= epoch: self.results["validation"].append({}) self.results["validation"][epoch] = metrics self.use_wandb: wandb.log({"validation": metrics}, step=epoch) def set_test_results(self, metrics: dict[str, float]): self.results["test"] = metrics self.use_wandb: wandb.log({"test": metrics}) def add_task_specific_result(self, task_id: str, metrics: dict[str, float]): task_id self.task_specific_results: self.task_specific_results[task_id] = {} self.task_specific_results[task_id].update(metrics) self.use_wandb: wandb.log({f"task_{task_id}": metrics}) def set_final_metrics(self, metrics: dict[str, float]): self.metrics = metrics self.use_wandb: wandb.log(metrics) def set_checkpoint_path(self, path: str): self.checkpoint_path = path self.use_wandb: wandb.save(path) def save_to_json(self, filepath: str): try: directory = os.path.dirname(filepath) directory os.path.exists(directory): os.makedirs(directory) data = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "config": self.config, "results": self.results, "metrics": self.metrics, "task_specific_results": self.task_specific_results, "environment": self.environment, "checkpoint_path": self.checkpoint_path } open(filepath, 'w') f: json.dump(data, f, indent=2) print(f"results saved {filepath}") except ioerror e: print(f"error saving results {filepath}: {e}") def _ensure_directory_exists(self, directory: str): os.path.exists(directory): os.makedirs(directory) def get_summary(self) -&gt; dict[str, any]: summary = { "experiment_id": self.experiment_id, "timestamp": self.timestamp, "final_train_loss": self.results["train"][-1]["loss"] self.results["train"] else none, "final_val_loss": self.results["validation"][-1]["loss"] self.results["validation"] else none, "test_accuracy": self.results["test"].get("accuracy"), "config": self._serialize_config(self.config) } return {k: self._make_serializable(v) k, v summary.items()} def _make_serializable(self, obj): isinstance(obj, (int, float, str, bool, type(none))): return obj elif isinstance(obj, (list, tuple)): return [self._make_serializable(item) item obj] elif isinstance(obj, dict): return {k: self._make_serializable(v) k, v obj.items()} else: return str(obj) def _serialize_config(self, config): return {k: self._make_serializable(v) k, v config.items()} def log_metric(self, name: str, value: float, step: optional[int] = none): self.use_wandb: try: wandb.log({name: value}, step=step) print(f"logged metric wandb: {name}={value}, step={step}") except exception e: print(f"error logging metric wandb: {str(e)}") # always log locally fallback self.metrics[name] = value print(f"logged metric locally: {name}={value}, step={step}") # add simple test __name__ == "__main__": config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": true} tracker = experimenttracker(config, project="test-project") tracker.start() tracker.log_metric("accuracy", 0.85, step=1) tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8}) tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75}) tracker.set_test_results({"loss": 0.55, "accuracy": 0.82}) tracker.add_task_specific_result("task_1", {"accuracy": 0.9}) tracker.set_final_metrics({"best_accuracy": 0.85}) tracker.set_checkpoint_path("model_checkpoint.pth") tracker.save_to_json("results.json") tracker.finish()</file><file name="src/utils/__init__.py">.grokfast import ( gradfilter_ma, gradfilter_ema ) .grokfast_callback import grokfastcallback</file><file name="src/utils/model_memory_estimator.py"># gpt2_arc/src/utils/model_memory_estimator.py import torch import math import psutil def calculate_params(n_layers, n_heads, d_model, mamba_ratio, d_state=16, d_conv=4, mamba_depth=1, mamba_expand=2): print("executing updated calculate_params mamba_ratio =", mamba_ratio) print("debug: called calculate_params mamba_ratio =", mamba_ratio) transformer_params_per_layer = ( 12 * d_model * d_model + 13 * d_model ) # calculate number mamba layers total_mamba_layers = int(n_layers * mamba_ratio) # calculate parameters mamba layers # assuming mambablock parameters based d_state, d_conv, depth, expand mamba_params_per_layer = ( d_state * d_conv * mamba_expand * mamba_depth # example calculation ) total_mamba_params = total_mamba_layers * mamba_params_per_layer # total parameters total_params = n_layers * transformer_params_per_layer + total_mamba_params return total_params def estimate_memory_usage(total_params, batch_size, height, width, d_model, dtype_size=4): model_memory = total_params * dtype_size # model parameters optimizer_memory = model_memory * 2 # adam optimizer uses 2x model size input_memory = batch_size * height * width * dtype_size # input tensors conv_output_memory = batch_size * height * width * d_model * dtype_size # conv layer activations_memory = batch_size * (height * width) * d_model * dtype_size * 2 # forward &amp; backward pass total_memory = model_memory + optimizer_memory + input_memory + conv_output_memory + activations_memory return total_memory / (1024**3) # convert gb def get_available_memory(): torch.cuda.is_available(): return torch.cuda.get_device_properties(0).total_memory / (1024**3) # convert gb else: return psutil.virtual_memory().total / (1024**3) # get actual system memory cpu def get_device_info(): torch.cuda.is_available(): return { "device": "gpu", "name": torch.cuda.get_device_name(0), "compute_capability": torch.cuda.get_device_capability(0), "total_memory": torch.cuda.get_device_properties(0).total_memory / (1024**3), "cuda_version": torch.version.cuda } else: return { "device": "cpu", "name": "system cpu", "total_memory": psutil.virtual_memory().total / (1024**3), "cpu_count": psutil.cpu_count(logical=false), "cpu_freq": psutil.cpu_freq().max psutil.cpu_freq() else "n/a" } def can_fit_model(estimated_memory, available_memory, threshold=0.9): return estimated_memory &lt; available_memory * threshold def estimate_single_configuration(n_layers, n_heads, d_model, batch_size, height, width): device_info = get_device_info() available_memory = get_available_memory() print(f"device information:") key, value device_info.items(): print(f" {key}: {value}") print(f"available memory: {available_memory:.2f} gb") total_params = calculate_params(n_layers, n_heads, d_model) estimated_memory = estimate_memory_usage(total_params, batch_size, height, width, d_model) print(f"\nconfiguration:") print(f" n_layers: {n_layers}") print(f" n_heads: {n_heads}") print(f" d_model: {d_model}") print(f" batch_size: {batch_size}") print(f" input_height: {height}") print(f" input_width: {width}") print(f"total parameters: {total_params:,}") print(f"estimated memory usage: {estimated_memory:.2f} gb") can_fit_model(estimated_memory, available_memory): print(f"model fit {device_info['device']} memory.") else: print(f"warning: model may large available {device_info['device']} memory!") print(f"memory utilization: {(estimated_memory / available_memory) * 100:.2f}%")</file><file name="src/data/__init__.py" /><file name="src/data/arc_dataset.py"># gp2_arc/src/data/arc_dataset.py import os import json import random typing import union, list, dict, tuple, import numpy np import pickle import hashlib import torch import torch.nn.functional f torch.utils.data import dataset import logging torch.utils.data import get_worker_info import math # import math module ceiling division try: arckit.data import taskset, task except importerror: taskset = none logger = logging.getlogger(__name__) logger.setlevel(logging.error) # set error default # create handler writes stderr handler = logging.streamhandler() handler.setlevel(logging.error) # create formatting logs formatter = logging.formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s') handler.setformatter(formatter) # add handler logger logger.addhandler(handler) # function set debug mode def set_debug_mode(debug=false): debug: logger.setlevel(logging.debug) handler.setlevel(logging.debug) else: logger.setlevel(logging.error) handler.setlevel(logging.error) class arcdataset(dataset): def __init__( self, data_source: union[str, list[dict], 'taskset', tuple[union[list, 'taskset'], str]], is_test: bool = false, num_symbols: int = 10, test_split: float = 0.2, debug=false, ): self.test_split = test_split self.is_test = is_test self.num_symbols = num_symbols self.data_files = [] # initialize data_files empty list self.data_source = data_source self.num_samples = 0 self.data = [] self.cache_path = self._generate_cache_path( data_source=self.data_source, num_symbols=self.num_symbols, is_test=self.is_test, test_split=self.test_split ) self._load_cache(self.cache_path): logger.debug("data loaded cache successfully.") return set_debug_mode(debug) logger.debug("starting arcdataset initialization") logger.debug(f"data_source type: {type(data_source)}") logger.debug(f"data_source content: {data_source}") logger.debug(f"self.test_split set to: {self.test_split}") isinstance(data_source, str): os.path.isdir(data_source): logger.debug("initializing dataset data directory") self.data_dir = data_source self.data_files = [ os.path.join(data_source, f) f os.listdir(data_source) f.endswith('.json') ] random.shuffle(self.data_files) file_path self.data_files: open(file_path, 'r') f: task_data = json.load(f) isinstance(task_data, dict): task_id = task_data.get('id', os.path.splitext(os.path.basename(file_path))[0]) samples = self._process_single_task(task_data, task_id=task_id) self.data.extend(samples) logger.debug(f"added {len(samples)} samples file {file_path} task_id: {task_id}") elif isinstance(task_data, list): task_id = os.path.splitext(os.path.basename(file_path))[0] samples = self._process_list_data(task_data, task_id=task_id) self.data.extend(samples) logger.debug(f"assigned task_id '{task_id}' list samples file {file_path}") else: logger.error(f"unexpected data format file {file_path}: {type(task_data)}") elif os.path.isfile(data_source): open(data_source, 'r') f: task_data = json.load(f) isinstance(task_data, dict): task_id = task_data.get('id', "default_task") samples = self._process_single_task(task_data, task_id=task_id) self.data.extend(samples) elif isinstance(task_data, list): samples = self._process_list_data(task_data) self.data.extend(samples) else: logger.error(f"unexpected data format file {data_source}: {type(task_data)}") else: raise filenotfounderror(f"data source file directory found: {data_source}") elif taskset none isinstance(data_source, taskset): logger.debug(f"taskset attributes access: {dir(data_source)}") logger.debug(f"does taskset 'dataset' attribute? {hasattr(data_source, 'dataset')}") samples = self._process_arckit_data(data_source) self.data.extend(samples) elif isinstance(data_source, list): samples = self._process_list_data(data_source) self.data.extend(samples) else: raise valueerror(f"unsupported data_source type: {type(data_source)}") self.num_samples = len(self.data) self._compute_and_cache_statistics() self._save_cache(self.cache_path) def _save_cache(self, cache_path: str): """ saves dataset statistics specified cache path using pickle. args: cache_path (str): file path cache saved. """ try: cache_data = { "data": self.data, "statistics": self.statistics } open(cache_path, 'wb') f: pickle.dump(cache_data, f) logger.debug(f"saved cache {cache_path}") except exception e: logger.error(f"failed save cache {cache_path}: {e}") # add data validation self._validate_data() self._validate_data() def _validate_data(self): """ validates dataset ensure sample contains required keys correct data types. raises: valueerror: sample missing required keys incorrect types. """ required_keys = {"input", "output", "task_id"} idx, sample enumerate(self.data): # check required keys required_keys.issubset(sample.keys()): missing = required_keys - sample.keys() raise keyerror(f"sample index {idx} missing keys: {missing}") # validate 'input' 'output' types key ["input", "output"]: isinstance(sample[key], torch.tensor): raise typeerror(f"sample index {idx} '{key}' type {type(sample[key])}, expected torch.tensor.") sample[key].ndimension() != 3 sample[key].shape[0] != 1: raise valueerror(f"sample index {idx} '{key}' shape {sample[key].shape}, expected shape (1, h, w).") # validate 'task_id' type isinstance(sample["task_id"], str): raise typeerror(f"sample index {idx} 'task_id' type {type(sample['task_id'])}, expected str.") logger.debug("all samples passed validation.") def __len__(self): return len(self.data) def get_num_samples(self): return self.num_samples def __getitem__(self, idx): sample = self.data[idx] return sample["input"], sample["output"], sample["task_id"] def _count_samples_in_directory(self, directory: str): num_samples = 0 file_list = [os.path.join(directory, f) f os.listdir(directory) f.endswith('.json')] logger.debug(f"counting samples {len(file_list)} files") file_path file_list: try: open(file_path, 'r') f: task_data = json.load(f) isinstance(task_data, dict): sample_count = len(task_data.get('test', [])) self.is_test else len(task_data.get('train', [])) num_samples += sample_count logger.debug(f"file {file_path}: {sample_count} samples") elif isinstance(task_data, list): num_samples += len(task_data) logger.debug(f"file {file_path}: {len(task_data)} samples") else: logger.error(f"unexpected data format file {file_path}") except exception e: logger.error(f"error processing file {file_path}: {e}", exc_info=true) continue # skip file proceed next logger.debug(f"total samples counted: {num_samples}") return num_samples @staticmethod def _generate_cache_path(data_source: union[str, list[dict], 'taskset', tuple[union[list, 'taskset'], str]], num_symbols: int, is_test: bool, test_split: float) -&gt; str: dataset_version = "v1" hash_input = json.dumps({ 'version': dataset_version, 'data_source': data_source isinstance(data_source, list) else data_source.__str__(), 'num_symbols': num_symbols, 'is_test': is_test, 'test_split': test_split }, sort_keys=true).encode('utf-8') hash_digest = hashlib.md5(hash_input).hexdigest() cache_filename = f"arc_dataset_cache_{hash_digest}.pkl" cache_dir = os.path.join(os.path.dirname(__file__), 'cache') os.makedirs(cache_dir, exist_ok=true) return os.path.join(cache_dir, cache_filename) def _load_cache(self, cache_path: str) -&gt; bool: os.path.exists(cache_path): try: open(cache_path, 'rb') f: cache_data = pickle.load(f) self.data = cache_data.get("data", []) self.statistics = cache_data.get("statistics", {}) self.num_samples = len(self.data) logger.debug(f"loaded cached data {cache_path}") return true except exception e: logger.error(f"failed load cache {cache_path}: {e}") return false def _compute_and_cache_statistics(self): """ computes dataset statistics caches alongside dataset cache. """ logger.debug("computing dataset statistics") grid_size_stats = self._compute_grid_size_stats() symbol_frequencies = self._compute_symbol_frequencies() statistics = { "grid_size_stats": grid_size_stats, "symbol_frequencies": symbol_frequencies } # update cache dictionary statistics self.statistics = statistics self._save_cache(self.cache_path) # ensure statistics saved cache logger.debug("dataset statistics computed cached successfully") def _process_list_data(self, data_list: list[dict], task_id: str) -&gt; list[dict]: processed_data = [] idx, example enumerate(data_list): 'input' example 'output' example isinstance(example['input'], (list, np.ndarray)) isinstance(example['output'], (list, np.ndarray)): # preprocess grids input_grid = self._preprocess_grid(example['input']) output_grid = self._preprocess_grid(example['output']) # assign task_id present # assign task_id parameter, overriding existing task_id data processed_data.append({ "input": input_grid, "output": output_grid, "task_id": task_id }) else: logger.warning(f"example index {idx} missing 'input' 'output' keys incorrect types.") # optionally, skip raise error # raise valueerror("unexpected item format data_source.") return processed_data def _combine_data(self, official_data, synthetic_data_path): official_processed = self._process_arckit_data(official_data) taskset none isinstance(official_data, taskset) else official_data synthetic_processed = self._process_synthetic_data(synthetic_data_path) return official_processed + synthetic_processed def _process_synthetic_data(self, directory: str): self.data_files = [] filename os.listdir(directory): filename.endswith('.json'): file_path = os.path.join(directory, filename) self.data_files.append(file_path) logger.debug(f"processing file: {file_path}") open(file_path, 'r') f: try: task_data = json.load(f) # assign task_id filename task_id = os.path.splitext(filename)[0] processed_samples = self._process_single_task(task_data, task_id=task_id) self.data.extend(processed_samples) except json.jsondecodeerror e: logger.error(f"error decoding json file {file_path}: {e}") def _process_arckit_data(self, taskset: 'taskset') -&gt; list[dict]: processed_data = [] logger.debug(f"processing taskset {len(taskset.tasks)} tasks") task taskset.tasks: logger.debug(f"processing task: {task.id}") logger.debug(f"train samples: {len(task.train)}, test samples: {len(task.test)}") # process training samples ex task.train: try: input_tensor = self._preprocess_grid(ex[0]) output_tensor = self._preprocess_grid(ex[1]) processed_data.append({ "input": input_tensor, "output": output_tensor, "task_id": task.id }) except exception e: logger.error(f"error processing training example task {task.id}: {e}", exc_info=true) # process testing samples ex task.test: try: input_tensor = self._preprocess_grid(ex[0]) output_tensor = self._preprocess_grid(ex[1]) processed_data.append({ "input": input_tensor, "output": output_tensor, "task_id": task.id }) except exception e: logger.error(f"error processing testing example task {task.id}: {e}", exc_info=true) logger.debug(f"processed task {task.id}: total samples added: {len(task.train) + len(task.test)}") logger.debug(f"total samples processed taskset: {len(processed_data)}") return processed_data def get_grid_size_stats(self) -&gt; dict[str, any]: """ returns precomputed grid size statistics. returns: dict[str, any]: dictionary containing grid size statistics. """ hasattr(self, 'statistics') 'grid_size_stats' self.statistics: return self.statistics['grid_size_stats'] else: logger.warning("grid size statistics available.") return {} def get_symbol_frequencies(self) -&gt; dict[str, float]: """ returns precomputed symbol frequencies. returns: dict[str, float]: dictionary mapping symbols frequencies. """ hasattr(self, 'statistics') 'symbol_frequencies' self.statistics: return self.statistics['symbol_frequencies'] else: logger.warning("symbol frequencies available.") return {} def _compute_grid_size_stats(self): max_height, max_width = 0, 0 sample self.data: # assuming sample["input"] sample["output"] shape [c, h, w] max_height = max(max_height, sample["input"].shape[1], sample["output"].shape[1]) max_width = max(max_width, sample["input"].shape[2], sample["output"].shape[2]) grid_size_stats = {"max_height": max_height, "max_width": max_width} self.max_grid_size = (max_height, max_width) return grid_size_stats def _compute_symbol_frequencies(self): symbol_counts = np.zeros(self.num_symbols, dtype=int) sample self.data: symbol_counts += np.bincount(sample["input"].flatten(), minlength=self.num_symbols) symbol_counts += np.bincount(sample["output"].flatten(), minlength=self.num_symbols) return symbol_counts / symbol_counts.sum() def _preprocess_grid(self, grid: union[dict, list, np.ndarray, torch.tensor]) -&gt; torch.tensor: logger.debug(f"preprocessing grid initial type: {type(grid)}") # convert grid torch.tensor list numpy array isinstance(grid, list): grid_tensor = torch.tensor(grid, dtype=torch.float32) logger.debug(f"converted list tensor shape: {grid_tensor.shape}") elif isinstance(grid, np.ndarray): grid_tensor = torch.from_numpy(grid).float() logger.debug(f"converted numpy array tensor shape: {grid_tensor.shape}") elif isinstance(grid, torch.tensor): grid_tensor = grid.float() logger.debug(f"using existing tensor shape: {grid_tensor.shape}") else: raise valueerror(f"unexpected grid type: {type(grid)}") # ensure grid_tensor three dimensions [c, h, w] grid_tensor.ndim == 2: logger.debug("grid tensor 2d. adding channel dimension.") grid_tensor = grid_tensor.unsqueeze(0) # add channel dimension logger.debug(f"grid tensor shape unsqueeze: {grid_tensor.shape}") elif grid_tensor.ndim != 3: raise valueerror(f"unexpected grid tensor dimensions: {grid_tensor.ndim}. expected 2d 3d tensor.") logger.debug(f"grid shape padding: {grid_tensor.shape}") # apply padding using pytorch's built-in functions padded_grid = self._pad_grid_torch(grid_tensor, height=30, width=30) logger.debug(f"grid shape padding: {padded_grid.shape}") return padded_grid def kronecker_scale(self, x, target_height=30, target_width=30): print(f"kronecker scaling input shape: {x.shape}") h, w = x.shape scale_h = target_height / h scale_w = target_width / w = int(np.floor(min(scale_h, scale_w))) x_scaled = np.kron(x, np.ones((d, d))) print(f"kronecker scaled output shape: {x_scaled.shape}") return x_scaled def reverse_scaling(self, x_orig, x_pred): print(f"reverse scaling - original shape: {x_orig.shape}, prediction shape: {x_pred.shape}") h, w = x_orig.shape # reshape x_pred 2d 1d x_pred.ndim == 1: x_pred = x_pred.reshape((int(np.sqrt(x_pred.size)), -1)) x_pred_cropped = x_pred[:h, :w] # crop original size h == x_pred.shape[0] w == x_pred.shape[1]: print("no rescaling needed") return x_pred_cropped # calculate downscale factor d_h = x_pred_cropped.shape[0] // h d_w = x_pred_cropped.shape[1] // w # ensure dimensions compatible reshaping d_h &gt; 0 d_w &gt; 0: try: x_rev = x_pred_cropped.reshape(h, d_h, w, d_w).mean(axis=(1, 3)) except valueerror e: print(f"error reshaping: {e}") print(f"x_pred_cropped shape: {x_pred_cropped.shape}, h: {h}, w: {w}, d_h: {d_h}, d_w: {d_w}") raise else: print(f"invalid downscale factors: d_h={d_h}, d_w={d_w}") raise valueerror("invalid dimensions reverse scaling") # resize result match original target shape result = np.resize(x_rev.round().astype(int), x_orig.shape) print(f"reverse scaled output shape: {result.shape}") return result def _scale_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray: return grid # scaling, preserve original size def _pad_grid_torch(self, grid: torch.tensor, height: int, width: int) -&gt; torch.tensor: """ pads input grid tensor specified height width using pytorch's functional padding. args: grid (torch.tensor): input grid tensor shape [c, h, w]. height (int): target height padding. width (int): target width padding. returns: torch.tensor: padded grid tensor. """ _, h, w = grid.shape pad_h = max((height - h) // 2, 0) pad_w = max((width - w) // 2, 0) # calculate padding top, bottom, left, right padding = (pad_w, width - w - pad_w, pad_h, height - h - pad_h) # (left, right, top, bottom) logger.debug(f"padding applied: left={pad_w}, right={width - w - pad_w}, top={pad_h}, bottom={height - h - pad_h}") # apply padding using pytorch's functional pad padded_grid = f.pad(grid, padding, mode='constant', value=0) return padded_grid @staticmethod def collate_fn(batch): # debugging: check batch size logger.debug(f"collating batch size: {len(batch)}") batch: logger.warning("empty batch received") return torch.tensor([]), torch.tensor([]), [] inputs, outputs, task_ids = zip(*batch) # find maximum dimensions batch max_h = max(input_tensor.size(1) input_tensor inputs) max_w = max(input_tensor.size(2) input_tensor outputs) # debugging: print maximum dimensions logger.debug(f"maximum height batch: {max_h}") logger.debug(f"maximum width batch: {max_w}") # pad inputs outputs maximum size padded_inputs = torch.stack([ f.pad(input_tensor, (0, max_w - input_tensor.size(2), 0, max_h - input_tensor.size(1))) input_tensor inputs ]) padded_outputs = torch.stack([ f.pad(output_tensor, (0, max_w - output_tensor.size(2), 0, max_h - output_tensor.size(1))) output_tensor outputs ]) # debugging: verify shapes padding print(f"padded inputs shape: {padded_inputs.shape}") print(f"padded outputs shape: {padded_outputs.shape}") return padded_inputs, padded_outputs, list(task_ids)</file><file name="src/training/trainer.py"># gpt2_arc/src/training/trainer.py import pytorch_lightning pl import torch import logging torch import nn, optim import time typing import any, dict, optional collections import deque torch.optim.lr_scheduler import lambdalr ..config import config ..utils.helpers import differential_pixel_accuracy ..utils.results_collector import resultscollector torch.utils.data import dataloader torch.utils.tensorboard import summarywriter pytorch_lightning.loggers import tensorboardlogger import os optuna.exceptions import trialpruned pytorch_lightning.callbacks import callback import torch logger = logging.getlogger(__name__) class nanlosspruningcallback(callback): def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx): # extract loss outputs loss = outputs.get('loss') isinstance(outputs, dict) else outputs loss none: torch.isnan(loss): logger.warning("nan loss detected. pruning trial.") raise trialpruned("nan loss encountered, pruning trial.") class arctrainer(pl.lightningmodule): def __init__(self, model, train_dataset, val_dataset, config: config, results_collector=none): super().__init__() self.model = model self.train_dataset = train_dataset self.val_dataset = val_dataset self.config = config self.batch_size = config.training.batch_size self.lr = config.training.learning_rate self.train_losses = [] self.logged_metrics = {} self.test_outputs = [] # initialize empty list store test outputs self.test_results = [] # initialize test results storing test outcomes self.best_val_loss = float('inf') self.best_epoch = 0 self.results_collector = results_collector results_collector else resultscollector(config) self.writer = summarywriter(f"runs/experiment_{self.results_collector.experiment_id}") hasattr(self.model, 'loss_fn') hasattr(self.model.loss_fn, 'weight'): logger.debug(f"trainer's loss function class weights: {self.model.loss_fn.weight}") else: logger.debug("trainer's loss function class weights.") def get_tensorboard_logger(self): logger self.trainer.loggers: isinstance(logger, tensorboardlogger): return logger.experiment print("debug: tensorboardlogger found trainer.loggers") return none def training_step(self, batch, batch_idx): logger.debug(f"training step - batch type: {type(batch)}, length: {len(batch)}") isinstance(batch, (list, tuple)) len(batch) &gt;= 2: inputs, labels = batch[:2] task_ids = batch[2] len(batch) &gt; 2 else none elif isinstance(batch, dict): inputs = batch.get("input_ids") labels = batch.get("labels") task_ids = batch.get("task_ids") else: raise valueerror(f"unexpected batch format: {type(batch)}. content: {batch}") # ensure inputs labels correct type inputs = inputs.float() labels = labels.long() outputs = self(inputs) loss = self.compute_loss(outputs, labels) hasattr(self, 'log'): self.log("train_loss", loss, on_step=true, on_epoch=true, prog_bar=true, logger=true) self.train_losses.append(loss.item()) self.results_collector.update_train_metrics(self.current_epoch, {"loss": loss.item()}) tb_logger = self.get_tensorboard_logger() tb_logger: tb_logger.add_scalar('train/loss', loss.item(), self.global_step) print(f"debug: logged training loss: {loss.item()} step {self.global_step}") else: print(f"debug: failed log training loss. tensorboard logger available.") return loss def validation_step(self, batch, batch_idx, dataloader_idx=0): logger.debug(f"validation step - batch type: {type(batch)}, length: {len(batch)}") isinstance(batch, (list, tuple)): len(batch) &lt; 2: logger.error(f"missing inputs labels batch. inputs: {batch[0] len(batch) &gt; 0 else none}, labels: {batch[1] len(batch) &gt; 1 else none}") raise valueerror("batch must contain inputs labels.") inputs, labels = batch[:2] task_ids = batch[2] len(batch) &gt; 2 else none elif isinstance(batch, dict): inputs = batch.get("input_ids") labels = batch.get("labels") task_ids = batch.get("task_ids") inputs none labels none: logger.error(f"missing inputs labels batch. inputs: {inputs}, labels: {labels}") raise valueerror("batch must contain inputs labels.") else: logger.error(f"unexpected batch format: {type(batch)}. content: {batch}") raise valueerror(f"unexpected batch format: {type(batch)}. content: {batch}") # ensure inputs labels correct type inputs = inputs.float() labels = labels.long() outputs = self(inputs) loss = self.compute_loss(outputs, labels) hasattr(self, 'log'): self.log("val_loss", loss, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.logged_metrics["val_loss"] = loss.item() self.results_collector.update_val_metrics(self.current_epoch, {"loss": loss.item()}) try: self.writer.add_scalar('val/loss', loss.item(), self.current_epoch) print(f"debug: logged validation loss: {loss.item()} epoch {self.current_epoch}") except exception e: print(f"debug: error logging validation step: {str(e)}") return loss def on_test_epoch_start(self): self.test_outputs = [] def test_step(self, batch, batch_idx): logger.debug(f"debug: test_step input - batch: {batch}, batch_idx: {batch_idx}") logger.debug(f"debug: test step - batch type: {type(batch)}, length: {len(batch)}") # unpack batch len(batch) == 3: inputs, outputs, task_ids = batch elif len(batch) == 2: inputs, outputs = batch task_ids = none # set none default value else: raise valueerror(f"unexpected batch format length {len(batch)}") logger.debug(f"debug: task ids batch: {task_ids}") inputs = inputs.float() outputs = outputs.long() attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32, device=inputs.device) model_outputs = self(inputs, attention_mask) loss = self.compute_loss(model_outputs, outputs) accuracies = [] diff_accuracies = [] range(len(inputs)): accuracy = self.compute_accuracy(model_outputs[i:i+1], outputs[i:i+1]) diff_accuracy, _, _ = differential_pixel_accuracy(inputs[i:i+1], outputs[i:i+1], model_outputs[i:i+1].argmax(dim=-1)) accuracies.append(accuracy.item()) diff_accuracies.append(diff_accuracy) logger.debug(f"debug: diff_accuracy type: {type(diff_accuracy)}, value: {diff_accuracy}") result = { 'test_loss': loss.item(), 'task_ids': task_ids, 'test_accuracy': sum(accuracies) / len(accuracies) accuracies else 0, 'test_diff_accuracy': sum(diff_accuracies) / len(diff_accuracies) diff_accuracies else 0, } logger.debug(f"debug: test loss: {result['test_loss']}, avg accuracy: {result['test_accuracy']}, avg diff accuracy: {result['test_diff_accuracy']}") # log task-specific metrics task_ids none: task_id, accuracy, diff_accuracy zip(task_ids, accuracies, diff_accuracies): result[f"{task_id}_test_accuracy"] = accuracy result[f"{task_id}_test_diff_accuracy"] = diff_accuracy self.log(f"{task_id}_test_accuracy", accuracy, on_step=false, on_epoch=true, prog_bar=true, logger=true) self.log(f"{task_id}_test_diff_accuracy", diff_accuracy, on_step=false, on_epoch=true, prog_bar=true, logger=true) try: self.writer.add_scalar('test/loss', result['test_loss'], self.current_epoch) self.writer.add_scalar('test/avg_accuracy', result['test_accuracy'], self.current_epoch) self.writer.add_scalar('test/diff_accuracy', result['test_diff_accuracy'], self.current_epoch) logger.debug(f"debug: logged test metrics epoch {self.current_epoch}: loss={result['test_loss']}, avg_accuracy={result['test_accuracy']}, diff_accuracy={result['test_diff_accuracy']}") except exception e: logger.error(f"debug: error logging test step: {str(e)}") logger.debug(f"debug: test_step output - result: {result}") logger.debug(f"debug: test step result: {result}") # append result self.test_outputs self.test_outputs.append({key: value.item() isinstance(value, torch.tensor) else value key, value result.items()}) return result def on_test_epoch_end(self): total_loss = torch.stack([torch.tensor(x['test_loss']) x self.test_outputs]).mean() all_accuracies = [] all_diff_accuracies = [] output self.test_outputs: 'test_accuracy' output: all_accuracies.append(output['test_accuracy']) 'test_diff_accuracy' output: all_diff_accuracies.append(output['test_diff_accuracy']) avg_accuracy = sum(all_accuracies) / len(all_accuracies) all_accuracies else 0 avg_diff_accuracy = sum(all_diff_accuracies) / len(all_diff_accuracies) all_diff_accuracies else 0 self.log('avg_test_loss', total_loss, prog_bar=true) self.log('avg_test_accuracy', avg_accuracy, prog_bar=true) self.log('avg_test_diff_accuracy', avg_diff_accuracy, prog_bar=true) print(f"debug: test epoch end - avg loss: {total_loss}, avg accuracy: {avg_accuracy}, avg diff accuracy: {avg_diff_accuracy}") def compute_accuracy(self, outputs, targets): predictions = outputs.argmax(dim=-1) # reshape predictions match target shape predictions = predictions.view(targets.size()) # calculate accuracy elements accuracy = (predictions == targets).float().mean() print(f"debug: compute_accuracy - accuracy: {accuracy.item()}") return accuracy def compute_diff_accuracy(self, inputs, targets, outputs): predictions = outputs.argmax(dim=-1) diff_accuracies, _, _ = differential_pixel_accuracy(inputs, targets, predictions) return diff_accuracies predictions = outputs.argmax(dim=-1) # reshape predictions match target shape predictions = predictions.view(targets.size()) # calculate accuracy sample batch accuracies = (predictions == targets).float().mean(dim=[1, 2, 3]) return accuracies def on_validation_epoch_end(self): # compute average validation loss val_loss = self.trainer.callback_metrics.get('val_loss') val_loss none: avg_val_loss = val_loss.item() else: avg_val_loss = float('inf') # default high value val_loss available # update best_val_loss best_epoch avg_val_loss &lt; self.best_val_loss: self.best_val_loss = avg_val_loss self.best_epoch = self.current_epoch # log validation metrics self.log('val_loss', avg_val_loss) self.log('best_val_loss', self.best_val_loss) self.log('best_epoch', self.best_epoch) # update results collector self.results_collector.update_val_metrics(self.current_epoch, { "avg_loss": avg_val_loss, "best_val_loss": self.best_val_loss, "best_epoch": self.best_epoch }) # log additional information self.log('epoch', self.current_epoch) def configure_optimizers(self): optimizer = optim.adam(self.model.parameters(), lr=self.lr) lr_scheduler = { 'scheduler': optim.lr_scheduler.steplr(optimizer, step_size=1, gamma=0.95), 'name': 'learning_rate', } return [optimizer], [lr_scheduler] def on_fit_end(self): self.results_collector.save_to_json(f"results/experiment_{self.results_collector.experiment_id}.json") try: self.writer.close() print("debug: tensorboard writer closed successfully.") except exception e: print(f"debug: error closing tensorboard writer: {str(e)}") print("debug: results saved tensorboard writer closed.") def train_dataloader(self): return dataloader(self.train_dataset, batch_size=self.batch_size, shuffle=true, num_workers=0) def val_dataloader(self): loader = dataloader(self.val_dataset, batch_size=self.batch_size, num_workers=0) logger.debug(f"debug: test dataloader created {len(loader)} batches") return loader def test_dataloader(self): return dataloader(self.val_dataset, batch_size=self.batch_size, num_workers=4) def compute_loss(self, outputs, labels): loss = nn.crossentropyloss()( outputs.view(-1, outputs.size(-1)), labels.view(-1) ) logger.debug(f"computed loss: {loss.item()}") return loss def forward(self, input_ids, attention_mask=none): return self.model(input_ids, attention_mask) def log_hyperparameters(self): hparams = { 'learning_rate': self.config.training.learning_rate, 'batch_size': self.config.training.batch_size, 'n_embd': self.config.model.n_embd, 'n_head': self.config.model.n_head, 'n_layer': self.config.model.n_layer, } metric_dict = { 'train_loss': 0, 'val_loss': 0, 'test_accuracy': 0, } try: self.writer.add_hparams(hparams, metric_dict) print(f"debug: successfully logged hyperparameters: {hparams}") except exception e: print(f"debug: error logging hyperparameters: {str(e)}")</file><file name="src/training/__init__.py" /><file name="src/training/train.py"># gpt2_arc/src/training/train.py import argparse import multiprocessing import sys import logging import os import json import datetime unittest.mock import magicmock, patch import optuna import arckit import numpy np import torch lightning.pytorch.profilers import pytorchprofiler pytorch_lightning.callbacks import callback torch.profiler import profileractivity torch.utils.data import dataloader, weightedrandomsampler # define base directory arc-neural-reasoning-model arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../..")) # add root directory project pythonpath project_root = arc_model_dir sys.path.insert(0, project_root) import pytorch_lightning pl #import torch.autograd.profiler profiler import torch pytorch_lightning.callbacks import modelcheckpoint pytorch_lightning.loggers import tensorboardlogger gpt2_arc.src.data.arc_dataset import arcdataset gpt2_arc.src.models.gpt2 import gpt2arc gpt2_arc.src.config import config, modelconfig, trainingconfig gpt2_arc.src.training.trainer import arctrainer gpt2_arc.src.utils.experiment_tracker import experimenttracker gpt2_arc.src.utils.results_collector import resultscollector gpt2_arc.src.utils import grokfastcallback def get_num_workers(): try: return multiprocessing.cpu_count() // 2 # use half available cpus except notimplementederror: return 4 # default fallback logger = logging.getlogger(__name__) class configsavingmodelcheckpoint(modelcheckpoint): def __init__(self, config, trial_num='na', task_id='na', iter_num='na', *args, **kwargs): super().__init__(*args, **kwargs) self.config = config self.trial_num = trial_num self.task_id = task_id self.iter_num = iter_num self.timestamp = datetime.datetime.now().strftime("%y%m%dt%h%m%s") # e.g., 20240308t153045 def on_save_checkpoint(self, trainer, pl_module, checkpoint): # add custom metadata checkpoint checkpoint['model_config'] = self.config.model.__dict__ checkpoint['trial_num'] = self.trial_num checkpoint['task_id'] = self.task_id checkpoint['iter_num'] = self.iter_num checkpoint['timestamp'] = self.timestamp # add current epoch checkpoint checkpoint['epoch'] = trainer.current_epoch super().on_save_checkpoint(trainer, pl_module, checkpoint) def format_checkpoint_name(self, metrics): """ override method include custom placeholders filename. """ return self.filename.format( trial_num=self.trial_num, task_id=self.task_id, iter_num=self.iter_num, val_loss=metrics.get("val_loss", 0.0), epoch=metrics.get("epoch", 0), timestamp=self.timestamp ) class modelconfigsaver(callback): def __init__(self, config): """ initialize modelconfigsaver callback current configuration. args: config (config): configuration object containing model parameters. """ super().__init__() self.config = config def on_save_checkpoint(self, trainer, pl_module, checkpoint): """ override checkpoint saving include model configuration. args: trainer (pl.trainer): trainer instance. pl_module (pl.lightningmodule): lightningmodule trained. checkpoint (dict): checkpoint dictionary modified. """ checkpoint['model_config'] = self.config.model.__dict__ def main(args): # set float32 matrix multiplication precision torch.set_float32_matmul_precision(args.matmul_precision) logger.info(f"set float32 matmul precision to: {args.matmul_precision}") log_level = getattr(logging, args.log_level.upper() hasattr(args, 'log_level') else 'debug', logging.debug) logging.basicconfig( level=log_level, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s' ) profiler = pytorchprofiler( dirpath=args.profiler_dirpath, filename=args.profiler_filename, activities=[profileractivity.cpu, profileractivity.cuda], # include cuda activities record_shapes=true, with_stack=true # enable stack tracing ) args.use_profiler else none logger.setlevel(logging.debug) # ensure logger set debug logger.info("starting main function") logger.debug(f"command line arguments: {args}") trainer = none # initialize trainer none try: args.use_optuna: logger.info("loading best hyperparameters optuna study") study_name = args.optuna_study_name study_name none: # retrieve study summaries storage study_summaries = optuna.get_all_study_summaries(storage=args.optuna_storage) study_names = [summary.study_name summary study_summaries] len(study_names) == 1: study_name = study_names[0] logger.info(f"automatically selected available study: {study_name}") elif len(study_names) == 0: logger.error("no studies found specified optuna storage.") sys.exit(1) else: logger.error("multiple studies found specified optuna storage. please specify study name using --optuna-study-name.") sys.exit(1) study = optuna.load_study(study_name=study_name, storage=args.optuna_storage) best_params = study.best_params logger.debug(f"loaded best parameters: {best_params}") n_head = 2 ** best_params['n_head_exp'] n_embd = n_head * best_params['n_embd_multiplier'] n_embd = 2 ** int(np.log2(n_embd)) model_config = modelconfig( n_embd=n_embd, n_head=n_head, n_layer=best_params['n_layer'], dropout=best_params['dropout'] ) training_config = trainingconfig( batch_size=best_params['batch_size'], learning_rate=best_params['learning_rate'], max_epochs=args.max_epochs, use_gpu=args.use_gpu, log_level=args.log_level, use_synthetic_data=args.use_synthetic_data, synthetic_data_path=args.synthetic_data_path ) training_config = trainingconfig( batch_size=best_params['batch_size'], learning_rate=best_params['learning_rate'], max_epochs=args.max_epochs # always use user-provided max_epochs ) else: logger.info("using provided default hyperparameters") model_config = modelconfig( n_embd=args.n_embd, n_head=args.n_head, n_layer=args.n_layer, mamba_ratio=args.mamba_ratio, d_state=args.d_state, d_conv=args.d_conv, dropout=args.dropout, mamba_depth=args.mamba_depth, mamba_expand=args.mamba_expand ) training_config = trainingconfig( batch_size=args.batch_size, learning_rate=args.learning_rate, max_epochs=args.max_epochs, use_gpu=args.use_gpu, log_level=args.log_level, use_synthetic_data=args.use_synthetic_data, synthetic_data_path=args.synthetic_data_path, use_grokfast=args.use_grokfast, grokfast_type=args.grokfast_type, grokfast_alpha=args.grokfast_alpha, grokfast_lamb=args.grokfast_lamb, grokfast_window_size=args.grokfast_window_size ) config = config(model=model_config, training=training_config) logger.debug(f"configuration: {config}") # load data logger.info("loading data") args.use_synthetic_data: args.synthetic_data_path: raise valueerror("synthetic data path provided") logger.info(f"loading synthetic data {args.synthetic_data_path}") synthetic_files = os.listdir(args.synthetic_data_path) logger.debug(f"total files synthetic data path: {len(synthetic_files)}") logger.debug(f"sample files: {synthetic_files[:5]}... (total {len(synthetic_files)})") train_data = arcdataset(args.synthetic_data_path) synthetic_files = os.listdir(args.synthetic_data_path) logger.debug(f"listing files synthetic data path validation: {synthetic_files[:5]}... (total {len(synthetic_files)})") val_data = arcdataset(args.synthetic_data_path, is_test=true) else: logger.info("loading arc dataset") train_set, eval_set = arckit.load_data() train_data = arcdataset(train_set) val_data = arcdataset(eval_set) # access dataset statistics train_grid_stats = train_data.get_grid_size_stats() train_symbol_freq = train_data.get_symbol_frequencies() val_grid_stats = val_data.get_grid_size_stats() val_symbol_freq = val_data.get_symbol_frequencies() # convert train_symbol_freq numpy array dictionary string keys train_symbol_freq_dict = {str(idx): float(freq) idx, freq enumerate(train_symbol_freq)} # update trainingconfig symbol_freq dictionary training_config.symbol_freq = train_symbol_freq_dict logger.info(f"train grid size stats: {train_grid_stats}") logger.info(f"train symbol frequencies: {train_symbol_freq_dict}") logger.info(f"validation grid size stats: {val_grid_stats}") logger.info(f"validation symbol frequencies: {val_symbol_freq}") # initialize experiment tracker tracker = experimenttracker(config, project=args.project) # log dataset statistics experimenttracker tracker.log_metric("train_max_grid_height", train_grid_stats.get("max_height", 0)) tracker.log_metric("train_max_grid_width", train_grid_stats.get("max_width", 0)) tracker.log_metric("train_symbol_frequencies", train_symbol_freq) tracker.log_metric("val_max_grid_height", val_grid_stats.get("max_height", 0)) tracker.log_metric("val_max_grid_width", val_grid_stats.get("max_width", 0)) tracker.log_metric("val_symbol_frequencies", val_symbol_freq) # example: adjust model configuration based grid size stats max_grid_height = max(train_grid_stats.get("max_height", 30), val_grid_stats.get("max_height", 30)) max_grid_width = max(train_grid_stats.get("max_width", 30), val_grid_stats.get("max_width", 30)) logger.debug(f"adjusted max grid size - height: {max_grid_height}, width: {max_grid_width}") # set number classes num_classes = 10 logger.info(f"number classes set to: {num_classes}") num_train_samples = train_data.get_num_samples() num_val_samples = val_data.get_num_samples() logger.info(f"number training examples: {num_train_samples}") logger.info(f"number validation examples: {num_val_samples}") num_train_samples == 0 num_val_samples == 0: logger.error("the dataset empty. please check synthetic data path dataset contents.") return logger.debug(f"train data size: {train_data.get_num_samples()}, validation data size: {val_data.get_num_samples()}") # set number classes num_classes = 10 logger.info(f"number classes set to: {num_classes}") # create dataloader instances logger.info("creating dataloader instances") # create dataloader instances logger.info("creating dataloader instances") config.training.balance_symbols: config.training.balancing_method == "weighting": # compute class weights (inverse frequencies) class_weights = 1.0 / torch.tensor(train_symbol_freq, dtype=torch.float) # removed weightedrandomsampler appropriate multi-class samples train_loader = dataloader( train_data, batch_size=config.training.batch_size, num_workers=get_num_workers(), shuffle=true, # enable shuffle pin_memory=true args.use_gpu else false, prefetch_factor=config.training.prefetch_factor, persistent_workers=config.training.persistent_workers ) logger.debug("class weights applied loss function. weightedrandomsampler removed.") elif config.training.balancing_method == "oversampling": # placeholder oversampling implementation logger.info("oversampling method selected, yet implemented.") # implement oversampling logic desired train_loader = dataloader( train_data, batch_size=config.training.batch_size, num_workers=get_num_workers(), shuffle=true, # enable shuffle using sampler pin_memory=true args.use_gpu else false, prefetch_factor=config.training.prefetch_factor, persistent_workers=config.training.persistent_workers ) else: logger.warning(f"unknown balancing method: {config.training.balancing_method}. skipping balancing.") train_loader = dataloader( train_data, batch_size=config.training.batch_size, num_workers=get_num_workers(), shuffle=true, # enable shuffle pin_memory=true args.use_gpu else false, prefetch_factor=config.training.prefetch_factor, persistent_workers=config.training.persistent_workers ) else: train_loader = dataloader( train_data, batch_size=config.training.batch_size, num_workers=get_num_workers(), shuffle=true, # enable shuffle pin_memory=true args.use_gpu else false, prefetch_factor=config.training.prefetch_factor, persistent_workers=config.training.persistent_workers ) val_loader = dataloader( val_data, batch_size=config.training.batch_size, num_workers=get_num_workers(), pin_memory=true args.use_gpu else false, prefetch_factor=config.training.prefetch_factor, persistent_workers=config.training.persistent_workers ) logger.debug(f"dataloaders created batch size {args.batch_size}") # initialize model logger.info("initializing model") model = gpt2arc(config=config, num_classes=num_classes, symbol_freq=train_symbol_freq_dict) logger.debug(f"model initialized config: {model_config}") # load checkpoint specified args.model_checkpoint: logger.info(f"loading model checkpoint: {args.model_checkpoint}") checkpoint = torch.load(args.model_checkpoint) 'model_config' checkpoint: model_config = modelconfig(**checkpoint['model_config']) model = gpt2arc(config=model_config) model.load_state_dict(checkpoint['state_dict']) # initialize results collector results_collector = resultscollector(config) # initialize experiment tracker tracker = experimenttracker(config, project=args.project) logger.debug("initializing experimenttracker") tracker = experimenttracker(config, project=args.project) logger.debug("initializing arctrainer") trainer = arctrainer( model=model, train_dataset=train_data, val_dataset=val_data, config=config ) trainer.log_hyperparameters() # determine accelerator parameters based --accelerator argument args.accelerator == "tpu": accelerator = 'tpu' devices = 'xla:1' # use 'xla:8' tpu v3-8 pods strategy = 'tpu_spawn' # recommended strategy tpu elif args.accelerator == "gpu": torch.cuda.is_available(): accelerator = 'gpu' devices = 1 else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' else: accelerator = 'cpu' devices = 1 strategy = 'auto' # changed none 'auto' # initialize callbacks list callbacks = [] # initialize grokfastcallback enabled config.training.use_grokfast: grokfast_callback = grokfastcallback( filter_type=config.training.grokfast_type, # 'ema' 'ma' alpha=config.training.grokfast_alpha, lamb=config.training.grokfast_lamb, window_size=config.training.grokfast_window_size config.training.grokfast_type == 'ma' else 100, # default warmup=true, trigger=false ) callbacks.append(grokfast_callback) logger.info("grokfastcallback added training callbacks.") else: logger.info("grokfast disabled; callback added.") # add standard modelcheckpoint callback args.no_checkpointing: checkpoint_callback = modelcheckpoint( dirpath="checkpoints", filename="checkpoint-{epoch:02d}-{val_loss:.4f}", save_top_k=3, monitor="val_loss", mode="min", ) callbacks.append(checkpoint_callback) # instantiate add modelconfigsaver callback model_config_saver = modelconfigsaver(config) callbacks.append(model_config_saver) logger.info("modelconfigsaver callback added training callbacks.") logger.info("setting pytorch lightning trainer") # define trial_num, task_id, iter_num trial_num = 0 # initialize 0 another appropriate default task_id = "default_task" # replace dynamic task identification necessary iter_num = 1 # initialize 1; increment needed within training loop # removed custom configsavingmodelcheckpoint needed args.no_logging: tb_logger = tensorboardlogger( save_dir="runs", name=f"experiment_{trainer.results_collector.experiment_id}" ) logger.debug(f"tensorboard logger initialized. log dir: {tb_logger.log_dir}") else: tb_logger = false logger.debug("logging disabled") pl_trainer = pl.trainer( max_epochs=config.training.max_epochs, logger=tb_logger, callbacks=callbacks callbacks else none, # includes modelcheckpoint enable_checkpointing=not args.no_checkpointing, enable_progress_bar=not args.no_progress_bar, fast_dev_run=args.fast_dev_run, # use command-line argument gradient_clip_val=1.0, # add gradient clipping precision=16, # enable automatic mixed precision accelerator=accelerator, devices=devices, strategy=strategy, profiler=profiler ) tb_logger: trainer.results_collector.set_tensorboard_log_path(tb_logger.log_dir) logger.debug(f"tensorboard log path set results collector: {tb_logger.log_dir}") # log initial memory usage args.use_gpu torch.cuda.is_available(): logger.info(f"initial cuda memory allocated: {torch.cuda.memory_allocated()} bytes") logger.info(f"initial cuda memory reserved: {torch.cuda.memory_reserved()} bytes") # train model logger.info("starting model training") pl_trainer.fit(trainer, train_dataloaders=train_loader, val_dataloaders=val_loader) # log memory usage training args.use_gpu torch.cuda.is_available(): logger.info(f"cuda memory allocated training: {torch.cuda.memory_allocated()} bytes") logger.info(f"cuda memory reserved training: {torch.cuda.memory_reserved()} bytes") # training, run test logger.info("running model evaluation") test_results = pl_trainer.test(trainer) test_results: avg_test_loss = sum(result['avg_test_loss'] result test_results) / len(test_results) avg_test_accuracy = sum(result['avg_test_accuracy'] result test_results) / len(test_results) avg_test_diff_accuracy = sum(result['avg_test_diff_accuracy'] result test_results) / len(test_results) logger.info(f"test results - loss: {avg_test_loss}, accuracy: {avg_test_accuracy}, diff accuracy: {avg_test_diff_accuracy}") results = { "avg_test_loss": avg_test_loss, "avg_test_accuracy": avg_test_accuracy, "avg_test_diff_accuracy": avg_test_diff_accuracy, } # add task-specific results result test_results: key, value result.items(): key.endswith('_test_accuracy') key.endswith('_test_diff_accuracy'): results[key] = value trainer.results_collector.set_test_results(results) trainer.results_collector.set_final_metrics({ "best_val_loss": trainer.best_val_loss, "best_epoch": trainer.best_epoch, "final_test_loss": avg_test_loss, "final_test_accuracy": avg_test_accuracy }) # save final model configuration logger.info("saving final model configuration") model_path = f"final_model_{trainer.results_collector.experiment_id}.pth" os.makedirs("checkpoints", exist_ok=true) torch.save({ 'state_dict': trainer.model.state_dict(), 'model_config': trainer.config.model.__dict__ }, model_path) trainer.results_collector.set_checkpoint_path(model_path) logger.debug(f"model configuration saved to: {model_path}") # save results logger.info("saving experiment results") os.makedirs("results", exist_ok=true) results_path = f"results/experiment_{trainer.results_collector.experiment_id}.json" trainer.results_collector.save_to_json(results_path) logger.debug(f"results saved to: {results_path}") except runtimeerror e: 'cuda memory' str(e): logger.error("cuda memory error occurred.") logger.error("consider reducing batch size model complexity.") raise runtimeerror("cuda memory error occurred.") else: logger.error(f"a runtime error occurred: {str(e)}", exc_info=true) raise runtimeerror(f"a runtime error occurred: {str(e)}") except exception e: logger.error(f"an unexpected error occurred: {str(e)}", exc_info=true) sys.exit(1) # exit program logging error finally: 'tracker' locals(): tracker.finish() trainer none: # ... proceed training ... pass else: logger.error("trainer initialized. exiting training loop.") __name__ == "__main__": parser = argparse.argumentparser(description="train arc neural reasoning model") group = parser.add_mutually_exclusive_group() group.add_argument("--use-profiler", action="store_true", help="enable custom profiler") group.add_argument("--fast-dev-run", action="store_true", help="run fast development test") parser.add_argument( "--optuna-study-name", type=str, default=none, help="name optuna study load. provided one study exists storage, used automatically." ) parser.add_argument("--optuna-storage", type=str, default="sqlite:///optuna_results.db", help="storage url optuna study") parser.add_argument("--n-embd", type=int, default=4, help="embedding dimension profiling") parser.add_argument("--n-head", type=int, default=1, help="number attention heads profiling") parser.add_argument("--n-layer", type=int, default=1, help="number transformer layers profiling") parser.add_argument("--batch-size", type=int, default=32, help="batch size profiling") parser.add_argument("--learning-rate", type=float, default=1e-4, help="learning rate") parser.add_argument("--max-epochs", type=int, required=true, help="maximum number epochs") parser.add_argument("--mamba-ratio", type=float, default=0.0, help="mamba ratio (float value)") parser.add_argument("--dropout", type=float, default=0.05, help="dropout rate") parser.add_argument("--d-state", type=int, default=4, help="mamba state dimension") parser.add_argument("--d-conv", type=int, default=1, help="mamba convolution dimension") parser.add_argument("--mamba-depth", type=int, default=1, help="depth mamba layer") parser.add_argument("--mamba-expand", type=int, default=2, help="expand factor mamba layer") parser.add_argument("--use-gpu", action="store_true", help="use gpu training available") parser.add_argument("--use-grokfast", action="store_true", help="enable grokfast gradient filtering.") parser.add_argument( "--grokfast-type", type=str, default="ema", choices=["ema", "ma"], help="type grokfast filter use: 'ema' 'ma'." ) parser.add_argument( "--grokfast-alpha", type=float, default=0.98, help="alpha parameter grokfast-ema." ) parser.add_argument( "--grokfast-lamb", type=float, default=2.0, help="lambda parameter grokfast filters." ) parser.add_argument( "--grokfast-window_size", type=int, default=100, help="window size grokfast-ma." ) parser.add_argument("--no-logging", action="store_true", help="disable logging") parser.add_argument("--no-checkpointing", action="store_true", help="disable checkpointing") parser.add_argument("--no-progress-bar", action="store_true", help="disable progress bar") parser.add_argument("--model_checkpoint", type=str, help="path model checkpoint resume training") parser.add_argument("--project", type=str, default="gpt2-arc", help="w&amp;b project name") parser.add_argument("--results-dir", type=str, default="./results", help="directory save results") parser.add_argument("--run-name", type=str, default="default_run", help="name run saving results") parser.add_argument("--use-synthetic-data", action="store_true", help="use synthetic data training") parser.add_argument( "--matmul-precision", type=str, default="medium", choices=["highest", "high", "medium"], help="set internal precision float32 matrix multiplications. options: 'highest', 'high', 'medium'. defaults 'medium'." ) parser.add_argument("--synthetic-data-path", type=str, help="path synthetic data directory") parser.add_argument("--log-level", type=str, default="info", help="logging level") parser.add_argument("--use-optuna", action="store_true", help="use best hyperparameters optuna study") parser.add_argument( "--accelerator", type=str, default="gpu", choices=["cpu", "gpu", "tpu"], help="accelerator use training: 'cpu', 'gpu', 'tpu'. defaults 'gpu'." ) parser.add_argument( "--profiler-dirpath", type=str, default="./profiler_logs", help="directory path profiler output files." ) parser.add_argument( "--profiler-filename", type=str, default="profile", help="filename profiler output." ) args = parser.parse_args() # validate mamba_ratio args.mamba_ratio &lt; 0.0: logger.error("invalid value --mamba-ratio: must non-negative.") sys.exit(1) main(args)</file><file name="src/UNKNOWN.egg-info/dependency_links.txt" /><file name="src/UNKNOWN.egg-info/SOURCES.txt">readme.md setup.py src/__init__.py src/checkpoint_evaluator.py src/config.py src/evaluate.py src/optimize_hyperparameters.py src/unknown.egg-info/pkg-info src/unknown.egg-info/sources.txt src/unknown.egg-info/dependency_links.txt src/unknown.egg-info/top_level.txt src/data/__init__.py src/data/arc_dataset.py src/models/__init__.py src/models/gpt2.py src/training/__init__.py src/training/train.py src/training/trainer.py src/utils/__init__.py src/utils/experiment_tracker.py src/utils/grokfast.py src/utils/grokfast_callback.py src/utils/helpers.py src/utils/model_memory_estimator.py src/utils/performance_metrics.py src/utils/results_collector.py tests/test_arc_dataset.py tests/test_arc_trainer.py tests/test_balancing.py tests/test_benchmark.py tests/test_checkpoint_loading.py tests/test_class_weights.py tests/test_dataset.py tests/test_differential_pixel_accuracy.py tests/test_end_to_end.py tests/test_evaluate.py tests/test_experiment_tracker.py tests/test_gpt2.py tests/test_grokfast.py tests/test_hyperparameter_optimization.py tests/test_integration_experiment.py tests/test_mamba_integration.py tests/test_metrics.py tests/test_model_evaluation.py tests/test_models.py tests/test_optimize_hyperparameters.py tests/test_pytest_error_fixer.py tests/test_pytorch_lightning_integration.py tests/test_results_collector.py tests/test_synthetic_arc_dataset.py tests/test_synthetic_data.py tests/test_train.py tests/test_trainer.py</file><file name="src/UNKNOWN.egg-info/top_level.txt">__init__ checkpoint_evaluator config data evaluate models optimize_hyperparameters training utils</file><file name="src/models/__init__.py" /><file name="src/models/gpt2.py"># gpt2_arc/src/models/gpt2.py import logging import torch import torch.nn.functional f import pytorch_lightning pl torch import nn typing import dict import torch.nn.init init bitnet import bitlinearnew zeta.nn import mambablock logging.basicconfig(level=logging.debug) logger = logging.getlogger(__name__) class attention(nn.module): def __init__(self, n_embd, n_head, dropout): super().__init__() self.n_head = n_head self.n_embd = n_embd self.key = bitlinearnew(n_embd, n_embd) self.query = bitlinearnew(n_embd, n_embd) self.value = bitlinearnew(n_embd, n_embd) self.proj = bitlinearnew(n_embd, n_embd) self.dropout = nn.dropout(dropout) # add line logger.debug(f"initialized attention n_embd={n_embd}, n_head={n_head}") def forward(self, x, mask=none): b, t, c = x.size() torch._dynamo.is_compiling(): logger.debug(f"attention input shape: {x.shape}") k = self.key(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) q = self.query(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) v = self.value(x).view(b, t, self.n_head, c // self.n_head).transpose(1, 2) att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32))) mask none: att = att.masked_fill(mask[:, none, none, :] == 0, float("-inf")) att = f.softmax(att, dim=-1) # apply dropout attention probabilities att = self.dropout(att) # add line = att @ v = y.transpose(1, 2).contiguous().view(b, t, c) output = self.proj(y) torch._dynamo.is_compiling(): logger.debug(f"attention output shape: {output.shape}") return output class feedforward(nn.module): def __init__(self, n_embd, dropout): super().__init__() self.net = nn.sequential( bitlinearnew(n_embd, 4 * n_embd), nn.relu(), nn.dropout(dropout), bitlinearnew(4 * n_embd, n_embd) ) logger.debug(f"initialized feedforward n_embd={n_embd}") def forward(self, x): torch._dynamo.is_compiling(): logger.debug(f"feedforward input shape: {x.shape}") output = self.net(x) torch._dynamo.is_compiling(): logger.debug(f"feedforward output shape: {output.shape}") return output class transformerblock(nn.module): def __init__(self, n_embd, n_head, dropout): super().__init__() self.attention = attention(n_embd, n_head, dropout) self.feed_forward = feedforward(n_embd, dropout) self.ln1 = nn.layernorm(n_embd) self.ln2 = nn.layernorm(n_embd) self.dropout = nn.dropout(dropout) logger.debug( f"initialized transformerblock n_embd={n_embd}, n_head={n_head}" ) def forward(self, x, mask=none): torch._dynamo.is_compiling(): logger.debug(f"transformerblock input shape: {x.shape}") # attention sublayer residual dropout attn_output = self.attention(self.ln1(x), mask) attn_output = self.dropout(attn_output) # apply dropout x = x + attn_output # feed-forward sublayer residual dropout ff_output = self.feed_forward(self.ln2(x)) ff_output = self.dropout(ff_output) # apply dropout x = x + ff_output torch._dynamo.is_compiling(): logger.debug(f"transformerblock output shape: {x.shape}") return x class mambalayer(nn.module): def __init__(self, n_embd, d_state, d_conv, dropout, depth, expand): super().__init__() self.dropout = nn.dropout(dropout) self.mamba_block = mambablock( dim=n_embd, depth=depth, # use depth config d_state=d_state, d_conv=d_conv, expand=expand, # use expand config conv_bias=true, # default value bias=false # default value ) self.layer_norm = nn.layernorm(n_embd) logger.debug( f"initialized mambalayer n_embd={n_embd}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}" ) def forward(self, x): torch._dynamo.is_compiling(): logger.debug(f"mambalayer input shape: {x.shape}") x_norm = self.layer_norm(x) x_mamba = self.mamba_block(x_norm) x_mamba = self.dropout(x_mamba) output = x + x_mamba torch._dynamo.is_compiling(): logger.debug(f"mambalayer output shape: {output.shape}") return output gpt2_arc.src.config import config class gpt2arc(pl.lightningmodule): def __init__(self, config: config, num_classes: int, symbol_freq: dict[str, float] = none): # define example input array model summary self.example_input_array = torch.zeros(1, 1, 6, 6) # adjust dimensions needed super().__init__() self.config = config self.symbol_freq = symbol_freq # replace token embedding convolutional layer self.conv1 = nn.conv2d( in_channels=1, out_channels=self.config.model.n_embd, # accessing 'model' attribute within config kernel_size=3, padding=1 ).to(torch.float32) # initialize blocks interleaved transformerblocks mambalayer(s) self.blocks = nn.modulelist() num_transformer_blocks = self.config.model.n_layer total_mamba_layers = int(num_transformer_blocks * self.config.model.mamba_ratio) logger.debug(f"total transformerblocks: {num_transformer_blocks}") logger.debug(f"total mambalayers add: {total_mamba_layers}") # distribute mambalayers across transformerblocks mamba_layer_positions = [] total_mamba_layers &gt; 0: step = num_transformer_blocks / total_mamba_layers mamba_layer_positions = [int(i * step) range(total_mamba_layers)] current_mamba_index = 0 layer_idx range(num_transformer_blocks): # add transformerblock self.blocks.append(transformerblock(self.config.model.n_embd, self.config.model.n_head, self.config.model.dropout)) logger.debug(f"layer {len(self.blocks)}: added transformerblock") # check add mambalayer transformerblock current_mamba_index &lt; len(mamba_layer_positions) layer_idx == mamba_layer_positions[current_mamba_index]: # add mambalayer self.blocks.append( mambalayer( n_embd=self.config.model.n_embd, d_state=self.config.model.d_state, d_conv=self.config.model.d_conv, dropout=self.config.model.dropout, depth=self.config.model.mamba_depth, expand=self.config.model.mamba_expand ) ) logger.debug(f"layer {len(self.blocks)}: added mambalayer transformerblock {layer_idx + 1}") current_mamba_index += 1 self.ln_f = nn.layernorm(self.config.model.n_embd) assert isinstance(self.config.model.n_embd, int), "model.n_embd must integer" assert isinstance(num_classes, int), "num_classes must integer" self.fc_out = bitlinearnew(int(self.config.model.n_embd), int(num_classes)) # add final linear layer # initialize loss function class weights needed self.config.training.balance_symbols self.config.training.balancing_method == "weighting": self.symbol_freq: raise valueerror("symbol_freq must provided balance_symbols true balancing_method 'weighting'") class_weights = 1.0 / torch.tensor(list(symbol_freq.values()), dtype=torch.float) self.loss_fn = nn.crossentropyloss(weight=class_weights) logger.debug(f"class weights: {class_weights}") # added line else: self.loss_fn = nn.crossentropyloss() # initialize weights self.apply(self._init_weights) def _init_weights(self, module): isinstance(module, nn.conv2d): # calculate fan_in conv2d fan_in = module.in_channels * module.kernel_size[0] * module.kernel_size[1] std = 1.0 / fan_in**0.5 init.normal_(module.weight, mean=0.0, std=std) module.bias none: init.zeros_(module.bias) elif isinstance(module, bitlinearnew): fan_in = module.in_features std = 1.0 / fan_in**0.5 init.normal_(module.weight, mean=0.0, std=std) module.bias none: init.zeros_(module.bias) # initialization nn.layernorm, using default def forward(self, input_ids, attention_mask=none): torch._dynamo.is_compiling(): logger.debug(f"gpt2arc input shape: {input_ids.shape}, dtype: {input_ids.dtype}") # check input_ids already correct shape input_ids.dim() == 4: x = input_ids.float() else: # reshape input_ids [batch_size, 1, height, width] batch_size = input_ids.size(0) seq_length = input_ids.size(1) height = width = int(seq_length ** 0.5) x = input_ids.float().view(batch_size, 1, height, width) x = self.conv1(x) logger.debug(f"after conv1 shape: {x.shape}") b, c, h, w = x.size() x = x.view(b, c, h * w) # flatten spatial dimensions x = x.permute(0, 2, 1) # rearrange (batch_size, sequence_length, channels) logger.debug(f"reshaped transformer blocks: {x.shape}") i, block enumerate(self.blocks): # check block transformerblock mambalayer isinstance(block, transformerblock): x = block(x, attention_mask) logger.debug(f"after transformerblock {i + 1}: shape {x.shape}") else: x = block(x) logger.debug(f"after mambalayer {i + 1}: shape {x.shape}") x = self.ln_f(x) x = self.fc_out(x) # apply final linear layer return x</file><file name="debug tips/test_train.md">encountering error `gpt2_arc/tests/test_train.py` test suite, relevant files examine debugging resolving issue directly imported utilized within test cases. here's breakdown key files focus on: 1. **source files test:** - **`gpt2_arc/src/training/train.py`** - **role:** contains `main` function orchestrates training process. - **why check:** since tests invoking `main(args)`, issues training initiated handled would likely originate here. - **`gpt2_arc/src/training/trainer.py`** - **role:** defines `arctrainer` class, subclass `pl.lightningmodule` responsible managing training loop. - **why check:** errors related training logic, training steps, validation steps, integration pytorch lightning, would stem file. - **`gpt2_arc/src/models/gpt2.py`** - **role:** implements `gpt2arc` model, including architecture forward pass. - **why check:** error pertains model initialization, forward propagation, layer-specific issues, primary file inspect. - **`gpt2_arc/src/data/arc_dataset.py`** - **role:** contains `arcdataset` class responsible data loading preprocessing. - **why check:** issues related data handling, dataset initialization, data preprocessing, data loader configuration, would originate here. - **`gpt2_arc/src/config.py`** - **role:** defines configuration dataclasses like `config`, `modelconfig`, `trainingconfig`. - **why check:** misconfigurations incorrect parameter settings affect training behavior would defined file. 2. **utility support files:** - **`gpt2_arc/src/utils/results_collector.py`** - **role:** implements `resultscollector` class aggregating managing training results. - **why check:** errors related result logging, metric collection, summary generation would found here. - **`gpt2_arc/src/utils/experiment_tracker.py`** - **role:** manages experiment tracking, possibly integrating tools like weights &amp; biases. - **why check:** error involves experiment tracking, logging configurations, integrations external tracking tools, file pertinent. 3. **additional considerations:** - **`gpt2_arc/benchmark.py` `gpt2_arc/src/evaluate.py`** - **role:** files focused benchmarking evaluation, respectively, might still interact training components. - **why check:** error indirectly involves evaluation metrics benchmarking training, reviewing files could provide insights. - **mock fixture implementations `test_train.py`:** - **role:** test file uses fixtures mocks extensively simulate different components. - **why check:** ensure mocks correctly mimic behavior actual classes fixtures set appropriately. errors test setup lead misleading test failures. 4. **logging configuration:** - **logging configuration `test_train.py`:** - **role:** test file sets logging levels configurations. - **why check:** misconfigured logging obscure error messages lead unexpected behaviors testing. 5. **dependencies environment:** - **external libraries:** - ensure dependencies like `pytorch_lightning`, `torch`, libraries correctly installed compatible codebase. - **environment variables paths:** - verify `sys.path` manipulations environment settings test file correctly point necessary modules path conflicts. **summary:** effectively debug resolve errors `test_train.py`: - **start source files tested** (`train.py`, `trainer.py`, `gpt2.py`, `arc_dataset.py`, `config.py`) identify underlying issues training pipeline. - **examine utility files** (`results_collector.py` `experiment_tracker.py`) problems related logging result management. - **review test setup itself**, ensuring mocks fixtures accurately represent real components setup-related errors. systematically inspecting areas, pinpoint root cause errors implement effective fixes.</file><file name="debug tips/test_trainer.md">encountering error `gpt2_arc/tests/test_trainer.py` file, relevant files examine debugging fixing issue test file directly interacts depends upon. here's breakdown key files roles: 1. **`src/config.py`** - **classes check:** - `config` - `modelconfig` - `trainingconfig` - **relevance:** file defines configuration classes used initialize models trainers. errors related configuration parameters, default values, initialization logic likely originate here. 2. **`src/data/arc_dataset.py`** - **classes functions check:** - `arcdataset` - `set_debug_mode` - `_process_synthetic_data` - `_process_arckit_data` - `_preprocess_grid` - **relevance:** file handles data preprocessing dataset creation. issues related data loading, preprocessing steps, dataset structure (e.g., unexpected data formats) would found here. 3. **`src/models/gpt2.py`** - **classes check:** - `gpt2arc` - `attention` - `feedforward` - `transformerblock` - `modelconfig` - **relevance:** file contains gpt-2 model architecture components. errors model's forward pass, layer configurations, parameter settings likely rooted file. 4. **`src/training/trainer.py`** - **classes methods check:** - `arctrainer` - `training_step` - `validation_step` - `configure_optimizers` - `train_dataloader` - `val_dataloader` - `test_step` - **relevance:** core training module orchestrates training validation processes. issues related training loop, optimizer configuration, data loaders, logging mechanisms would addressed here. 5. **`src/utils/experiment_tracker.py`** - **classes methods check:** - `experimenttracker` - `log_metric` - `update_train_metrics` - `update_val_metrics` - `set_test_results` - `save_to_json` - **relevance:** tests involve tracking experiments logging metrics, errors related metric logging, experiment initialization, result serialization would involve file. 6. **`src/utils/results_collector.py`** - **classes methods check:** - `resultscollector` - `update_train_metrics` - `update_val_metrics` - `set_test_results` - `add_task_specific_result` - `save_to_json` - **relevance:** similar `experiment_tracker.py`, file manages collection storage results. errors aggregating storing test results would pertinent here. ### steps debug: 1. **identify error message:** - start looking exact error message stack trace. often point directly file line number issue originated. 2. **trace dependencies:** - understand `test_trainer.py` interacts modules. instance, there's issue model initialization, focus `src/config.py` `src/models/gpt2.py`. 3. **check configurations:** - ensure configuration objects (`config`, `modelconfig`, `trainingconfig`) correctly set required parameters provided. 4. **validate data handling:** - error related data loading preprocessing, review methods `arc_dataset.py` ensure data processed expected. 5. **inspect model architecture:** - issues forward pass model outputs, delve `gpt2.py` verify layer configurations data flow within model. 6. **examine training logic:** - error occurs training validation steps, scrutinize `arctrainer` class `trainer.py`, focusing methods like `training_step` `validation_step`. 7. **review utility functions:** - issues related logging result collection, check utility files ensure metrics recorded stored correctly. ### additional tips: - **use debugging tools:** - incorporate debugging statements use tools like `pdb` step code inspect variable states different execution points. - **isolate issue:** - temporarily simplify tests mock certain components isolate error occurring. - **check dependencies versions:** - ensure dependencies (e.g., pytorch, pytest) date compatible codebase. - **consult documentation:** - review documentation third-party libraries frameworks youre using ensure adhering best practices usage patterns. systematically examining files following structured debugging approach, able identify resolve error `test_trainer.py` code.</file><file name="debug tips/test_arc_dataset.md">effectively troubleshoot fix errors `gpt2_arc/tests/test_arc_dataset.py` test suite, focus following key files repository: 1. **`gpt2_arc/src/data/arc_dataset.py`** - **why:** primary file `arcdataset` class `set_debug_mode` function defined. since tests directly interacting components, issues related dataset initialization, data preprocessing, utility functions likely originate here. - **what look for:** - **initialization logic:** ensure `__init__` method correctly handles different types `data_source` inputs (`str`, `list[dict]`, `taskset`, etc.). - **data processing methods:** check methods like `_process_synthetic_data`, `_process_arckit_data`, `_preprocess_grid` logical errors incorrect handling data. - **debug mode handling:** verify `set_debug_mode` function correctly toggles debug state debug-related logging behavior `arcdataset` functioning expected. 2. **`gpt2_arc/src/utils/experiment_tracker.py`** - **why:** directly referenced test file, utility classes like `experimenttracker` influence behavior dataset, especially used logging tracking metrics dataset processing. - **what look for:** - **logging configuration:** ensure logging correctly set interfere dataset operations. - **serialization methods:** check methods like `_make_serializable` `_serialize_config` ensure configurations correctly handled, affect dataset initialization configurations passed around. 3. **`gpt2_arc/src/models/gpt2.py`** - **why:** although tests focus dataset, models often interact closely datasets training evaluation. issues model configurations data handling within model indirectly affect dataset behavior. - **what look for:** - **data expectations:** ensure model correctly expects data shapes types provided `arcdataset`. - **integration points:** verify integration points model dataset (if present) correctly implemented. 4. **dependencies external libraries (`arckit`)** - **why:** tests import `taskset` `arckit.data`, suggests `arckit` external dependency. issues within library propagate dataset tests. - **what look for:** - **compatibility:** ensure version `arckit` using compatible dataset known bugs affecting `taskset`. - **mock implementations:** since use `unittest.mock.mock` `taskset`, ensure mock accurately reflects structure behavior expected `arcdataset`. 5. **test file (`gpt2_arc/tests/test_arc_dataset.py`)** - **why:** sometimes, issue might reside within test logic rather implementation. reviewing test file help identify incorrect assumptions faulty test setups. - **what look for:** - **test fixtures:** ensure fixtures like `sample_data` `mock_taskset` provide correct data structures expected `arcdataset`. - **assertions:** verify assertions correctly reflect intended behavior overly restrictive incorrectly specified. - **skipped tests:** review certain tests skipped determine need updated fixed included test suite. 6. **additional configuration files (`gpt2_arc/src/config.py`)** - **why:** configuration files often dictate datasets models initialized interacted with. errors configurations lead unexpected behaviors testing. - **what look for:** - **model dataset configurations:** ensure necessary configurations dataset correctly defined accessible. - **defaults overrides:** check default configurations set overridden, ensuring consistency across different test scenarios. **summary:** - **primary focus:** `gpt2_arc/src/data/arc_dataset.py` - **secondary focus:** utility files like `experiment_tracker.py`, model definitions `gpt2.py`, configuration files `config.py` - **dependencies:** ensure external libraries like `arckit` functioning expected - **test integrity:** verify correctness test setups assertions within `test_arc_dataset.py` systematically reviewing areas, able identify resolve errors within test suite effectively.</file><file name="debug tips/test_end_to_end.md">troubleshooting errors `test_end_to_end.py` script, several files within repository likely provide relevant information help identify fix issue. here's breakdown key files examine based different parts test script: ### 1. **data handling preprocessing** - **`src/data/arc_dataset.py`** - **relevance:** file contains `arcdataset` class, crucial loading preprocessing arc dataset. errors related data loading, dataset splitting, preprocessing steps (like `_process_synthetic_data` `_preprocess_grid`) likely originate here. - **what check:** - ensure dataset paths correct. - verify data processing methods handling data expected. - check issues `collate_fn` used batching data. - **`arckit` module** - **relevance:** test script uses `arckit.load_data()` load dataset. issues data loading process structure loaded data would tied module. - **what check:** - ensure `arckit` correctly installed accessible. - verify `load_data` function returns data expected format. ### 2. **model definition** - **`src/models/gpt2.py`** - **relevance:** file defines `gpt2arc` model components (`attention`, `feedforward`, `transformerblock`). errors related model architecture, layer mismatches incorrect configurations, likely originate here. - **what check:** - ensure model configuration (`modelconfig`) matches expected architecture. - verify layers correctly defined initialized. - check type mismatches tensor dimension issues within model. ### 3. **training logic** - **`src/training/trainer.py`** - **relevance:** file contains `arctrainer` class, manages training loop, loss computation, metric tracking. errors training, issues optimizer, loss functions, training steps, likely stem here. - **what check:** - ensure training configurations (`trainingconfig`) correctly set. - verify implementation training validation steps. - check runtime errors forward backward passes. ### 4. **configuration management** - **`src/config.py`** - **relevance:** file defines configuration data classes (`config`, `modelconfig`, `trainingconfig`). misconfigurations, incorrect hyperparameters mismatched settings, lead errors model initialization training. - **what check:** - ensure configuration parameters correctly set passed components. - verify default values appropriate overrides correctly applied. ### 5. **utility functions experiment tracking** - **`src/utils/experiment_tracker.py`** - **relevance:** test script involves experiment tracking logging metrics, issues could affect logging tracking experiments. - **what check:** - ensure experiment tracker correctly initialized configured. - verify metrics logged saved expected. - **`src/utils/results_collector.py`** - **relevance:** file manages collection storage results training evaluation. errors related result aggregation storage likely originate here. - **what check:** - ensure results correctly collected serialized. - verify issues saving loading result data. ### 6. **evaluation process** - **`src/evaluate.py`** - **relevance:** although directly referenced test script, evaluation logic invoked shared scripts, issues could affect evaluation metrics. - **what check:** - ensure evaluation metrics correctly computed. - verify evaluation data correctly processed fed model. ### 7. **other potential sources** - **`benchmark.py` `train.py`** - **relevance:** likely related running benchmarks training outside tests, shared components configurations could indirectly affect tests. - **what check:** - ensure shared utilities configurations used scripts consistent error-free. ### **general debugging tips:** 1. **logging:** test script extensive logging enabled (`logging.basicconfig(level=logging.debug)`). review debug logs pinpoint error occurs. logs provide step-by-step insights test execution flow. 2. **assertions error messages:** pay close attention assertion statements error messages produce. guide exact point failure. 3. **dependencies:** ensure dependencies (like `arckit`, `torch`, `pytorch_lightning`, etc.) correctly installed compatible other. 4. **environment issues:** sometimes, errors arise environment (e.g., incorrect cuda setup, incompatible library versions). verify environment matches expected setup. 5. **isolate issue:** possible, try running individual components smaller tests isolate error occurring. help narrow problematic file section code. ### **next steps:** reviewing files still unable identify issue, consider following: - **provide specific error messages:** sharing exact error messages stack traces help diagnosing problem accurately. - **add relevant files:** issue seems originate specific file listed here, feel free add content chat in-depth analysis. systematically reviewing files following debugging tips, able identify resolve errors end-to-end test script effectively.</file><file name="debug tips/test_differential_pixel_accuracy.md">identify fix errors `test_differential_pixel_accuracy.py` test script, essential examine files modules test interacts with. based provided code summaries repository files, following files likely candidates investigate: 1. **`gpt2_arc/src/utils/helpers.py`** - **reason:** file contains `differential_pixel_accuracy` function, central test cases. there's error related accuracy computation, implementation first place check. - **action:** review implementation `differential_pixel_accuracy` potential bugs inconsistencies. ensure correctly handles different tensor shapes, data types, edge cases like empty tensors. 2. **`gpt2_arc/src/models/gpt2.py`** - **reason:** `gpt2arc` model instantiated used one tests (`test_differential_pixel_accuracy_with_arckit_data`). errors related model initialization, prediction generation, tensor shapes likely rooted here. - **action:** - verify `gpt2arc` model correctly defined, especially forward pass. - ensure model's output dimensions match expected shapes used test. - check potential issues model's layers (e.g., `attention`, `feedforward`, `transformerblock`) might affect predictions. 3. **`gpt2_arc/src/config.py`** - **reason:** `modelconfig` dataclass used configure `gpt2arc` model. misconfigurations lead unexpected behaviors mismatches model parameters. - **action:** - ensure necessary configuration parameters correctly defined passed. - check consistency configuration used tests model's requirements. 4. **`gpt2_arc/src/data/arc_dataset.py`** - **reason:** `arcdataset` class responsible data loading preprocessing, critical generating valid input target tensors tests. - **action:** - verify data preprocessing methods (e.g., `_process_arckit_data`, `_preprocess_grid`) correctly handle data. - ensure dataset returns tensors expected shapes types. - check `reverse_scaling` method confirm accurately reverses scaling applied preprocessing. 5. **external dependency: `arckit`** - **reason:** test `test_differential_pixel_accuracy_with_arckit_data` relies `arckit` library load task data. issues data loading compatibility stem here. - **action:** - ensure `arckit` library correctly installed compatible project. - verify `task_id` used (`"007bbfb7"`) exists `arckit.load_single(task_id)` returns expected data structure. - check updates changes `arckit` api might affect data loading. 6. **additional considerations:** - **environment dependencies:** - ensure dependencies (e.g., pytorch, `arckit`) up-to-date compatible other. - verify python environment necessary packages installed. - **test environment:** - confirm test run environment relative paths module imports correctly resolved. - check recent changes project structure might affect import statements. 7. **debugging tips:** - **verbose logging:** enhance test functions detailed logging pinpoint error occurs. example, print shapes data types tensors operation. - **isolate tests:** run individual test functions separately identify specific test failing. - **use assertions carefully:** ensure assertions accurately reflect expected outcomes. example, floating-point comparisons might require tolerance level instead exact equality. 8. **if error persists:** - **provide error messages:** sharing specific error messages stack traces help diagnosing issue effectively. - **check version control:** review recent commits identify changes might introduced error. - **consult documentation:** refer documentation external libraries like `arckit` breaking changes known issues. systematically examining files following debugging steps, able identify resolve error test code effectively. need assistance specific files error messages, feel free share relevant code snippets details.</file><file name="debug tips/test_integration_experiment.md">debugging errors `test_integration_experiment.py` test file, several parts codebase likely provide valuable insights. based imports structure project, following files relevant diagnosing fixing potential issues: 1. **data handling preprocessing:** - **`gpt2_arc/src/data/arc_dataset.py`** - **why:** file contains `arcdataset` class, crucial data loading preprocessing. errors related data formatting, missing fields, incorrect data types often originate here. - **key sections review:** - `__init__` method: ensure dataset initialized correctly provided data sources. - `_process_arckit_data` `_process_synthetic_data` methods: verify data `arckit` processed expected. - debug logging statements might help trace data issues. 2. **model definition:** - **`gpt2_arc/src/models/gpt2.py`** - **why:** file defines `gpt2arc` model related components like `attention`, `feedforward`, `transformerblock`. errors related model architecture, layer configurations, forward passes stem here. - **key sections review:** - `gpt2arc` class initialization: check layers correctly instantiated right configurations. - forward methods: ensure data flows correctly model without shape mismatches issues. - custom configurations modifications standard gpt-2 architecture. 3. **training logic:** - **`gpt2_arc/src/training/trainer.py`** - **why:** file contains `arctrainer` class, manages training loop, loss calculations, metric updates. issues like improper training steps, incorrect loss functions, metric logging problems found here. - **key sections review:** - `__init__` method: ensure datasets, model, configurations correctly set up. - training step methods: verify loss calculations backpropagation implemented correctly. - integration pytorch lightning: check compatibility correct usage lightning's `trainer`. 4. **configuration management:** - **`gpt2_arc/src/config.py`** - **why:** file defines configuration classes like `config`, `modelconfig`, `trainingconfig`. misconfigurations, incorrect hyperparameters missing configuration fields, lead errors data loading, model initialization, training. - **key sections review:** - default values data types configuration parameters. - methods manipulate validate configurations. - integration points configurations passed components like model trainer. 5. **results collection logging:** - **`gpt2_arc/src/utils/results_collector.py`** - **why:** file manages collection summarization training validation results. errors related metric logging, result storage, summary generation originate here. - **key sections review:** - methods updating retrieving metrics. - serialization saving results. - integration components ensure metrics correctly passed stored. 6. **additional considerations:** - **`arckit` library:** - **why:** test setup relies `arckit` library load task data. issues tasks loaded structured, could affect tests. - **action:** ensure `arckit` correctly installed task ids used tests (`"007bbfb7"`) valid accessible. - **pytorch lightning integration:** - **files involved:** listed explicitly, test uses pytorch lightning's `trainer`. ensure integrations lightning correctly implemented `arctrainer` class callbacks like `modelcheckpoint` properly configured. - **logging debug statements:** - **why:** test includes several `print` statements debugging. ensure logs provide meaningful information interfere test flow. 7. **common error scenarios file associations:** - **import errors:** - **files check:** `arc_dataset.py`, `gpt2.py`, `trainer.py`, `config.py`, `results_collector.py` - **reason:** missing incorrect imports usually point issues modules. - **attribute errors missing methods:** - **files check:** `gpt2.py`, `trainer.py`, `arc_dataset.py` - **reason:** ensure necessary methods attributes defined correctly named. - **data shape mismatches:** - **files check:** `arc_dataset.py`, `gpt2.py` - **reason:** verify data shapes consistent throughout data pipeline model. - **configuration mismatches:** - **files check:** `config.py`, `trainer.py`, `gpt2.py` - **reason:** ensure components receive use configurations correctly. 8. **next steps:** - **review relevant files:** start examining files listed above, focusing sections likely related error. - **add detailed logging:** already present, consider adding detailed logging within files trace flow data identify things might going wrong. - **isolate issue:** determine whether error related data loading, model initialization, training steps, configuration. help narrow file focus on. - **run tests incrementally:** use pytest's verbose mode selectively run tests get context failure occurs. identify specific file section causing issue need assistance, feel free share relevant code snippets adding files chat. allow targeted help resolving problem.</file><file name="debug tips/test_gpt2.md">encountering error `gpt2_arc/tests/test_gpt2.py` test suite, relevant files examine troubleshooting define components tested. here's breakdown primary files investigate: 1. **`gpt2_arc/src/models/gpt2.py`** - **why:** file contains definitions `gpt2arc` class well constituent modules like `attention`, `feedforward`, `transformerblock`. since tests directly interacting classes (e.g., initializing `gpt2arc`, performing forward passes, etc.), issues related model architecture, initialization, forward computations would likely originate here. - **key sections check:** - `gpt2arc` class initialization attributes (`conv1`, `blocks`, `ln_f`, `config`). - implementation details `attention`, `feedforward`, `transformerblock` modules. - custom methods overrides might affect model's behavior testing. 2. **`gpt2_arc/src/config.py`** - **why:** `modelconfig` class file used configure `gpt2arc` model initialization tests. errors related configuration parameters, default values, structure configuration lead issues model instantiation behavior. - **key sections check:** - definition `modelconfig` fields. - methods default values set model's configuration. - interactions `modelconfig` parts model (e.g., ensuring necessary configuration parameters correctly passed utilized). 3. **additional files consider:** - **`gpt2_arc/src/utils/experiment_tracker.py` &amp; `gpt2_arc/src/utils/results_collector.py`:** - **why:** tests involve tracking experiments collecting results, issues utility classes might indirectly affect tests. instance, incorrect logging result serialization could lead unexpected behavior errors test execution. - **`gpt2_arc/src/data/arc_dataset.py`:** - **why:** tests rely specific data preprocessing dataset structures, bugs changes data handling could impact inputs tests use. ensuring data correctly processed fed model crucial accurate testing. 4. **test-specific considerations:** - **duplicate test function:** - **issue:** `test_gpt2.py`, two functions named `test_gpt2arc_forward_pass`. python support function overloading, second definition overwrite first. could lead unexpected test behaviors skipped tests. - **solution:** rename one test functions ensure test unique name, `test_gpt2arc_forward_pass_with_mask` `test_gpt2arc_forward_pass_without_mask`. 5. **logging output:** - **why:** since test file configured `logging.debug`, reviewing log outputs provide detailed insights error might occurring. ensure logs captured review identify anomalies error messages test execution. 6. **dependencies environment:** - **why:** ensure dependencies (like `torch`, `pytest`, etc.) correctly installed compatible codebase. sometimes, version mismatches lead unexpected errors. - **solution:** verify `requirements.txt` environment configuration ensure packages up-to-date compatible. 7. **version control recent changes:** - **why:** error new, reviewing recent commits changes related files help pinpoint source issue. - **solution:** use `git` commands like `git blame` `git diff` identify recent modifications might introduced error. **next steps:** 1. **identify specific error:** - encounter error message traceback, use determine part code failing. 2. **inspect relevant files:** - focus files highlighted above, especially `gpt2_arc/src/models/gpt2.py` `gpt2_arc/src/config.py`. 3. **request additional file contents needed:** - determine specific section file might causing issue need details, feel free ask include file's content analysis. systematically reviewing areas, able identify resolve error test suite effectively.</file><file name="debug tips/test_benchmark.md">encountering error `gpt2_arc/tests/test_benchmark.py` test suite, essential identify relevant files could provide insights issue. here's prioritized list files examine, along brief explanations likely involved: 1. **`gpt2_arc/benchmark.py`** - **reason:** primary module tested. functions like `benchmark_model` `main` directly imported invoked test cases. issues functions (e.g., logic errors, incorrect handling inputs/outputs) likely manifest testing. 2. **`gpt2_arc/src/models/gpt2.py`** - **reason:** `gpt2arc` class core component mocked used tests. errors related model configuration, initialization, methods (like `forward`) affect benchmark tests. 3. **`gpt2_arc/src/config.py`** - **reason:** `modelconfig` dataclass imported potentially used within `benchmark.py` model modules. misconfigurations incorrect parameter settings lead unexpected behaviors benchmarking. 4. **`gpt2_arc/src/data/arc_dataset.py`** - **reason:** dataset (`arcdataset`) mocked tests, underlying issues data processing, loading, preprocessing actual implementation cause tests fail behave unpredictably. 5. **`gpt2_arc/src/utils/experiment_tracker.py`** - **reason:** `benchmark_model` related functions utilize `experimenttracker` logging tracking experiments, bugs exceptions within utility propagate tests. 6. **`gpt2_arc/src/utils/results_collector.py`** - **reason:** similar `experiment_tracker.py`, results collection part benchmarking process, issues `resultscollector` affect output validation tests. 7. **`gpt2_arc/src/training/train.py` &amp; `gpt2_arc/src/training/trainer.py`** - **reason:** directly invoked provided test code, modules may indirectly involved `benchmark_model` interacts training routines utilizes components scripts. 8. **external dependencies (e.g., `torch`, `pytest`, `unittest.mock`)** - **reason:** although less likely, issues external libraries mocked tests also lead errors. ensure versions compatible mocks correctly set up. ### steps diagnose error: 1. **examine error message:** - start looking exact error message traceback provided test fails. often point directly problematic file line number. 2. **check `benchmark.py`:** - since main module test, review functions `benchmark_model` `main` logical errors incorrect handling inputs outputs. 3. **validate mocks fixtures:** - ensure mocks (e.g., `mock_model`, `mock_dataset`, `mock_dataloader`) accurately represent behavior real objects. incorrect mocking lead misleading test results. 4. **review dependencies `gpt2_arc/src/models/gpt2.py`:** - look issues `gpt2arc` class, especially methods invoked benchmarking, `forward`. 5. **inspect configuration `gpt2_arc/src/config.py`:** - verify necessary configurations correctly set mismatches expected actual parameters. 6. **analyze data handling `gpt2_arc/src/data/arc_dataset.py`:** - ensure data loading preprocessing steps functioning intended. errors lead incorrect inputs fed model benchmarking. 7. **evaluate utility modules:** - check `experiment_tracker.py` `results_collector.py` bugs exceptions might interfere benchmarking process. 8. **run isolated tests:** - consider running individual tests components isolation pinpoint failure occurs. 9. **check environment issues:** - sometimes, errors arise testing environment, incompatible library versions insufficient resources (e.g., gpu availability). ensure environment matches expectations set tests. ### additional tips: - **enable verbose logging:** - add logging statements within `benchmark.py` related modules trace flow execution identify things might going wrong. - **use debugging tools:** - utilize debugging tools like `pdb` step test execution inspect state variables different points. - **review recent changes:** - tests passing previously, review recent changes codebase might introduced error. systematically examining files following diagnostic steps, able identify resolve error test suite effectively.</file><file name="debug tips/test_results_collector.md">debugging errors `test_results_collector.py` test suite, relevant files examine directly involved functionality tested. here's prioritized list files likely provide information help fix issues: 1. **`gpt2_arc/src/utils/results_collector.py`** - **reason:** primary module tested. errors initialization, metric updates, result handling likely originating here. - **key components check:** - `resultscollector` class implementation. - methods like `update_train_metrics`, `update_val_metrics`, `set_test_results`, `add_task_specific_result`, `get_summary`. - initialization logic, especially `experiment_id`, `timestamp`, `config` set up. 2. **`gpt2_arc/src/config.py`** - **reason:** test initializes `resultscollector` using configurations defined file. errors related configuration attributes (e.g., `n_embd`, `n_head`, `n_layer`, `batch_size`, etc.) may stem issues configuration classes. - **key components check:** - `config`, `modelconfig`, `trainingconfig` dataclasses. - methods default values manipulate validate configuration parameters. 3. **`gpt2_arc/src/utils/experiment_tracker.py`** - **reason:** `resultscollector` may internally utilize `experimenttracker` logging tracking experiments. issues experiment tracking could affect results collection process. - **key components check:** - `experimenttracker` class methods, especially related logging metrics handling experiment ids. - initialization interactions external services like wandb (if `use_wandb` enabled). 4. **dependencies external modules:** - **`gpt2_arc/src/utils/results_collector.py` dependencies:** - ensure utility functions classes used within `resultscollector` functioning correctly. - **environment configuration files:** - check environment-specific configurations dependencies might affect test execution. 5. **test environment setup:** - although test file primarily testing, ensure `setup` method correctly initializes necessary components. misconfigurations incorrect setups lead misleading test failures. **steps diagnose issue:** 1. **identify error message:** - start examining exact error message traceback failed test. often point directly problematic line code. 2. **trace source:** - use traceback trace back source file line number error originated. help determine whether issue within `results_collector.py`, `config.py`, another related module. 3. **review recent changes:** - tests previously passing, consider recent changes made related modules might introduced error. 4. **check dependency issues:** - ensure dependencies correctly installed compatible other, especially updates packages like `torch`, `pytorch_lightning`, others used project. 5. **isolate problem:** - temporarily simplify isolate parts `resultscollector` identify specific method component causing failure. systematically reviewing files following diagnostic steps, able identify fix error test suite.</file><file name="debug tips/test_pytest_error_fixer.md">effectively diagnose fix errors `test_pytest_error_fixer.py` test script, want focus several key files within repository. here's breakdown relevant files provide necessary information: 1. **primary module test:** - **`pytest_error_fixer.py`**: main module tested `test_pytest_error_fixer.py` script. errors tests likely related implementation details within file. since list file summaries, ensure available consider sharing contents need detailed assistance. 2. **configuration files:** - **`gpt2_arc/src/config.py`**: file contains configuration classes (`modelconfig` `config`) might used `pytesterrorfixer`. misconfigurations lead issues initializing running fixer. 3. **utility modules:** - **`gpt2_arc/src/utils/experiment_tracker.py`** **`gpt2_arc/src/utils/results_collector.py`**: utility classes (`experimenttracker` `resultscollector`) might dependencies `pytesterrorfixer`. errors utilities propagate tests. 4. **data handling:** - **`gpt2_arc/src/data/arc_dataset.py`**: `pytesterrorfixer` interacts datasets relies data preprocessing, issues module affect tests. 5. **model training modules:** - **`gpt2_arc/src/models/gpt2.py`**: file defines `gpt2arc` model related classes. `pytesterrorfixer` interacts model components, ensure issues here. - **`gpt2_arc/src/training/train.py`** **`gpt2_arc/src/training/trainer.py`**: modules handle training process. integration `pytesterrorfixer` training pipeline verified. 6. **evaluation benchmarking:** - **`gpt2_arc/src/evaluate.py`** **`gpt2_arc/benchmark.py`**: scripts essential evaluating model performance. ensure `pytesterrorfixer` correctly interacts evaluation metrics applicable. 7. **experiment tracking:** - **`gpt2_arc/src/utils/experiment_tracker.py`**: utility crucial logging tracking experiments. issues affect errors progress logged `pytesterrorfixer`. 8. **results collection:** - **`gpt2_arc/src/utils/results_collector.py`**: similar experiment tracker, module handles collection storage results, might integral `pytesterrorfixer` operates. ### next steps: - **check `pytest_error_fixer.py`**: start reviewing implementation `pytesterrorfixer` class `pytest_error_fixer.py`. look obvious issues dependencies might properly handled. - **verify dependencies**: ensure dependencies (`experiment_tracker.py`, `results_collector.py`, etc.) correctly implemented free errors. - **review configuration**: double-check configurations `config.py` ensure align requirements `pytesterrorfixer`. - **mock external interactions**: tests, youre using mocks subprocess calls `coder` object. ensure mocks accurately represent behavior actual components. - **add missing files needed**: encounter issues trace back files listed (like `pytest_error_fixer.py`), please add chat provide targeted assistance. focusing files, able identify resolve errors test script effectively. need detailed help, feel free share contents `pytest_error_fixer.py` relevant files.</file><file name="debug tips/test_model_evaluation.md">identify fix errors `test_model_evaluation.py` file, essential focus dependencies modules test script interacts with. here's breakdown relevant files repository likely provide useful information debugging: 1. **`src/models/gpt2.py`** - **why:** file defines `gpt2arc` class, core component tested. issues related model architecture, forward pass, specific layers (like `attention`, `feedforward`, `transformerblock`) originate here. - **what check:** - initialization `gpt2arc` model. - implementation `forward` method. - custom layers operations might affect model outputs. 2. **`src/config.py`** - **why:** file contains configuration classes (`config`, `modelconfig`, `trainingconfig`) used instantiate models training parameters. misconfigurations lead unexpected behaviors initialization errors tests. - **what check:** - correct definitions default values dataclasses. - dependencies validations within configuration classes. - ensure required fields correctly passed utilized. 3. **`src/training/trainer.py`** - **why:** `arctrainer` class imported used fixtures. issues related training loop, validation steps, trainer interacts model manifest tests. - **what check:** - initialization setup `arctrainer`. - implementation methods like `validation_step`, explicitly tested. - handling incorrect batch formats error raising mechanisms. 4. **`src/utils/helpers.py`** - **why:** file includes utility functions like `differential_pixel_accuracy`, directly used tests. bugs unexpected behaviors helper functions cause test failures. - **what check:** - correct implementation `differential_pixel_accuracy`. - edge case handling input validations within helper functions. 5. **`src/data/arc_dataset.py`** - **why:** although directly imported test script, `dataloader` relies `arcdataset` class defined here. issues data preprocessing, batching, dataset splitting indirectly affect tests. - **what check:** - data loading preprocessing logic. - handling different data sources formats. - transformations applied data fed model. 6. **checkpoint files (`checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt`)** - **why:** tests involve loading model checkpoints. problems checkpoint integrity, missing keys, incompatible configurations lead errors model loading evaluation. - **what check:** - ensure checkpoint file exists accessible. - verify checkpoint contains necessary keys (`config`, `state_dict`, etc.). - confirm `modelconfig` checkpoint matches expected configuration code. 7. **`src/utils/experiment_tracker.py` &amp; `src/utils/results_collector.py`** - **why:** utilities handle experiment tracking results collection, influence metrics configurations logged stored. issues affect integrity metrics tested. - **what check:** - correct logging metrics configurations. - proper serialization deserialization experiment data. - error handling edge case management tracking methods. 8. **logging configuration `test_model_evaluation.py`** - **why:** since test script sets logging, misconfigurations obscure error messages make debugging challenging. - **what check:** - ensure logging level appropriately set (`debug` case). - verify log messages correctly formatted informative. ### steps diagnose fix errors: 1. **identify error source:** - look error message traceback pinpoint error originates. guide relevant file(s). 2. **check dependencies:** - know part code failing, inspect corresponding file(s) mentioned potential issues. 3. **validate configurations:** - ensure configurations passed models trainers correct complete. 4. **verify data integrity:** - make sure data loaded processed matches expected format structure required models trainers. 5. **inspect checkpoints:** - confirm checkpoint files corrupted contain necessary components reconstruct model state. 6. **enhance logging:** - utilize debug logs set gain insights internal states data flow test execution. systematically reviewing files following diagnostic steps, able identify resolve errors `test_model_evaluation.py` script effectively.</file></source>