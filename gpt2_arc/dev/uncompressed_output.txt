<source type="local_directory" path="/workspaces/arc-neural-reasoning-model/gpt2_arc">
<file name="requirements.txt">
absl-py==2.1.0
accelerate==0.33.0
aider-chat==0.59.1
aiohappyeyeballs==2.4.0
aiohttp==3.10.5
aiosignal==1.3.1
alembic==1.13.3
altair==5.3.0
annotated-types==0.7.0
anyio==4.6.0
arckit==0.1.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==2.4.1
async-lru==2.0.4
attrs==24.2.0
babel==2.16.0
backoff==2.2.1
beartype==0.18.5
beautifulsoup4==4.12.3
bitnet==0.2.5
bitsandbytes==0.44.1
black==24.8.0
bleach==6.1.0
blinker==1.8.2
bottle==0.13.1
build==1.2.2
CacheControl==0.14.0
cachetools==5.3.3
certifi==2024.8.30
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.3.2
cleo==2.1.0
click==8.1.7
colorama==0.4.6
colorlog==6.8.2
CoLT5-attention==0.10.19
comm==0.2.2
commonmark==0.9.1
ConfigArgParse==1.7
contourpy==1.3.0
coverage==7.6.1
crashtest==0.4.1
cryptography==43.0.1
cycler==0.12.1
datasets==2.14.4
debugpy==1.8.5
decorator==5.1.1
defusedxml==0.7.1
diff-match-patch==20230430
dill==0.3.7
diskcache==5.6.3
distlib==0.3.8
distro==1.9.0
docker-pycreds==0.4.0
drawsvg==2.4.0
dulwich==0.21.7
e==1.4.5
einops==0.8.0
einops-exts==0.0.4
einx==0.3.0
entrypoints==0.4
executing==2.1.0
fairscale==0.4.13
fastjsonschema==2.20.0
filelock==3.16.1
flake8==7.1.1
fonttools==4.54.0
fqdn==1.5.1
frozendict==2.4.4
frozenlist==1.4.1
fsspec==2024.9.0
gitdb==4.0.11
GitPython==3.1.43
google-ai-generativelanguage==0.6.2
google-api-core==2.19.0
google-api-python-client==2.128.0
google-auth==2.29.0
google-auth-httplib2==0.2.0
google-generativeai==0.5.2
googleapis-common-protos==1.63.0
greenlet==3.0.3
grep-ast==0.3.3
grpcio==1.63.0
grpcio-status==1.62.2
h11==0.14.0
httpcore==1.0.5
httplib2==0.22.0
httpx==0.27.2
huggingface-hub==0.25.0
hypothesis==6.115.0
idna==3.10
importlib_metadata==7.2.1
importlib_resources==6.4.5
iniconfig==2.0.0
installer==0.7.0
ipykernel==6.29.5
ipython==8.27.0
isoduration==20.11.0
isort==5.13.2
jaraco.classes==3.4.0
jedi==0.19.1
jeepney==0.8.0
Jinja2==3.1.4
jiter==0.5.0
joblib==1.4.2
json5==0.9.25
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2023.12.1
jupyter-events==0.10.0
jupyter-lsp==2.2.5
jupyter-server-mathjax==0.2.6
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.14.2
jupyter_server_terminals==0.5.3
jupyterlab==4.2.5
jupyterlab_git==0.50.1
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
keyring==24.3.1
kiwisolver==1.4.7
libcst==1.1.0
lightning==2.4.0
lightning-utilities==0.11.7
lion-pytorch==0.2.2
litellm==1.47.0
local-attention==1.9.15
loguru==0.7.2
Mako==1.3.5
Markdown==3.7
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.9.2
matplotlib-inline==0.1.7
mccabe==0.7.0
mdurl==0.1.2
memory-profiler==0.61.0
mistune==0.8.4
more-itertools==10.5.0
mpmath==1.3.0
msgpack==1.1.0
multidict==6.1.0
multiprocess==0.70.15
mypy==1.11.2
mypy-extensions==1.0.0
nbclient==0.10.0
nbconvert==6.5.0
nbdime==4.0.2
nbformat==5.4.0
nest-asyncio==1.6.0
networkx==3.2.1
nltk==3.7
notebook_shim==0.2.4
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.6.77
nvidia-nvtx-cu12==12.1.105
openai==1.47.0
optuna==4.0.0
optuna-dashboard==0.16.2
optuna-integration==4.0.0
overrides==7.7.0
packaging==24.1
pandas==2.2.2
pandocfilters==1.5.1
parso==0.8.4
pathspec==0.12.1
pexpect==4.9.0
pillow==10.4.0
pkginfo==1.11.1
platformdirs==4.3.6
playwright==1.43.0
plotly==5.24.1
pluggy==1.5.0
poetry==1.8.3
poetry-core==1.9.0
poetry-plugin-export==1.8.0
prometheus_client==0.21.0
prompt_toolkit==3.0.47
proto-plus==1.23.0
protobuf==4.25.3
psutil==6.0.0
ptyprocess==0.7.0
pure_eval==0.2.3
pyarrow==16.0.0
pyasn1==0.6.0
pyasn1_modules==0.4.0
pycodestyle==2.12.1
pycparser==2.22
pydantic==2.9.2
pydantic_core==2.23.4
pydeck==0.9.0
pydub==0.25.1
pyee==11.1.0
pyflakes==3.2.0
Pygments==2.18.0
pyngrok==7.2.0
pynvml==11.5.3
pypandoc==1.13
pyparsing==3.1.2
PyPDF2==2.10.0
pyperclip==1.9.0
pyproject-api==1.6.1
pyproject_hooks==1.2.0
pytest==8.3.2
pytest-cov==5.0.0
pytest-mock==3.14.0
python-dateutil==2.9.0.post0
python-dotenv==1.0.1
python-json-logger==2.0.7
pytorch-lightning==2.4.0
pytz==2024.1
PyYAML==6.0.2
pyzmq==26.2.0
RapidFuzz==3.10.0
referencing==0.35.1
regex==2024.9.11
requests==2.32.3
requests-toolbelt==1.0.0
rfc3339-validator==0.1.4
rfc3986-validator==0.1.1
rich==13.8.1
rpds-py==0.20.0
rsa==4.9
ruff==0.6.9
safetensors==0.4.5
scikit-learn==1.5.2
scipy==1.13.1
seaborn==0.13.2
SecretStorage==3.3.3
Send2Trash==1.8.3
sentencepiece==0.2.0
sentry-sdk==2.15.0
setproctitle==1.3.3
setuptools==75.1.0
shellingham==1.5.4
six==1.16.0
smmap==5.0.1
sniffio==1.3.1
sortedcontainers==2.4.0
sounddevice==0.5.0
soundfile==0.12.1
soupsieve==2.6
SQLAlchemy==2.0.35
stack-data==0.6.3
streamlit==1.34.0
sympy==1.12
tenacity==8.3.0
tensorboard==2.18.0
tensorboard-data-server==0.7.2
terminado==0.18.1
threadpoolctl==3.5.0
tiktoken==0.7.0
timm==1.0.9
tinycss2==1.3.0
tokenizers==0.19.1
tokenmonster==1.1.12
toml==0.10.2
tomlkit==0.13.2
toolz==0.12.1
torch==2.4.1
torchdiffeq==0.2.4
TorchFix==0.6.0
torchmetrics==1.4.2
torchsummary==1.5.1
torchvision==0.19.1
tornado==6.4
tox==4.15.1
tqdm==4.66.5
traitlets==5.14.3
transformers==4.44.2
tree-sitter==0.21.3
tree-sitter-languages==1.10.2
triton==3.0.0
trove-classifiers==2024.9.12
types-python-dateutil==2.9.0.20240906
typing==3.7.4.3
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.1
ultralytics-thop==2.0.8
uri-template==1.3.0
uritemplate==4.1.1
urllib3==2.2.3
vector-quantize-pytorch==1.12.0
virtualenv==20.26.6
wandb==0.18.3
watchdog==5.0.3
wcwidth==0.2.13
webcolors==24.8.0
webencodings==0.5.1
websocket-client==1.8.0
Werkzeug==3.0.4
wget==3.2
xxhash==3.5.0
yarl==1.11.1
youtube-transcript-api==0.4.1
zetascale==0.9.1
zipp==3.20.2
</file>
<file name="benchmark.py">
# gp2_arc/benchmark.py
import sys
import os

# Add the project root directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch._dynamo
import csv
import uuid
from datetime import datetime
import os
import torch
from torch.utils.data import DataLoader
import arckit
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import ModelConfig
import time
from torch.amp import autocast
import psutil
import logging
import argparse
import statistics
import numpy as np
from scipy import stats

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Dynamically adjustable baseline values for CPU, GPU, and MPS
BASELINES = {
    'cpu': {'total_time': 1.6391, 'grids_per_second': 199.27},
    'cuda': {'total_time': 0.0481, 'grids_per_second': 13774.98},
    'mps': {'total_time': 0.0481, 'grids_per_second': 13774.98}  # Updated baselines for MPS
}

def benchmark_model(model, dataset, batch_size=1, num_batches=1, num_runs=1, device_type='cpu', precision='medium', model_cheText preprocessing completed with XML structure preserved.

Compressed Token Count: 76007
Uncompressed Token Count: 90496

compressed_output.txt and uncompressed_output.txt have been created in the working directory.

An error occurred: Pyperclip could not find a copy/paste mechanism for your system. For more information, 
please visit https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error
On Linux, you can run `sudo apt-get install xclip` or `sudo apt-get install xselect` to install a copy/paste 
mechanism.

Please check your input and try again.
Processing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00
Traceback (most recent call last):
  File "/workspaces/arc-neural-reasoning-model/tmp/1filellm/onefilellm.py", line 678, in <module>
    main()
  File "/workspaces/arc-neural-reasoning-model/tmp/1filellm/onefilellm.py", line 669, in main
    pyperclip.copy(uncompressed_text)
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyperclip/__init__.py", line 622, in lazy_load_stub_copy
    return copy(text)
           ^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyperclip/__init__.py", line 295, in __call__
    raise PyperclipException('Pyperclip could not find a copy/paste mechanism for your system. For more information, please visit https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error' + additionalInfo)
pyperclip.PyperclipException: Pyperclip could not find a copy/paste mechanism for your system. For more information, please visit https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error
On Linux, you can run `sudo apt-get install xclip` or `sudo apt-get install xselect` to install a copy/paste mechanism.
overhead", fullgraph=True)
            else:
                compiled_model = model  # Use the model directly for MPS
        except ImportError as e:
            logger.warning(f"Compilation failed with error: {e}. Falling back to eager execution.")
            compiled_model = model

    try:
        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=ARCDataset.collate_fn)
        total_time = 0.0
        total_grids = 0

        for i, batch in enumerate(dataloader):
            if i &gt;= num_batches:
                break
            print(f"Processing batch {i+1}/{num_batches}")

            logger.debug(f"Batch content before unpacking: {batch}")
            if len(batch) != 3:
                raise ValueError(f"Unexpected batch format. Expected 3 items, got {len(batch)}")
            inputs, outputs, task_ids = batch

            print(f"Inputs type: {type(inputs)}")
            if hasattr(inputs, 'shape'):
                print(f"Inputs shape: {inputs.shape}")
            else:
                print("Inputs shape: N/A")
            print(f"Outputs type: {type(outputs)}, shape: {outputs.shape if torch.is_tensor(outputs) else 'N/A'}")
            print(f"Task IDs: {task_ids}")

            if inputs is None or not isinstance(inputs, torch.Tensor):
                raise ValueError(f"Expected inputs to be a torch.Tensor, got {type(inputs)}")

            if inputs.numel() == 0:
                raise ValueError("Inputs tensor is empty")
            print(f"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}, Task IDs: {task_ids}")
            
            if inputs.dim() == 2:
                # If inputs is 2D (batch_size, sequence_length), reshape it to 4D
                height = width = int(inputs.size(1)**0.5)
                inputs = inputs.view(inputs.size(0), 1, height, width)
            elif inputs.dim() == 3:
                # If inputs is 3D (batch_size, height, width), add a channel dimension
                inputs = inputs.unsqueeze(1)
            elif inputs.dim() != 4:
                raise ValueError(f"Unexpected input dimensions: {inputs.dim()}. Expected 2, 3, or 4 dimensions.")
            
            attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32)
            inputs, attention_mask = inputs.to(device), attention_mask.to(device)

            # Log system load and system state before processing the batch
            cpu_percent = psutil.cpu_percent(interval=None)
            memory_info = psutil.virtual_memory()
            cpu_usages.append(cpu_percent)
            memory_usages.append(memory_info.percent)
            if device.type == 'cuda':
                gpu_utilization = torch.cuda.utilization(device.index)
                gpu_usages.append(gpu_utilization)
                logger.info(f"Batch {i+1}: CPU Usage: {cpu_percent}%, Memory Usage: {memory_info.percent}%, GPU Utilization: {gpu_utilization}%")
            else:
                logger.info(f"Batch {i+1}: CPU Usage: {cpu_percent}%, Memory Usage: {memory_info.percent}%")

                # Measure the time taken to process the batch
                start_time = time.time()

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                logger.debug("Invoking the model with inputs and attention_mask")
                with torch.no_grad():
                    if device.type == 'cuda':
                        with autocast(device_type=device.type, dtype=torch.float16):
                            compiled_model(inputs, attention_mask)
                    else:
                        compiled_model(inputs, attention_mask)

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                end_time = time.time()

                batch_time = end_time - start_time
                print(f"Batch time: {batch_time}")
                if batch_time &lt;= 0:
                    print(f"WARNING: Invalid batch time: {batch_time}. Skipping this batch.")
                    continue
                total_time += batch_time
                total_grids += len(inputs)
    except Exception as e:
        logger.error(f"An error occurred during benchmarking: {e}")
        raise

    print(f"Benchmark completed. Total time: {total_time}, Total grids: {total_grids}")
    # Calculate average and standard deviation for the runs
    num_runs = len(total_time_runs)
    avg_total_time = np.mean(total_time_runs)
    std_total_time = np.std(total_time_runs)
    avg_grids_per_second = np.mean(grids_per_second_runs)
    std_grids_per_second = np.std(grids_per_second_runs)
    if total_time &gt; 0:
        grids_per_second = total_grids / total_time
    else:
        grids_per_second = 0.0  # Avoid division by zero
        logger.warning("Total time is zero. Setting grids_per_second to 0.0 to avoid division by zero.")

    logger.info(f"Total Time: {total_time:.4f} seconds, Grids per Second: {grids_per_second:.2f}")
    
    # Store the results of the run
    run_results.append({
        'run_id': run_id,
        'datetime': current_time,
        'total_time': total_time,
        'grids_per_second': grids_per_second,
        'cpu_usage': np.mean(cpu_usages),
        'memory_usage': np.mean(memory_usages),
        'gpu_usage': np.mean(gpu_usages) if gpu_usages else None,
        'batch_size': batch_size,
        'num_batches': num_batches,
        'device': device.type,
        'n_embd': model.config.n_embd,
        'n_head': model.config.n_head,
        'n_layer': model.config.n_layer,
        'precision': precision,  # Add precision here
        'checkpoint_used': checkpoint_used,
        'checkpoint_info': checkpoint_info
    })

    total_time_runs.append(total_time)
    grids_per_second_runs.append(grids_per_second)

    if total_time &lt;= 0 or total_grids &lt;= 0:
        logger.warning(f"ERROR: Invalid total time ({total_time}) or total grids ({total_grids}). Check the benchmark implementation.")
        return 0.0, 0.0  # Return sensible defaults instead of infinity

    avg_total_time = total_time
    avg_grids_per_second = total_grids / total_time if total_time &gt; 0 else 0.0

    logger.info(f"Total Time: {avg_total_time:.4f} seconds, Grids per Second: {avg_grids_per_second:.2f}")


    # Perform statistical analysis (confidence intervals, effect size, etc.)
    confidence_level = 0.95
    z_score = stats.norm.ppf((1 + confidence_level) / 2)

    ci_total_time = z_score * (std_total_time / np.sqrt(num_runs))
    ci_grids_per_second = z_score * (std_grids_per_second / np.sqrt(num_runs))

    effect_size_time = (avg_total_time - BASELINES[device.type]['total_time']) / std_total_time
    effect_size_grids = (avg_grids_per_second - BASELINES[device.type]['grids_per_second']) / std_grids_per_second

    # Calculate improvements and regressions based on averages
    time_improvement = BASELINES[device.type]['total_time'] - avg_total_time
    time_improvement_percent = (time_improvement / BASELINES[device.type]['total_time']) * 100
    time_regression = avg_total_time - BASELINES[device.type]['total_time']
    time_regression_percent = (time_regression / BASELINES[device.type]['total_time']) * 100

    grids_per_second_improvement = avg_grids_per_second - BASELINES[device.type]['grids_per_second']
    grids_per_second_improvement_percent = (grids_per_second_improvement / BASELINES[device.type]['grids_per_second']) * 100
    grids_per_second_regression = BASELINES[device.type]['grids_per_second'] - avg_grids_per_second
    grids_per_second_regression_percent = (grids_per_second_regression / BASELINES[device.type]['grids_per_second']) * 100

    # Determine if there was an improvement
    improvement_time = avg_total_time &lt; BASELINES[device.type]['total_time']
    improvement_grids = avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']

    # Log improvements or regressions based on averages
    if avg_total_time &lt; BASELINES[device.type]['total_time']:
        logger.info(f"Improvement in average total time: -{time_improvement:.4f} seconds ({time_improvement_percent:.2f}%)")
    else:
        logger.info(f"Regression in average total time: +{time_regression:.4f} seconds ({time_regression_percent:.2f}%)")

    if avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']:
        logger.info(f"Improvement in average grids per second: +{grids_per_second_improvement:.2f} ({grids_per_second_improvement_percent:.2f}%)")
    else:
        logger.info(f"Regression in average grids per second: -{grids_per_second_regression:.2f} ({grids_per_second_regression_percent:.2f}%)")

    # Update practical significance checks
    practical_significance_time = time_improvement_percent &gt;= practical_threshold
    practical_significance_grids = grids_per_second_improvement_percent &gt;= practical_threshold

    # Log practical significance
    if improvement_time:
        if practical_significance_time:
            logger.info("The improvement in average total time is practically significant.")
        else:
            logger.info("The improvement in average total time is not practically significant.")
    else:
        if practical_significance_time:
            logger.info("The regression in average total time is practically significant.")
        else:
            logger.info("The regression in average total time is not practically significant.")

    if improvement_grids:
        if practical_significance_grids:
            logger.info("The improvement in average grids per second is practically significant.")
        else:
            logger.info("The improvement in average grids per second is not practically significant.")
    else:
        if practical_significance_grids:
            logger.info("The regression in average grids per second is practically significant.")
        else:
            logger.info("The regression in average grids per second is not practically significant.")

    # Perform a one-sample t-test
    t_stat_time, p_value_time = stats.ttest_1samp(total_time_runs, BASELINES[device.type]['total_time'])
    t_stat_grids, p_value_grids = stats.ttest_1samp(grids_per_second_runs, BASELINES[device.type]['grids_per_second'])

    logger.info(f"T-Test for total time: t-statistic = {t_stat_time:.4f}, p-value = {p_value_time:.4f}")
    logger.info(f"T-Test for grids per second: t-statistic = {t_stat_grids:.4f}, p-value = {p_value_grids:.4f}")

    # Log the results including confidence intervals
    logger.info(f"Run Summary:")
    logger.info(f" • Avg Total Time: {avg_total_time:.4f}s (CI 95%: ±{ci_total_time:.4f}s)")
    logger.info(f" • Avg Grids per Second: {avg_grids_per_second:.2f} (CI 95%: ±{ci_grids_per_second:.2f})")
    logger.info(f" • Effect Size (Total Time): {effect_size_time:.4f}, Effect Size (Grids per Second): {effect_size_grids:.4f}")

    # Determine if there was an improvement
    improvement_time = avg_total_time &lt; BASELINES[device.type]['total_time']
    improvement_grids = avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']
    csv_file_path = 'benchmark_results.csv'
    file_exists = os.path.isfile(csv_file_path)
    with open(csv_file_path, 'a', newline='') as csvfile:
        fieldnames = [
            'run_id', 'datetime', 'run', 'total_time', 'grids_per_second', 'cpu_usage', 'memory_usage',
            'batch_size', 'num_batches', 'device', 'n_embd', 'n_head', 'n_layer', 'gpu_usage', 'precision',
            'checkpoint_used', 'checkpoint_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        for result in run_results:
            writer.writerow(result)

    # Write statistical summary to CSV
    stats_csv_file_path = 'benchmark_statistics.csv'
    stats_file_exists = os.path.isfile(stats_csv_file_path)
    with open(stats_csv_file_path, 'a', newline='') as csvfile:
        fieldnames = [
            'run_id', 'datetime', 'avg_total_time', 'std_total_time', 'ci_total_time',
            'avg_grids_per_second', 'std_grids_per_second', 'ci_grids_per_second',
            'effect_size_time', 'effect_size_grids', 'percent_change_time', 'percent_change_grids',
            't_stat_time', 'p_value_time', 't_stat_grids', 'p_value_grids',
            'improvement_time', 'improvement_grids',
            'practical_significance_time', 'practical_significance_grids', 'precision',
            'checkpoint_used', 'checkpoint_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not stats_file_exists:
            writer.writeheader()
        writer.writerow({
            'run_id': run_id,
            'datetime': current_time,
            'avg_total_time': avg_total_time,
            'std_total_time': std_total_time,
            'ci_total_time': ci_total_time,
            'avg_grids_per_second': avg_grids_per_second,
            'std_grids_per_second': std_grids_per_second,
            'ci_grids_per_second': ci_grids_per_second,
            'effect_size_time': effect_size_time,
            'effect_size_grids': effect_size_grids,
            'percent_change_time': time_improvement_percent if improvement_time else time_regression_percent,
            'percent_change_grids': grids_per_second_improvement_percent if improvement_grids else grids_per_second_regression_percent,
            't_stat_time': t_stat_time,
            'p_value_time': p_value_time,
            't_stat_grids': t_stat_grids,
            'p_value_grids': p_value_grids,
            'improvement_time': improvement_time,
            'improvement_grids': improvement_grids,
            'practical_significance_time': practical_significance_time,
            'practical_significance_grids': practical_significance_grids,
            'precision': precision,  # Add precision here
            'checkpoint_used': checkpoint_used,
            'checkpoint_info': checkpoint_info
        })

    print(f"Benchmark completed. Final results - avg_time: {avg_total_time}, avg_grids: {avg_grids_per_second}")
    return avg_total_time, avg_grids_per_second


def main(args):
    print(f"Starting main function with args: {args}")
    # Set the float32 matmul precision
    torch.set_float32_matmul_precision(args.precision)
    train_set, _ = arckit.load_data()
    full_dataset = ARCDataset(train_set, is_test=False)

    # Create the model configuration
    model_config = ModelConfig(
        n_embd=args.n_embd,
        n_head=args.n_head,
        n_layer=args.n_layer,
        mamba_ratio=args.mamba_ratio,
        d_state=args.d_state,
        d_conv=args.d_conv
    )
    model = GPT2ARC(model_config, num_classes=args.num_classes)

    # Run the benchmark for different configurations
    for run_num in range(args.num_full_runs):
        logger.info(f"Starting full benchmark run {run_num + 1}/{args.num_full_runs}")
        avg_time, avg_grids = benchmark_model(
            model, full_dataset, batch_size=args.batch_size, num_batches=args.num_batches, num_runs=args.num_runs, device_type=args.device, precision=args.precision, model_checkpoint=args.model_checkpoint
        )
        logger.info(f"Full run {run_num + 1} - Avg Time: {avg_time:.4f}s, Avg Grids per Second: {avg_grids:.2f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Benchmark the GPT2ARC model.")
    parser.add_argument('--model_checkpoint', type=str, help='Path to the model checkpoint')
    parser.add_argument('--num-runs', type=int, default=20, help='Number of runs for each configuration')
    parser.add_argument('--num-full-runs', type=int, default=1, help='Number of full configurations to run')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for each run')
    parser.add_argument('--num-batches', type=int, default=10, help='Number of batches per run')
    parser.add_argument('--n-embd', type=int, default=64, help='Number of embeddings for the model')
    parser.add_argument('--n-head', type=int, default=2, help='Number of attention heads')
    parser.add_argument('--n-layer', type=int, default=1, help='Number of layers')
    parser.add_argument('--mamba-ratio', type=int, default=7, help='Number of Mamba layers per Transformer layer')
    parser.add_argument('--d-state', type=int, default=16, help='Mamba state dimension')
    parser.add_argument('--d-conv', type=int, default=4, help='Mamba convolution dimension')
    parser.add_argument('--device', choices=['cpu', 'cuda', 'mps'], default='cpu', help='Device to run the benchmark on (cpu, cuda, or mps)')
    parser.add_argument('--precision', choices=['highest', 'high', 'medium'], default='highest', help='Precision level for float32 matrix multiplications')
    parser.add_argument('--num-classes', type=int, default=10, help='Number of classes for the model')
    
    args = parser.parse_args()
    main(args)

</file>
<file name="README.md">
# GPT-2 ARC Neural Reasoning Model

This project implements a neural reasoning model based on the GPT-2 architecture to solve tasks from the Abstraction and Reasoning Corpus (ARC) challenge.

## Features

- **Data Handling**: Utilizes a custom `ArcDataset` class for handling and preprocessing ARC data.
- **Model Architecture**: Implements a `GPT2ARC` model leveraging the pre-trained GPT-2 architecture.
- **Training**: Includes a `train.py` script for training the model using PyTorch Lightning, with support for logging and checkpointing.
- **Testing**: Comprehensive test suite using `pytest` to ensure model and data integrity.

## Installation

Clone the repository and install the required packages:

```bash
git clone https://github.com/yourusername/arc-neural-reasoning-model.git
cd arc-neural-reasoning-model
pip install -e .
```

For development, install the extra dependencies:

```bash
pip install -e ".[dev]"
```

## Usage

### Training the Model

To train the model, use the following command:

```
python src/train.py --train_data path/to/train_data --val_data path/to/val_data --batch_size 32 --learning_rate 1e-4 --max_epochs 10 --use_gpu
```

Adjust the parameters as needed. The trained model checkpoints will be saved in the `checkpoints` directory.

### Evaluating the Model

To evaluate a trained model on a test set, use the following command:

```
python src/evaluate.py --test_data path/to/test_data --model_checkpoint path/to/model_checkpoint.ckpt --batch_size 32
```

This will output the evaluation metrics for the model on the test dataset.

## Running Tests

To run the tests, use the following command:

```
pytest -v
```

This will run all tests and display the results, including test coverage.

## Contributing

[Add contribution guidelines here]

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

</file>
<file name="setup.py">
from setuptools import setup

setup()

</file>
<file name="scripts/memory_test.py">
import sys
import os

# Determine the absolute path to the project root
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))

# Add the project root to the Python path
sys.path.insert(0, project_root)

import torch
from torch.utils.data import DataLoader
from gpt2_arc.src.data.arc_dataset import ARCDataset
import psutil
from arckit import load_data
import gc
from memory_profiler import profile
import json

def get_system_memory_usage():
    process = psutil.Process(os.getpid())
    mem_bytes = process.memory_info().rss  # Resident Set Size
    mem_mb = mem_bytes / (1024 ** 2)  # Convert to MB
    return mem_mb

@profile
def initialize_dataset():
    # Load data using arckit.load_data() as in training.py
    train_set, eval_set = load_data()
    
    # Initialize ARCDataset with the training set
    dataset = ARCDataset(
        data_source=train_set,
        is_test=False,
        num_symbols=10,
        test_split=0.2,
        debug=True  # Enable debug mode if needed
    )
    return dataset

@profile
def main():
    # Perform garbage collection to ensure accurate measurements
    gc.collect()

    # Measure memory before loading the dataset
    mem_before = get_system_memory_usage()
    print(f"Memory before loading dataset: {mem_before:.2f} MB")

    # Initialize the dataset
    dataset = initialize_dataset()

    # Measure memory after loading the dataset
    mem_after = get_system_memory_usage()
    print(f"Memory after loading dataset: {mem_after:.2f} MB")

    # Calculate memory used by the dataset
    mem_used = mem_after - mem_before
    print(f"Total Memory Used by Dataset: {mem_used:.2f} MB")

    # Calculate per-example memory usage
    total_samples = len(dataset)
    if total_samples == 0:
        print("Dataset is empty.")
        return

    per_example_mem = mem_used / total_samples
    print(f"Estimated Memory Usage per Example: {per_example_mem:.2f} KB")

    # Define the range of batch sizes to test
    batch_sizes = [1, 2, 4, 8, 16, 32]  # Extend as needed

    all_memory_data = []

    for batch_size in batch_sizes:
        print(f"\nTesting with Batch Size: {batch_size}")
        # Perform garbage collection before each batch test
        gc.collect()

        # Measure memory before processing the batch
        mem_before_batch = get_system_memory_usage()

        # Initialize DataLoader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=0,  # Ensure main process data loading
            pin_memory=False  # Disable pin_memory for CPU
        )

        # Iterate through a subset of batches to measure memory
        num_batches = 10
        for i, batch in enumerate(dataloader):
            if i &gt;= num_batches:
                break

            inputs, outputs, task_ids = batch  # Unpack the batch

            # Optionally, validate tensor shapes
            print(f"Batch {i+1}: Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}")

        # Measure memory after processing the batch
        mem_after_batch = get_system_memory_usage()
        mem_used_batch = mem_after_batch - mem_before_batch

        memory_records = {
            "batch_size": batch_size,
            "memory_used_mb": mem_used_batch
        }

        all_memory_data.append(memory_records)

        print(f"Batch Size: {batch_size} | Memory Used: {mem_used_batch:.2f} MB")

    # Save the memory usage data to a JSON file
    output_path = os.path.join("gpt2_arc", "memory_usage_results.json")
    with open(output_path, 'w') as f:
        json.dump(all_memory_data, f, indent=4)
    
    print(f"\nMemory usage data saved to {output_path}")

if __name__ == "__main__":
    main()

</file>
<file name="tests/test_gpt2.py">
# gpt2_arc/tests/test_gpt2.py
import logging

import pytest
import torch
from src.config import ModelConfig
from src.models.gpt2 import GPT2ARC, Attention, FeedForward, TransformerBlock

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


@pytest.fixture
def model():
    config = ModelConfig()
    return GPT2ARC(config)


def test_gpt2arc_initialization(model):
    assert isinstance(model, GPT2ARC)
    assert hasattr(model, "conv1")  # Check for conv1 instead of token_embedding
    assert hasattr(model, "blocks")
    assert hasattr(model, "ln_f")
    assert hasattr(model, "config")


def test_gpt2arc_forward_pass(model):
    batch_size = 2
    height = 30
    width = 30
    input_ids = torch.randn(batch_size, 1, height, width)  # Simulate image-like input
    attention_mask = torch.ones((batch_size, height * width))

    output = model(input_ids, attention_mask)

    assert isinstance(output, torch.Tensor)
    assert output.shape == (batch_size, height * width, model.config.n_embd)

    logger.debug(f"Output shape: {output.shape}")


def test_gpt2arc_output_values(model):
    logger.debug("Testing GPT2ARC output values")
    batch_size = 1
    channels = 1
    height = 30
    width = 30
    input_ids = torch.randn(batch_size, channels, height, width)  # Simulate image-like input
    attention_mask = torch.ones((batch_size, height * width))

    output = model(input_ids, attention_mask)

    assert not torch.isnan(output).any(), "Output contains NaN values"


def test_gpt2arc_forward_pass(model):
    batch_size = 2
    channels = 1
    height = 30
    width = 30
    input_ids = torch.randn(batch_size, channels, height, width)  # Simulate image-like input
    attention_mask = torch.ones((batch_size, height * width))

    output_with_mask = model(input_ids, attention_mask)
    output_without_mask = model(input_ids)

    logger.debug(
        f"Difference between outputs: {(output_with_mask - output_without_mask).abs().mean()}"
    )


def test_attention_module():
    logger.debug("Testing Attention module")
    attention = Attention(n_embd=768, n_head=12)
    x = torch.randn(2, 10, 768)
    output = attention(x)
    assert output.shape == x.shape
    logger.debug(f"Attention input shape: {x.shape}, output shape: {output.shape}")


def test_feedforward_module():
    logger.debug("Testing FeedForward module")
    ff = FeedForward(n_embd=768)
    x = torch.randn(2, 10, 768)
    output = ff(x)
    assert output.shape == x.shape
    logger.debug(f"FeedForward input shape: {x.shape}, output shape: {output.shape}")


def test_transformer_block():
    logger.debug("Testing TransformerBlock")
    block = TransformerBlock(n_embd=768, n_head=12)
    x = torch.randn(2, 10, 768)
    output = block(x)
    assert output.shape == x.shape
    logger.debug(
        f"TransformerBlock input shape: {x.shape}, output shape: {output.shape}"
    )

</file>
<file name="tests/test_dataset.py">
import unittest
import torch
from gpt2_arc.src.data.arc_dataset import ARCDataset

class TestARCDataset(unittest.TestCase):
    def setUp(self):
        # Initialize dataset with a mock data source
        self.dataset = ARCDataset(data_source="path/to/mock_data")

    def test_task_ids_loaded_from_filenames(self):
        # Mock the os.listdir to return predefined filenames
        synthetic_filenames = ['task_alpha.json', 'task_beta.json']
        with patch('os.listdir', return_value=synthetic_filenames):
            # Mock the open function to return empty JSON data
            mock_data = json.dumps({'train': [], 'test': []})
            with patch('builtins.open', mock_open(read_data=mock_data)):
                dataset = ARCDataset(data_source='path/to/synthetic_data', debug=True)
                expected_task_ids = ['task_alpha', 'task_beta']
                actual_task_ids = [sample['task_id'] for sample in dataset.data]
                self.assertEqual(actual_task_ids, expected_task_ids, "Task IDs do not match filenames.")
        # Initialize dataset with a mock directory
        dataset = ARCDataset(data_source="path/to/mock_directory")
        self.assertGreater(len(dataset), 0, "Dataset should contain samples loaded from the directory.")

    def test_dataset_preprocessing(self):
        # Initialize dataset with a mock data source
        dataset = ARCDataset(data_source="path/to/mock_data")
        input_tensor, output_tensor, task_id = dataset[0]
        self.assertEqual(input_tensor.shape, (1, 30, 30), "Input tensor should be padded to (1, 30, 30).")
        self.assertEqual(output_tensor.shape, (1, 30, 30), "Output tensor should be padded to (1, 30, 30).")

    def test_symbol_frequencies(self):
        # Test symbol frequency calculation
        frequencies = self.dataset.get_symbol_frequencies()
        self.assertIsInstance(frequencies, dict, "Symbol frequencies should be a dictionary.")

    def test_dataset_length_and_item_retrieval(self):
        # Test dataset length and item retrieval
        self.assertEqual(len(self.dataset), self.dataset.get_num_samples(), "Dataset length should match number of samples.")
        input_tensor, output_tensor, task_id = self.dataset[0]
        self.assertIsInstance(input_tensor, torch.Tensor, "Input should be a torch.Tensor.")
        self.assertIsInstance(output_tensor, torch.Tensor, "Output should be a torch.Tensor.")
        self.assertIsInstance(task_id, str, "Task ID should be a string.")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_train.py">
# gpt2_arc/tests/test_train.py
import os
import sys

sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))
import os
import sys

import pytest
import logging

logger = logging.getLogger(__name__)

def set_logging_level(level=logging.ERROR):
    logger = logging.getLogger()
    logger.setLevel(level)

# Add the project root to the PYTHONPATH
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "../../")))
import argparse
from unittest.mock import ANY, MagicMock, patch

import pytorch_lightning as pl
import torch

from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.train import main
from gpt2_arc.src.training.trainer import ARCTrainer


@pytest.fixture
def mock_args():
    args = argparse.Namespace()
    args.train_data = "mock_train_data.json"
    args.val_data = "mock_val_data.json"
    args.batch_size = 32
    args.learning_rate = 1e-4
    args.max_epochs = 10
    args.use_gpu = False
    args.no_logging = False
    args.no_checkpointing = False
    args.no_progress_bar = False
    args.log_level = "INFO"  # Add log_level attribute
    args.fast_dev_run = False  # Add fast_dev_run attribute
    args.project = "test_project"  # Add a project attribute to mock_args
    return args


@pytest.fixture
def mock_dataset():
    dataset = MagicMock(spec=ARCDataset)
    dataset.data = [{"input": "mock input", "output": "mock output"}]
    dataset.__len__.return_value = 100
    return dataset


from src.config import Config, ModelConfig, TrainingConfig


@pytest.fixture
def model():
    config = Config(model=ModelConfig(), training=TrainingConfig())
    return GPT2ARC(config.model)


@pytest.fixture
def trainer():
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    return ARCTrainer(model, None, None, config)


@pytest.fixture
def mock_pl_trainer():
    return MagicMock(spec=pl.Trainer)


# Existing GPT2ARC model tests


def test_gpt2arc_initialization(model):
    assert isinstance(model, GPT2ARC)
    assert hasattr(model, "conv1")  # Check for conv1 instead of token_embedding
    assert hasattr(model, "blocks")
    assert hasattr(model, "ln_f")
    assert hasattr(model, "config")


def test_gpt2arc_forward_pass(model):
    batch_size = 2
    height = width = 30
    seq_length = height * width
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output_with_mask = model(input_ids, attention_mask)
    output_without_mask = model(input_ids)

    assert isinstance(output_with_mask, torch.Tensor)
    assert output_with_mask.shape == (batch_size, seq_length, model.config.n_embd)
    assert isinstance(output_without_mask, torch.Tensor)
    assert output_without_mask.shape == (batch_size, seq_length, model.config.n_embd)

    logger.debug(f"Difference between outputs: {(output_with_mask - output_without_mask).abs().mean()}")


def test_gpt2arc_output_values(model):
    logger.debug("Testing GPT2ARC output values")
    batch_size = 1
    height = width = 30
    seq_length = height * width
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output = model(input_ids, attention_mask)

    assert not torch.isnan(output).any(), "Output contains NaN values"
    assert not torch.isinf(output).any(), "Output contains infinity values"


def test_gpt2arc_attention_mask(model):
    batch_size = 2
    channels = 1
    height = 30
    width = 30
    input_ids = torch.randint(0, 2, (batch_size, channels, height, width))
    attention_mask = torch.zeros((batch_size, height * width))
    attention_mask[:, :450] = 1  # Only attend to first half of the pixels
    output_with_mask = model(input_ids, attention_mask)
    output_without_mask = model(input_ids)
    assert not torch.allclose(output_with_mask, output_without_mask), "Attention mask should affect the output"


# New tests for train.py


def test_logging(mock_args, mock_dataset, model, mock_pl_trainer):
    print("Entering test_logging")
    with patch(
        "gpt2_arc.src.training.train.ARCDataset", return_value=mock_dataset
    ), patch("gpt2_arc.src.training.train.GPT2ARC", return_value=model), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
    ) as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer", return_value=mock_pl_trainer
    ), patch("gpt2_arc.src.training.train.TensorBoardLogger") as mock_logger, patch(
        "gpt2_arc.src.training.train.ModelCheckpoint"
    ), patch("torch.utils.data.DataLoader") as mock_dataloader:
        mock_dataloader.return_value = MagicMock()

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "1234",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }
        mock_trainer_instance.results_collector = mock_results_collector

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_logger.assert_called_once_with("tb_logs", name="arc_model")


def test_fit_call(mock_args, mock_dataset, model):
    mock_pl_trainer = MagicMock()
    mock_pl_trainer.fit = MagicMock()
    print("Entering test_fit_call")
    with patch(
        "gpt2_arc.src.training.train.ARCDataset", return_value=mock_dataset
    ), patch("gpt2_arc.src.training.train.GPT2ARC", return_value=model), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ) as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer", return_value=mock_pl_trainer
    ), patch("gpt2_arc.src.training.train.TensorBoardLogger"), patch(
        "gpt2_arc.src.training.train.ModelCheckpoint"
    ), patch("torch.utils.data.DataLoader", new_callable=MagicMock) as mock_dataloader:
        mock_dataloader.return_value = MagicMock()

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_pl_trainer.fit.assert_called_once_with(mock_trainer_instance)


def test_data_loading(mock_args):
    with patch(
        "gpt2_arc.src.data.arc_dataset.ARCDataset.__init__", return_value=None
    ) as mock_init:
        ARCDataset(mock_args.train_data)
        mock_init.assert_called_once_with(mock_args.train_data)


def test_trainer_initialization(model, mock_dataset):
    config = Config(model=ModelConfig(), training=TrainingConfig())
    trainer = ARCTrainer(
        model=model, train_dataset=mock_dataset, val_dataset=mock_dataset, config=config
    )
    assert isinstance(trainer, ARCTrainer)
    assert trainer.model == model
    assert trainer.train_dataset == mock_dataset
    assert trainer.val_dataset == mock_dataset
    assert trainer.batch_size == 32
    assert trainer.lr == 1e-4


@pytest.mark.parametrize("batch_size", [1, 1000000])
def test_batch_size_extremes(mock_args, batch_size):
    model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=batch_size, learning_rate=5e-4, max_epochs=10))
    mock_args.batch_size = batch_size
    mock_args.no_logging = True
    mock_args.no_checkpointing = True
    mock_args.no_progress_bar = True
    mock_args.use_gpu = False
    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch("gpt2_arc.src.training.trainer.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ) as mock_trainer, patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        main(mock_args)

        mock_trainer.assert_called_with(
            max_epochs=config.training.max_epochs,
            logger=False,
            callbacks=None,
            enable_checkpointing=False,
            enable_progress_bar=False,
            fast_dev_run=False,  # Include fast_dev_run in the expected call
            gradient_clip_val=1.0,
            accelerator='cpu'
        )


@pytest.mark.parametrize("learning_rate", [1e-10, 1000])
def test_learning_rate_extremes(mock_args, learning_rate):
    set_logging_level(logging.WARNING)  # Suppress INFO and DEBUG messages
    mock_args.learning_rate = learning_rate
    logger.debug(f"Testing with learning_rate: {learning_rate}")
    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ), patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "1234",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }
        main(mock_args)  # Should not raise an exception


def test_non_existent_train_data(mock_args):
    mock_args.train_data = "non_existent_path.json"
    with pytest.raises(FileNotFoundError):
        if not os.path.exists(mock_args.train_data):
            raise FileNotFoundError(f"File not found: {mock_args.train_data}")
        main(mock_args)


def test_gpu_not_available(mock_args):
    mock_args.use_gpu = True
    mock_args.no_logging = False
    mock_args.no_checkpointing = False
    mock_args.no_progress_bar = False
    with patch("torch.cuda.is_available", return_value=False), patch(
        "gpt2_arc.src.training.train.ARCDataset"
    ), patch("gpt2_arc.src.training.train.GPT2ARC"), patch(
        "gpt2_arc.src.training.train.ARCTrainer"
        "gpt2_arc.src.training.train.ARCTrainer"
    ), patch("gpt2_arc.src.training.train.pl.Trainer") as mock_trainer, \
         patch("gpt2_arc.src.utils.results_collector.ResultsCollector.get_summary") as mock_get_summary:

        # Mock the get_summary method to return a serializable dictionary
        mock_get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }
        # Use a simple function instead of MagicMock for main
        def simple_main(args):
            pass

        simple_main(mock_args)
        mock_trainer.assert_called_with(
            max_epochs=mock_args.max_epochs,
            logger=ANY,
            callbacks=ANY,
            enable_checkpointing=True,
            enable_progress_bar=True,
            fast_dev_run=False,
            gradient_clip_val=1.0,
            accelerator='cpu'
        )


from hypothesis import HealthCheck, given, settings
from hypothesis import strategies as st


@settings(suppress_health_check=[HealthCheck.function_scoped_fixture], deadline=None)
@given(batch_size=st.integers(min_value=1, max_value=1024))
def test_valid_batch_sizes(mock_args, batch_size):
    mock_args.batch_size = batch_size
    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ), patch("gpt2_arc.src.training.train.ResultsCollector.get_summary", return_value={
        "experiment_id": "test_id",
        "timestamp": "2023-10-01 12:00:00",
        "final_train_loss": 0.1,
        "final_val_loss": 0.2,
        "test_accuracy": 0.95,
        "config": {"model": {}, "training": {}}
    }), patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        main(mock_args)  # Should not raise an exception


@settings(suppress_health_check=[HealthCheck.function_scoped_fixture], deadline=None)
@given(
    learning_rate=st.floats(
        min_value=1e-6, max_value=1.0, allow_nan=False, allow_infinity=False
    )
)
def test_valid_learning_rates(mock_args, learning_rate):
    mock_args.learning_rate = learning_rate
    import glob
    import os

    with patch("gpt2_arc.src.training.train.ARCDataset"), patch(
        "gpt2_arc.src.training.train.GPT2ARC"
    ), patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, patch(
        "gpt2_arc.src.training.train.pl.Trainer"
    ) as mock_trainer, patch(
        "torch.utils.data.DataLoader"
    ) as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        try:
            # Set up the ARCTrainer mock instance
            mock_trainer_instance = mock_ARCTrainer.return_value

            # Create a mock ResultsCollector with a real get_summary() method
            mock_results_collector = MagicMock()
            mock_results_collector.get_summary.return_value = {
                "experiment_id": "test_id",
                "timestamp": "2023-10-01 12:00:00",
                "final_train_loss": 0.1,
                "final_val_loss": 0.2,
                "test_accuracy": 0.95,
                "config": {"model": {}, "training": {}}
            }
            mock_results_collector.config = {"model": {}, "training": {}}

            # Assign the mock ResultsCollector to the trainer instance
            mock_trainer_instance.results_collector = mock_results_collector

            main(mock_args)  # Should not raise an exception
        finally:
            # Ensure cleanup of generated files
            for file in glob.glob("results/summary_*.json"):
                os.remove(file)


def test_end_to_end_training(mock_args, tmp_path):
    model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=5e-4, max_epochs=2))
    checkpoint_dir = tmp_path / "checkpoints"
    checkpoint_dir.mkdir()
    mock_args.checkpoint_dir = str(checkpoint_dir)

    with patch("gpt2_arc.src.training.train.ARCDataset"), \
         patch("gpt2_arc.src.training.train.GPT2ARC"), \
         patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, \
         patch("gpt2_arc.src.training.train.pl.Trainer") as mock_trainer, \
         patch("gpt2_arc.src.training.train.ModelCheckpoint") as mock_checkpoint, \
         patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_trainer.return_value.fit.assert_called_once()
        mock_checkpoint.assert_called_once()


def test_tensorboard_logging(mock_args, tmp_path):
    log_dir = tmp_path / "tb_logs"
    log_dir.mkdir()

    with patch("gpt2_arc.src.training.train.ARCDataset"), \
         patch("gpt2_arc.src.training.train.GPT2ARC"), \
         patch("gpt2_arc.src.training.train.ARCTrainer") as mock_ARCTrainer, \
         patch("gpt2_arc.src.training.train.pl.Trainer"), \
         patch("gpt2_arc.src.training.train.TensorBoardLogger") as mock_logger, \
         patch("torch.utils.data.DataLoader") as mock_dataloader:
        # Directly return a mock DataLoader instance
        mock_dataloader.return_value = MagicMock(spec=torch.utils.data.DataLoader)

        # Set up the ARCTrainer mock instance
        mock_trainer_instance = mock_ARCTrainer.return_value

        # Create a mock ResultsCollector with a real get_summary() method
        mock_results_collector = MagicMock()
        mock_results_collector.get_summary.return_value = {
            "experiment_id": "test_id",
            "timestamp": "2023-10-01 12:00:00",
            "final_train_loss": 0.1,
            "final_val_loss": 0.2,
            "test_accuracy": 0.95,
            "config": {"model": {}, "training": {}}
        }

        # Assign the mock ResultsCollector to the trainer instance
        mock_trainer_instance.results_collector = mock_results_collector

        main(mock_args)

        mock_logger.assert_called_once_with("tb_logs", name="arc_model")


# Additional test for GPT2ARC model in training context


def test_arctrainer_forward_pass(trainer):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output = trainer(input_ids, attention_mask)

    assert isinstance(output, torch.Tensor)
    assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd)

def test_arctrainer_training_step(trainer):
    batch_size = 2
    height = width = 30  # 30x30 grid
    seq_length = height * width
    vocab_size = 10  # Use a small vocab size for testing
    batch = (
        torch.randint(0, vocab_size, (batch_size, seq_length)).long(),  # inputs
        torch.ones((batch_size, seq_length)).float(),                   # labels
        torch.randint(0, vocab_size, (batch_size, seq_length)).long()   # task_ids
    )
    pl_trainer = MagicMock()
    pl_trainer.validate = MagicMock()
    pl_trainer.validate(trainer, dataloaders=[batch])

@pytest.mark.parametrize("batch_format", ["tuple", "dict"])
def test_arctrainer_batch_format(trainer, batch_format):
    batch_size = 2
    height = width = 30  # 30x30 grid
    seq_length = height * width
    vocab_size = 10  # Use a small vocab size for testing

    if batch_format == "tuple":
        batch = (
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            torch.ones((batch_size, seq_length)).float(),
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        )
    else:
        batch = {
            "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            "attention_mask": torch.ones((batch_size, seq_length)).float(),
            "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        }

    loss = trainer.training_step(batch, 0)

    assert isinstance(loss, torch.Tensor)
    assert loss.shape == torch.Size([])  # Loss should be a scalar
    assert not torch.isnan(loss).any(), "Loss contains NaN values"
    assert not torch.isinf(loss).any(), "Loss contains infinity values"

</file>
<file name="tests/test_differential_pixel_accuracy.py">
# gpt2_arc/tests/test_differential_pixel_accuracy.py
import sys
import os

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

import torch
from gpt2_arc.src.utils.helpers import differential_pixel_accuracy
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import ModelConfig
from gpt2_arc.src.data.arc_dataset import ARCDataset
import arckit

def test_identical_inputs_and_targets():
    input_tensor = torch.tensor([[1, 2], [3, 4]])
    target_tensor = torch.tensor([[1, 2], [3, 4]])
    prediction_tensor = torch.tensor([[1, 2], [3, 4]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for identical input and target"

def test_completely_different_inputs_and_targets():
    input_tensor = torch.tensor([[1, 1], [1, 1]])
    target_tensor = torch.tensor([[0, 0], [0, 0]])
    prediction_tensor = torch.tensor([[0, 0], [0, 0]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for correct prediction of all differing pixels"

def test_partial_differences():
    input_tensor = torch.tensor([[1, 2], [3, 4]])
    target_tensor = torch.tensor([[1, 0], [3, 0]])
    prediction_tensor = torch.tensor([[1, 0], [3, 4]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 0.5, "Expected accuracy of 0.5 for partial correct predictions"

def test_empty_tensors():
    input_tensor = torch.tensor([])
    target_tensor = torch.tensor([])
    prediction_tensor = torch.tensor([])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for empty tensors"

def test_single_pixel_difference():
    input_tensor = torch.tensor([[1]])
    target_tensor = torch.tensor([[0]])
    prediction_tensor = torch.tensor([[0]])
    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    assert accuracy == 1.0, "Expected accuracy of 1.0 for single pixel difference"

def test_differential_pixel_accuracy_with_arckit_data():
    print("Starting test_differential_pixel_accuracy_with_arckit_data")
    task_id = "007bbfb7"
    task_data = arckit.load_single(task_id)

    print(f"Loaded task data: {task_data}")
    print(f"Debug: task_data type: {type(task_data)}")
    print(f"Debug: task_data attributes: {dir(task_data)}")

    dataset = ARCDataset([task_data])  # Wrap in list to simulate multiple tasks
    input_tensor, target_tensor, _ = dataset[0]

    print(f"Dataset input tensor shape: {input_tensor.shape}")
    print(f"Dataset target tensor shape: {target_tensor.shape}")

    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    model = GPT2ARC(model_config)
    model.eval()

    print("Model initialized and set to eval mode")

    with torch.no_grad():
        prediction_tensor = model(input_tensor.unsqueeze(0))

    print(f"Model prediction tensor shape: {prediction_tensor.shape}")

    # Reverse scaling for evaluation
    original_input = task_data.train[0][0]
    original_target = task_data.train[0][1]
    
    print(f"Original input shape: {original_input.shape}")
    print(f"Original target shape: {original_target.shape}")

    prediction_np = prediction_tensor.squeeze().argmax(dim=0).numpy()
    print(f"Prediction numpy array shape: {prediction_np.shape}")

    reversed_prediction = dataset.reverse_scaling(original_input, prediction_np)
    print(f"Reversed prediction shape: {reversed_prediction.shape}")

    # Convert back to tensors for differential_pixel_accuracy
    # Ensure all tensors have the same shape
    input_tensor = torch.tensor(original_input, dtype=torch.float32).resize_(original_target.shape)
    target_tensor = torch.tensor(original_target, dtype=torch.float32)
    prediction_tensor = torch.tensor(reversed_prediction, dtype=torch.float32).resize_(original_target.shape)

    print(f"Final input tensor shape: {input_tensor.shape}")
    print(f"Final target tensor shape: {target_tensor.shape}")
    print(f"Final prediction tensor shape: {prediction_tensor.shape}")

    accuracy, _, _ = differential_pixel_accuracy(input_tensor, target_tensor, prediction_tensor)
    print(f"Differential Pixel Accuracy for task {task_id}: {accuracy}")

    assert 0 &lt;= accuracy &lt;= 1, f"Accuracy should be between 0 and 1, but got {accuracy}"

# Run the tests
if __name__ == "__main__":
    test_identical_inputs_and_targets()
    test_completely_different_inputs_and_targets()
    test_partial_differences()
    test_empty_tensors()
    test_single_pixel_difference()
    print("All tests passed!")

</file>
<file name="tests/test_models.py">
import unittest
import torch
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig

class TestGPT2ARC(unittest.TestCase):
    def setUp(self):
        # Define model and training configurations
        model_config = ModelConfig(
            n_embd=16,
            n_head=2,
            n_layer=2,
            mamba_ratio=1,
            d_state=4,
            d_conv=1,
            dropout=0.05
        )
        training_config = TrainingConfig(
            batch_size=2,
            learning_rate=0.001,
            max_epochs=10,
            use_gpu=False,
            log_level="DEBUG",
            use_synthetic_data=False,
            balance_symbols=True,
            balancing_method="weighting",
            synthetic_data_path=None,
            symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05}
        )
        self.config = Config(model=model_config, training=training_config)
        self.model = GPT2ARC(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq)

    def test_model_initialization_with_class_weights(self):
        expected_weights = torch.tensor([2.0, 5.0, 10.0, 10.0, 20.0, 20.0])
        self.assertTrue(torch.allclose(self.model.loss_fn.weight, expected_weights),
                        "Class weights in loss function do not match expected values.")

    def test_model_forward_pass(self):
        dummy_input = torch.zeros(1, 1, 6, 6)
        output = self.model(dummy_input)
        self.assertEqual(output.shape, (1, 1, 6), "Model output shape mismatch.")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_benchmark.py">
# gpt2_arc/tests/test_benchmark.py

import pytest
import torch
import numpy as np
from unittest.mock import MagicMock, patch
from benchmark import benchmark_model, main, BASELINES
from src.config import ModelConfig
from src.models.gpt2 import GPT2ARC

# Mock classes and fixtures

@pytest.fixture
def mock_model():
    model = MagicMock(spec=GPT2ARC)
    model.forward = MagicMock()  # Ensure forward is a mock
    return model

@pytest.fixture
def mock_dataset():
    dataset = MagicMock()
    dataset.__getitem__.return_value = (
        torch.randn(1, 30, 30),  # inputs
        torch.randint(0, 10, (1, 30, 30)),  # outputs
        "task_1"  # task_id
    )
    dataset.__len__.return_value = 100
    return dataset

@pytest.fixture
def mock_dataloader():
    dataloader = MagicMock()
    dataloader.__iter__.return_value = iter([
        (
            torch.randn(32, 1, 30, 30),  # inputs
            torch.randn(32, 1, 30, 30),  # outputs
            f"task_{i}"                  # task_ids
        )
        for i in range(10)
    ])
    return dataloader


# Tests for benchmark_model function

def test_benchmark_model_basic(mock_model, mock_dataset, mock_dataloader):
    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader), \
         patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=False), \
         patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=False):
        avg_time, avg_grids = benchmark_model(mock_model, mock_dataset)
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))
    assert avg_time &gt; 0
    assert avg_grids &gt; 0

@pytest.mark.parametrize("batch_size,num_batches,num_runs", [
    (16, 5, 10),
    (64, 20, 5),
    (128, 2, 3)
])
def test_benchmark_model_parameters(mock_model, mock_dataset, mock_dataloader, batch_size, num_batches, num_runs):
    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader), \
         patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=False), \
         patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=False):
        avg_time, avg_grids = benchmark_model(
            mock_model, mock_dataset, batch_size=batch_size, num_batches=num_batches, num_runs=num_runs
        )
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))

def test_benchmark_model_cuda(mock_model, mock_dataset, mock_dataloader):
    with patch('benchmark.torch.cuda.is_available', return_value=True), \
         patch('benchmark.torch.cuda.synchronize'), \
         patch('benchmark.DataLoader', return_value=mock_dataloader), \
         patch('benchmark.torch.compile', return_value=mock_model):
        
        if not torch.cuda.is_available():
            pytest.skip("CUDA is not available on this system")
        
        try:
            avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='cuda')
        except AssertionError as e:
            if "Torch not compiled with CUDA enabled" in str(e):
                pytest.skip("PyTorch not compiled with CUDA support")
            else:
                raise
        
        assert isinstance(avg_time, float)
        assert isinstance(avg_grids, float)
        assert avg_time &gt;= 0
        assert avg_grids &gt;= 0

def test_benchmark_model_mps(mock_model, mock_dataset, mock_dataloader):
    with patch('benchmark.torch.backends.mps.is_available', return_value=True), \
         patch('benchmark.DataLoader', return_value=mock_dataloader):
        avg_time, avg_grids = benchmark_model(mock_model, mock_dataset, device_type='mps')
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))

def test_benchmark_model_error_handling(mock_model, mock_dataset):
    with pytest.raises(ValueError, match="Invalid device type"):
        benchmark_model(mock_model, mock_dataset, device_type='invalid_device')

# Tests for main function

@pytest.fixture
def mock_argparse():
    with patch('benchmark.argparse.ArgumentParser') as mock_argparse:
        mock_args = MagicMock()
        mock_args.num_runs = 5
        mock_args.num_full_runs = 1
        mock_args.batch_size = 32
        mock_args.num_batches = 10
        mock_args.n_embd = 64
        mock_args.n_head = 2
        mock_args.n_layer = 1
        mock_args.device = 'cpu'
        mock_args.precision = 'highest'
        mock_argparse.return_value.parse_args.return_value = mock_args
        yield mock_argparse

def test_main_function(mock_argparse, mock_dataset, mock_model):
    with patch('benchmark.arckit.load_data', return_value=(mock_dataset, None)), \
         patch('benchmark.ARCDataset', return_value=mock_dataset), \
         patch('benchmark.GPT2ARC', return_value=mock_model), \
         patch('benchmark.benchmark_model', return_value=(1.0, 100.0)):
        main(mock_argparse.return_value.parse_args())

# Performance tests

@pytest.mark.benchmark(group="benchmark_model")
def test_benchmark_model_performance(benchmark, mock_model):
    # Create a mock dataset with one item
    mock_dataset = MagicMock()
    mock_dataset.__len__.return_value = 1
    mock_dataset.__getitem__.return_value = (
        torch.randn(1, 30, 30),  # input
        torch.randint(0, 10, (1, 30, 30)),  # output
        "task_1"  # task_id
    )

    # Create a mock dataloader that returns the mock dataset item
    mock_dataloader = MagicMock()
    mock_dataloader.__iter__.return_value = iter([mock_dataset.__getitem__()])

    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader), \
         patch('gpt2_arc.benchmark.torch.cuda.is_available', return_value=False), \
         patch('gpt2_arc.benchmark.torch.backends.mps.is_available', return_value=False):
        result = benchmark(
            benchmark_model,
            mock_model,
            mock_dataset,
            batch_size=1,
            num_batches=1,
            device_type='cpu',
            precision='medium',
            model_checkpoint=None
        )

    assert isinstance(result, tuple), f"Expected tuple, got {type(result)}"
    assert len(result) == 2, f"Expected tuple of length 2, got length {len(result)}"
    
    avg_time, grids_per_second = result
    print(f"Benchmark result - Average Time: {avg_time}, Grids per Second: {grids_per_second}")
    
    assert isinstance(avg_time, float), f"Expected float for avg_time, got {type(avg_time)}"
    assert isinstance(grids_per_second, float), f"Expected float for grids_per_second, got {type(grids_per_second)}"
    assert avg_time &gt;= 0, f"Average time should be non-negative, got {avg_time}"
    assert grids_per_second &gt;= 0, f"Grids per second should be non-negative, got {grids_per_second}"

    if avg_time &gt; 0:
        assert grids_per_second &gt; 0, f"Grids per second should be positive when avg_time &gt; 0, got {grids_per_second}"

# Edge case tests

def test_benchmark_model_empty_dataset(mock_model):
    empty_dataset = MagicMock()
    empty_dataset.__len__.return_value = 0

    with pytest.raises(ValueError, match="Dataset is empty"):
        benchmark_model(mock_model, empty_dataset)

def test_benchmark_model_single_item_dataset(mock_model):
    single_item_dataset = MagicMock()
    single_item_dataset.__len__.return_value = 1
    mock_dataloader = MagicMock()
    mock_dataloader.__iter__.return_value = iter([
        (torch.randn(1, 1, 30, 30), torch.randn(1, 1, 30, 30), "task_1")
    ])
    
    with patch('benchmark.DataLoader', return_value=mock_dataloader):
        avg_time, avg_grids = benchmark_model(mock_model, single_item_dataset, batch_size=1, num_batches=1)
    
    assert isinstance(avg_time, (float, int))
    assert isinstance(avg_grids, (float, int))

# Error handling tests

def test_benchmark_model_with_correct_data(mock_model, mock_dataset, mock_dataloader):
    with patch('benchmark.DataLoader', return_value=mock_dataloader):
        avg_time, avg_grids = benchmark_model(mock_model, mock_dataset)
        
        assert isinstance(avg_time, float), "avg_time should be a float"
        assert isinstance(avg_grids, float), "avg_grids should be a float"
        assert avg_time &gt; 0, "avg_time should be positive"
        assert avg_grids &gt; 0, "avg_grids should be positive"

def test_benchmark_model_model_error(mock_model, mock_dataset, mock_dataloader):
    # Mock the model's forward method to raise a RuntimeError during execution
    mock_model.forward = MagicMock(side_effect=RuntimeError("Model execution failed"))
    
    with patch('gpt2_arc.benchmark.DataLoader', return_value=mock_dataloader):
        with pytest.raises(RuntimeError, match="Model execution failed"):
            print("DEBUG: Invoking benchmark_model")
            benchmark_model(mock_model, mock_dataset, device_type='cpu')
        # Ensure the forward method is called
        assert mock_model.forward.call_count &gt; 0, "DEBUG: forward method was not called"
        print(f"DEBUG: forward method call count: {mock_model.forward.call_count}")

#skip
@pytest.mark.skip(reason="I dont want to crash my computer")
def test_benchmark_model_out_of_memory(mock_model, mock_dataset, mock_dataloader):
    mock_model.side_effect = torch.cuda.OutOfMemoryError("CUDA out of memory")
    
    with patch('benchmark.DataLoader', return_value=mock_dataloader), \
         patch('benchmark.torch.cuda.is_available', return_value=True), \
         pytest.raises(torch.cuda.OutOfMemoryError, match="CUDA out of memory"):
        benchmark_model(mock_model, mock_dataset, device_type='cuda')

# Precision tests

@pytest.fixture
def mock_torch():
    return MagicMock()

@pytest.mark.parametrize("precision", ['highest', 'high', 'medium'])
def test_benchmark_model_precision(mock_model, mock_dataset, mock_torch, precision):
    with patch('gpt2_arc.benchmark.DataLoader') as mock_dataloader_class:
        mock_dataloader = MagicMock()
        mock_dataloader.__iter__.return_value = iter([
            (
                torch.randn(1, 1, 30, 30),  # inputs
                torch.randint(0, 10, (1, 30, 30)),  # outputs
                "task_1"  # task_id
            )
        ])
        mock_dataloader_class.return_value = mock_dataloader

        with patch('gpt2_arc.benchmark.torch.set_float32_matmul_precision') as mock_set_precision:
            benchmark_model(mock_model, mock_dataset, precision=precision)
    
    mock_set_precision.assert_called_once_with(precision)

# CSV output tests

def test_csv_output(mock_model, mock_dataset, mock_dataloader, tmp_path):
    csv_file = tmp_path / "benchmark_results.csv"
    stats_csv_file = tmp_path / "benchmark_statistics.csv"
    
    with patch('benchmark.DataLoader', return_value=mock_dataloader), \
         patch('benchmark.csv.writer') as mock_csv_writer:
        benchmark_model(mock_model, mock_dataset)
    
    assert mock_csv_writer.call_count == 2  # One for results, one for statistics

# Test suite execution

if __name__ == '__main__':
    pytest.main(['-v', '--cov=benchmark', '--cov-report=term-missing'])

</file>
<file name="tests/test_synthetic_data.py">
import json
import tempfile
import os
from pathlib import Path
import pytest
from unittest.mock import patch, MagicMock
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.training.train import main
import torch

@pytest.fixture
def synthetic_data():
    with tempfile.TemporaryDirectory() as tmpdir:
        data_path = Path(tmpdir) / "synthetic_data"
        data_path.mkdir()
        with open(data_path / "task1.json", "w") as f:
            json.dump({
                "train": [{"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}],
                "test": [{"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]}]
            }, f)
        yield str(data_path)

def test_synthetic_data_loading(synthetic_data):
    dataset = ARCDataset(synthetic_data)
    assert len(dataset) &gt; 0
    sample = dataset[0]
    assert isinstance(sample, tuple)
    assert len(sample) == 3  # input, output, task_id
    assert sample[0].shape == (1, 2, 2)  # Assuming 2x2 grid
    assert sample[1].shape == (1, 2, 2)

def test_short_training_run():
    args = MagicMock()
    args.use_synthetic_data = True
    args.synthetic_data_path = "gpt2_arc/src/data/SyntheticARC/tasks"
    args.max_epochs = 1
    args.fast_dev_run = True
    args.use_gpu = torch.cuda.is_available()
    args.no_logging = True
    args.no_checkpointing = True
    args.no_progress_bar = True
    args.project = "test_project"
    args.log_level = "DEBUG"
    args.batch_size = 1
    args.learning_rate = 1e-4
    args.n_embd = 32
    args.n_head = 2
    args.n_layer = 2

    with patch("gpt2_arc.src.training.train.pl.Trainer") as mock_pl_trainer, \
         patch("gpt2_arc.src.training.train.ARCTrainer") as mock_arc_trainer, \
         patch("gpt2_arc.src.training.train.ARCDataset") as mock_dataset:
        
        # Mock the dataset to return a single sample
        mock_dataset.return_value = [
            (torch.rand(1, 30, 30), torch.rand(1, 30, 30), 0)
        ]
        
        main(args)
        
        mock_pl_trainer.assert_called_once()
        mock_arc_trainer.assert_called_once()

@pytest.mark.parametrize("use_synthetic", [True, False])
def test_main_with_synthetic_data(synthetic_data, use_synthetic):
    args = MagicMock()
    args.use_synthetic_data = use_synthetic
    args.synthetic_data_path = synthetic_data if use_synthetic else None
    args.max_epochs = 1
    args.fast_dev_run = True
    args.use_gpu = False
    args.no_logging = True
    args.no_checkpointing = True
    args.no_progress_bar = True
    args.project = "test_project"
    args.log_level = "DEBUG"
    args.batch_size = 1  # Set batch_size to a positive integer
    args.learning_rate = 1e-4  # Set a default learning rate
    print(f"DEBUG: args.batch_size = {args.batch_size}")
    print(f"DEBUG: args.learning_rate = {args.learning_rate}")

    with patch("gpt2_arc.src.training.train.pl.Trainer") as mock_pl_trainer, \
         patch("gpt2_arc.src.training.train.ARCDataset") as mock_dataset, \
         patch("gpt2_arc.src.training.train.GPT2ARC") as mock_model, \
         patch("gpt2_arc.src.training.train.ARCTrainer") as mock_arc_trainer:
        print("DEBUG: Inside test_main_with_synthetic_data")
        print(f"DEBUG: mock_pl_trainer = {mock_pl_trainer}")
        print(f"DEBUG: mock_arc_trainer = {mock_arc_trainer}")
        main(args)
        mock_pl_trainer.assert_called_once()

def test_synthetic_data_argument_parsing():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--use-synthetic-data", action="store_true")
    parser.add_argument("--synthetic-data-path", type=str)

    # Test with synthetic data
    args = parser.parse_args(["--use-synthetic-data", "--synthetic-data-path", "/path/to/data"])
    assert args.use_synthetic_data
    assert args.synthetic_data_path == "/path/to/data"

    # Test without synthetic data
    args = parser.parse_args([])
    assert not args.use_synthetic_data
    assert args.synthetic_data_path is None

</file>
<file name="tests/test_evaluate.py">
import unittest
import torch
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.data.arc_dataset import ARCDataset

class TestEvaluationMetrics(unittest.TestCase):
    def setUp(self):
        # Define model and training configurations
        model_config = ModelConfig(
            n_embd=16,
            n_head=2,
            n_layer=2,
            mamba_ratio=1,
            d_state=4,
            d_conv=1,
            dropout=0.05
        )
        training_config = TrainingConfig(
            batch_size=2,
            learning_rate=0.001,
            max_epochs=10,
            use_gpu=False,
            log_level="DEBUG",
            use_synthetic_data=False,
            balance_symbols=True,
            balancing_method="weighting",
            synthetic_data_path=None,
            symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05}
        )
        self.config = Config(model=model_config, training=training_config)
        self.model = GPT2ARC(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq)
        self.train_dataset = ARCDataset(data_source="path/to/mock_data")
        self.val_dataset = ARCDataset(data_source="path/to/mock_data")

    def test_evaluation_metrics_computation(self):
        trainer = ARCTrainer(model=self.model, train_dataset=self.train_dataset, val_dataset=self.val_dataset, config=self.config)
        # Create dummy outputs and labels
        outputs = torch.tensor([[0.1, 0.6, 0.3, 0.0, 0.0, 0.0],
                                [0.3, 0.3, 0.2, 0.1, 0.05, 0.05]], requires_grad=True)
        labels = torch.tensor([1, 2])
        loss = trainer.compute_loss(outputs, labels)
        self.assertGreater(loss.item(), 0, "Loss should be positive.")
        accuracy = trainer.compute_accuracy(outputs, labels)
        self.assertGreaterEqual(accuracy.item(), 0, "Accuracy should be non-negative.")
        self.assertLessEqual(accuracy.item(), 1, "Accuracy should not exceed 1.")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_end_to_end.py">
# gpt2_arc/tests/test_end_to_end.py
import pytest
import torch
import numpy as np
from src.data.arc_dataset import ARCDataset
from src.models.gpt2 import GPT2ARC
from src.training.trainer import ARCTrainer
from src.config import Config, ModelConfig, TrainingConfig
import pytorch_lightning as pl
import time
import logging
import os
from thop import profile, clever_format  # Import THOP
from pytest import approx

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@pytest.fixture
def arc_data_path():
    # Adjust this path to the location of your ARC dataset JSON file
    return "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/syntheticARC/tasks/1c786137.json"

import arckit

def test_end_to_end():
    logger.debug("Starting end-to-end test")

    try:
        # Load data using arckit
        logger.debug("Loading data using arckit")
        train_set, eval_set = arckit.load_data()
        
        # Create datasets using ARCDataset
        logger.debug("Creating train and validation datasets")
        full_dataset = ARCDataset(train_set, is_test=False)
        # Use a smaller subset of the dataset
        subset_size = int(0.1 * len(full_dataset))  # Use 10% of the dataset
        train_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size])
        val_dataset, _ = torch.utils.data.random_split(full_dataset, [subset_size, len(full_dataset) - subset_size])
        logger.debug(f"Train dataset size: {len(train_dataset)}, Validation dataset size: {len(val_dataset)}")

        # Create a custom collate function to handle the data format
        def collate_fn(batch):
            inputs = [item[0].to(torch.float32) for item in batch]  # Convert to float32
            outputs = [item[1].to(torch.float32) for item in batch]  # Convert to float32
            logger.debug(f"Batch input dtypes before stack: {[item[0].dtype for item in batch]}")
            logger.debug(f"Batch output dtypes before stack: {[item[1].dtype for item in batch]}")

            # Inputs and outputs are already tensors, so we just need to stack them
            input_stack = torch.stack(inputs)
            output_stack = torch.stack(outputs)

            # Log data types after stacking
            logger.debug(f"Collate function input_stack dtype: {input_stack.dtype}")
            logger.debug(f"Collate function output_stack dtype: {output_stack.dtype}")

            # Create a dummy attention mask (all ones)
            attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32)

            logger.debug(f"Collate function attention_mask dtype: {attention_mask.dtype}")
            
            # Generate dummy task_ids for each item in the batch
            task_ids = [f"task_{i}" for i in range(len(batch))]
            
            return input_stack, attention_mask, output_stack, task_ids
            logger.debug(f"Batch output dtypes before stack: {[item[1].dtype for item in batch]}")

            # Inputs and outputs are already tensors, so we just need to stack them
            input_stack = torch.stack(inputs)
            output_stack = torch.stack(outputs)

            # Create a dummy attention mask (all ones)
            attention_mask = torch.ones(input_stack.size(0), input_stack.size(2) * input_stack.size(3), dtype=torch.float32)

            logger.debug(f"Collate function input dtype: {input_stack.dtype}")
            return input_stack, attention_mask, output_stack

        # Initialize model
        logger.debug("Initializing model")
        model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)  # Use smaller model configuration
        model = GPT2ARC(model_config).to(torch.float32)
        logger.debug(f"Model initialized with config: {model_config}")

        # # THOP Profiling - Commented out due to TypeError with MPS Tensors
        # logger.debug("Profiling model with THOP")
        # dummy_input = torch.randn(1, 1, 28, 28, dtype=torch.float32)  # Example input shape
        # macs, params = profile(model, inputs=(dummy_input,))
        # macs, params = clever_format([macs, params], "%.3f")
        # logger.info(f"MACs: {macs}, Parameters: {params}")

        # Initialize trainer
        logger.debug("Initializing trainer")
        config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))  # Reduce epochs to 2
        trainer = ARCTrainer(model, train_dataset, val_dataset, config)
        trainer.train_dataloader = lambda: torch.utils.data.DataLoader(train_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0)
        trainer.val_dataloader = lambda: torch.utils.data.DataLoader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0)
        trainer.test_dataloader = lambda: torch.utils.data.DataLoader(val_dataset, batch_size=config.training.batch_size, collate_fn=collate_fn, num_workers=0)
        logger.debug(f"Trainer initialized with config: {config}")

        # Create PyTorch Lightning trainer
        logger.debug("Creating PyTorch Lightning trainer")
        # Measure training time
        start_time = time.time()
        
        pl_trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            logger=False,
            enable_checkpointing=False,
            enable_progress_bar=False
        )
        logger.debug("PyTorch Lightning trainer created")

        # Evaluate model before training to get initial accuracy
        logger.info("Evaluating model before training")
        initial_val_results = pl_trainer.test(trainer, verbose=False)
        logger.debug(f"Initial validation results: {initial_val_results}")
        initial_accuracy = initial_val_results[0].get('test_accuracy')
        initial_loss = initial_val_results[0].get('test_loss')

        print(f"Initial validation results: {initial_val_results}")
        assert initial_accuracy is not None, "Initial validation results missing 'test_accuracy'"
        assert initial_loss is not None, "Initial validation results missing 'test_loss'"
        logger.info(f"Initial validation accuracy: {initial_accuracy}, Initial loss: {initial_loss}")
        print(f"Initial validation accuracy: {initial_accuracy}, Initial loss: {initial_loss}")
        logger.debug("Starting model training")
        pl_trainer.fit(trainer)
        end_time = time.time()
        training_time = end_time - start_time
        logger.info(f"Total training time: {training_time:.2f} seconds")
        logger.debug("Model training completed")

        # Check that loss decreased
        train_losses = trainer.train_losses
        logger.info(f"Training losses: {train_losses}")
        assert train_losses[-1] &lt; train_losses[0], f"Training loss did not decrease. Initial loss: {train_losses[0]}, Final loss: {train_losses[-1]}"
        
        # Check that the final loss is lower than the initial loss
        assert train_losses[-1] &lt; train_losses[0], "Final training loss should be lower than initial loss"

        # Check that the average loss per epoch decreases
        epoch_losses = [sum(train_losses[i:i+33])/33 for i in range(0, len(train_losses), 33)]
        assert all(epoch_losses[i] &gt; epoch_losses[i+1] for i in range(len(epoch_losses)-1)), "Average training loss per epoch did not consistently decrease"

        # Evaluate model after training
        logger.debug("Evaluating model after training")
        final_val_results = pl_trainer.test(trainer, verbose=False)
        final_accuracy = final_val_results[0]['test_accuracy']
        final_loss = final_val_results[0]['test_loss']
        logger.info(f"Final validation accuracy: {final_accuracy}, Final loss: {final_loss}")
        print(f"Final validation accuracy: {final_accuracy}, Final loss: {final_loss}")

        # Check that validation accuracy improved
        assert final_accuracy &gt; initial_accuracy, f"Validation accuracy did not improve. Initial accuracy: {initial_accuracy}, Final accuracy: {final_accuracy}"

        logger.info(f"Final training loss: {train_losses[-1]:.4f}")
        logger.info(f"Validation accuracy: {final_accuracy:.4f}")

        # Check model parameters
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        assert total_params &gt; 0, "Model has no parameters"
        assert trainable_params &gt; 0, "Model has no trainable parameters"
        assert trainable_params == total_params, "Not all parameters are trainable"

        logger.debug(f"Total parameters: {total_params}")
        logger.debug(f"Trainable parameters: {trainable_params}")

        logger.debug("End-to-end test completed successfully")
    except Exception as e:
        logger.error(f"End-to-end test failed with error: {str(e)}")
        raise

def test_evaluation_process_with_arckit_data():
    logger.debug("Starting evaluation process test with arckit data")

    # Load data using arckit
    _ , evaluation_data = arckit.load_data()

    # Log the structure of evaluation data
    logger.debug(f"Evaluation data structure: {evaluation_data}")
    logger.debug(f"Evaluation data structure: {evaluation_data}")
    test_dataset = ARCDataset(evaluation_data, is_test=True)

    # Initialize the model and trainer
    model_configuration = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    model = GPT2ARC(model_configuration)
    training_configuration = Config(model=model_configuration, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))
    trainer = ARCTrainer(model, None, test_dataset, training_configuration)

    # Run the evaluation
    lightning_trainer = pl.Trainer(logger=False, enable_checkpointing=False, enable_progress_bar=False)
    evaluation_results = lightning_trainer.test(trainer)

    # Access the test results from the trainer
    evaluation_results = trainer.test_results

    # Log the evaluation results
    logger.debug(f"Evaluation results: {evaluation_results}")
    for result in evaluation_results:
        task_ids = result.get('task_ids', [])
        if not task_ids:
            logger.error(f"Missing task_ids in result: {result}")
        else:
            for task_id in task_ids:
                logger.info(f"Task {task_id}: Loss={result['test_loss']}, Accuracy={result['test_accuracy']}")

    # Check for duplicate metrics
    unique_task_ids = set(task_id for result in evaluation_results for task_id in result.get('task_ids', []))
    print("All task IDs:", [task_id for result in evaluation_results for task_id in result.get('task_ids', [])])
    print("Unique task IDs:", unique_task_ids)
    print(f"Number of evaluation results: {len(evaluation_results)}")
    print(f"Number of unique task IDs: {len(unique_task_ids)}")

    if len(unique_task_ids) != len(evaluation_results):
        print("Warning: Number of unique task IDs doesn't match number of evaluation results")
        duplicate_tasks = [task_id for task_id in unique_task_ids if sum(task_id in result.get('task_ids', []) for result in evaluation_results) &gt; 1]
        print(f"Duplicate task IDs: {duplicate_tasks}")
        for task_id in duplicate_tasks:
            print(f"Results for task {task_id}:")
            for result in evaluation_results:
                if task_id in result.get('task_ids', []):
                    print(result)
    print(f"Unique task IDs: {unique_task_ids}")
    print(f"Evaluation results: {evaluation_results}")
    assert len(unique_task_ids) &gt; 0, "No tasks were evaluated"

    logger.debug("Completed evaluation process test with arckit data")

</file>
<file name="tests/test_mamba_integration.py">
import unittest
import torch
import inspect
from gpt2_arc.src.models.gpt2 import MambaLayer, GPT2ARC
from zeta.nn import MambaBlock
from gpt2_arc.src.config import ModelConfig

class TestMambaLayer(unittest.TestCase):
    def test_mamba_layer_forward(self):
        # Print the __init__ method signature of MambaBlock
        print(f"MambaBlock.__init__ signature: {inspect.signature(MambaBlock.__init__)}")
        n_embd = 64
        d_state = 16
        d_conv = 4
        dropout = 0.1
        mamba_layer = MambaLayer(n_embd, d_state, d_conv, dropout)

        # Create a sample input tensor
        batch_size = 2
        seq_len = 10
        x = torch.randn(batch_size, seq_len, n_embd)

        # Forward pass
        output = mamba_layer(x)

        # Assert output shape is correct
        self.assertEqual(output.shape, x.shape)

        # Optional: Check for NaNs or infinite values
        self.assertTrue(torch.all(torch.isfinite(output)))

class TestGPT2ARCWithMamba(unittest.TestCase):
    def test_gpt2arc_with_mamba_forward(self):
        # Define model configuration with Mamba parameters
        model_config = ModelConfig(
            n_embd=64,
            n_head=4,
            n_layer=2,
            mamba_ratio=1,
            d_state=16,
            d_conv=4,
            dropout=0.1
        )
        num_classes = 10  # Adjust based on your dataset
        model = GPT2ARC(config=model_config, num_classes=num_classes)

        # Create a sample input tensor (e.g., a batch of grids)
        batch_size = 2
        height = width = 6
        x = torch.randint(0, num_classes, (batch_size, 1, height, width), dtype=torch.long)

        # Forward pass
        output = model(x)

        # Assert output shape is correct
        expected_output_shape = (batch_size, height * width, num_classes)
        self.assertEqual(output.shape, expected_output_shape)

        # Optional: Check for NaNs or infinite values
        self.assertTrue(torch.all(torch.isfinite(output)))

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_experiment_tracker.py">
import unittest
import json
from gpt2_arc.src.utils.experiment_tracker import ExperimentTracker
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig

class TestExperimentTracker(unittest.TestCase):
    def setUp(self):
        # Define model and training configurations
        model_config = ModelConfig(
            n_embd=16,
            n_head=2,
            n_layer=2,
            mamba_ratio=1,
            d_state=4,
            d_conv=1,
            dropout=0.05
        )
        training_config = TrainingConfig(
            batch_size=2,
            learning_rate=0.001,
            max_epochs=10,
            use_gpu=False,
            log_level="DEBUG",
            use_synthetic_data=False,
            balance_symbols=True,
            balancing_method="weighting",
            synthetic_data_path=None,
            symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05}
        )
        self.config = Config(model=model_config, training=training_config)

    def test_experiment_tracker_logging(self):
        tracker = ExperimentTracker(config=self.config, project="test_project")
        tracker.log_metric("test_metric", 0.95)
        self.assertIn("test_metric", tracker.metrics, "Metric should be logged in tracker.metrics.")
        self.assertEqual(tracker.metrics["test_metric"], 0.95, "Logged metric value mismatch.")

    def test_experiment_tracker_save_to_json(self):
        tracker = ExperimentTracker(config=self.config, project="test_project")
        tracker.log_metric("test_metric", 0.95)
        tracker.save_to_json("test_results.json")
        with open("test_results.json", 'r') as f:
            data = json.load(f)
        self.assertIn("test_metric", data, "Metric should be present in saved JSON.")
        self.assertEqual(data["test_metric"], 0.95, "Saved metric value mismatch.")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_synthetic_arc_dataset.py">
# gpt2_arc/tests/test_synthetic_arc_dataset.py
import os
import pytest
import logging
from gpt2_arc.src.data.arc_dataset import ARCDataset

SYNTHETIC_DATA_PATH = "/workspaces/arc-neural-reasoning-model/gpt2_arc/src/data/SyntheticARC/small_tasks"

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@pytest.fixture
def synthetic_dataset():
    logger.debug("Creating synthetic dataset")
    return ARCDataset(SYNTHETIC_DATA_PATH)

def test_synthetic_data_loading(synthetic_dataset):
    logger.debug("Testing synthetic data loading")
    assert len(synthetic_dataset) &gt; 0, "Synthetic dataset is empty"

def test_synthetic_data_structure(synthetic_dataset):
    logger.debug("Testing synthetic data structure")
    sample = synthetic_dataset[0]
    assert isinstance(sample, tuple), "Sample should be a tuple"
    assert len(sample) == 3, "Sample should contain input, output, and task_id"
    input_grid, output_grid, task_id = sample
    
    assert input_grid.dim() == 3, "Input grid should be 3-dimensional (channel, height, width)"
    assert output_grid.dim() == 3, "Output grid should be 3-dimensional (channel, height, width)"
    assert isinstance(task_id, (int, str)), "Task ID should be an integer or string"

def test_all_synthetic_files_loaded():
    logger.debug("Testing all synthetic files loaded")
    file_count = len([f for f in os.listdir(SYNTHETIC_DATA_PATH) if f.endswith('.json')])
    dataset = ARCDataset(SYNTHETIC_DATA_PATH)
    assert len(dataset.data) == file_count, f"Number of loaded tasks ({len(dataset.data)}) doesn't match the number of JSON files ({file_count})"

def test_synthetic_data_content(synthetic_dataset):
    logger.debug("Testing synthetic data content")
    for i in range(len(synthetic_dataset)):
        input_grid, output_grid, _ = synthetic_dataset[i]
        assert input_grid.min() &gt;= 0 and input_grid.max() &lt;= 9, f"Input grid values should be between 0 and 9 (sample {i})"
        assert output_grid.min() &gt;= 0 and output_grid.max() &lt;= 9, f"Output grid values should be between 0 and 9 (sample {i})"

if __name__ == "__main__":
    pytest.main([__file__])

</file>
<file name="tests/test_pytest_error_fixer.py">
# gpt2_arc/tests/test_pytest_error_fixer.py
import os
import json
import pytest
from unittest.mock import patch, MagicMock
from pytest_error_fixer import PytestErrorFixer

# Reusable fixtures for test setup
@pytest.fixture
def error_fixer(tmp_path):
    # Initialize PytestErrorFixer with a temporary directory for the progress log
    fixer = PytestErrorFixer("test_project_dir")
    fixer.progress_log = tmp_path / "test_progress_log.json"
    fixer.error_log = tmp_path / "test_error_log.json"
    return fixer

@pytest.fixture
def sample_errors():
    # Sample errors for testing
    return {
        "gpt2_arc/test_file.py": [
            "test_function AssertionError: assert 1 == 2",
            "test_another_function TypeError: unsupported operand type(s) for +: 'int' and 'str'"
        ]
    }

# 1. Test for progress log initialization
def test_init_progress_log(error_fixer):
    error_fixer.init_progress_log()
    assert os.path.exists(error_fixer.progress_log)
    with open(error_fixer.progress_log, 'r') as f:
        assert json.load(f) == []  # Ensure the log is empty upon initialization

# 2. Test for logging progress in the progress log
def test_log_progress(error_fixer):
    error_fixer.init_progress_log()
    error_fixer.log_progress("fixed", "test error", "test_file.py")
    with open(error_fixer.progress_log, 'r') as f:
        log = json.load(f)
        assert len(log) == 1
        assert log[0] == {"error": "test error", "file": "test_file.py", "status": "fixed"}

# 3. Test for running full test suite and capturing output
@patch('subprocess.run')
def test_run_full_test(mock_run, error_fixer):
    # Mock the output of subprocess.run to simulate pytest execution
    mock_run.return_value = MagicMock(stdout="Test output", stderr="Test error")
    stdout, stderr = error_fixer.run_full_test()
    
    # Assert that stdout and stderr are captured correctly
    assert stdout == "Test output"
    assert stderr == "Test error"
    mock_run.assert_called_once()

# 4. Test for parsing errors from pytest output
def test_parse_errors(error_fixer):
    # Simulate pytest output with multiple errors
    sample_output = """
    gpt2_arc/test_file.py::test_function FAILED
    gpt2_arc/another_file.py::test_another_function FAILED
    """
    errors = error_fixer.parse_errors(sample_output)
    
    # Verify that errors are correctly parsed and associated with the right test files
    assert "gpt2_arc/test_file.py" in errors
    assert "gpt2_arc/another_file.py" in errors
    assert "test_function FAILED" in errors["gpt2_arc/test_file.py"]
    assert "test_another_function FAILED" in errors["gpt2_arc/another_file.py"]

# 5. Test for saving and loading errors to/from a JSON file
def test_save_and_load_errors(error_fixer, sample_errors):
    # Save errors to a file
    error_fixer.save_errors(sample_errors)
    
    # Load errors back and verify they match the original data
    loaded_errors = error_fixer.load_errors()
    assert loaded_errors == sample_errors

# 6. Test for predicting relevant files using aider's output
@patch.object(PytestErrorFixer, 'coder')
def test_predict_relevant_files(mock_coder, error_fixer):
    # Mock aider's file prediction output
    mock_coder.run.return_value = "The files likely involved are gpt2_arc/file1.py and gpt2_arc/file2.py"
    
    # Predict files for a test error
    files = error_fixer.predict_relevant_files("test error")
    
    # Assert that the correct files are predicted
    assert files == ["gpt2_arc/file1.py", "gpt2_arc/file2.py"]
    mock_coder.run.assert_called_once()

# 7. Test for fixing errors and retrying if needed
@patch('subprocess.run')
@patch.object(PytestErrorFixer, 'coder')
def test_fix_error(mock_coder, mock_run, error_fixer):
    # Simulate failed and successful pytest runs
    mock_run.side_effect = [
        MagicMock(stdout="Test failed", stderr="Error occurred"),
        MagicMock(stdout="Test PASSED", stderr="")
    ]
    
    # Simulate aider suggesting fixes
    mock_coder.run.return_value = "Suggested fix"
    
    # Run the fix_error method and verify it retries and eventually succeeds
    result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function")
    
    # Assert that the error is eventually fixed
    assert result == True
    assert mock_run.call_count == 2
    mock_coder.run.assert_called_once()

# 8. Edge case: Test for handling invalid error output (additional coverage)
def test_parse_errors_invalid_format(error_fixer):
    invalid_output = "This is not a valid pytest output"
    errors = error_fixer.parse_errors(invalid_output)
    assert errors == {}

# 9. Edge case: Test for retry exhaustion when errors remain unfixed
@patch('subprocess.run')
@patch.object(PytestErrorFixer, 'coder')
def test_retry_exhaustion(mock_coder, mock_run, error_fixer):
    # Simulate constant failure in pytest runs
    mock_run.side_effect = [
        MagicMock(stdout="Test failed", stderr="Error occurred")
    ] * 3  # Retry the maximum number of times
    
    mock_coder.run.return_value = "Suggested fix"
    
    # Run the fix_error method and ensure it retries up to the max limit
    result = error_fixer.fix_error("gpt2_arc/test_file.py", "test_function")
    
    # Assert that the retries are exhausted
    assert result == False
    assert mock_run.call_count == 3  # Ensure the retry mechanism works
    mock_coder.run.assert_called_once()

</file>
<file name="tests/test_trainer.py">
# gpt2_arc/tests/test_trainer.py
import pytest
import torch
from src.config import Config, ModelConfig, TrainingConfig
from src.data.arc_dataset import ARCDataset
from src.models.gpt2 import GPT2ARC
from src.training.trainer import ARCTrainer


@pytest.fixture
def sample_data():
    return [
        {
            "train": [
                {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]}
            ],
            "test": [
                {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]}
            ]
        }
    ]


@pytest.fixture
def model():
    config = ModelConfig()
    return GPT2ARC(config)


@pytest.fixture
def trainer(model, sample_data):
    config = Config(model=ModelConfig(), training=TrainingConfig())
    train_dataset = ARCDataset(sample_data)
    val_dataset = ARCDataset(sample_data)
    trainer = ARCTrainer(model, train_dataset, val_dataset, config)
    trainer.logged_metrics = {}
    trainer.config.training.log_level = "INFO"  # Add this line
    trainer.log = lambda name, value, on_step=None, on_epoch=None, prog_bar=None, logger=None: trainer.logged_metrics.update({name: value})
    return trainer


def test_arctrainer_initialization(trainer):
    assert isinstance(trainer, ARCTrainer)
    assert hasattr(trainer, "model")
    assert hasattr(trainer, "train_dataset")
    assert hasattr(trainer, "val_dataset")


def test_arctrainer_forward_pass(trainer):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    input_ids = torch.randint(0, 2, (batch_size, seq_length))
    attention_mask = torch.ones((batch_size, seq_length))

    output = trainer(input_ids, attention_mask)

    assert isinstance(output, torch.Tensor)
    assert output.shape == (batch_size, seq_length, trainer.model.config.n_embd)


@pytest.mark.parametrize("batch_format", ["tuple", "dict"])
def test_arctrainer_training_step(trainer, batch_format):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    vocab_size = 10  # Use a small vocab size for testing
    if batch_format == "tuple":
        batch = (
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            torch.ones((batch_size, seq_length)).float(),
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        )
    else:
        batch = {
            "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            "attention_mask": torch.ones((batch_size, seq_length)).float(),
            "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        }
    loss = trainer.training_step(batch, 0)

    assert isinstance(loss, torch.Tensor)
    assert loss.shape == torch.Size([])
    assert not torch.isnan(loss).any(), "Loss contains NaN values"
    assert not torch.isinf(loss).any(), "Loss contains infinity values"


def test_training_step_with_list_input():
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=2, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    trainer = ARCTrainer(model, None, None, config)

    batch_size = 2
    vocab_size = 10

    # Create a batch as a tuple of length 3 (input_ids, attention_mask, labels)
    inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float()
    inputs_flat = inputs.view(batch_size, -1)

    attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float()

    labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long()
    labels_flat = labels.view(batch_size, -1)

    batch = (
        inputs_flat,       # input_ids
        attention_mask,    # attention_mask
        labels_flat        # labels
    )

    loss = trainer.training_step(batch, 0)

    assert isinstance(loss, torch.Tensor), "Loss should be a torch.Tensor"
    assert loss.shape == torch.Size([]), "Loss should be a scalar"
    assert not torch.isnan(loss).any(), "Loss should not be NaN"
    assert not torch.isinf(loss).any(), "Loss should not be infinity"

def test_validation_step_with_list_input():
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=2, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    trainer = ARCTrainer(model, None, None, config)

    batch_size = 2
    vocab_size = 10

    # Create inputs and labels
    inputs = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).float()
    inputs_flat = inputs.view(batch_size, -1)  # Flatten inputs

    labels = torch.randint(0, vocab_size, (batch_size, 1, 30, 30)).long()
    labels_flat = labels.view(batch_size, -1)  # Flatten labels

    # Create an attention mask
    attention_mask = torch.ones((batch_size, inputs_flat.shape[1])).float()

    # Create batch as a tuple of length 3
    batch = (
        inputs_flat,       # input_ids
        attention_mask,    # attention_mask
        labels_flat        # labels
    )

    trainer.validation_step(batch, 0)

    assert "val_loss" in trainer.logged_metrics, "Validation loss should be logged"
    assert isinstance(trainer.logged_metrics["val_loss"], float), "Logged validation loss should be a float"

@pytest.mark.parametrize("batch_format", ["tuple", "dict"])
def test_arctrainer_validation_step(trainer, batch_format):
    batch_size = 2
    seq_length = 900  # 30x30 grid
    vocab_size = 10  # Use a small vocab size for testing
    if batch_format == "tuple":
        batch = (
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            torch.ones((batch_size, seq_length)).float(),
            torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        )
    else:
        batch = {
            "input_ids": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
            "attention_mask": torch.ones((batch_size, seq_length)).float(),
            "labels": torch.randint(0, vocab_size, (batch_size, seq_length)).long(),
        }
    trainer.validation_step(batch, 0)

    # Check if val_loss is logged
    assert "val_loss" in trainer.logged_metrics


def test_arctrainer_configure_optimizers(trainer):
    optimizers, schedulers = trainer.configure_optimizers()
    assert any(isinstance(opt, torch.optim.Adam) for opt in optimizers), "Expected an Adam optimizer"
    assert any(sch['scheduler'].__class__.__name__ == 'StepLR' for sch in schedulers), "Expected a StepLR scheduler"


def test_arctrainer_train_dataloader(trainer):
    dataloader = trainer.train_dataloader()
    assert isinstance(dataloader, torch.utils.data.DataLoader)
    assert len(dataloader.dataset) == len(trainer.train_dataset)


def test_arctrainer_val_dataloader(trainer):
    dataloader = trainer.val_dataloader()
    assert isinstance(dataloader, torch.utils.data.DataLoader)
    assert len(dataloader.dataset) == len(trainer.val_dataset)
def test_arctrainer_test_step_with_task_ids(trainer):
    batch_size = 2
    height = width = 30
    num_symbols = 10
    
    # Create a mock batch
    inputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).float()
    outputs = torch.randint(0, num_symbols, (batch_size, 1, height, width)).long()
    task_ids = ['task1', 'task2']
    
    batch = (inputs, outputs, task_ids)
    
    # Run the test step
    result = trainer.test_step(batch, 0)
    
    # Check if the result contains the expected keys
    assert 'test_loss' in result
    assert 'test_accuracy' in result
    assert 'task_ids' in result
    
    # Check if 'test_loss', 'test_accuracy', 'test_diff_accuracy' are logged
    assert 'test_loss' in trainer.logged_metrics
    assert 'test_accuracy' in trainer.logged_metrics
    assert 'test_diff_accuracy' in trainer.logged_metrics

    # Check if task-specific metrics were logged
    for task_id in task_ids:
        assert f'{task_id}_test_loss' in trainer.logged_metrics
        assert f'{task_id}_test_accuracy' in trainer.logged_metrics
        assert f'{task_id}_test_diff_accuracy' in trainer.logged_metrics


</file>
<file name="tests/__init__.py">
# This file can be empty, or include initialization code if necessary

</file>
<file name="tests/test_results_collector.py">
# gpt2_arc/tests/test_results_collector.py
import unittest
from gpt2_arc.src.utils.results_collector import ResultsCollector
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig

class TestResultsCollector(unittest.TestCase):
    def setUp(self):
        model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
        training_config = TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=10)
        config = Config(model=model_config, training=training_config)
        self.results_collector = ResultsCollector(config)

    def test_initialization(self):
        self.assertIsNotNone(self.results_collector.experiment_id)
        self.assertIsNotNone(self.results_collector.timestamp)
        self.assertEqual(self.results_collector.config['model']['n_embd'], 96)

    def test_update_train_metrics(self):
        self.results_collector.update_train_metrics(1, {"loss": 0.5})
        self.assertIn(1, self.results_collector.results["train"])
        self.assertEqual(self.results_collector.results["train"][1]["loss"], 0.5)

    def test_update_val_metrics(self):
        self.results_collector.update_val_metrics(1, {"loss": 0.3})
        self.assertIn(1, self.results_collector.results["validation"])
        self.assertEqual(self.results_collector.results["validation"][1]["loss"], 0.3)

    def test_set_test_results(self):
        self.results_collector.set_test_results({"accuracy": 0.8})
        self.assertEqual(self.results_collector.results["test"]["accuracy"], 0.8)

    def test_add_task_specific_result(self):
        self.results_collector.add_task_specific_result("task_1", {"accuracy": 0.9})
        self.assertIn("task_1", self.results_collector.task_specific_results)
        self.assertEqual(self.results_collector.task_specific_results["task_1"]["accuracy"], 0.9)

    def test_get_summary(self):
        summary = self.results_collector.get_summary()
        self.assertEqual(summary["experiment_id"], self.results_collector.experiment_id)

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_pytorch_lightning_integration.py">
import unittest
import torch
from torch.utils.data import DataLoader
import pytorch_lightning as pl
from pytorch_lightning.loggers import TensorBoardLogger
from pytorch_lightning.callbacks import ModelCheckpoint
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.data.arc_dataset import ARCDataset

class TestPyTorchLightningIntegration(unittest.TestCase):
    def setUp(self):
        # Define model and training configurations
        model_config = ModelConfig(
            n_embd=16,
            n_head=2,
            n_layer=2,
            mamba_ratio=1,
            d_state=4,
            d_conv=1,
            dropout=0.05
        )
        training_config = TrainingConfig(
            batch_size=2,
            learning_rate=0.001,
            max_epochs=10,
            use_gpu=False,
            log_level="DEBUG",
            use_synthetic_data=False,
            balance_symbols=True,
            balancing_method="weighting",
            synthetic_data_path=None,
            symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05}
        )
        self.config = Config(model=model_config, training=training_config)
        self.model = GPT2ARC(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq)
        self.train_dataset = ARCDataset(data_source="path/to/mock_data")
        self.val_dataset = ARCDataset(data_source="path/to/mock_data")
        self.train_loader = DataLoader(self.train_dataset, batch_size=2)
        self.val_loader = DataLoader(self.val_dataset, batch_size=2)

    def test_pytorch_lightning_trainer_initialization(self):
        trainer = pl.Trainer(
            max_epochs=10,
            logger=TensorBoardLogger(save_dir="logs"),
            callbacks=[ModelCheckpoint(monitor="val_loss")],
            accelerator='cpu',
            devices=1
        )
        self.assertEqual(trainer.max_epochs, 10, "Trainer max_epochs mismatch.")
        self.assertIsInstance(trainer.logger, TensorBoardLogger, "Logger is not TensorBoardLogger.")
        self.assertEqual(len(trainer.callbacks), 1, "Unexpected number of callbacks initialized.")

    def test_training_loop_with_mock_data(self):
        trainer = pl.Trainer(max_epochs=1, logger=False, enable_checkpointing=False, accelerator='cpu', devices=1)
        trainer.fit(self.model, train_dataloaders=self.train_loader, val_dataloaders=self.val_loader)
        # Assert that training completed without errors
        self.assertTrue(True, "Training loop executed successfully.")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_integration_experiment.py">
# gpt2_arc/tests/test_integration_experiment.py
import sys
import os

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))
sys.path.insert(0, project_root)

import pytest
import torch
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.utils.results_collector import ResultsCollector
from pytorch_lightning import Trainer
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

import arckit

@pytest.fixture
def setup_experiment():
    # Load a sample task from arckit
    task_id = "007bbfb7"  # Example task ID
    task_data = arckit.load_single(task_id)
    train_data = task_data.train
    val_data = task_data.test
    print(f"DEBUG: task_data type: {type(task_data)}")
    print(f"DEBUG: task_data attributes: {dir(task_data)}")
    print(f"DEBUG: train_data type: {type(train_data)}")
    print(f"DEBUG: train_data content: {train_data}")
    print(f"DEBUG: val_data type: {type(val_data)}")
    print(f"DEBUG: val_data content: {val_data}")
    train_dataset = ARCDataset([{"train": train_data, "test": val_data}])
    val_dataset = ARCDataset([{"train": train_data, "test": val_data}])

    # Model and config setup
    model_config = ModelConfig(n_embd=64, n_head=2, n_layer=1)
    training_config = TrainingConfig(batch_size=1, learning_rate=1e-4, max_epochs=1)
    config = Config(model=model_config, training=training_config)
    model = GPT2ARC(config=model_config)

    # Trainer setup
    trainer = ARCTrainer(model=model, train_dataset=train_dataset, val_dataset=val_dataset, config=config)
    return trainer, config

def test_full_experiment_run(setup_experiment):
    trainer, config = setup_experiment

    # PyTorch Lightning Trainer
    pl_trainer = Trainer(
        max_epochs=config.training.max_epochs,
        logger=TensorBoardLogger("tb_logs", name="arc_model_test"),
        callbacks=[ModelCheckpoint(dirpath="checkpoints", save_top_k=1, monitor="val_loss")],
        enable_checkpointing=True,
        enable_progress_bar=False,
        fast_dev_run=True
    )

    # Run training
    pl_trainer.fit(trainer)

    # Verify results
    results_summary = trainer.results_collector.get_summary()
    assert results_summary["experiment_id"] is not None
    assert "final_train_loss" in results_summary
    assert "final_val_loss" in results_summary

@pytest.mark.parametrize("invalid_data", [
    ({"input": [[0] * 30 for _ in range(30)]}),  # Missing output
    ({"output": [[0] * 30 for _ in range(30)]}),  # Missing input
])
def test_invalid_data_handling(invalid_data):
    with pytest.raises(ValueError):
        ARCDataset([invalid_data])

def test_model_convergence_issue(setup_experiment):
    trainer, config = setup_experiment
    trainer.config.training.learning_rate = 1e-10  # Set an inappropriate learning rate

    # PyTorch Lightning Trainer
    pl_trainer = Trainer(
        max_epochs=config.training.max_epochs,
        logger=False,
        enable_checkpointing=False,
        enable_progress_bar=False,
        fast_dev_run=True
    )

    # Run training
    pl_trainer.fit(trainer)

    # Verify that the model did not converge
    results_summary = trainer.results_collector.get_summary()
    assert results_summary["final_train_loss"] is not None
    assert results_summary["final_train_loss"] &gt; 1.0  # Assuming a high loss indicates non-convergence

</file>
<file name="tests/test_model_evaluation.py">
# gpt2_arc/tests/test_model_evaluation.py
import sys
import os

# Add the root directory of the project to the PYTHONPATH
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))
sys.path.insert(0, project_root)
print("Current PYTHONPATH:", sys.path)

import sys
import os
import json
import pytest
import torch
from pytest_mock import mocker
from src.models.gpt2 import GPT2ARC
from src.config import Config, ModelConfig, TrainingConfig
from torch.utils.data import DataLoader
from src.utils.helpers import differential_pixel_accuracy
from src.training.trainer import ARCTrainer

# Add the project root to the Python path
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)
print(f"Updated Python path: {sys.path}")

@pytest.fixture
def trainer():
    model_config = ModelConfig(n_embd=96, n_head=3, n_layer=1)
    config = Config(model=model_config, training=TrainingConfig(batch_size=32, learning_rate=1e-4, max_epochs=2))
    model = GPT2ARC(config.model)
    return ARCTrainer(model, None, None, config)
import logging
from unittest.mock import Mock


# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

@pytest.fixture
def model(mocker):
    mock_model = mocker.Mock()
    mock_model.eval = mocker.Mock()
    mock_model.side_effect = lambda inputs, attention_mask=None: torch.randn(1, 4, 2, 2)
    logger.debug(f"Created mock model")
    return mock_model

@pytest.fixture
def inputs():
    # Use a predetermined input
    inputs = torch.tensor([[[[1.0, 0.0], [0.0, 1.0]]]])
    logger.debug(f"Input shape: {inputs.shape}, dtype: {inputs.dtype}")
    return inputs

@pytest.fixture
def targets():
    # Use a predetermined target
    targets = torch.tensor([[[1, 0], [0, 1]]])
    logger.debug(f"Targets shape: {targets.shape}, dtype: {targets.dtype}")
    return targets

@pytest.fixture
def attention_mask():
    mask = torch.ones(1, 4)
    logger.debug(f"Attention mask shape: {mask.shape}, dtype: {mask.dtype}")
    return mask

@pytest.fixture
def dataloader(inputs, targets, attention_mask):
    dataset = list(zip(inputs, targets, attention_mask))
    loader = DataLoader(dataset, batch_size=1)
    logger.debug(f"Dataloader created with {len(loader)} batches")
    return loader

def test_no_grad_calculation(model, inputs, attention_mask):
    logger.debug("Starting test_no_grad_calculation")
    with torch.no_grad():
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Output shape: {outputs.shape}, requires_grad: {outputs.requires_grad}")
        assert not outputs.requires_grad, "Gradients should not be tracked in evaluation mode."

def test_data_loop_for_evaluation(model, dataloader):
    logger.debug("Starting test_data_loop_for_evaluation")
    model.eval()
    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Batch {batch_idx}: Input shape: {inputs.shape}, Output shape: {outputs.shape}")
        assert outputs is not None, f"Model returned None for batch {batch_idx}"
        assert outputs.shape == (1, 4, 2, 2), f"Expected output shape (1, 4, 2, 2), got {outputs.shape}"

def test_model_predictions(model, inputs, attention_mask):
    logger.debug("Starting test_model_predictions")
    outputs = model(inputs, attention_mask=attention_mask)
    logger.debug(f"Model output shape: {outputs.shape}, dtype: {outputs.dtype}")
    initial_output = model(inputs, attention_mask=attention_mask)
    logger.debug(f"Initial output shape: {initial_output.shape}")
    
    # Change input and check if output changes
    modified_inputs = inputs + 1
    modified_output = model(modified_inputs, attention_mask=attention_mask)
    logger.debug(f"Modified output shape: {modified_output.shape}")
    
    assert not torch.allclose(initial_output, modified_output), "Output should change when input changes"

@pytest.mark.skip(reason="Needs to be checked against a known value from the ARC data")
def test_standard_pixel_accuracy(model, inputs, targets):
    logger.debug("Starting test_standard_pixel_accuracy")
    outputs = model(inputs)
    logger.debug(f"Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
    outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
    predicted = outputs.argmax(dim=1)
    accuracy = (predicted == targets).float().mean().item()
    logger.debug(f"Calculated accuracy: {accuracy}")
    assert 0.0 &lt;= accuracy &lt;= 1.0, f"Accuracy should be between 0 and 1, got {accuracy}"
    
    # Test with known values
    known_outputs = torch.FloatTensor([[[[0.9, 0.1], [0.1, 0.9]]]])
    known_targets = torch.tensor([[[0, 1], [1, 0]]])
    known_accuracy = (known_outputs.argmax(dim=1) == known_targets).float().mean().item()
    logger.debug(f"Known accuracy: {known_accuracy}")
    assert known_accuracy == 1.0, f"Expected known accuracy to be 1.0, got {known_accuracy}"

def test_differential_pixel_accuracy(model, inputs, targets):
    logger.debug("Starting test_differential_pixel_accuracy")
    outputs = model(inputs)
    logger.debug(f"Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
    outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
    predicted = outputs.argmax(dim=1)
    diff_accuracy, _, _ = differential_pixel_accuracy(inputs, targets, predicted)
    logger.debug(f"Calculated differential accuracy: {diff_accuracy}")
    assert 0.0 &lt;= diff_accuracy &lt;= 1.0, f"Differential pixel accuracy should be between 0 and 1, got {diff_accuracy}"

    # Test with known values
    known_inputs = torch.tensor([[[[1, 0], [0, 1]]]])
    known_targets = torch.tensor([[[0, 1], [1, 0]]])
    known_predicted = torch.tensor([[[0, 1], [1, 0]]])
    known_diff_accuracy, known_total_diff, known_correct_diff = differential_pixel_accuracy(known_inputs, known_targets, known_predicted)
    logger.debug(f"Known differential accuracy: {known_diff_accuracy}, Total diff: {known_total_diff}, Correct diff: {known_correct_diff}")
    assert known_diff_accuracy == 1.0, f"Expected known differential accuracy to be 1.0, got {known_diff_accuracy}"

def test_task_accuracies_tracking(model, dataloader, is_training=False):
    logger.debug("Starting test_task_accuracies_tracking")
    task_accuracies = {}
    model.eval()
    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Batch {batch_idx}: Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
        outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
        accuracy = (outputs.argmax(dim=1) == targets).float().mean().item()
        task_id = getattr(dataloader, 'task_id', 'default_task')
        if task_id not in task_accuracies:
            task_accuracies[task_id] = {'train': [], 'test': []}
        task_accuracies[task_id]['train' if is_training else 'test'].append(accuracy)
        logger.debug(f"Task accuracies after batch {batch_idx}: {task_accuracies}")
    assert task_accuracies, "Task accuracies dictionary should not be empty"
    assert 'default_task' in task_accuracies, "Default task should be logged in task accuracies"
    assert 'test' in task_accuracies['default_task'], "Test accuracies should be logged for default task"

def test_final_metric_calculation(model, dataloader, attention_mask):
    logger.debug("Starting test_final_metric_calculation")
    model.eval()
    total_loss, total_accuracy = 0, 0
    num_batches = 0
    for batch_idx, (inputs, targets, attention_mask) in enumerate(dataloader):
        outputs = model(inputs, attention_mask=attention_mask)
        logger.debug(f"Batch {batch_idx}: Outputs shape: {outputs.shape}, Targets shape: {targets.shape}")
        outputs = outputs.view(targets.shape[0], -1, targets.shape[1], targets.shape[2])
        loss = torch.nn.functional.cross_entropy(outputs.view(-1, outputs.size(1)), targets.view(-1))
        total_loss += loss.item()
        accuracy = (outputs.argmax(dim=1) == targets).float().mean().item()
        logger.debug(f"Batch {batch_idx}: Loss: {loss.item()}, Accuracy: {accuracy}")
        total_accuracy += accuracy
        num_batches += 1
    avg_loss = total_loss / num_batches
    avg_accuracy = total_accuracy / num_batches
    logger.debug(f"Final metrics - Average loss: {avg_loss}, Average accuracy: {avg_accuracy}")
    assert avg_loss &gt;= 0, f"Average loss should be non-negative, got {avg_loss}"
    assert 0.0 &lt;= avg_accuracy &lt;= 1.0, f"Average accuracy should be between 0 and 1, got {avg_accuracy}"

def test_return_of_evaluation_results(model, dataloader, mocker):
    logger.debug("Starting test_return_of_evaluation_results")
    # Simulate a simple evaluation result
    model.evaluate = lambda dataloader: {'loss': 0.5, 'accuracy': 0.75}
    results = model.evaluate(dataloader)
    logger.debug(f"Evaluation results: {results}")
    assert "loss" in results and "accuracy" in results, "Evaluation results should return loss and accuracy."
    assert isinstance(results["loss"], float), f"Loss should be a float, got {type(results['loss'])}"
    assert 0.0 &lt;= results["accuracy"] &lt;= 1.0, f"Accuracy should be between 0 and 1, got {results['accuracy']}"
def test_validation_step_with_incorrect_batch_format(trainer):
    """Test that the validation_step raises a ValueError for an incorrect batch format."""

    # Create a batch with an incorrect format (e.g., a list)
    incorrect_batch = [
        torch.randint(0, 10, (2, 900)),  # Random input data
        # Labels are missing
    ]

    logger.debug(f"Testing with incorrect batch format: {type(incorrect_batch)}")
    with pytest.raises(ValueError, match="Batch must contain inputs and labels."):
        trainer.validation_step(incorrect_batch, 0)

def test_model_loading_from_checkpoint(mocker):
    logger.debug("Starting test_model_loading_from_checkpoint")
    
    # Load the model checkpoint
    checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt"
    logger.debug(f"Attempting to load checkpoint from: {checkpoint_path}")

    # Check if the checkpoint file exists
    if not os.path.isfile(checkpoint_path):
        pytest.skip(f"Checkpoint file not found: {checkpoint_path}")

    try:
        checkpoint = torch.load(checkpoint_path)
        logger.debug(f"Checkpoint loaded successfully. Keys: {checkpoint.keys()}")
    except FileNotFoundError as e:
        logger.error(f"Failed to load checkpoint: {str(e)}")
        pytest.fail(f"Failed to load checkpoint: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        pytest.fail(f"Unexpected error: {str(e)}")

    # Extract and print the config from the checkpoint
    if 'config' in checkpoint:
        config_dict = checkpoint['config']
        logger.debug(f"Config found in checkpoint: {json.dumps(config_dict, indent=2)}")
    else:
        logger.error("Config not found in checkpoint")
        pytest.fail("Config not found in checkpoint")

    # Reconstruct ModelConfig
    try:
        model_config = ModelConfig(
            n_embd=config_dict['n_embd'],
            n_head=config_dict['n_head'],
            n_layer=config_dict['n_layer'],
            dropout=config_dict['dropout']
        )
        logger.debug(f"ModelConfig reconstructed: {model_config}")
    except KeyError as e:
        logger.error(f"Missing key in config_dict: {str(e)}")
        pytest.fail(f"Failed to reconstruct ModelConfig: {str(e)}")

    # Initialize the model
    try:
        model = GPT2ARC(model_config)
        logger.debug("Model initialized successfully")
    except Exception as e:
        logger.error(f"Failed to initialize model: {str(e)}")
        pytest.fail(f"Failed to initialize model: {str(e)}")

    # Load the state dict
    try:
        state_dict = {k.replace("model.", ""): v for k, v in checkpoint['state_dict'].items()}
        model.load_state_dict(state_dict)
        logger.debug("State dict loaded successfully")
    except Exception as e:
        logger.error(f"Failed to load state dict: {str(e)}")
        pytest.fail(f"Failed to load state dict: {str(e)}")

    # Ensure the model is in evaluation mode
    model.eval()
    assert not model.training, "Model should be in evaluation mode after calling eval()"
    
    logger.debug("Completed test_model_loading_from_checkpoint")


def test_checkpoint_contains_model_config():
    checkpoint_path = "checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt"
    logger.debug(f"Checking for checkpoint file at: {checkpoint_path}")

    if not os.path.isfile(checkpoint_path):
        logger.warning(f"Checkpoint file not found: {checkpoint_path}")
        pytest.skip(f"Checkpoint file not found: {checkpoint_path}")

    try:
        checkpoint = torch.load(checkpoint_path)
        # Log the keys in the checkpoint
        logger.debug(f"Checkpoint keys: {checkpoint.keys()}")
    except FileNotFoundError as e:
        logger.error(f"FileNotFoundError: {str(e)}")
        pytest.fail(f"FileNotFoundError: {str(e)}")
    except Exception as e:
        logger.error(f"Unexpected error: {str(e)}")
        pytest.fail(f"Unexpected error: {str(e)}")

    # Check for model configuration
    assert 'config' in checkpoint, "Model configuration not found in checkpoint."
    model_config = checkpoint['config']
    logger.debug(f"Model configuration found in checkpoint: {model_config}")
    print("Model configuration found in checkpoint:", model_config)

</file>
<file name="tests/test_checkpoint_loading.py">
import torch
import pytest

def test_actual_checkpoint_loading():
    # Path to the actual checkpoint file
    checkpoint_path = 'final_model_4fe9801e-c839-454f-a46c-6e94e3c04e81.pth'
    
    # Load the checkpoint
    checkpoint = torch.load(checkpoint_path)
    
    # Print the keys for debugging purposes
    print("Checkpoint keys:", checkpoint.keys())
    
    # Check if the checkpoint contains expected keys
    expected_keys = ['conv1.weight', 'conv1.bias', 'blocks.0.attention.key.weight']
    for key in expected_keys:
        assert key in checkpoint, f"Checkpoint does not contain expected key: {key}"

if __name__ == "__main__":
    pytest.main([__file__])

</file>
<file name="tests/test_hyperparameter_optimization.py">
import unittest
from unittest.mock import patch
import sys
import os

# Adjust the import path as necessary
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'src')))
from gpt2_arc.src.optimize_hyperparameters import run_optimization

class TestHyperparameterOptimization(unittest.TestCase):
    @patch('optuna.create_study')
    def test_run_optimization(self, mock_create_study):
        # Mock the study object and its methods
        mock_study = mock_create_study.return_value
        mock_study.optimize.return_value = None
        mock_study.best_trial = None  # Simulate no successful trials

        # Call the function with minimal parameters
        try:
            run_optimization(n_trials=1, n_jobs=1)
            success = True
        except Exception as e:
            success = False
            print(f"Optimization failed with exception: {e}")

        self.assertTrue(success, "Hyperparameter optimization should run without errors")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_optimize_hyperparameters.py">
import unittest
from unittest.mock import patch
import optuna
from gpt2_arc.src.training.train import main

class TestHyperparameterOptimization(unittest.TestCase):
    def test_load_best_hyperparameters(self):
        # Mock an Optuna study with predefined best_params
        best_params = {'n_head_exp': 2, 'n_embd_multiplier': 4, 'n_layer': 3, 'dropout': 0.1, 'batch_size': 16, 'learning_rate': 0.001}
        with patch('optuna.load_study') as mocked_load_study:
            mocked_load_study.return_value.best_params = best_params
            # Assuming a function get_best_hyperparameters exists
            best_hyperparams = main.get_best_hyperparameters(study_name="test_study", storage="sqlite:///test.db")
            self.assertEqual(best_hyperparams, best_params, "Loaded best hyperparameters do not match expected values.")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_class_weights.py">

import unittest
import torch
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.models.gpt2 import GPT2ARC

class TestClassWeights(unittest.TestCase):
    def setUp(self):
        # Define a sample symbol frequency dictionary
        self.symbol_freq = {
            "0": 0.5,
            "1": 0.2,
            "2": 0.1,
            "3": 0.1,
            "4": 0.05,
            "5": 0.05
        }
        
        # Define model and training configurations
        model_config = ModelConfig(
            n_embd=16,
            n_head=2,
            n_layer=2,
            mamba_ratio=1,
            d_state=4,
            d_conv=1,
            dropout=0.05
        )
        training_config = TrainingConfig(
            batch_size=2,
            learning_rate=0.001,
            max_epochs=10,
            use_gpu=False,
            log_level="DEBUG",
            use_synthetic_data=False,
            balance_symbols=True,
            balancing_method="weighting",
            synthetic_data_path=None,
            symbol_freq=self.symbol_freq
        )
        
        # Create a Config object
        self.config = Config(model=model_config, training=training_config)
        
        # Initialize the GPT2ARC model with symbol_freq
        self.model = GPT2ARC(config=self.config, num_classes=len(self.symbol_freq), symbol_freq=self.symbol_freq)
    
    def test_class_weights_correctness(self):
        # Calculate expected class weights
        expected_weights = {k: 1.0 / v for k, v in self.symbol_freq.items()}
        expected_weights_tensor = torch.tensor([expected_weights[str(i)] for i in range(len(self.symbol_freq))])
        
        # Retrieve class weights from the model's loss function
        if hasattr(self.model, 'loss_fn') and hasattr(self.model.loss_fn, 'weight'):
            actual_weights = self.model.loss_fn.weight
            # Assert that actual_weights matches expected_weights_tensor
            self.assertTrue(torch.allclose(actual_weights, expected_weights_tensor), 
                            f"Actual class weights {actual_weights} do not match expected {expected_weights_tensor}")
        else:
            self.fail("The model's loss function does not have 'weight' attribute.")

    def test_class_weights_application(self):
        # Create dummy outputs and labels
        outputs = torch.tensor([[0.1, 0.6, 0.3, 0.0, 0.0, 0.0],
                                [0.3, 0.3, 0.2, 0.1, 0.05, 0.05]], requires_grad=True)
        labels = torch.tensor([1, 2])
        
        # Compute loss
        loss = self.model.loss_fn(outputs, labels)
        
        # Manually calculate expected loss
        expected_loss = (-torch.log(torch.tensor(0.6)) / self.symbol_freq["1"]) + (-torch.log(torch.tensor(0.2)) / self.symbol_freq["2"])
        expected_loss = expected_loss / 2  # Average over batch
        
        # Check if calculated loss matches expected loss
        self.assertAlmostEqual(loss.item(), expected_loss.item(), places=4, 
                               msg=f"Computed loss {loss.item()} does not match expected {expected_loss.item()}")

if __name__ == '__main__':
    unittest.main()

</file>
<file name="tests/test_arc_dataset.py">
# gpt2_arc/tests/test_arc_dataset.py

import os
import numpy as np
import pytest
import torch
import random
import logging
import arckit
from torch.utils.data import DataLoader
from src.data.arc_dataset import ARCDataset, set_debug_mode
from src.utils.experiment_tracker import ExperimentTracker

# Set up logging for tests
logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)

@pytest.fixture(scope="module")
def debug_mode():
    set_debug_mode(True)
    yield
    set_debug_mode(False)
from unittest.mock import Mock
from arckit.data import TaskSet


@pytest.fixture
def sample_data():
    return [
        {'input': [[1, 0], [0, 1]], 'output': [[0, 1], [1, 0]]},
        {'input': [[0, 1], [1, 0]], 'output': [[1, 0], [0, 1]]}
    ]


@pytest.fixture
def mock_taskset():
    mock_task = Mock()
    mock_task.id = "mock_task_1"
    mock_task.train = [
        (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])),
        (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]]))
    ]
    mock_task.test = [
        (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]]))
    ]
    
    mock_taskset = Mock(spec=TaskSet)
    mock_taskset.tasks = [mock_task]
    return mock_taskset
def test_dataset_statistics_computation(sample_data):
    dataset = ARCDataset(sample_data, debug=True)
    
    # Retrieve statistics
    grid_size_stats = dataset.get_grid_size_stats()
    symbol_frequencies = dataset.get_symbol_frequencies()
    
    # Assertions for grid size statistics
    assert "max_height" in grid_size_stats, "Grid size stats should include 'max_height'"
    assert "max_width" in grid_size_stats, "Grid size stats should include 'max_width'"
    assert grid_size_stats["max_height"] == 2, "Max height should be 2 for sample data"
    assert grid_size_stats["max_width"] == 2, "Max width should be 2 for sample data"
    
    # Assertions for symbol frequencies
    expected_frequencies = {
        0: 4 / 8,  # 4 occurrences of symbol '0'
        1: 4 / 8,  # 4 occurrences of symbol '1'
        # Assuming num_symbols=10, remaining symbols have frequency 0
        2: 0.0,
        3: 0.0,
        4: 0.0,
        5: 0.0,
        6: 0.0,
        7: 0.0,
        8: 0.0,
        9: 0.0
    }
    for symbol, freq in expected_frequencies.items():
        assert symbol_frequencies.get(symbol, 0.0) == freq, f"Frequency for symbol {symbol} should be {freq}"

def test_dataset_statistics_caching(sample_data):
    dataset = ARCDataset(sample_data, debug=True)
    
    # Ensure that statistics are cached
    assert hasattr(dataset, 'statistics'), "Dataset should have a 'statistics' attribute after caching"
    assert "grid_size_stats" in dataset.statistics, "Statistics should include 'grid_size_stats'"
    assert "symbol_frequencies" in dataset.statistics, "Statistics should include 'symbol_frequencies'"

    # Reload the dataset to ensure statistics are loaded from cache
    dataset_reloaded = ARCDataset(sample_data, debug=True)
    grid_size_stats = dataset_reloaded.get_grid_size_stats()
    symbol_frequencies = dataset_reloaded.get_symbol_frequencies()
    
    # Assertions to verify consistency
    assert grid_size_stats == dataset.statistics["grid_size_stats"], "Grid size stats should match after reloading from cache"
    assert symbol_frequencies == dataset.statistics["symbol_frequencies"], "Symbol frequencies should match after reloading from cache"
    dataset = ARCDataset(sample_data, debug=True)
    logger.debug(f"Dataset length: {len(dataset)}, expected: {len(sample_data)}")
    assert len(dataset) == len(sample_data), "Dataset length mismatch"
    
    input_grid, output_grid, *_ = dataset[0]
    logger.debug(f"Input grid shape: {input_grid.shape}, expected: (1, 30, 30)")
    logger.debug(f"Output grid shape: {output_grid.shape}, expected: (1, 30, 30)")
    
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    
    # Update the shape check to match the new preprocessing logic
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Verify that the original data is preserved in the center of the padded grid
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    
    logger.debug(f"Center input:\n{center_input}")
    logger.debug(f"Center output:\n{center_output}")
    
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"
    dataset = ARCDataset(sample_data)
    assert len(dataset) == 2, "Dataset should have 2 samples"
    
    input_grid, output_grid, *_ = dataset[0]
    
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    
    # Update the shape check to match the new preprocessing logic
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Verify that the original data is preserved in the center of the padded grid
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"

#Skip
@pytest.mark.skip(reason="Skipping test for synthetic data because test is problematic")
def test_arc_dataset_synthetic_data(debug_mode):
    synthetic_data_path = "/Volumes/Totallynotaharddrive/arc-neural-reasoning-model/syntheticARC/tasks"
    assert os.path.isdir(synthetic_data_path), f"Directory does not exist: {synthetic_data_path}"
    train_dataset = ARCDataset(synthetic_data_path, is_test=False, debug=True)
    test_dataset = ARCDataset(synthetic_data_path, is_test=True, debug=True)

    assert len(train_dataset) &gt; 0, "Synthetic train dataset should not be empty"
    assert len(test_dataset) &gt; 0, "Synthetic test dataset should not be empty"
    logger.debug(f"Loaded {len(train_dataset.data)} synthetic tasks")
    logger.debug(f"Total train dataset length: {len(train_dataset)}")
    logger.debug(f"Total test dataset length: {len(test_dataset)}")

    total_train = sum(len(task['train']) for task in train_dataset.data)
    total_test = sum(len(task['test']) for task in test_dataset.data)
    logger.debug(f"Total train samples: {total_train}")
    logger.debug(f"Total test samples: {total_test}")

    for i, task in enumerate(train_dataset.data):
        print(f"Task {i} - Train samples: {len(task['train'])}, Test samples: {len(task['test'])}")

    assert len(train_dataset) == total_train, f"Train dataset length ({len(train_dataset)}) should match total train samples ({total_train})"
    assert len(test_dataset) == total_test, f"Test dataset length ({len(test_dataset)}) should match total test samples ({total_test})"

    if len(train_dataset) == 0:
        pytest.skip("Train dataset is empty; skipping random sample tests.")

    print(f"Train dataset size: {len(train_dataset)}")
    print(f"Test dataset size: {len(test_dataset)}")
    
    if len(train_dataset) &lt; 3:
        pytest.skip("Not enough data in the train dataset for random sampling tests.")
    
    # Test a few random samples from the train dataset
    for i in range(3):
        idx = random.choice(range(len(train_dataset)))
        try:
            print(f"\nTrain Sample {i + 1}:")
            print(f"Generated index: {idx}")
            input_grid, output_grid = train_dataset[idx]
            print(f"Input grid shape: {input_grid.shape}")
            print(f"Output grid shape: {output_grid.shape}")
        except IndexError as e:
            print(f"Error: Attempted to access index {idx} which is out of range. Train dataset size is {len(train_dataset)}.")
            pytest.fail(f"Generated index {idx} out of range for train dataset size {len(train_dataset)}: {str(e)}")

    # Verify grid sizes
    max_h, max_w = train_dataset.max_grid_size
    assert max_h &gt; 0 and max_w &gt; 0, "Grid size should be positive"
    print(f"Maximum grid size: {train_dataset.max_grid_size}")

    # Verify access to train and test splits
    assert len(train_dataset.data) &gt; 0, "Dataset should contain at least one task"
    assert 'train' in train_dataset.data[0], "Each task should have a 'train' split"
    assert 'test' in train_dataset.data[0], "Each task should have a 'test' split"

    print(f"Train dataset length: {len(train_dataset)}")
    print(f"Test dataset length: {len(test_dataset)}")





def test_data_loader_parameters(sample_data):                         
    dataset = ARCDataset(sample_data)
    num_workers = 4  # Example value
    prefetch_factor = 2
    persistent_workers = True

    train_loader = DataLoader(
        dataset,
        batch_size=2,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers
    )

    assert train_loader.num_workers == num_workers, "num_workers should be set correctly"
    assert train_loader.prefetch_factor == prefetch_factor, "prefetch_factor should be set correctly"
    assert train_loader.persistent_workers == persistent_workers, "persistent_workers should be set correctly"

def test_dataloader_loading(sample_data):
    dataset = ARCDataset(sample_data)
    data_loader = DataLoader(
        dataset,
        batch_size=1,
        num_workers=2,
        prefetch_factor=1,
        persistent_workers=True
    )

    for batch in data_loader:
        inputs, outputs, task_ids = batch
        assert inputs.shape == (1, 1, 30, 30), "Input shape mismatch"
        assert outputs.shape == (1, 1, 30, 30), "Output shape mismatch"
    dataset = ARCDataset(sample_data)
    input_grid, output_grid, *_ = dataset[0]

    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"

    # Check if the original data is preserved in the center
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"


def test_arc_dataset_len(sample_data):
    print("Debugging: Entering test_arc_dataset_len")
    print(f"Debugging: sample_data = {sample_data}")
    dataset = ARCDataset(sample_data)
    print(f"Debugging: len(dataset) = {len(dataset)}, len(sample_data) = {len(sample_data)}")
    assert len(dataset) == len(sample_data), "Dataset length should match input data length"
    print("Debugging: Exiting test_arc_dataset_len")


def test_arc_dataset_invalid_data(sample_data):
    invalid_data = [{"input": [1, 0], "output": [[0, 1], [1, 0]]}]
    with pytest.raises(ValueError):
        ARCDataset(invalid_data)

    invalid_data = [{"input": [[1, 0], [0, 1]], "output": "not a list"}]
    with pytest.raises(ValueError):
        ARCDataset(invalid_data)

def test_arc_dataset_preprocess_grid(sample_data):
    dataset = ARCDataset(sample_data, num_symbols=10)
    input_grid, output_grid, *_ = dataset[0]

    print(f"Input grid shape: {input_grid.shape}")
    print(f"Output grid shape: {output_grid.shape}")
    print(f"Input grid content:\n{input_grid}")
    print(f"Output grid content:\n{output_grid}")

    # Check that the grids are indeed 3D
    assert input_grid.ndim == 3, f"Expected 3D input grid, got {input_grid.ndim}D"
    assert output_grid.ndim == 3, f"Expected 3D output grid, got {output_grid.ndim}D"

    # Check the shape (1, 30, 30)
    assert input_grid.shape == (1, 30, 30), f"Preprocessed grid should have shape (1, 30, 30), but got {input_grid.shape}"
    assert output_grid.shape == (1, 30, 30), f"Preprocessed grid should have shape (1, 30, 30), but got {output_grid.shape}"

    # Check if the original data is preserved in the center
    expected_input = torch.zeros((1, 30, 30))
    expected_input[0, 14:16, 14:16] = torch.tensor([[1., 0.], [0., 1.]])

    expected_output = torch.zeros((1, 30, 30))
    expected_output[0, 14:16, 14:16] = torch.tensor([[0., 1.], [1., 0.]])

    print(f"Expected input:\n{expected_input}")
    print(f"Expected output:\n{expected_output}")

    assert torch.allclose(input_grid, expected_input), "Input grid data mismatch"
    assert torch.allclose(output_grid, expected_output), "Output grid data mismatch"

@pytest.fixture
def mock_taskset():
    mock_task = Mock()
    mock_task.id = "mock_task_1"
    mock_task.train = [
        (np.array([[1, 0], [0, 1]]), np.array([[0, 1], [1, 0]])),
        (np.array([[0, 1], [1, 0]]), np.array([[1, 0], [0, 1]]))
    ]
    mock_task.test = [
        (np.array([[1, 1], [0, 0]]), np.array([[0, 0], [1, 1]]))
    ]
    
    mock_taskset = Mock(spec=TaskSet)
    mock_taskset.tasks = [mock_task]
    return mock_taskset
def test_experiment_tracker_logs_dataset_statistics(sample_data):
    # Initialize ARCDataset
    dataset = ARCDataset(sample_data, debug=True)
    
    # Initialize ExperimentTracker with mock config
    mock_config = {"dummy_key": "dummy_value"}
    tracker = ExperimentTracker(config=mock_config, project="test_project")
    
    # Retrieve dataset statistics
    train_grid_stats = dataset.get_grid_size_stats()
    train_symbol_freq = dataset.get_symbol_frequencies()
    val_grid_stats = dataset.get_grid_size_stats()  # Assuming same dataset for simplicity
    val_symbol_freq = dataset.get_symbol_frequencies()
    
    # Log metrics
    tracker.log_metric("train_max_grid_height", train_grid_stats.get("max_height", 0))
    tracker.log_metric("train_max_grid_width", train_grid_stats.get("max_width", 0))
    tracker.log_metric("train_symbol_frequencies", train_symbol_freq)

    tracker.log_metric("val_max_grid_height", val_grid_stats.get("max_height", 0))
    tracker.log_metric("val_max_grid_width", val_grid_stats.get("max_width", 0))
    tracker.log_metric("val_symbol_frequencies", val_symbol_freq)
    
    # Assertions to verify metrics are logged correctly
    assert tracker.metrics["train_max_grid_height"] == train_grid_stats.get("max_height", 0)
    assert tracker.metrics["train_max_grid_width"] == train_grid_stats.get("max_width", 0)
    assert tracker.metrics["train_symbol_frequencies"] == train_symbol_freq

    assert tracker.metrics["val_max_grid_height"] == val_grid_stats.get("max_height", 0)
    assert tracker.metrics["val_max_grid_width"] == val_grid_stats.get("max_width", 0)
    assert tracker.metrics["val_symbol_frequencies"] == val_symbol_freq
def test_collate_fn_output(): 
    sample_data = [
        {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]},
        {"input": [[0, 1], [1, 0]], "output": [[1, 0], [0, 1]]},
    ]
    dataset = ARCDataset(sample_data)
    dataloader = DataLoader(dataset, batch_size=2, collate_fn=ARCDataset.collate_fn)
    batch = next(iter(dataloader))

    assert isinstance(batch, list), "Collate function should return a list"
    assert len(batch) == 3, "Collate function should return a list with 3 elements"
    assert isinstance(batch[0], torch.Tensor), "First element should be a tensor (inputs)"
    assert isinstance(batch[1], torch.Tensor), "Second element should be a tensor (outputs)"
    assert batch[0].shape == (2, 1, 30, 30), "Input tensor should have shape (batch_size, 1, 30, 30)"
    assert batch[1].shape == (2, 1, 30, 30), "Output tensor should have shape (batch_size, 1, 30, 30)"
    assert batch[0].dtype == torch.float32, "Input tensor should be of type float32"
    assert batch[1].dtype == torch.float32, "Output tensor should be of type float32"

def test_getitem_output():
    sample_data = [
        {"input": [[1, 0], [0, 1]], "output": [[0, 1], [1, 0]]},
    ]
    dataset = ARCDataset(sample_data)
    input_grid, output_grid, *_ = dataset[0]

    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    assert input_grid.dtype == torch.float32, "Input grid should be float32"
    assert output_grid.dtype == torch.float32, "Output grid should be float32"

#Skip
@pytest.mark.skip(reason="Skipping because test is problematic")
def test_arc_dataset_taskset_initialization(mock_taskset):
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logger = logging.getLogger(__name__)
    
    logger.debug(f"Mock TaskSet: {mock_taskset}")
    logger.debug(f"Mock TaskSet attributes: {dir(mock_taskset)}")
    
    print(f"Mock task train data: {mock_taskset.tasks[0].train}")
    print(f"Mock task test data: {mock_taskset.tasks[0].test}")
    dataset = ARCDataset(mock_taskset)
    
    logger.debug(f"Dataset length: {len(dataset)}")
    print(f"Dataset length: {len(dataset)}, Expected: 3")
    
    assert len(dataset) == 3, "Dataset should have 3 samples (2 train + 1 test)"
    input_grid, output_grid, *_ = dataset[0]
    print(f"Input grid shape: {input_grid.shape}, Expected: (1, 30, 30)")
    print(f"Output grid shape: {output_grid.shape}, Expected: (1, 30, 30)")
    
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Check if the original data is preserved in the center
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    print(f"Center input: {center_input}")
    print(f"Center output: {center_output}")
    
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"
    import logging
    logging.basicConfig(level=logging.DEBUG)
    logger = logging.getLogger(__name__)
    
    logger.debug(f"Mock TaskSet: {mock_taskset}")
    logger.debug(f"Mock TaskSet attributes: {dir(mock_taskset)}")
    
    dataset = ARCDataset(mock_taskset)
    
    logger.debug(f"Dataset length: {len(dataset)}")
    
    assert len(dataset) == 3, "Dataset should have 3 samples (2 train + 1 test)"
    input_grid, output_grid = dataset[0]
    assert isinstance(input_grid, torch.Tensor), "Input should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output should be a torch.Tensor"
    assert input_grid.shape == (1, 30, 30), "Input grid should have shape (1, 30, 30)"
    assert output_grid.shape == (1, 30, 30), "Output grid should have shape (1, 30, 30)"
    
    # Check if the original data is preserved in the center
    center_input = input_grid[0, 14:16, 14:16]
    center_output = output_grid[0, 14:16, 14:16]
    assert torch.allclose(center_input, torch.tensor([[1., 0.], [0., 1.]])), "Input data not preserved correctly"
    assert torch.allclose(center_output, torch.tensor([[0., 1.], [1., 0.]])), "Output data not preserved correctly"

from torch.utils.data import DataLoader

def test_arc_dataset_collate_fn(sample_data):
    logger.debug("Starting test_arc_dataset_collate_fn")
    dataset = ARCDataset(sample_data)
    dataloader = DataLoader(dataset, batch_size=2, collate_fn=ARCDataset.collate_fn)
    batch = next(iter(dataloader))
    input_batch, output_batch, *_ = batch
    logger.debug(f"Collated batch shapes - inputs: {input_batch.shape}, outputs: {output_batch.shape}")
    assert input_batch.shape == (2, 1, 30, 30), "Batched input should have shape (2, 1, 30, 30)"
    assert output_batch.shape == (2, 1, 30, 30), "Batched output should have shape (2, 1, 30, 30)"
    logger.debug("Completed test_arc_dataset_collate_fn")

def test_arc_dataset_variable_size_grids(sample_data):
    logger.debug("Starting test_arc_dataset_variable_size_grids")
    variable_data = sample_data + [{"input": [[1, 0, 2], [0, 2, 1], [2, 1, 0]], "output": [[2, 1, 0], [1, 0, 2], [0, 2, 1]]}]
    dataset = ARCDataset(variable_data)
    
    # Check first sample (2x2)
    input_grid_1, output_grid_1, *_ = dataset[0]
    assert input_grid_1.shape == (1, 30, 30), "First sample should have shape (1, 30, 30)"
    assert output_grid_1.shape == (1, 30, 30), "First sample should have shape (1, 30, 30)"
    
    # Check center of first sample (2x2)
    center_input_1 = input_grid_1[0, 14:16, 14:16]
    center_output_1 = output_grid_1[0, 14:16, 14:16]
    assert torch.allclose(center_input_1, torch.tensor([[1., 0.], [0., 1.]])), "First sample input data not preserved correctly"
    assert torch.allclose(center_output_1, torch.tensor([[0., 1.], [1., 0.]])), "First sample output data not preserved correctly"
    
    # Check third sample (3x3)
    input_grid_2, output_grid_2, *_ = dataset[2]
    assert input_grid_2.shape == (1, 30, 30), "Third sample should have shape (1, 30, 30)"
    assert output_grid_2.shape == (1, 30, 30), "Third sample should have shape (1, 30, 30)"
    
    # Check center of third sample (3x3)
    center_input_2 = input_grid_2[0, 13:16, 13:16]
    center_output_2 = output_grid_2[0, 13:16, 13:16]
    assert torch.allclose(center_input_2, torch.tensor([[1., 0., 2.], [0., 2., 1.], [2., 1., 0.]])), f"Third sample input data not preserved correctly. Got:\n{center_input_2}"
    assert torch.allclose(center_output_2, torch.tensor([[2., 1., 0.], [1., 0., 2.], [0., 2., 1.]])), f"Third sample output data not preserved correctly. Got:\n{center_output_2}"
    
    logger.debug("Completed test_arc_dataset_variable_size_grids")

def test_arc_dataset_with_arckit_data_get_task_id():
    # Load data using arckit
    train_set, _ = arckit.load_data()

    # Initialize the dataset
    dataset = ARCDataset(train_set, is_test=False)

    # Check that __getitem__ returns the correct structure
    input_grid, output_grid, task_id = dataset[0]
    assert isinstance(input_grid, torch.Tensor), "Input grid should be a torch.Tensor"
    assert isinstance(output_grid, torch.Tensor), "Output grid should be a torch.Tensor"
    assert isinstance(task_id, str), "Task ID should be a string"

    # Test the collate_fn
    batch = [dataset[i] for i in range(2)]  # Create a batch of two samples
    collated_inputs, collated_outputs, collated_task_ids = ARCDataset.collate_fn(batch)

    assert len(collated_task_ids) == 2, "Batch size should be 2"
    assert collated_inputs.shape[0] == 2, "Batch size should be 2"
    assert collated_outputs.shape[0] == 2, "Batch size should be 2"

</file>
<file name="tests/test_arc_trainer.py">
import unittest
import torch
from torch.utils.data import DataLoader
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.data.arc_dataset import ARCDataset

class TestARCTrainer(unittest.TestCase):
    def setUp(self):
        # Define model and training configurations
        model_config = ModelConfig(
            n_embd=16,
            n_head=2,
            n_layer=2,
            mamba_ratio=1,
            d_state=4,
            d_conv=1,
            dropout=0.05
        )
        training_config = TrainingConfig(
            batch_size=2,
            learning_rate=0.001,
            max_epochs=10,
            use_gpu=False,
            log_level="DEBUG",
            use_synthetic_data=False,
            balance_symbols=True,
            balancing_method="weighting",
            synthetic_data_path=None,
            symbol_freq={"0": 0.5, "1": 0.2, "2": 0.1, "3": 0.1, "4": 0.05, "5": 0.05}
        )
        self.config = Config(model=model_config, training=training_config)
        self.model = GPT2ARC(config=self.config, num_classes=6, symbol_freq=self.config.training.symbol_freq)
        self.train_dataset = ARCDataset(data_source="path/to/mock_data")
        self.val_dataset = ARCDataset(data_source="path/to/mock_data")

    def test_training_step_computes_loss(self):
        trainer = ARCTrainer(model=self.model, train_dataset=self.train_dataset, val_dataset=self.val_dataset, config=self.config)
        batch = next(iter(DataLoader(self.train_dataset, batch_size=2)))
        loss = trainer.training_step(batch, batch_idx=0)
        self.assertIsInstance(loss, torch.Tensor, "Loss should be a torch.Tensor instance.")
        self.assertGreater(loss.item(), 0, "Loss should be positive.")

    def test_configure_optimizers(self):
        trainer = ARCTrainer(model=self.model, train_dataset=self.train_dataset, val_dataset=self.val_dataset, config=self.config)
        optimizers, schedulers = trainer.configure_optimizers()
        self.assertIsInstance(optimizers[0], torch.optim.Adam, "Optimizer should be Adam.")
        self.assertIsInstance(schedulers[0]['scheduler'], torch.optim.lr_scheduler.StepLR, "Scheduler should be StepLR.")

if __name__ == '__main__':
    unittest.main()
</file>
<file name="tests/test_metrics.py">
# gpt2_arc/tests/test_metrics.py                                                                                                                                     
                                                                                                                                                                    
import pytest                                                                                                                                                        
import torch                                                                                                                                                         
from gpt2_arc.src.models.gpt2 import GPT2ARC                                                                                                                         
from gpt2_arc.src.config import Config, TrainingConfig, ModelConfig                                                                                                  
                                                                                                                                                                    
def generate_test_data(num_samples=100):                                                                                                                             
    """                                                                                                                                                              
    Generates synthetic predictions and targets for testing.                                                                                                         
    Returns:                                                                                                                                                         
        preds (torch.Tensor): Predicted class indices.                                                                                                               
        targets (torch.Tensor): Ground truth class indices.                                                                                                          
    """                                                                                                                                                              
    preds = torch.randint(0, 3, (num_samples,))                                                                                                                      
    targets = torch.randint(0, 3, (num_samples,))                                                                                                                    
    return preds, targets                                                                                                                                            
                                                                                                                                                                    
@pytest.fixture                                                                                                                                                      
def model():                                                                                                                                                         
    config = Config(                                                                                                                                                 
        model=ModelConfig(n_embd=4, n_head=2, n_layer=3, dropout=0.1, mamba_ratio=1.0, d_state=2, d_conv=4),                                                         
        training=TrainingConfig(balance_symbols=True, balancing_method="weighting"),                                                                                 
        evaluation=None                                                                                                                                              
    )                                                                                                                                                                
    symbol_freq_dict = {'0': 0.5, '1': 0.3, '2': 0.2}                                                                                                                
    return GPT2ARC(config=config, num_classes=3, symbol_freq=symbol_freq_dict)                                                                                       
                                                                                                                                                                    
def test_training_metrics(model):                                                                                                                                    
    preds, targets = generate_test_data()                                                                                                                            
    # Mock a batch                                                                                                                                                   
    batch = (torch.tensor([0]*len(preds)), targets, ['task']*len(preds))                                                                                             
                                                                                                                                                                    
    # Perform a training step                                                                                                                                        
    loss = model.training_step(batch, batch_idx=0)                                                                                                                   
                                                                                                                                                                    
    # Check that metrics are computed and logged                                                                                                                     
    assert 'train_loss' in model.logged_metrics                                                                                                                      
    assert 'train_precision' in model.logged_metrics                                                                                                                 
    assert 'train_recall' in model.logged_metrics                                                                                                                    
    assert 'train_f1' in model.logged_metrics                                                                                                                        
                                                                                                                                                                    
    # Verify that the loss is a scalar tensor                                                                                                                        
    assert isinstance(loss, torch.Tensor)                                                                                                                            
    assert loss.dim() == 0, "Loss should be a scalar tensor"                                                                                                         
                                                                                                                                                                    
def test_validation_metrics(model):                                                                                                                                  
    preds, targets = generate_test_data()                                                                                                                            
    # Mock a batch                                                                                                                                                   
    batch = (torch.tensor([0]*len(preds)), targets, ['task']*len(preds))                                                                                             
                                                                                                                                                                    
    # Perform a validation step                                                                                                                                      
    model.validation_step(batch, batch_idx=0)                                                                                                                        
                                                                                                                                                                    
    # Check that metrics are computed and logged                                                                                                                     
    assert 'val_loss' in model.logged_metrics                                                                                                                        
    assert 'val_precision' in model.logged_metrics                                                                                                                   
    assert 'val_recall' in model.logged_metrics                                                                                                                      
    assert 'val_f1' in model.logged_metrics             
</file>
<file name="tests/test_grokfast.py">
import pytest
from collections import deque
from gpt2_arc.src.utils.grokfast import gradfilter_ma, gradfilter_ema
from gpt2_arc.src.utils.grokfast_callback import GrokfastCallback
import torch.nn as nn
import torch
from unittest.mock import MagicMock

def test_gradfilter_ma():
    # Initialize a simple model
    model = nn.Linear(10, 10)
    
    # Create dummy gradients and initialize the grads dictionary
    initial_grad = torch.ones(10, 10)
    model.weight.grad = initial_grad.clone()
    grads = {name: deque(maxlen=5) for name, param in model.named_parameters() if param.requires_grad}
    
    # Apply gradfilter_ma
    updated_grads = gradfilter_ma(
        m=model,
        grads=grads,
        window_size=5,
        lamb=2.0,
        filter_type='mean',
        warmup=False,
        trigger=False
    )
    
    # Assertions to verify gradients are updated correctly
    assert "weight" in updated_grads
    assert len(updated_grads["weight"]) == 1
    expected_grad = initial_grad + (initial_grad * 2.0)
    assert torch.allclose(model.weight.grad, expected_grad)

def test_gradfilter_ema():
    # Initialize a simple model
    model = nn.Linear(10, 10)
    
    # Create dummy gradients and initialize the grads dictionary
    initial_grad = torch.ones(10, 10)
    model.weight.grad = initial_grad.clone()
    grads = None  # For EMA, grads start as None
    
    # Apply gradfilter_ema for the first time
    updated_grads = gradfilter_ema(
        m=model,
        grads=grads,
        alpha=0.9,
        lamb=2.0
    )
    
    # Expected gradient after first update
    expected_grad = initial_grad * 2.0
    assert "weight" in updated_grads
    assert torch.allclose(model.weight.grad, expected_grad)
    
    # Apply gradfilter_ema again with new gradients
    new_grad = torch.ones(10, 10) * 2.0
    model.weight.grad = new_grad.clone()
    updated_grads = gradfilter_ema(
        m=model,
        grads=updated_grads,
        alpha=0.9,
        lamb=2.0
    )
    
    # Expected gradient after second update
    expected_grad = new_grad + (updated_grads["weight"] * 2.0)
    assert torch.allclose(model.weight.grad, expected_grad)

class TestGrokfastCallback:
    def test_grokfast_callback_ema(self):
        # Initialize a simple model
        model = nn.Linear(10, 10)
        
        # Initialize the callback with EMA settings
        callback = GrokfastCallback(
            filter_type='ema',
            alpha=0.9,
            lamb=2.0,
            window_size=100,
            warmup=True,
            trigger=False
        )
        
        # Mock trainer and pl_module
        trainer = MagicMock()
        pl_module = MagicMock()
        pl_module.model = model
        
        # Set initial gradients
        initial_grad = torch.ones(10, 10)
        model.weight.grad = initial_grad.clone()
        
        # Call the callback after backward
        callback.on_after_backward(trainer, pl_module)
        
        # Expected gradient update
        expected_grad = initial_grad + (initial_grad * 2.0)
        assert torch.allclose(model.weight.grad, expected_grad)
    
    def test_grokfast_callback_ma(self):
        # Initialize a simple model
        model = nn.Linear(10, 10)
        
        # Initialize the callback with MA settings
        callback = GrokfastCallback(
            filter_type='ma',
            alpha=0.9,  # Not used for MA but required parameter
            lamb=2.0,
            window_size=5,
            warmup=False,
            trigger=False
        )
        
        # Mock trainer and pl_module
        trainer = MagicMock()
        pl_module = MagicMock()
        pl_module.model = model
        
        # Set initial gradients
        initial_grad = torch.ones(10, 10)
        model.weight.grad = initial_grad.clone()
        
        # Call the callback after backward
        callback.on_after_backward(trainer, pl_module)
        
        # Expected gradient update
        expected_grad = initial_grad + (initial_grad * 2.0)
        assert torch.allclose(model.weight.grad, expected_grad)
        
        # Apply again to test window functionality
        new_grad = torch.ones(10, 10) * 2.0
        model.weight.grad = new_grad.clone()
        callback.on_after_backward(trainer, pl_module)
        
        # Expected gradient after second update
        expected_grad = new_grad + (initial_grad * 2.0)
        assert torch.allclose(model.weight.grad, expected_grad)

</file>
<file name="src/evaluate.py">
# gpt2_arc/src/evaluate.py
import sys
import sys
import os
import json
import argparse
import pytorch_lightning as pl
import os
import torch
import wandb
import numpy as np
from datetime import datetime
from pytorch_lightning.utilities.model_summary import ModelSummary
from torchsummary import summary
from pytorch_lightning.utilities.model_summary import ModelSummary

# Define the base directory for the arc-neural-reasoning-model
arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))

# Add the root directory of the project to the PYTHONPATH
project_root = arc_model_dir
sys.path.insert(0, project_root)

from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig, EvaluationConfig
import arckit
import logging
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.utils.helpers import differential_pixel_accuracy

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def evaluate(model, test_dataset, config, batch_size=32):
    trainer = ARCTrainer(model, None, test_dataset, config=config)
    pl_trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu')
    results = pl_trainer.test(trainer)
    logger.debug(f"DEBUG: Raw results from test: {results}")

    avg_test_loss = pl_trainer.callback_metrics.get('avg_test_loss')
    avg_test_accuracy = pl_trainer.callback_metrics.get('avg_test_accuracy')
    avg_test_diff_accuracy = pl_trainer.callback_metrics.get('avg_test_diff_accuracy')

    # Convert tensors to Python floats if necessary
    if avg_test_loss is not None:
        avg_test_loss = avg_test_loss.item()
    if avg_test_accuracy is not None:
        avg_test_accuracy = avg_test_accuracy.item()
    if avg_test_diff_accuracy is not None:
        avg_test_diff_accuracy = avg_test_diff_accuracy.item()

    aggregated_results = {
        'test_loss': avg_test_loss,
        'test_accuracy': avg_test_accuracy,
        'test_diff_accuracy': avg_test_diff_accuracy,
    }

    print(f"DEBUG: Logged metrics - Avg test loss: {avg_test_loss}, Avg test accuracy: {avg_test_accuracy}, Avg diff accuracy: {avg_test_diff_accuracy}")

    # Collect individual task metrics
    individual_metrics = {}
    for key, value in pl_trainer.callback_metrics.items():
        if '_test_accuracy' in key or '_test_diff_accuracy' in key:
            if isinstance(value, torch.Tensor):
                value = value.item()
            # Key format: 'taskid_test_accuracy' or 'taskid_test_diff_accuracy'
            task_id, metric_name = key.split('_test_')
            if task_id not in individual_metrics:
                individual_metrics[task_id] = {}
            individual_metrics[task_id][f'test_{metric_name}'] = value

    # Compute complete task accuracy (fraction of tasks with perfect accuracy)
    num_tasks = len(individual_metrics)
    perfect_accuracy_threshold = config.evaluation.perfect_accuracy_threshold / 100.0  # Convert percentage to fraction

    num_complete_accuracy = 0
    for task_id, metrics in individual_metrics.items():
        test_accuracy = metrics.get('test_accuracy', 0)
        # Determine if the task is completely solved
        completely_solved = test_accuracy &gt;= perfect_accuracy_threshold
        metrics['completely_solved'] = completely_solved
        if completely_solved:
            num_complete_accuracy += 1

    complete_task_accuracy = num_complete_accuracy / num_tasks if num_tasks &gt; 0 else 0.0
    aggregated_results['complete_task_accuracy'] = complete_task_accuracy

    print(f"DEBUG: Computed complete task accuracy: {complete_task_accuracy}")

    return aggregated_results, individual_metrics

def load_config_from_json(json_path):
    with open(json_path, 'r') as f:
        data = json.load(f)
    return data['config']

def save_results(results, individual_metrics, output_dir, model_name, model_summary):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{model_name}_eval_results_{timestamp}.json"
    output_path = os.path.join(output_dir, filename)

    data_to_save = {
        "aggregate_results": results,
        "individual_metrics": {task_id: metrics for task_id, metrics in individual_metrics.items()},
        "model_summary": str(model_summary)  # Convert ModelSummary to string
    }

    logger.debug(f"DEBUG: Data to be saved: {data_to_save}")

    os.makedirs(output_dir, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(data_to_save, f, indent=2)

    logger.info(f"Results saved to {output_path}")
    return output_path

def main(args):
    if args.use_wandb:
        api_key = os.getenv("WANDB_API_KEY")
        if api_key:
            wandb.login(key=api_key)
            wandb.init(project=args.wandb_project, name=args.wandb_run_name)
        else:
            print("WARNING: WANDB_API_KEY not found in environment variables.")
            print("Weights &amp; Biases logging is disabled.")
            args.use_wandb = False
    else:
        print("Weights &amp; Biases logging is disabled.")

    # Load the test data using arckit
    _, test_set = arckit.load_data()
    test_data = ARCDataset(test_set)

    # Compute symbol frequencies from the test dataset
    symbol_freq_array = test_data.get_symbol_frequencies()
    symbol_freq = {str(i): float(freq) for i, freq in enumerate(symbol_freq_array)}
    checkpoint = torch.load(args.model_checkpoint, map_location='cpu')

    # Extract and convert the model configuration from the checkpoint
    if 'model_config' in checkpoint:
        model_config_dict = checkpoint['model_config']
        # Convert dict to ModelConfig object
        model_config = ModelConfig(**model_config_dict)
    else:
        logger.error("Model configuration not found in checkpoint. Please ensure the checkpoint includes 'model_config'.")
        raise ValueError("Model configuration not found in checkpoint. Ensure that the training process includes the ModelConfigSaver callback.")

    # Create configuration
    config = Config(
        model=model_config,
        training=TrainingConfig(),
        evaluation=EvaluationConfig()
    )

    # Determine the number of classes from the test dataset
    max_label_test = max([sample[1].max().item() for sample in test_data])
    num_classes = int(max_label_test) + 1  # Ensure num_classes is an integer
    config.training.symbol_freq = symbol_freq

    # Initialize the model with the complete Config object and symbol frequencies
    model = GPT2ARC(config, num_classes=num_classes, symbol_freq=symbol_freq)
    try:
        # Remove the "model." prefix from state dict keys
        state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items()}
        model.load_state_dict(state_dict)
    except Exception as e:
        logger.error(f"Error while loading state_dict: {e}")
        logger.error(f"Available keys in checkpoint: {list(checkpoint.keys())}")
        raise

    model.eval()

    # Generate model summary
    print("DEBUG: Attempting to generate model summary")
    try:
        model_summary = str(ModelSummary(model, max_depth=-1))
        print("DEBUG: Model summary generated successfully")
    except Exception as e:
        print(f"DEBUG: Error generating model summary - {str(e)}")
        model_summary = "Error generating model summary"

    print("DEBUG: Model summary:")
    print(model_summary)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    logger.info(f"Model moved to device: {device}")

    # Define the input size based on your model's expected input
    sequence_length = 100  # Example value; adjust as needed
    input_size = (1, 1, sequence_length)  # Adjusted to match (batch_size, channels, sequence_length)
    logger.info(f"Defined input_size for summary: {input_size}")

    # Extract model name from the checkpoint path and sanitize it
    model_name = os.path.basename(args.model_checkpoint).split('.')[0]
    # Sanitize model_name to contain only valid characters
    model_name = ''.join(c if c.isalnum() or c in '-_.' else '_' for c in model_name)

    # Debugging statements
    logger.debug(f"Sanitized model_name: {model_name}")
    print(f"DEBUG: Sanitized model_name: {model_name}")

    # Verify that model_name contains only allowed characters
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_.")
    invalid_chars = set(model_name) - allowed_chars
    if invalid_chars:
        logger.error(f"Model name contains invalid characters after sanitization: {invalid_chars}")
        print(f"ERROR: Model name contains invalid characters after sanitization: {invalid_chars}")
    else:
        logger.debug("Model name contains only valid characters.")
        print("DEBUG: Model name contains only valid characters.")


    # Create configuration
    config = Config(
        model=model_config,
        training=TrainingConfig(),
        evaluation=EvaluationConfig()
    )

    # Evaluate the model
    results, individual_metrics = evaluate(model, test_data, config, args.batch_size)

    logger.debug(f"DEBUG: Evaluation results: {results}")
    logger.debug(f"DEBUG: Individual metrics: {individual_metrics}")

    logger.info("Evaluation Results:")
    for metric, value in results.items():
        if metric != 'complete_task_accuracy':
            print(f"{metric}: {value}")
            if args.use_wandb:
                wandb.log({f"eval/{metric}": value})

    # Print complete_task_accuracy at the bottom
    if 'complete_task_accuracy' in results:
        print(f"complete_task_accuracy: {results['complete_task_accuracy']}")
        if args.use_wandb:
            wandb.log({"eval/complete_task_accuracy": results['complete_task_accuracy']})

    # Log individual task metrics
    for task_id, metrics in individual_metrics.items():
        # Ensure metrics are not already floats
        if isinstance(metrics['test_accuracy'], list):
            metrics['test_accuracy'] = sum(metrics['test_accuracy']) / len(metrics['test_accuracy'])
        if isinstance(metrics['test_diff_accuracy'], list):
            metrics['test_diff_accuracy'] = sum(metrics['test_diff_accuracy']) / len(metrics['test_diff_accuracy'])
        logger.info(f"Task {task_id}: Accuracy = {metrics['test_accuracy']:.4f}, Diff Accuracy = {metrics['test_diff_accuracy']:.4f}")

    # Save results regardless of wandb usage
    results_path = save_results(results, individual_metrics, args.output_dir, model_name, model_summary)

    if args.use_wandb:
        # Wandb artifact creation and logging
        logger.debug(f"Creating wandb Artifact with name: {model_name}")
        print(f"DEBUG: Creating wandb Artifact with name: {model_name}")

        try:
            artifact = wandb.Artifact(name=model_name, type='evaluation')
            artifact.add_file(results_path)
            wandb.log_artifact(artifact)
            logger.debug("Artifact created and logged successfully.")
            print("DEBUG: Artifact created and logged successfully.")
        except ValueError as ve:
            logger.error(f"Failed to create wandb Artifact: {ve}")
            print(f"ERROR: Failed to create wandb Artifact: {ve}")
            raise ve

        wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the ARC Neural Reasoning Model")
    parser.add_argument("--model_checkpoint", type=str, required=True, help="Path to the model checkpoint")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for evaluation")
    parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="Directory to save evaluation results")
    parser.add_argument("--log-level", type=str, default="INFO", help="Set the logging level (e.g., DEBUG, INFO, WARNING)")
    parser.add_argument("--wandb_project", type=str, default="arc-evaluation", help="Weights &amp; Biases project name")
    parser.add_argument("--wandb_run_name", type=str, default=None, help="Weights &amp; Biases run name")

    parser.add_argument("--use_wandb", action='store_true', help="Use Weights &amp; Biases for logging")
    args = parser.parse_args()

    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)

    # Set logging level
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), None))
    
    main(args)

</file>
<file name="src/optimize_hyperparameters.py">
# gpt2_arc/src/optimize_hyperparameters.py
import argparse
import optuna
import logging
import sys
import os
import torch
import gc
import pytorch_lightning as pl
import numpy as np
from pytorch_lightning.utilities.model_summary import ModelSummary
from optuna.pruners import PercentilePruner
from optuna.samplers import TPESampler
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger
from gpt2_arc.src.training.trainer import NanLossPruningCallback
from gpt2_arc.src.training.train import ModelConfigSaver

from gpt2_arc.src.utils.model_memory_estimator import (
    calculate_params,
    estimate_memory_usage,
    get_available_memory,
    can_fit_model
)

class CustomPruningCallback(pl.Callback):
    def __init__(self, trial, monitor="val_loss"):
        super().__init__()
        self.trial = trial
        self.monitor = monitor

    def on_validation_end(self, trainer, pl_module):
        epoch = trainer.current_epoch
        current_score = trainer.callback_metrics.get(self.monitor)
        if current_score is None:
            return
        self.trial.report(current_score, step=epoch)
        if self.trial.should_prune():
            raise optuna.TrialPruned()

# Define the base directory for the arc-neural-reasoning-model
arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))

# Add the project root to the Python path
project_root = arc_model_dir
sys.path.insert(0, project_root)

from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.data.arc_dataset import ARCDataset
import arckit
from gpt2_arc.src.utils.performance_metrics import calculate_mamba_efficiency

# Set up logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout):
    """Validate that hyperparameters meet necessary constraints."""
    logger.debug(f"Validating hyperparameters: n_embd={n_embd}, n_head={n_head}, n_layer={n_layer}, "
                 f"mamba_ratio={mamba_ratio}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}")
    assert n_embd % n_head == 0, f"n_embd ({n_embd}) must be divisible by n_head ({n_head})"
    assert n_embd &gt;= n_head, f"n_embd ({n_embd}) must be greater than or equal to n_head ({n_head})"
    assert n_layer &gt; 0, f"n_layer ({n_layer}) must be positive"
    assert d_state &gt; 0, f"d_state ({d_state}) must be positive"
    assert d_conv &gt; 0, f"d_conv ({d_conv}) must be positive"
    assert 0.0 &lt;= dropout &lt;= 1.0, f"dropout ({dropout}) must be between 0.0 and 1.0"
    logger.debug("Hyperparameters validated successfully")
    return True


def calculate_symbol_freq(dataset):
    """Calculate the frequency of each symbol in the dataset."""
    symbol_counts = {}
    total_symbols = 0
    for input_tensor, output_tensor, task_id in dataset:
        # Assuming symbols are represented as integers in the tensors
        input_symbols = input_tensor.flatten().tolist()
        output_symbols = output_tensor.flatten().tolist()
        symbols = input_symbols + output_symbols
        for symbol in symbols:
            symbol_counts[symbol] = symbol_counts.get(symbol, 0) + 1
            total_symbols += 1
    
    if total_symbols == 0:
        raise ValueError("The dataset contains no symbols to calculate frequencies.")
    
    # Calculate normalized frequencies
    symbol_freq = {symbol: count / total_symbols for symbol, count in symbol_counts.items()}
    return symbol_freq



def objective(trial, args):
    model = None
    trainer = None
    arc_trainer = None
    logger.info(f"Starting trial {trial.number}")
    try:
        # Set float32 matrix multiplication precision
        torch.set_float32_matmul_precision(args.matmul_precision)
        logger.info(f"Trial {trial.number}: Set float32 matmul precision to: {args.matmul_precision}")
        n_head_exp = trial.suggest_int("n_head_exp", args.n_head_exp_min, args.n_head_exp_max)
        n_head = 2 ** n_head_exp
        logger.debug(f"Suggested n_head: {n_head} (2^{n_head_exp})")

        # Suggest n_embd as a multiple of n_head and ensure it's a power of 2
        n_embd_multiplier = trial.suggest_int("n_embd_multiplier", args.n_embd_multiplier_min, args.n_embd_multiplier_max)
        n_embd = n_head * n_embd_multiplier
        n_embd = 2 ** int(np.log2(n_embd))
        logger.debug(f"Adjusted n_embd: {n_embd}")

        # Suggest n_layer
        n_layer = trial.suggest_int("n_layer", args.n_layer_min, args.n_layer_max)
        logger.debug(f"Suggested n_layer: {n_layer}")

        # Suggest Mamba-specific hyperparameters
        mamba_ratio = trial.suggest_float("mamba_ratio", args.mamba_ratio_min, args.mamba_ratio_max, step=args.mamba_ratio_step)
        d_state = trial.suggest_int("d_state", args.d_state_min, args.d_state_max)
        d_conv = trial.suggest_int("d_conv_min", args.d_conv_min, args.d_conv_max)

        # Suggest dropout rate
        dropout = trial.suggest_float("dropout", args.dropout_min, args.dropout_max, step=args.dropout_step)
        mamba_depth = trial.suggest_int("mamba_depth", args.mamba_depth_min, args.mamba_depth_max)
        logger.debug(f"Suggested mamba_depth: {mamba_depth}")

        mamba_expand = trial.suggest_int("mamba_expand", args.mamba_expand_min, args.mamba_expand_max)
        logger.debug(f"Suggested mamba_expand: {mamba_expand}")

        validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout)

        # Suggest whether to use Grokfast
        use_grokfast = trial.suggest_categorical("use_grokfast", [True, False])

        if use_grokfast:
            # Suggest Grokfast type based on command-line choices
            grokfast_type = trial.suggest_categorical("grokfast_type", args.grokfast_type_choices)

            # Suggest Grokfast alpha within specified range
            grokfast_alpha = trial.suggest_float("grokfast_alpha", args.grokfast_alpha_min, args.grokfast_alpha_max)

            # Suggest Grokfast lambda within specified range
            grokfast_lamb = trial.suggest_float("grokfast_lamb", args.grokfast_lamb_min, args.grokfast_lamb_max)

            # If using 'ma', suggest window_size within specified range
            if grokfast_type == "ma":
                grokfast_window_size = trial.suggest_int("grokfast_window_size", args.grokfast_window_size_min, args.grokfast_window_size_max)
            else:
                grokfast_window_size = None
        else:
            grokfast_type = None
            grokfast_alpha = None
            grokfast_lamb = None
            grokfast_window_size = None
        batch_size = trial.suggest_int("batch_size", args.batch_size_min, args.batch_size_max)
        learning_rate = trial.suggest_float("learning_rate", args.learning_rate_min, args.learning_rate_max, log=True)
        max_epochs = trial.suggest_int("max_epochs", args.max_epochs_min, args.max_epochs_max)

        # Ensure hyperparameters are within the new limits
        n_head = min(n_head, 2 ** args.n_head_exp_max)
        n_embd = min(n_embd, 2 ** int(np.log2(args.n_embd_multiplier_max * n_head)))
        n_layer = min(n_layer, args.n_layer_max)
        mamba_ratio = min(mamba_ratio, args.mamba_ratio_max)
        d_state = min(d_state, args.d_state_max)
        d_conv = min(d_conv, args.d_conv_max)
        batch_size = min(batch_size, args.batch_size_max)
        # Optionally, log the clamped values
        logger.debug(f"Clamped hyperparameters: n_head={n_head}, n_embd={n_embd}, n_layer={n_layer}, \
            mamba_ratio={mamba_ratio}, d_state={d_state}, d_conv={d_conv}, batch_size={batch_size}")

        # Check if the model will fit in memory
        # Adjust the total number of layers to include Mamba layers
        total_mamba_layers = int(n_layer * mamba_ratio)
        total_layers = n_layer + total_mamba_layers

        # Recalculate total parameters based on total_layers
        total_params = calculate_params(
            n_layers=total_layers,
            n_heads=n_head,
            d_model=n_embd,
            mamba_ratio=mamba_ratio,
            d_state=d_state,
            d_conv=d_conv,
            mamba_depth=mamba_depth,
            mamba_expand=mamba_expand
        )
        estimated_memory = estimate_memory_usage(
            total_params=total_params,
            batch_size=batch_size,
            height=30,  # Adjust as necessary based on your data
            width=30,   # Adjust as necessary
            d_model=n_embd
        )
        available_memory = get_available_memory()

        logger.debug(f"Trial {trial.number}: Estimated memory usage: {estimated_memory:.2f} GB")
        logger.debug(f"Trial {trial.number}: Available memory: {available_memory:.2f} GB")

        # Prune the trial if estimated memory exceeds 80% of available memory
        if not can_fit_model(estimated_memory, available_memory * 0.8):
            logger.warning(f"Trial {trial.number}: Model too large for available memory. Skipping.")
            raise optuna.exceptions.TrialPruned()

        logger.debug(f"Suggested dropout rate: {dropout}")

        model_config = ModelConfig(
            n_embd=n_embd,
            n_head=n_head,
            n_layer=n_layer,
            dropout=dropout,
            mamba_ratio=mamba_ratio,
            d_state=d_state,
            d_conv=d_conv
        )
        logger.debug(f"Model config: {model_config}")

        # Create TrainingConfig with Grokfast parameters from args
        training_config = TrainingConfig(
            batch_size=batch_size,
            learning_rate=learning_rate,
            max_epochs=max_epochs,
            use_grokfast=use_grokfast,
            grokfast_type=grokfast_type,
            grokfast_alpha=grokfast_alpha,
            grokfast_lamb=grokfast_lamb,
            grokfast_window_size=grokfast_window_size
        )

        config = Config(model=model_config, training=training_config)
        config.estimated_memory = estimated_memory
        config.available_memory = available_memory
        logger.debug(f"Suggested Mamba parameters - mamba_ratio: {mamba_ratio}, d_state: {d_state}, d_conv: {d_conv}")
        trial.set_user_attr("mamba_ratio", mamba_ratio)
        trial.set_user_attr("d_state", d_state)
        trial.set_user_attr("d_conv", d_conv)
        trial.set_user_attr("mamba_depth", mamba_depth)
        trial.set_user_attr("mamba_expand", mamba_expand)
        logger.debug(f"Full config: {config}")

        # Instantiate the ModelConfigSaver callback with the current config
        model_config_saver = ModelConfigSaver(config)

        # Load data
        if args.use_synthetic_data:
            if not args.synthetic_data_path:
                raise ValueError("Synthetic data path not provided")
            logger.info(f"Loading synthetic data from {args.synthetic_data_path}")
            train_data = ARCDataset(
                data_source=args.synthetic_data_path,
                is_test=False,
                num_symbols=config.model.n_embd
            )
            val_data = ARCDataset(
                data_source=args.synthetic_data_path,
                is_test=True,
                num_symbols=config.model.n_embd
            )
            logger.debug(f"Synthetic training data size: {len(train_data)}")
            logger.debug(f"Synthetic validation data size: {len(val_data)}")
        else:
            logger.info("Loading ARC dataset")
            train_set, eval_set = arckit.load_data()
            train_data = ARCDataset(train_set)
            val_data = ARCDataset(eval_set)
            logger.debug(f"ARC training data size: {len(train_data)}")
            logger.debug(f"ARC validation data size: {len(val_data)}")

        # Calculate Symbol Frequencies
        if args.use_synthetic_data:
            logger.debug("Calculating symbol frequencies for synthetic training set")
            symbol_freq = train_data.get_symbol_frequencies()
        else:
            logger.debug("Calculating symbol frequencies for ARC training set")
            symbol_freq = train_data.get_symbol_frequencies()

        logger.debug(f"Computed symbol frequencies: {symbol_freq}")

        # Convert symbol_freq from NumPy array to dictionary
        symbol_freq_dict = {str(i): float(freq) for i, freq in enumerate(symbol_freq)}
        logger.debug(f"Converted symbol frequencies to dictionary: {symbol_freq_dict}")

        # Assign the converted symbol_freq to the training configuration
        config.training.symbol_freq = symbol_freq_dict

        # Validate that symbol_freq_dict is not empty
        if not symbol_freq_dict:
            logger.error("symbol_freq_dict is empty. Cannot proceed with balance_symbols=True and balancing_method='weighting'.")
            raise ValueError("symbol_freq must be provided and non-empty when balance_symbols is True and balancing_method is 'weighting'.")

        # Create model and trainer
        logger.debug("Creating model and trainer")
        num_classes = 10  # Set this to the appropriate number of classes for your task
        model = GPT2ARC(config, num_classes=num_classes, symbol_freq=symbol_freq_dict)
        
        # Generate model summary
        print("DEBUG: Attempting to generate model summary")
        try:
            model_summary = str(ModelSummary(model, max_depth=-1))
            print("DEBUG: Model summary generated successfully")
        except Exception as e:
            print(f"DEBUG: Error generating model summary - {str(e)}")
            model_summary = "Error generating model summary"

        # Save model summary to trial user attributes
        print("DEBUG: Attempting to save model summary to trial user attributes")
        try:
            trial.set_user_attr("model_summary", model_summary)
            print("DEBUG: Model summary saved to trial user attributes")
        except Exception as e:
            print(f"DEBUG: Error saving model summary to trial - {str(e)}")

        print("DEBUG: Model summary:")
        print(model_summary)

        # Calculate Mamba efficiency metrics
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.debug("Calculating Mamba efficiency metrics")
        sample_input = torch.randn(1, 1, 6, 6).to(device)
        model.to(device)
        mamba_metrics = calculate_mamba_efficiency(model, sample_input)
        for key, value in mamba_metrics.items():
            trial.set_user_attr(key, value)
            logger.debug(f"Mamba metric - {key}: {value}")

        arc_trainer = ARCTrainer(model, train_data, val_data, config)

        # Set up PyTorch Lightning trainer with custom pruning callback
        pruning_callback = CustomPruningCallback(trial, monitor="val_loss")
        early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=0.00, patience=3, verbose=False, mode="min")
        # Determine accelerator parameters based on the --accelerator argument
        if args.accelerator == "tpu":
            accelerator = 'tpu'
            devices = 'xla:1'  # Use 'xla:8' for TPU v3-8 pods
            strategy = 'tpu_spawn'  # Recommended strategy for TPU
        elif args.accelerator == "gpu":
            if torch.cuda.is_available():
                accelerator = 'gpu'
                devices = 1
            else:
                accelerator = 'cpu'
                devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
        else:
            accelerator = 'cpu'
            devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
            
        nan_loss_pruning_callback = NanLossPruningCallback()
        #callbacks.append(nan_loss_pruning_callback)
        logger.info("NanLossPruningCallback added to the training callbacks.")
        experiment_id = f"optuna_trial_{trial.number}"
        tb_logger = TensorBoardLogger(save_dir="runs", name=f"experiment_{experiment_id}")
        print(f"DEBUG: Optuna trial TensorBoard logger initialized. Log dir: {tb_logger.log_dir}")
        
        # Extract trial number
        trial_num = trial.number

        # Define task_id (assuming a single task; modify as needed for multiple tasks)
        task_id = "main_task"  # Replace with dynamic task identification if necessary

        # Define iter_num (e.g., based on trial.number or another tracking mechanism)
        iter_num = 1  # Initialize to 1; increment as needed within your optimization loop

        # Initialize the checkpoint callback with descriptive filename
        checkpoint_callback = ModelCheckpoint(
            dirpath="checkpoints",
            filename="trial_{trial_num}-epoch_{epoch:02d}-val_loss_{val_loss:.4f}",
            save_top_k=3,
            monitor="val_loss",
            mode="min",
        )
        logger.info("Standard ModelCheckpoint callback added to the training callbacks.")

        # Initialize PyTorch Lightning Trainer with the checkpoint callback
        trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            callbacks=[pruning_callback, early_stop_callback, nan_loss_pruning_callback, checkpoint_callback, model_config_saver],
            logger=tb_logger,
            gradient_clip_val=1.0,    # Add gradient clipping
            precision=16,             # Enable Automatic Mixed Precision
            enable_checkpointing=True,
            accelerator=accelerator,
            devices=devices,
            strategy=strategy,
        )
        print("DEBUG: Trainer created for Optuna trial with TensorBoard logger")
        logger.debug(f"Trainer created with config: {trainer.state}")

        # Ensure model is in train mode before training
        model.train()
        logger.debug("Model set to train mode before training")

        # Enhanced Logging: Log model mode before training
        logger.info("Before training:")
        for name, module in model.named_modules():
            logger.debug(f"{name}: {'train' if module.training else 'eval'}")

        # Train and evaluate
        logger.debug("Starting training")
        trainer.fit(arc_trainer)

        # Enhanced Logging: Log model mode after training
        logger.info("After training:")
        for name, module in model.named_modules():
            logger.debug(f"{name}: {'train' if module.training else 'eval'}")

        # Update iter_num if needed (e.g., if multiple iterations per trial)
        iter_num += 1

        # Retrieve the best validation loss for optimization
        best_val_loss = trainer.callback_metrics.get("val_loss").item()
        logger.info(f"Trial {trial.number} completed. Best validation loss: {best_val_loss}")

        return best_val_loss

    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            logger.error(f"Trial {trial.number}: CUDA out of memory error.")
            logger.error("Pruning trial and suggesting to adjust hyperparameters.")
            trial.set_user_attr('failed_reason', 'CUDA out of memory')
            raise optuna.exceptions.TrialPruned()
        else:
            logger.error(f"Trial {trial.number}: A runtime error occurred: {str(e)}", exc_info=True)
            raise RuntimeError(f"Trial {trial.number}: A runtime error occurred: {str(e)}")
    except Exception as e:
        if "symbol_freq" in str(e):
            logger.error(f"Trial {trial.number}: 'symbol_freq' is missing. Ensure it is calculated and passed correctly.", exc_info=True)
        else:
            logger.error(f"Trial {trial.number}: An unexpected error occurred: {str(e)}", exc_info=True)
        raise optuna.exceptions.TrialPruned(f"Trial {trial.number}: An unexpected error occurred: {str(e)}")
    finally:
        # Ensure Proper Cleanup Between Trials
        logger.debug(f"Cleaning up after trial {trial.number}")
        if model is not None:
            del model
        if trainer is not None:
            del trainer
        if arc_trainer is not None:
            del arc_trainer
        gc.collect()
        torch.cuda.empty_cache()
        logger.debug(f"Cleanup completed for trial {trial.number}")



from functools import partial

def run_optimization(n_trials=100, storage_name="sqlite:///optuna_results.db", n_jobs=-1, args=None, study_name="gpt2_arc_optimization_v2"):

    pruner = PercentilePruner(percentile=25, n_startup_trials=5, n_warmup_steps=2, interval_steps=1)
    sampler = TPESampler(n_startup_trials=5)

    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        load_if_exists=True,
        direction="minimize",
        pruner=pruner,
        sampler=sampler
    )

    logger.info(f"Starting optimization with {n_trials} trials using {n_jobs} parallel jobs")
    study.optimize(partial(objective, args=args), n_trials=n_trials, n_jobs=n_jobs)

    logger.info("Optimization completed")

    if study.best_trial:
        print("DEBUG: Best trial found, attempting to retrieve model summary")
        best_model_summary = study.best_trial.user_attrs.get("model_summary")
        if best_model_summary:
            print("DEBUG: Model summary retrieved successfully")
            logger.info("Model summary for the best trial:")
            logger.info(best_model_summary)
        else:
            print("DEBUG: No model summary found for the best trial")
    else:
        logger.warning("No successful trials found. Please check the trial configurations and constraints.")
        logger.info(f"Best trial: {study.best_trial.number}")
        logger.info(f"Best value: {study.best_value}")
        
        best_trial = study.best_trial
        best_trial.set_user_attr("mamba_ratio", best_trial.params.get("mamba_ratio"))
        best_trial.set_user_attr("d_state", best_trial.params.get("d_state"))
        best_trial.set_user_attr("d_conv", best_trial.params.get("d_conv"))

        logger.info("Best Mamba metrics:")
        for key in ['mamba_forward_pass_time', 'mamba_params', 'mamba_params_ratio']:
            value = study.best_trial.user_attrs.get(key)
            if value is not None:
                logger.info(f"  {key}: {value}")

        logger.info("Best hyperparameters:")
        for key, value in study.best_params.items():
            logger.info(f"  {key}: {value}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimize hyperparameters for GPT2ARC model.")
    parser.add_argument("--n_trials", type=int, default=10, help="Number of trials for optimization.")
    parser.add_argument("--storage", type=str, default="sqlite:///optuna_results.db", help="Storage path for Optuna results.")
    parser.add_argument("--n_jobs", type=int, default=1, help="Number of parallel jobs. -1 means using all available cores.")
    parser.add_argument(
        "--study-name",
        type=str,
        default="gpt2_arc_optimization_v2",
        help="Name of the Optuna study."
    )

    parser.add_argument("--n_embd_min", type=int, default=4, help="Minimum value for n_embd")
    parser.add_argument("--n_embd_max", type=int, default=8, help="Maximum value for n_embd")
    parser.add_argument("--n_head_min", type=int, default=2, help="Minimum value for n_head")
    parser.add_argument("--n_head_max", type=int, default=16, help="Maximum value for n_head")
    parser.add_argument("--n_head_exp_min", type=int, default=1, help="Minimum exponent for n_head (2^x)")
    parser.add_argument("--n_head_exp_max", type=int, default=3, help="Maximum exponent for n_head (2^x)")
    parser.add_argument("--n_embd_multiplier_min", type=int, default=1, help="Minimum multiplier for n_embd")
    parser.add_argument("--n_embd_multiplier_max", type=int, default=2, help="Maximum multiplier for n_embd")
    parser.add_argument("--n_layer_min", type=int, default=4, help="Minimum value for n_layer")
    parser.add_argument("--n_layer_max", type=int, default=8, help="Maximum value for n_layer")
    parser.add_argument("--batch_size_min", type=int, default=32, help="Minimum value for batch_size")
    parser.add_argument("--batch_size_max", type=int, default=128, help="Maximum value for batch_size")
    parser.add_argument("--learning_rate_min", type=float, default=1e-5, help="Minimum value for learning_rate")
    parser.add_argument("--learning_rate_max", type=float, default=1e-2, help="Maximum value for learning_rate")
    parser.add_argument("--max_epochs_min", type=int, default=1, help="Minimum value for max_epochs")
    parser.add_argument("--max_epochs_max", type=int, default=20, help="Maximum value for max_epochs")

    parser.add_argument("--mamba_ratio_min", type=float, default=0.0, help="Minimum value for mamba_ratio")
    parser.add_argument("--mamba_ratio_max", type=float, default=8.0, help="Maximum value for mamba_ratio")
    parser.add_argument("--mamba_ratio_step", type=float, default=0.25, help="Step size for mamba_ratio")
    parser.add_argument("--d_state_min", type=int, default=1, help="Minimum value for d_state")
    parser.add_argument("--d_state_max", type=int, default=2, help="Maximum value for d_state")
    parser.add_argument("--d_conv_min", type=int, default=1, help="Minimum value for d_conv")
    parser.add_argument("--d_conv_max", type=int, default=2, help="Maximum value for d_conv")

    parser.add_argument("--dropout_min", type=float, default=0.0, help="Minimum value for dropout")
    parser.add_argument("--mamba_depth_min", type=int, default=1, help="Minimum value for mamba_depth")
    parser.add_argument("--mamba_depth_max", type=int, default=3, help="Maximum value for mamba_depth")
    parser.add_argument("--mamba_expand_min", type=int, default=2, help="Minimum value for mamba_expand")
    parser.add_argument("--mamba_expand_max", type=int, default=4, help="Maximum value for mamba_expand")
    parser.add_argument("--dropout_max", type=float, default=0.5, help="Maximum value for dropout")
    parser.add_argument("--dropout_step", type=float, default=0.1, help="Step size for dropout")
    parser.add_argument("--use_gpu", action="store_true", help="Flag to indicate whether to use GPU for training.")
    parser.add_argument("--use_synthetic_data", action="store_true", help="Flag to indicate whether to use synthetic data for training.")
    parser.add_argument(
        "--matmul-precision",
        type=str,
        default="medium",
        choices=["highest", "high", "medium"],
        help="Set the internal precision of float32 matrix multiplications for optimization trials. Options: 'highest', 'high', 'medium'. Defaults to 'medium'."
    )
    parser.add_argument("--synthetic_data_path", type=str, default="", help="Path to synthetic data for training.")
    parser.add_argument("--log_level", type=str, default="INFO", help="Logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL).")
    parser.add_argument(
        "--accelerator",
        type=str,
        default="gpu",
        choices=["cpu", "gpu", "tpu"],
        help="Accelerator to use for training: 'cpu', 'gpu', or 'tpu'. Defaults to 'gpu'."
    )

    # Grokfast parameter ranges
    parser.add_argument("--grokfast_alpha_min", type=float, default=0.9, help="Minimum value for grokfast_alpha.")
    parser.add_argument("--grokfast_alpha_max", type=float, default=0.99, help="Maximum value for grokfast_alpha.")
    parser.add_argument("--grokfast_lamb_min", type=float, default=1.0, help="Minimum value for grokfast_lamb.")
    parser.add_argument("--grokfast_lamb_max", type=float, default=3.0, help="Maximum value for grokfast_lamb.")
    parser.add_argument("--grokfast_window_size_min", type=int, default=50, help="Minimum value for grokfast_window_size.")
    parser.add_argument("--grokfast_window_size_max", type=int, default=200, help="Maximum value for grokfast_window_size.")
    parser.add_argument("--grokfast_type_choices", type=str, nargs='+', default=["ema", "ma"], choices=["ema", "ma"], help="List of Grokfast types to consider during tuning.")


    args = parser.parse_args()

    # Ensure the storage_name has the correct SQLite prefix and handle relative paths
    import os  # Ensure os is imported at the top of the file

    if not args.storage.startswith("sqlite:///"):
        if os.path.isabs(args.storage):
            args.storage = f"sqlite:////{args.storage}"
        else:
            args.storage = f"sqlite:///{os.path.abspath(args.storage)}"
    
    logger.debug(f"Optuna storage URL set to: {args.storage}")
    run_optimization(
        n_trials=args.n_trials,
        storage_name=args.storage,
        n_jobs=args.n_jobs,
        args=args,
        study_name=args.study_name
    )

</file>
<file name="src/__init__.py">
# This file allows the src directory to be recognized as a package.

</file>
<file name="src/checkpoint_evaluator.py">
#!/usr/bin/env python3
"""
checkpoint_evaluator.py

A script to monitor a directory for new model checkpoints and evaluate them using a specified evaluation script.
Logs are maintained, and resource usage is monitored.

Usage:
    python checkpoint_evaluator.py \
        --output_dir /path/to/output \
        --arc_model_dir /path/to/arc_model \
        --date_folder 2024-10-07 \
        --wandb_project arc-evaluation \
        [optional arguments]
"""

import os
import sys
import time
import subprocess
import threading
import shutil
import logging
import argparse
from datetime import datetime

import psutil  # For resource monitoring
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

def parse_arguments():
    parser = argparse.ArgumentParser(description="Monitor directories for model checkpoints and evaluate them.")
    
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Directory to store logs and evaluation results.')
    parser.add_argument('--arc_model_dir', type=str, required=True,
                        help='Directory containing the arc model scripts.')
    parser.add_argument('--date_folder', type=str, required=True,
                        help='Date folder name (e.g., 2024-10-07) to organize outputs.')
    parser.add_argument('--wandb_project', type=str, default='arc-evaluation',
                        help='Weights &amp; Biases project name.')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size for evaluation.')
    parser.add_argument('--log_level', type=str, default='DEBUG',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                        help='Logging level.')
    parser.add_argument('--resource_monitor_interval', type=int, default=60,
                        help='Interval in seconds for resource monitoring logs.')
    
    return parser.parse_args()

def setup_logging(output_dir, log_level):
    os.makedirs(output_dir, exist_ok=True)
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(output_dir, "checkpoint_evaluator.log")),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger("CheckpointEvaluator")
    return logger

def load_evaluated_models(evaluated_models_file, logger):
    evaluated_models = set()
    if os.path.exists(evaluated_models_file):
        try:
            with open(evaluated_models_file, "r") as f:
                evaluated_models.update(line.strip() for line in f)
            logger.info(f"Loaded evaluated models from {evaluated_models_file}")
        except Exception as e:
            logger.error(f"Error loading evaluated models from {evaluated_models_file}: {e}")
    else:
        logger.info("No previously evaluated models found. Starting fresh.")
    return evaluated_models

def save_evaluated_model(evaluated_models_file, model_path, logger):
    try:
        with open(evaluated_models_file, "a") as f:
            f.write(model_path + "\n")
        logger.debug(f"Recorded evaluation of {model_path} in {evaluated_models_file}")
    except Exception as e:
        logger.error(f"Error writing to evaluated models file {evaluated_models_file}: {e}")

def wait_for_file_stable(file_path, wait_time=1.0, max_retries=10, logger=None):
    """Wait until the file is stable (not changing size)"""
    previous_size = -1
    retries = 0
    while retries &lt; max_retries:
        if not os.path.exists(file_path):
            if logger:
                logger.warning(f"File {file_path} does not exist.")
            return False
        current_size = os.path.getsize(file_path)
        if current_size == previous_size:
            return True
        else:
            previous_size = current_size
            time.sleep(wait_time)
            retries += 1
    if logger:
        logger.warning(f"File {file_path} is not stable after {max_retries} retries.")
    return False

class CheckpointHandler(FileSystemEventHandler):
    def __init__(self, evaluated_models, temp_checkpoint_dir, evaluate_callback, logger):
        super().__init__()
        self.evaluated_models = evaluated_models
        self.temp_checkpoint_dir = temp_checkpoint_dir
        self.evaluate_callback = evaluate_callback
        self.logger = logger

    def on_created(self, event):
        if event.is_directory:
            return
        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):
            self.evaluate_model(event.src_path)

    def evaluate_model(self, model_path):
        model_file = os.path.basename(model_path)
        if model_path in self.evaluated_models:
            self.logger.info(f"Skipping already evaluated model: {model_file}")
            return  # Skip if the model was already evaluated

        # Wait for the file to be stable
        if not wait_for_file_stable(model_path, logger=self.logger):
            self.logger.warning(f"File {model_file} is not stable. Skipping evaluation.")
            return

        # Copy the checkpoint file to temp_checkpoint_dir
        temp_model_path = os.path.join(self.temp_checkpoint_dir, model_file)
        try:
            shutil.copy2(model_path, temp_model_path)
            self.logger.info(f"Copied {model_file} to temporary directory.")
        except Exception as e:
            self.logger.error(f"Error copying {model_file} to temporary directory: {e}")
            return

        # Extract epoch and val_loss from the filename for run_name
        try:
            parts = model_file.replace('.ckpt', '').replace('.pth', '').split('-')
            epoch = None
            val_loss = None
            for part in parts:
                if part.startswith('epoch='):
                    epoch = part.split('=')[1]
                elif part.startswith('val_loss='):
                    val_loss = part.split('=')[1]
            if epoch is not None and val_loss is not None:
                run_name = f"evaluation-epoch{epoch}-val_loss{val_loss}"
            else:
                run_name = f"evaluation-{model_file}"
            self.logger.debug(f"Parsed run name: {run_name}")
        except Exception as e:
            self.logger.error(f"Error parsing run name from filename {model_file}: {e}")
            run_name = f"evaluation-{model_file}"

        # Define the evaluation command
        eval_command = [
            "python", os.path.join(args.arc_model_dir, "gpt2_arc/src/evaluate.py"),
            "--model_checkpoint", temp_model_path,
            "--batch_size", str(args.batch_size),
            "--output_dir", args.output_dir,
            "--wandb_project", args.wandb_project,
            "--wandb_run_name", run_name
        ]
        self.logger.info(f"Evaluating model: {model_file} with run name: {run_name}")

        # Define the log file path
        log_file_path = os.path.join(args.output_dir, f"{model_file}_evaluation.log")
        try:
            with open(log_file_path, "w") as log_file:
                # Run the evaluation command and redirect stdout and stderr to the log file
                subprocess.run(
                    eval_command,
                    check=True,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    text=True  # Automatically decode bytes to string
                )
            self.logger.info(f"Successfully evaluated model: {model_file}. Logs at {log_file_path}")
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Error during evaluation of {model_file}. See log at {log_file_path}")
        except Exception as ex:
            self.logger.exception(f"An unexpected error occurred while evaluating {model_file}: {ex}")

        self.evaluated_models.add(model_path)
        save_evaluated_model(os.path.join(args.output_dir, "evaluated_models.txt"), model_path, self.logger)

        # Delete the temp model file
        try:
            os.remove(temp_model_path)
            self.logger.debug(f"Deleted temporary model file: {temp_model_path}")
        except Exception as e:
            self.logger.error(f"Error deleting temp model file {temp_model_path}: {e}")

def get_all_checkpoint_files(directory):
    checkpoint_files = []
    for root, _, files in os.walk(directory):
        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])
    return checkpoint_files

def start_observer(model_dir, handler, logger):
    # Set up and start the watchdog observer
    observer = Observer()
    observer.schedule(handler, model_dir, recursive=True)
    observer.start()

    logger.info("Watching for new checkpoints and final models in all subdirectories...")
    logger.info("This script will continue running in the background.")

    try:
        while True:
            time.sleep(10)
            # Optionally, you can implement additional periodic checks here
    except KeyboardInterrupt:
        observer.stop()
        logger.info("Observer stopped by user.")
    except FileNotFoundError as fnf_error:
        logger.error(f"FileNotFoundError: {fnf_error}")
        logger.error(f"Please ensure that the directory '{model_dir}' exists.")
    except Exception as e:
        logger.exception(f"An error occurred in the observer: {e}")
    finally:
        observer.join()
        logger.info("Checkpoint and final model evaluation completed.")

def monitor_resources(logger, interval=60):
    while True:
        try:
            memory = psutil.virtual_memory()
            cpu = psutil.cpu_percent(interval=1)
            logger.debug(f"Memory Usage: {memory.percent}%")
            logger.debug(f"CPU Usage: {cpu}%")
            time.sleep(interval)
        except Exception as e:
            logger.error(f"Error in resource monitoring: {e}")

def main(args):
    logger = setup_logging(args.output_dir, args.log_level)
    logger.info("Starting Checkpoint Evaluator Script")
    logger.debug(f"Arguments: {args}")

    model_dir = os.path.join(args.date_folder, "checkpoints")
    logger.debug(f"Watching for new models in directory: {model_dir}")

    # Create necessary directories
    os.makedirs(model_dir, exist_ok=True)
    temp_checkpoint_dir = os.path.join(args.output_dir, "temp_checkpoints")
    os.makedirs(temp_checkpoint_dir, exist_ok=True)

    # Load previously evaluated models
    evaluated_models_file = os.path.join(args.output_dir, "evaluated_models.txt")
    evaluated_models = load_evaluated_models(evaluated_models_file, logger)

    # Set up the event handler
    handler = CheckpointHandler(evaluated_models, temp_checkpoint_dir, None, logger)

    # Initialize watchdog event handler with the ability to evaluate models
    event_handler = CheckpointHandler(evaluated_models, temp_checkpoint_dir, None, logger)

    # Start the observer in a separate thread
    observer_thread = threading.Thread(target=start_observer, args=(model_dir, event_handler, logger))
    observer_thread.daemon = True  # Ensures the thread will exit when the main program exits
    observer_thread.start()
    logger.info("Background checkpoint observer started.")

    # Start the resource monitor in a background thread
    resource_monitor_thread = threading.Thread(target=monitor_resources, args=(logger, args.resource_monitor_interval))
    resource_monitor_thread.daemon = True
    resource_monitor_thread.start()
    logger.info("Background resource monitor started.")

    # Keep the main thread alive
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("Script terminated by user.")
    except Exception as e:
        logger.exception(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    args = parse_arguments()
    main(args)

</file>
<file name="src/config.py">
from typing import Optional
# gpt2_arc/src/config.py
from dataclasses import dataclass, asdict, field
from typing import Optional
import multiprocessing

@dataclass
class ModelConfig:
    n_embd: int = 768
    n_head: int = 12
    n_layer: int = 12
    dropout: float = 0.1
    mamba_ratio: float = 1.0  # Number of Mamba layers per Transformer layer
    d_state: int = 16     # Mamba state dimension
    d_conv: int = 4       # Mamba convolution dimension
    mamba_depth: int = 1  # Depth of each Mamba layer
    mamba_expand: int = 2  # Expand factor for each Mamba layer

    def __post_init__(self):
        assert self.n_embd % self.n_head == 0, f"n_embd ({self.n_embd}) must be divisible by n_head ({self.n_head})"
        assert self.n_embd &gt;= self.n_head, f"n_embd ({self.n_embd}) must be greater than or equal to n_head ({self.n_head})"
        assert self.n_layer &gt; 0, f"n_layer ({self.n_layer}) must be positive"
        assert self.d_state &gt;= 1, f"d_state ({self.d_state}) must be at least 1"
        assert self.d_conv &gt;= 1, f"d_conv ({self.d_conv}) must be at least 1"
        assert self.mamba_depth &gt;= 1, f"mamba_depth ({self.mamba_depth}) must be at least 1"
        assert self.mamba_expand &gt;= 2, f"mamba_expand ({self.mamba_expand}) must be at least 2"

from dataclasses import dataclass, field
import multiprocessing

@dataclass
class TrainingConfig:
    # Grokfast-specific parameters
    use_grokfast: bool = False
    grokfast_type: Optional[str] = field(default=None)  # 'ema' or 'ma'
    grokfast_alpha: float = field(default=0.98)
    grokfast_lamb: float = field(default=2.0)
    grokfast_window_size: Optional[int] = field(default=100)  # Only relevant if grokfast_type == 'ma'
    batch_size: int = 32
    learning_rate: float = 1e-4
    max_epochs: int = 10
    num_workers: int = multiprocessing.cpu_count() // 2 if multiprocessing.cpu_count() else 4
    symbol_freq: Optional[dict] = None
    prefetch_factor: int = 2
    persistent_workers: bool = True
    use_gpu: bool = True
    log_level: str = "INFO"
    use_synthetic_data: bool = False
    balance_symbols: bool = True  # Enable balancing
    balancing_method: str = "weighting"  # Options: "weighting", "oversampling"
    synthetic_data_path: Optional[str] = None

@dataclass
class EvaluationConfig:
    perfect_accuracy_threshold: float = 99.9  # Set to 99.9 for near-perfect accuracy

@dataclass
class Config:
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    evaluation: EvaluationConfig = field(default_factory=EvaluationConfig)
    estimated_memory: Optional[float] = None
    available_memory: Optional[float] = None

    def to_dict(self):
        return asdict(self)

</file>
<file name="src/utils/helpers.py">
# gpt2_arc/src/utils/helpers.py
import torch

def differential_pixel_accuracy(input, target, prediction):
    print(f"Differential pixel accuracy - Input shape: {input.shape}, Target shape: {target.shape}, Prediction shape: {prediction.shape}")
    
    assert isinstance(input, torch.Tensor) and isinstance(target, torch.Tensor) and isinstance(prediction, torch.Tensor), "All inputs must be torch.Tensor"
    assert input.numel() == target.numel() == prediction.numel(), "Input, target, and prediction must have the same number of elements"

    input = input.view_as(target)
    prediction = prediction.view_as(target)
    
    print(f"Reshaped - Input: {input.shape}, Target: {target.shape}, Prediction: {prediction.shape}")

    input_target_diff = input != target
    correct_diff_predictions = (prediction == target) &amp; input_target_diff

    total_diff_pixels = input_target_diff.sum().item()
    correct_diff_pixels = correct_diff_predictions.sum().item()

    print(f"Total different pixels: {total_diff_pixels}")
    print(f"Correctly predicted different pixels: {correct_diff_pixels}")

    if total_diff_pixels &gt; 0:
        accuracy = correct_diff_pixels / total_diff_pixels
    else:
        accuracy = 1.0  # If no pixels differ, consider it 100% accurate

    print(f"Calculated accuracy: {accuracy}")
    return accuracy, input_target_diff, correct_diff_predictions

</file>
<file name="src/utils/grokfast_callback.py">

from pytorch_lightning.callbacks import Callback
from typing import Optional, Dict
import torch.nn as nn
import logging
from .grokfast import gradfilter_ema, gradfilter_ma

logger = logging.getLogger(__name__)


class GrokfastCallback(Callback):
    def __init__(
        self,
        filter_type: str = 'ema',  # 'ema' or 'ma'
        alpha: float = 0.98,
        lamb: float = 2.0,
        window_size: int = 100,
        warmup: bool = True,
        trigger: bool = False,  # For ablation study.
    ):
        """
        Initializes the Grokfast callback.

        Args:
            filter_type (str): Type of Grokfast filter ('ema' or 'ma').
            alpha (float): Momentum parameter for EMA.
            lamb (float): Amplifying factor.
            window_size (int): Window size for MA.
            warmup (bool): Whether to use warmup for MA.
            trigger (bool): For ablation studies.
        """
        super().__init__()
        self.filter_type = filter_type
        self.alpha = alpha
        self.lamb = lamb
        self.window_size = window_size
        self.warmup = warmup
        self.trigger = trigger
        self.grads = None  # Will hold the state across batches

    def on_after_backward(self, trainer, pl_module):
        """
        Called after the backward pass and before the optimizer step.

        Args:
            trainer: The trainer instance.
            pl_module: The LightningModule instance.
        """
        model = pl_module.model  # Adjust if your model is nested differently

        if self.filter_type == 'ema':
            self.grads = gradfilter_ema(
                m=model,  # Pass the actual model
                grads=self.grads,
                alpha=self.alpha,
                lamb=self.lamb
            )
            logger.debug("Applied Grokfast-EMA filter.")
        elif self.filter_type == 'ma':
            self.grads = gradfilter_ma(
                m=model,  # Pass the actual model
                grads=self.grads,
                window_size=self.window_size,
                lamb=self.lamb,
                filter_type='mean',  # or 'sum' based on preference
                warmup=self.warmup,
                trigger=self.trigger
            )
            logger.debug("Applied Grokfast-MA filter.")
        else:
            logger.warning(f"Unknown Grokfast filter type: {self.filter_type}. Skipping gradient filtering.")

</file>
<file name="src/utils/grokfast.py">
from collections import deque
from typing import Dict, Optional, Literal
import torch
import torch.nn as nn


def gradfilter_ma(
    m: nn.Module,
    grads: Optional[Dict[str, deque]] = None,
    window_size: int = 100,
    lamb: float = 5.0,
    filter_type: Literal['mean', 'sum'] = 'mean',
    warmup: bool = True,
    trigger: bool = False, # For ablation study.
) -&gt; Dict[str, deque]:
    if grads is None:
        grads = {n: deque(maxlen=window_size) for n, p in m.named_parameters() if p.requires_grad and p.grad is not None}

    for n, p in m.named_parameters():
        if p.requires_grad and p.grad is not None:
            grads[n].append(p.grad.data.detach().clone())

            # Modify the gradients.
            if not warmup or len(grads[n]) == window_size and not trigger:
                if filter_type == "mean":
                    avg = sum(grads[n]) / len(grads[n])
                elif filter_type == "sum":
                    avg = sum(grads[n])
                else:
                    raise ValueError(f"Unrecognized filter_type {filter_type}")
                p.grad.data = p.grad.data + avg * lamb

    return grads


def gradfilter_ema(
    m: nn.Module,
    grads: Optional[Dict[str, torch.Tensor]] = None,
    alpha: float = 0.98,
    lamb: float = 2.0,
) -&gt; Dict[str, torch.Tensor]:
    if grads is None:
        grads = {n: p.grad.data.detach().clone() for n, p in m.named_parameters() if p.requires_grad and p.grad is not None}

    for n, p in m.named_parameters():
        if p.requires_grad and p.grad is not None:
            grads[n] = grads[n] * alpha + p.grad.data.detach() * (1 - alpha)
            p.grad.data = p.grad.data + grads[n] * lamb

    return grads

</file>
<file name="src/utils/performance_metrics.py">
import torch
import time

def calculate_mamba_efficiency(model, input_data):
    """
    Calculates performance metrics specific to Mamba layers in the model.

    Args:
        model: The GPT2ARC model instance.
        input_data: A sample input tensor.

    Returns:
        A dictionary containing Mamba-specific performance metrics.
    """
    metrics = {}
    model.eval()  # Set model to evaluation mode

    # Ensure model and input data are on the same device
    device = next(model.parameters()).device
    input_data = input_data.to(device)

    # Measure the forward pass time
    with torch.no_grad():
        start_time = time.time()
        _ = model(input_data)
        total_time = time.time() - start_time

    metrics['mamba_forward_pass_time'] = total_time

    # Count the number of parameters in Mamba layers
    mamba_params = 0
    total_params = 0
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        if 'mamba_block' in name:
            mamba_params += param_count

    metrics['mamba_params'] = mamba_params
    metrics['total_params'] = total_params
    metrics['mamba_params_ratio'] = mamba_params / total_params if total_params &gt; 0 else 0

    return metrics

</file>
<file name="src/utils/results_collector.py">
# gpt2_arc/src/utils/results_collector.py
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
from typing import Dict, Any

class ResultsCollector:
    def __init__(self, config):
        """Initialize the ResultsCollector with a given configuration."""
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = asdict(config)
        self.results = {
            "train": {},
            "validation": {},
            "test": {}
        }
        self.used_synthetic_data = config.training.use_synthetic_data
        print(f"DEBUG: Initialized self.results['train'] as {type(self.results['train'])}")
        self._log_results_type("After initialization")
        self.metrics = {}
        self.task_specific_results = {}
        self.environment = self._get_environment_info()
        self.checkpoint_path = None
        self.tensorboard_log_path = None

    def set_tensorboard_log_path(self, path):
        self.tensorboard_log_path = path
        print(f"DEBUG: Set TensorBoard log path in ResultsCollector: {path}")

    def _get_environment_info(self) -&gt; Dict[str, str]:
        """Retrieve environment information such as Python and PyTorch versions."""
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    def _log_results_type(self, context: str):
        """Log the type of self.results['train'] for debugging."""
    
    def update_train_metrics(self, epoch: int, metrics: Dict[str, float]):
        # print(f"DEBUG: self.results['train'] is of type {type(self.results['train'])}")
        """Update training metrics for a specific epoch."""
        self._log_results_type("Before checking 'train' in results")
        if "train" not in self.results:
            self.results["train"] = {}
        self._log_results_type("Before type check")
        if not isinstance(self.results["train"], dict):
            raise TypeError(f"Expected self.results['train'] to be a dict, but got {type(self.results['train'])}")
        self._log_results_type("Before setting default")
        # print(f"DEBUG: Before setting default, self.results['train'] is of type {type(self.results['train'])}")
        self.results["train"].setdefault(epoch, {})
        self._log_results_type("After setting default")
        # print(f"DEBUG: After setting default, self.results['train'] is of type {type(self.results['train'])}")
        self.results["train"][epoch].update(metrics)

    def update_val_metrics(self, epoch: int, metrics: Dict[str, float]):
        """Update validation metrics for a specific epoch."""
        if "validation" not in self.results:
            self.results["validation"] = {}
        self.results["validation"][epoch] = metrics

    def set_test_results(self, metrics: Dict[str, float]):
        """Set the test results metrics."""
        self.results["test"] = metrics

    def add_task_specific_result(self, task_id: str, metrics: Dict[str, float]):
        """Add task-specific results for a given task ID."""
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        self.task_specific_results[task_id].update(metrics)

    def set_final_metrics(self, metrics: Dict[str, float]):
        """Set the final metrics after training."""
        self.metrics = metrics

    def set_checkpoint_path(self, path: str):
        """Set the path to the model checkpoint."""
        self.checkpoint_path = path

    def save_to_json(self, filepath: str):
        """Save the collected results to a JSON file."""
        try:
            self._ensure_directory_exists(os.path.dirname(filepath))
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path,
                "used_synthetic_data": self.used_synthetic_data
            }
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    def _ensure_directory_exists(self, directory: str):
        """Ensure that the directory exists; create it if it does not."""
        if not os.path.exists(directory):
            os.makedirs(directory)

    def get_summary(self) -&gt; Dict[str, Any]:
        """Get a summary of the results."""
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_accuracy": self.results["test"].get("accuracy"),
            "config": self._serialize_config(self.config),
            "tensorboard_log_path": self.tensorboard_log_path
        }
        print(f"DEBUG: Added TensorBoard log path to results: {summary['tensorboard_log_path']}")
        return {k: self._make_serializable(v) for k, v in summary.items()}

    def _make_serializable(self, obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

    def _serialize_config(self, config):
        return {k: self._make_serializable(v) for k, v in config.items()}

</file>
<file name="src/utils/experiment_tracker.py">
# gpt2_arc/src/utils/experiment_tracker.py
import wandb
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
from typing import Dict, Any, Optional

class ExperimentTracker:
    def __init__(self, config: Dict[str, Any], project: str, entity: Optional[str] = None, use_wandb: bool = False):
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = config.to_dict() if hasattr(config, 'to_dict') else self._config_to_dict(config)
        self.project = project
        self.entity = entity
        self.run = None
        self.use_wandb = use_wandb
        self.metrics = {}
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False

        self.results = {
            "train": [],
            "validation": [],
            "test": {}
        }
        self.metrics = {}
        self.task_specific_results = {}
        self.environment = self._get_environment_info()
        self.checkpoint_path = None

        # Add debug logging
        print(f"ExperimentTracker initialized with config: {json.dumps(self.config, indent=2)}")
        print(f"Project: {project}, Entity: {entity}")
        print(f"use_wandb: {self.use_wandb}")

    def _get_environment_info(self) -&gt; Dict[str, str]:
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    def _config_to_dict(self, config):
        if isinstance(config, dict):
            return {k: self._config_to_dict(v) for k, v in config.items()}
        elif hasattr(config, '__dict__'):
            return {k: self._config_to_dict(v) for k, v in config.__dict__.items() if not k.startswith('_')}
        else:
            return config
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False
        
        if not self.use_wandb:
            print("Using local logging only")

    def finish(self):
        if self.use_wandb and self.run:
            try:
                wandb.finish()
                print("Wandb run finished")
            except Exception as e:
                print(f"Error finishing wandb run: {str(e)}")
        else:
            print("Experiment finished. Metrics:", self.metrics)

    def log_metric(self, name: str, value: float, step: Optional[int] = None):
        if self.use_wandb:
            try:
                wandb.log({name: value}, step=step)
                print(f"Logged metric to wandb: {name}={value}, step={step}")
            except Exception as e:
                print(f"Error logging metric to wandb: {str(e)}")
        
        # Always log locally as a fallback
        print(f"Logged metric locally: {name}={value}, step={step}")

    def update_train_metrics(self, epoch: int, metrics: Dict[str, float]):
        if "train" not in self.results:
            self.results["train"] = []
        while len(self.results["train"]) &lt;= epoch:
            self.results["train"].append({})
        self.results["train"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"train": metrics}, step=epoch)

    def update_val_metrics(self, epoch: int, metrics: Dict[str, float]):
        if "validation" not in self.results:
            self.results["validation"] = []
        while len(self.results["validation"]) &lt;= epoch:
            self.results["validation"].append({})
        self.results["validation"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"validation": metrics}, step=epoch)

    def set_test_results(self, metrics: Dict[str, float]):
        self.results["test"] = metrics
        if self.use_wandb:
            wandb.log({"test": metrics})

    def add_task_specific_result(self, task_id: str, metrics: Dict[str, float]):
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        self.task_specific_results[task_id].update(metrics)
        if self.use_wandb:
            wandb.log({f"task_{task_id}": metrics})

    def set_final_metrics(self, metrics: Dict[str, float]):
        self.metrics = metrics
        if self.use_wandb:
            wandb.log(metrics)

    def set_checkpoint_path(self, path: str):
        self.checkpoint_path = path
        if self.use_wandb:
            wandb.save(path)

    def save_to_json(self, filepath: str):
        try:
            directory = os.path.dirname(filepath)
            if directory and not os.path.exists(directory):
                os.makedirs(directory)
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path
            }
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"Results saved to {filepath}")
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    def _ensure_directory_exists(self, directory: str):
        if not os.path.exists(directory):
            os.makedirs(directory)

    def get_summary(self) -&gt; Dict[str, Any]:
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_accuracy": self.results["test"].get("accuracy"),
            "config": self._serialize_config(self.config)
        }
        return {k: self._make_serializable(v) for k, v in summary.items()}

    def _make_serializable(self, obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

    def _serialize_config(self, config):
        return {k: self._make_serializable(v) for k, v in config.items()}

    def log_metric(self, name: str, value: float, step: Optional[int] = None):
        if self.use_wandb:
            try:
                wandb.log({name: value}, step=step)
                print(f"Logged metric to wandb: {name}={value}, step={step}")
            except Exception as e:
                print(f"Error logging metric to wandb: {str(e)}")
        
        # Always log locally as a fallback
        self.metrics[name] = value
        print(f"Logged metric locally: {name}={value}, step={step}")

# Add a simple test
if __name__ == "__main__":
    config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": True}
    tracker = ExperimentTracker(config, project="test-project")
    tracker.start()
    tracker.log_metric("accuracy", 0.85, step=1)
    tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8})
    tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75})
    tracker.set_test_results({"loss": 0.55, "accuracy": 0.82})
    tracker.add_task_specific_result("task_1", {"accuracy": 0.9})
    tracker.set_final_metrics({"best_accuracy": 0.85})
    tracker.set_checkpoint_path("model_checkpoint.pth")
    tracker.save_to_json("results.json")
    tracker.finish()

</file>
<file name="src/utils/__init__.py">
from .grokfast import (
    gradfilter_ma,
    gradfilter_ema
)
from .grokfast_callback import GrokfastCallback

</file>
<file name="src/utils/model_memory_estimator.py">
# gpt2_arc/src/utils/model_memory_estimator.py
import torch
import math
import psutil

def calculate_params(n_layers, n_heads, d_model, mamba_ratio, d_state=16, d_conv=4, mamba_depth=1, mamba_expand=2):
    print("Executing updated calculate_params with mamba_ratio =", mamba_ratio)
    print("DEBUG: Called calculate_params with mamba_ratio =", mamba_ratio)
    transformer_params_per_layer = (
        12 * d_model * d_model + 13 * d_model
    )
    
    # Calculate the number of Mamba layers
    total_mamba_layers = int(n_layers * mamba_ratio)
    
    # Calculate parameters for Mamba layers
    # Assuming MambaBlock has parameters based on d_state, d_conv, depth, and expand
    mamba_params_per_layer = (
        d_state * d_conv * mamba_expand * mamba_depth  # Example calculation
    )
    total_mamba_params = total_mamba_layers * mamba_params_per_layer
    
    # Total parameters
    total_params = n_layers * transformer_params_per_layer + total_mamba_params
    return total_params

def estimate_memory_usage(total_params, batch_size, height, width, d_model, dtype_size=4):
    model_memory = total_params * dtype_size  # Model parameters
    optimizer_memory = model_memory * 2  # Adam optimizer uses 2x model size
    input_memory = batch_size * height * width * dtype_size  # Input tensors
    conv_output_memory = batch_size * height * width * d_model * dtype_size  # After conv layer
    activations_memory = batch_size * (height * width) * d_model * dtype_size * 2  # Forward &amp; backward pass
    total_memory = model_memory + optimizer_memory + input_memory + conv_output_memory + activations_memory
    return total_memory / (1024**3)  # Convert to GB

def get_available_memory():
    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB
    else:
        return psutil.virtual_memory().total / (1024**3)  # Get actual system memory for CPU

def get_device_info():
    if torch.cuda.is_available():
        return {
            "device": "GPU",
            "name": torch.cuda.get_device_name(0),
            "compute_capability": torch.cuda.get_device_capability(0),
            "total_memory": torch.cuda.get_device_properties(0).total_memory / (1024**3),
            "cuda_version": torch.version.cuda
        }
    else:
        return {
            "device": "CPU",
            "name": "System CPU",
            "total_memory": psutil.virtual_memory().total / (1024**3),
            "cpu_count": psutil.cpu_count(logical=False),
            "cpu_freq": psutil.cpu_freq().max if psutil.cpu_freq() else "N/A"
        }

def can_fit_model(estimated_memory, available_memory, threshold=0.9):
    return estimated_memory &lt; available_memory * threshold

def estimate_single_configuration(n_layers, n_heads, d_model, batch_size, height, width):
    device_info = get_device_info()
    available_memory = get_available_memory()
    
    print(f"Device Information:")
    for key, value in device_info.items():
        print(f"  {key}: {value}")
    print(f"Available memory: {available_memory:.2f} GB")

    total_params = calculate_params(n_layers, n_heads, d_model)
    estimated_memory = estimate_memory_usage(total_params, batch_size, height, width, d_model)
    
    print(f"\nConfiguration:")
    print(f"  n_layers: {n_layers}")
    print(f"  n_heads: {n_heads}")
    print(f"  d_model: {d_model}")
    print(f"  batch_size: {batch_size}")
    print(f"  input_height: {height}")
    print(f"  input_width: {width}")
    print(f"Total parameters: {total_params:,}")
    print(f"Estimated memory usage: {estimated_memory:.2f} GB")
    
    if can_fit_model(estimated_memory, available_memory):
        print(f"Model should fit in {device_info['device']} memory.")
    else:
        print(f"Warning: Model may be too large for available {device_info['device']} memory!")
    
    print(f"Memory utilization: {(estimated_memory / available_memory) * 100:.2f}%")

</file>
<file name="src/data/__init__.py">

</file>
<file name="src/data/arc_dataset.py">
# gp2_arc/src/data/arc_dataset.py
import os
import json
import random
from typing import Union, List, Dict, Tuple, Any
import numpy as np
import pickle
import hashlib
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset
import logging
from torch.utils.data import get_worker_info
import math  # Import math module for ceiling division

try:
    from arckit.data import TaskSet, Task
except ImportError:
    TaskSet = None

logger = logging.getLogger(__name__)
logger.setLevel(logging.ERROR)  # Set to ERROR by default

# Create a handler that writes to stderr
handler = logging.StreamHandler()
handler.setLevel(logging.ERROR)

# Create a formatting for the logs
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)

# Add the handler to the logger
logger.addHandler(handler)

# Function to set debug mode
def set_debug_mode(debug=False):
    if debug:
        logger.setLevel(logging.DEBUG)
        handler.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.ERROR)
        handler.setLevel(logging.ERROR)

class ARCDataset(Dataset):
    def __init__(
        self,
        data_source: Union[str, List[Dict], 'TaskSet', Tuple[Union[List, 'TaskSet'], str]],
        is_test: bool = False,
        num_symbols: int = 10,
        test_split: float = 0.2,
        debug=False,
    ):
        self.test_split = test_split
        self.is_test = is_test
        self.num_symbols = num_symbols
        self.data_files = []  # Initialize data_files as an empty list
        self.data_source = data_source
        self.num_samples = 0
        self.data = []
        self.cache_path = self._generate_cache_path(
            data_source=self.data_source,
            num_symbols=self.num_symbols,
            is_test=self.is_test,
            test_split=self.test_split
        )

        if self._load_cache(self.cache_path):
            logger.debug("Data loaded from cache successfully.")
            return
        set_debug_mode(debug)
        logger.debug("Starting ARCDataset initialization")
        logger.debug(f"data_source type: {type(data_source)}")
        logger.debug(f"data_source content: {data_source}")
        logger.debug(f"self.test_split is set to: {self.test_split}")

        if isinstance(data_source, str):
            if os.path.isdir(data_source):
                logger.debug("Initializing dataset with data from directory")
                self.data_dir = data_source
                self.data_files = [
                    os.path.join(data_source, f)
                    for f in os.listdir(data_source)
                    if f.endswith('.json')
                ]
                random.shuffle(self.data_files)
                for file_path in self.data_files:
                    with open(file_path, 'r') as f:
                        task_data = json.load(f)
                    if isinstance(task_data, dict):
                        task_id = task_data.get('id', os.path.splitext(os.path.basename(file_path))[0])
                        samples = self._process_single_task(task_data, task_id=task_id)
                        self.data.extend(samples)
                        logger.debug(f"Added {len(samples)} samples from file {file_path} with task_id: {task_id}")
                    elif isinstance(task_data, list):
                        task_id = os.path.splitext(os.path.basename(file_path))[0]
                        samples = self._process_list_data(task_data, task_id=task_id)
                        self.data.extend(samples)
                        logger.debug(f"Assigned task_id '{task_id}' to list samples from file {file_path}")
                    else:
                        logger.error(f"Unexpected data format in file {file_path}: {type(task_data)}")
            elif os.path.isfile(data_source):
                with open(data_source, 'r') as f:
                    task_data = json.load(f)
                if isinstance(task_data, dict):
                    task_id = task_data.get('id', "default_task")
                    samples = self._process_single_task(task_data, task_id=task_id)
                    self.data.extend(samples)
                elif isinstance(task_data, list):
                    samples = self._process_list_data(task_data)
                    self.data.extend(samples)
                else:
                    logger.error(f"Unexpected data format in file {data_source}: {type(task_data)}")
            else:
                raise FileNotFoundError(f"Data source file or directory not found: {data_source}")
        elif TaskSet is not None and isinstance(data_source, TaskSet):
            logger.debug(f"TaskSet attributes before access: {dir(data_source)}")
            logger.debug(f"Does TaskSet have 'dataset' attribute? {hasattr(data_source, 'dataset')}")
            samples = self._process_arckit_data(data_source)
            self.data.extend(samples)
        elif isinstance(data_source, list):
            samples = self._process_list_data(data_source)
            self.data.extend(samples)
        else:
            raise ValueError(f"Unsupported data_source type: {type(data_source)}")

        self.num_samples = len(self.data)
        self._compute_and_cache_statistics()
        self._save_cache(self.cache_path)

    def _save_cache(self, cache_path: str):
        """
        Saves the dataset and its statistics to the specified cache path using pickle.
        
        Args:
            cache_path (str): The file path where the cache will be saved.
        """
        try:
            cache_data = {
                "data": self.data,
                "statistics": self.statistics
            }
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            logger.debug(f"Saved cache to {cache_path}")
        except Exception as e:
            logger.error(f"Failed to save cache to {cache_path}: {e}")

        # Add data validation
        self._validate_data()
        self._validate_data()

    def _validate_data(self):
        """
        Validates the dataset to ensure each sample contains the required keys and correct data types.
        Raises:
            ValueError: If any sample is missing required keys or has incorrect types.
        """
        required_keys = {"input", "output", "task_id"}
        for idx, sample in enumerate(self.data):
            # Check for required keys
            if not required_keys.issubset(sample.keys()):
                missing = required_keys - sample.keys()
                raise KeyError(f"Sample at index {idx} is missing keys: {missing}")
            
            # Validate 'input' and 'output' types
            for key in ["input", "output"]:
                if not isinstance(sample[key], torch.Tensor):
                    raise TypeError(f"Sample at index {idx} has '{key}' of type {type(sample[key])}, expected torch.Tensor.")
                
                if sample[key].ndimension() != 3 or sample[key].shape[0] != 1:
                    raise ValueError(f"Sample at index {idx} has '{key}' with shape {sample[key].shape}, expected shape (1, H, W).")
            
            # Validate 'task_id' type
            if not isinstance(sample["task_id"], str):
                raise TypeError(f"Sample at index {idx} has 'task_id' of type {type(sample['task_id'])}, expected str.")
        
        logger.debug("All samples passed validation.")
    
    def __len__(self):
        return len(self.data)

    def get_num_samples(self):
        return self.num_samples
    def __getitem__(self, idx):
        sample = self.data[idx]
        return sample["input"], sample["output"], sample["task_id"]

    def _count_samples_in_directory(self, directory: str):
        num_samples = 0
        file_list = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.json')]
        logger.debug(f"Counting samples in {len(file_list)} files")
        for file_path in file_list:
            try:
                with open(file_path, 'r') as f:
                    task_data = json.load(f)
                if isinstance(task_data, dict):
                    sample_count = len(task_data.get('test', [])) if self.is_test else len(task_data.get('train', []))
                    num_samples += sample_count
                    logger.debug(f"File {file_path}: {sample_count} samples")
                elif isinstance(task_data, list):
                    num_samples += len(task_data)
                    logger.debug(f"File {file_path}: {len(task_data)} samples")
                else:
                    logger.error(f"Unexpected data format in file {file_path}")
            except Exception as e:
                logger.error(f"Error processing file {file_path}: {e}", exc_info=True)
                continue  # Skip this file and proceed to the next
        logger.debug(f"Total samples counted: {num_samples}")
        return num_samples


    @staticmethod
    def _generate_cache_path(data_source: Union[str, List[Dict], 'TaskSet', Tuple[Union[List, 'TaskSet'], str]], num_symbols: int, is_test: bool, test_split: float) -&gt; str:
        dataset_version = "v1"
        hash_input = json.dumps({
            'version': dataset_version,
            'data_source': data_source if isinstance(data_source, list) else data_source.__str__(),
            'num_symbols': num_symbols,
            'is_test': is_test,
            'test_split': test_split
        }, sort_keys=True).encode('utf-8')
        hash_digest = hashlib.md5(hash_input).hexdigest()
        cache_filename = f"arc_dataset_cache_{hash_digest}.pkl"
        cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
        os.makedirs(cache_dir, exist_ok=True)
        return os.path.join(cache_dir, cache_filename)

    def _load_cache(self, cache_path: str) -&gt; bool:
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    cache_data = pickle.load(f)
                self.data = cache_data.get("data", [])
                self.statistics = cache_data.get("statistics", {})
                self.num_samples = len(self.data)
                logger.debug(f"Loaded cached data from {cache_path}")
                return True
            except Exception as e:
                logger.error(f"Failed to load cache from {cache_path}: {e}")
        return False

    def _compute_and_cache_statistics(self):
        """
        Computes dataset statistics and caches them alongside the dataset cache.
        """
        logger.debug("Computing dataset statistics")
        grid_size_stats = self._compute_grid_size_stats()
        symbol_frequencies = self._compute_symbol_frequencies()
        
        statistics = {
            "grid_size_stats": grid_size_stats,
            "symbol_frequencies": symbol_frequencies
        }
        
        # Update the cache dictionary with statistics
        self.statistics = statistics
        self._save_cache(self.cache_path)  # Ensure statistics are saved in the cache
        logger.debug("Dataset statistics computed and cached successfully")


    def _process_list_data(self, data_list: List[Dict], task_id: str) -&gt; List[Dict]:
        processed_data = []
        for idx, example in enumerate(data_list):
            if 'input' in example and 'output' in example and isinstance(example['input'], (list, np.ndarray)) and isinstance(example['output'], (list, np.ndarray)):
                # Preprocess the grids
                input_grid = self._preprocess_grid(example['input'])
                output_grid = self._preprocess_grid(example['output'])

                # Assign task_id if not present
                # Assign task_id from parameter, overriding any existing task_id in the data

                processed_data.append({
                    "input": input_grid,
                    "output": output_grid,
                    "task_id": task_id
                })
            else:
                logger.warning(f"Example at index {idx} missing 'input' or 'output' keys or has incorrect types.")
                # Optionally, skip or raise an error
                # raise ValueError("Unexpected item format in data_source.")
        return processed_data


    def _combine_data(self, official_data, synthetic_data_path):
        official_processed = self._process_arckit_data(official_data) if TaskSet is not None and isinstance(official_data, TaskSet) else official_data
        synthetic_processed = self._process_synthetic_data(synthetic_data_path)
        return official_processed + synthetic_processed

    def _process_synthetic_data(self, directory: str):
        self.data_files = []
        for filename in os.listdir(directory):
            if filename.endswith('.json'):
                file_path = os.path.join(directory, filename)
                self.data_files.append(file_path)
                logger.debug(f"Processing file: {file_path}")
                with open(file_path, 'r') as f:
                    try:
                        task_data = json.load(f)
                        # Assign task_id from filename
                        task_id = os.path.splitext(filename)[0]
                        processed_samples = self._process_single_task(task_data, task_id=task_id)
                        self.data.extend(processed_samples)
                    except json.JSONDecodeError as e:
                        logger.error(f"Error decoding JSON from file {file_path}: {e}")





    def _process_arckit_data(self, taskset: 'TaskSet') -&gt; List[Dict]:
        processed_data = []
        logger.debug(f"Processing TaskSet with {len(taskset.tasks)} tasks")
        for task in taskset.tasks:
            logger.debug(f"Processing task: {task.id}")
            logger.debug(f"Train samples: {len(task.train)}, Test samples: {len(task.test)}")
            # Process training samples
            for ex in task.train:
                try:
                    input_tensor = self._preprocess_grid(ex[0])
                    output_tensor = self._preprocess_grid(ex[1])
                    processed_data.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task.id
                    })
                except Exception as e:
                    logger.error(f"Error processing training example in task {task.id}: {e}", exc_info=True)
            
            # Process testing samples
            for ex in task.test:
                try:
                    input_tensor = self._preprocess_grid(ex[0])
                    output_tensor = self._preprocess_grid(ex[1])
                    processed_data.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task.id
                    })
                except Exception as e:
                    logger.error(f"Error processing testing example in task {task.id}: {e}", exc_info=True)
            
            logger.debug(f"Processed task {task.id}: Total samples added: {len(task.train) + len(task.test)}")
        
        logger.debug(f"Total samples processed from TaskSet: {len(processed_data)}")
        return processed_data


    def get_grid_size_stats(self) -&gt; Dict[str, Any]:
        """
        Returns the precomputed grid size statistics.
        
        Returns:
            Dict[str, Any]: A dictionary containing grid size statistics.
        """
        if hasattr(self, 'statistics') and 'grid_size_stats' in self.statistics:
            return self.statistics['grid_size_stats']
        else:
            logger.warning("Grid size statistics not available.")
            return {}
    
    def get_symbol_frequencies(self) -&gt; Dict[str, float]:
        """
        Returns the precomputed symbol frequencies.
        
        Returns:
            Dict[str, float]: A dictionary mapping symbols to their frequencies.
        """
        if hasattr(self, 'statistics') and 'symbol_frequencies' in self.statistics:
            return self.statistics['symbol_frequencies']
        else:
            logger.warning("Symbol frequencies not available.")
            return {}

    def _compute_grid_size_stats(self):
        max_height, max_width = 0, 0
        for sample in self.data:
            # Assuming sample["input"] and sample["output"] have shape [C, H, W]
            max_height = max(max_height, sample["input"].shape[1], sample["output"].shape[1])
            max_width = max(max_width, sample["input"].shape[2], sample["output"].shape[2])
        grid_size_stats = {"max_height": max_height, "max_width": max_width}
        self.max_grid_size = (max_height, max_width)
        return grid_size_stats

    def _compute_symbol_frequencies(self):
        symbol_counts = np.zeros(self.num_symbols, dtype=int)
        for sample in self.data:
            symbol_counts += np.bincount(sample["input"].flatten(), minlength=self.num_symbols)
            symbol_counts += np.bincount(sample["output"].flatten(), minlength=self.num_symbols)
        return symbol_counts / symbol_counts.sum()
    
    def _preprocess_grid(self, grid: Union[Dict, List, np.ndarray, torch.Tensor]) -&gt; torch.Tensor:
        logger.debug(f"Preprocessing grid with initial type: {type(grid)}")
        
        # Convert grid to torch.Tensor if it's a list or numpy array
        if isinstance(grid, list):
            grid_tensor = torch.tensor(grid, dtype=torch.float32)
            logger.debug(f"Converted list to tensor with shape: {grid_tensor.shape}")
        elif isinstance(grid, np.ndarray):
            grid_tensor = torch.from_numpy(grid).float()
            logger.debug(f"Converted numpy array to tensor with shape: {grid_tensor.shape}")
        elif isinstance(grid, torch.Tensor):
            grid_tensor = grid.float()
            logger.debug(f"Using existing tensor with shape: {grid_tensor.shape}")
        else:
            raise ValueError(f"Unexpected grid type: {type(grid)}")
    
        # Ensure grid_tensor has three dimensions [C, H, W]
        if grid_tensor.ndim == 2:
            logger.debug("Grid tensor is 2D. Adding channel dimension.")
            grid_tensor = grid_tensor.unsqueeze(0)  # Add channel dimension
            logger.debug(f"Grid tensor shape after unsqueeze: {grid_tensor.shape}")
        elif grid_tensor.ndim != 3:
            raise ValueError(f"Unexpected grid tensor dimensions: {grid_tensor.ndim}. Expected 2D or 3D tensor.")

        logger.debug(f"Grid shape before padding: {grid_tensor.shape}")

        # Apply padding using PyTorch's built-in functions
        padded_grid = self._pad_grid_torch(grid_tensor, height=30, width=30)

        logger.debug(f"Grid shape after padding: {padded_grid.shape}")
        return padded_grid
    def kronecker_scale(self, X, target_height=30, target_width=30):
        print(f"Kronecker scaling input shape: {X.shape}")
        h, w = X.shape
        scale_h = target_height / h
        scale_w = target_width / w
        d = int(np.floor(min(scale_h, scale_w)))
        
        X_scaled = np.kron(X, np.ones((d, d)))
        print(f"Kronecker scaled output shape: {X_scaled.shape}")
        return X_scaled


    def reverse_scaling(self, X_orig, X_pred):
        print(f"Reverse scaling - Original shape: {X_orig.shape}, Prediction shape: {X_pred.shape}")
        h, w = X_orig.shape
        # Reshape X_pred to 2D if it's 1D
        if X_pred.ndim == 1:
            X_pred = X_pred.reshape((int(np.sqrt(X_pred.size)), -1))
        
        X_pred_cropped = X_pred[:h, :w]  # Crop to original size
        
        if h == X_pred.shape[0] and w == X_pred.shape[1]:
            print("No rescaling needed")
            return X_pred_cropped
        
        # Calculate the downscale factor
        d_h = X_pred_cropped.shape[0] // h
        d_w = X_pred_cropped.shape[1] // w
        
        # Ensure the dimensions are compatible for reshaping
        if d_h &gt; 0 and d_w &gt; 0:
            try:
                X_rev = X_pred_cropped.reshape(h, d_h, w, d_w).mean(axis=(1, 3))
            except ValueError as e:
                print(f"Error during reshaping: {e}")
                print(f"X_pred_cropped shape: {X_pred_cropped.shape}, h: {h}, w: {w}, d_h: {d_h}, d_w: {d_w}")
                raise
        else:
            print(f"Invalid downscale factors: d_h={d_h}, d_w={d_w}")
            raise ValueError("Invalid dimensions for reverse scaling")
        # Resize the result to match the original target shape
        result = np.resize(X_rev.round().astype(int), X_orig.shape)
        print(f"Reverse scaled output shape: {result.shape}")
        return result

    def _scale_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray:
        return grid  # No scaling, preserve original size

    def _pad_grid_torch(self, grid: torch.Tensor, height: int, width: int) -&gt; torch.Tensor:
        """
        Pads the input grid tensor to the specified height and width using PyTorch's functional padding.
        
        Args:
            grid (torch.Tensor): The input grid tensor with shape [C, H, W].
            height (int): The target height after padding.
            width (int): The target width after padding.
        
        Returns:
            torch.Tensor: The padded grid tensor.
        """
        _, h, w = grid.shape
        pad_h = max((height - h) // 2, 0)
        pad_w = max((width - w) // 2, 0)

        # Calculate padding for top, bottom, left, and right
        padding = (pad_w, width - w - pad_w, pad_h, height - h - pad_h)  # (left, right, top, bottom)
        logger.debug(f"Padding applied: left={pad_w}, right={width - w - pad_w}, top={pad_h}, bottom={height - h - pad_h}")
    
        # Apply padding using PyTorch's functional pad
        padded_grid = F.pad(grid, padding, mode='constant', value=0)
        return padded_grid


    @staticmethod
    def collate_fn(batch):
        # Debugging: Check batch size
        logger.debug(f"Collating batch of size: {len(batch)}")
        
        if not batch:
            logger.warning("Empty batch received")
            return torch.tensor([]), torch.tensor([]), []

        inputs, outputs, task_ids = zip(*batch)

        # Find maximum dimensions in the batch
        max_h = max(input_tensor.size(1) for input_tensor in inputs)
        max_w = max(input_tensor.size(2) for input_tensor in outputs)

        # Debugging: Print maximum dimensions
        logger.debug(f"Maximum height in batch: {max_h}")
        logger.debug(f"Maximum width in batch: {max_w}")

        # Pad inputs and outputs to the maximum size
        padded_inputs = torch.stack([
            F.pad(input_tensor, (0, max_w - input_tensor.size(2), 0, max_h - input_tensor.size(1)))
            for input_tensor in inputs
        ])

        padded_outputs = torch.stack([
            F.pad(output_tensor, (0, max_w - output_tensor.size(2), 0, max_h - output_tensor.size(1)))
            for output_tensor in outputs
        ])

        # Debugging: Verify shapes after padding
        print(f"Padded inputs shape: {padded_inputs.shape}")
        print(f"Padded outputs shape: {padded_outputs.shape}")

        return padded_inputs, padded_outputs, list(task_ids)

</file>
<file name="src/training/trainer.py">
# gpt2_arc/src/training/trainer.py
import pytorch_lightning as pl
import torch
import logging
from torch import nn, optim
import time
from typing import Any, Dict, Optional
from collections import deque
from torch.optim.lr_scheduler import LambdaLR
from ..config import Config
from ..utils.helpers import differential_pixel_accuracy
from ..utils.results_collector import ResultsCollector
from torch.utils.data import DataLoader
from torch.utils.tensorboard import SummaryWriter
from pytorch_lightning.loggers import TensorBoardLogger
import os
from optuna.exceptions import TrialPruned
from pytorch_lightning.callbacks import Callback
import torch

logger = logging.getLogger(__name__)

class NanLossPruningCallback(Callback):
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        # Extract loss from outputs
        loss = outputs.get('loss') if isinstance(outputs, dict) else outputs
        if loss is not None:
            if torch.isnan(loss):
                logger.warning("NaN loss detected. Pruning the trial.")
                raise TrialPruned("NaN loss encountered, pruning this trial.")


class ARCTrainer(pl.LightningModule):
    def __init__(self, model, train_dataset, val_dataset, config: Config, results_collector=None):
        super().__init__()
        self.model = model
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config
        self.batch_size = config.training.batch_size
        self.lr = config.training.learning_rate
        self.train_losses = []
        self.logged_metrics = {}
        self.test_outputs = []  # Initialize an empty list to store test outputs
        self.test_results = []  # Initialize test results for storing test outcomes
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.results_collector = results_collector if results_collector else ResultsCollector(config)
        self.writer = SummaryWriter(f"runs/experiment_{self.results_collector.experiment_id}")
        
        if hasattr(self.model, 'loss_fn') and hasattr(self.model.loss_fn, 'weight'):
            logger.debug(f"Trainer's loss function class weights: {self.model.loss_fn.weight}")
        else:
            logger.debug("Trainer's loss function does not have class weights.")

    def get_tensorboard_logger(self):
        for logger in self.trainer.loggers:
            if isinstance(logger, TensorBoardLogger):
                return logger.experiment
        print("DEBUG: No TensorBoardLogger found in trainer.loggers")
        return None

    def training_step(self, batch, batch_idx):
        logger.debug(f"Training step - Batch type: {type(batch)}, length: {len(batch)}")
        
        if isinstance(batch, (list, tuple)) and len(batch) &gt;= 2:
            inputs, labels = batch[:2]
            task_ids = batch[2] if len(batch) &gt; 2 else None
        elif isinstance(batch, dict):
            inputs = batch.get("input_ids")
            labels = batch.get("labels")
            task_ids = batch.get("task_ids")
        else:
            raise ValueError(f"Unexpected batch format: {type(batch)}. Content: {batch}")

        # Ensure inputs and labels are the correct type
        inputs = inputs.float()
        labels = labels.long()

        outputs = self(inputs)
        loss = self.compute_loss(outputs, labels)
        
        if hasattr(self, 'log'):
            self.log("train_loss", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.train_losses.append(loss.item())
        self.results_collector.update_train_metrics(self.current_epoch, {"loss": loss.item()})
        
        tb_logger = self.get_tensorboard_logger()
        if tb_logger:
            tb_logger.add_scalar('train/loss', loss.item(), self.global_step)
            print(f"DEBUG: Logged training loss: {loss.item()} at step {self.global_step}")
        else:
            print(f"DEBUG: Failed to log training loss. No TensorBoard logger available.")
        
        return loss

    def validation_step(self, batch, batch_idx, dataloader_idx=0):
        logger.debug(f"Validation step - Batch type: {type(batch)}, length: {len(batch)}")
        
        if isinstance(batch, (list, tuple)):
            if len(batch) &lt; 2:
                logger.error(f"Missing inputs or labels in batch. Inputs: {batch[0] if len(batch) &gt; 0 else None}, Labels: {batch[1] if len(batch) &gt; 1 else None}")
                raise ValueError("Batch must contain inputs and labels.")
            inputs, labels = batch[:2]
            task_ids = batch[2] if len(batch) &gt; 2 else None
        elif isinstance(batch, dict):
            inputs = batch.get("input_ids")
            labels = batch.get("labels")
            task_ids = batch.get("task_ids")
            if inputs is None or labels is None:
                logger.error(f"Missing inputs or labels in batch. Inputs: {inputs}, Labels: {labels}")
                raise ValueError("Batch must contain inputs and labels.")
        else:
            logger.error(f"Unexpected batch format: {type(batch)}. Content: {batch}")
            raise ValueError(f"Unexpected batch format: {type(batch)}. Content: {batch}")

        # Ensure inputs and labels are the correct type
        inputs = inputs.float()
        labels = labels.long()

        outputs = self(inputs)
        loss = self.compute_loss(outputs, labels)
        
        if hasattr(self, 'log'):
            self.log("val_loss", loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.logged_metrics["val_loss"] = loss.item()
        self.results_collector.update_val_metrics(self.current_epoch, {"loss": loss.item()})
        
        try:
            self.writer.add_scalar('val/loss', loss.item(), self.current_epoch)
            print(f"DEBUG: Logged validation loss: {loss.item()} for epoch {self.current_epoch}")
        except Exception as e:
            print(f"DEBUG: Error logging validation step: {str(e)}")
        
        return loss

    def on_test_epoch_start(self):
        self.test_outputs = []

    def test_step(self, batch, batch_idx):
        logger.debug(f"DEBUG: test_step input - batch: {batch}, batch_idx: {batch_idx}")
        logger.debug(f"DEBUG: Test step - Batch type: {type(batch)}, length: {len(batch)}")

        # Unpack batch
        if len(batch) == 3:
            inputs, outputs, task_ids = batch
        elif len(batch) == 2:
            inputs, outputs = batch
            task_ids = None  # Set to None or a default value
        else:
            raise ValueError(f"Unexpected batch format with length {len(batch)}")
        logger.debug(f"DEBUG: Task IDs in batch: {task_ids}")

        inputs = inputs.float()
        outputs = outputs.long()

        attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32, device=inputs.device)

        model_outputs = self(inputs, attention_mask)
        loss = self.compute_loss(model_outputs, outputs)

        accuracies = []
        diff_accuracies = []
        
        for i in range(len(inputs)):
            accuracy = self.compute_accuracy(model_outputs[i:i+1], outputs[i:i+1])
            diff_accuracy, _, _ = differential_pixel_accuracy(inputs[i:i+1], outputs[i:i+1], model_outputs[i:i+1].argmax(dim=-1))
            accuracies.append(accuracy.item())
            diff_accuracies.append(diff_accuracy)
            logger.debug(f"DEBUG: diff_accuracy type: {type(diff_accuracy)}, value: {diff_accuracy}")

        result = {
            'test_loss': loss.item(),
            'task_ids': task_ids,
            'test_accuracy': sum(accuracies) / len(accuracies) if accuracies else 0,
            'test_diff_accuracy': sum(diff_accuracies) / len(diff_accuracies) if diff_accuracies else 0,
        }
        logger.debug(f"DEBUG: Test loss: {result['test_loss']}, Avg accuracy: {result['test_accuracy']}, Avg diff accuracy: {result['test_diff_accuracy']}")

        # Log task-specific metrics
        if task_ids is not None:
            for task_id, accuracy, diff_accuracy in zip(task_ids, accuracies, diff_accuracies):
                result[f"{task_id}_test_accuracy"] = accuracy
                result[f"{task_id}_test_diff_accuracy"] = diff_accuracy
                self.log(f"{task_id}_test_accuracy", accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)
                self.log(f"{task_id}_test_diff_accuracy", diff_accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)

        try:
            self.writer.add_scalar('test/loss', result['test_loss'], self.current_epoch)
            self.writer.add_scalar('test/avg_accuracy', result['test_accuracy'], self.current_epoch)
            self.writer.add_scalar('test/diff_accuracy', result['test_diff_accuracy'], self.current_epoch)
            logger.debug(f"DEBUG: Logged test metrics for epoch {self.current_epoch}: loss={result['test_loss']}, avg_accuracy={result['test_accuracy']}, diff_accuracy={result['test_diff_accuracy']}")
        except Exception as e:
            logger.error(f"DEBUG: Error logging test step: {str(e)}")

        logger.debug(f"DEBUG: test_step output - result: {result}")
        logger.debug(f"DEBUG: Test step result: {result}")

        # Append the result to self.test_outputs
        self.test_outputs.append({key: value.item() if isinstance(value, torch.Tensor) else value for key, value in result.items()})

        return result

    def on_test_epoch_end(self):
        total_loss = torch.stack([torch.tensor(x['test_loss']) for x in self.test_outputs]).mean()
        all_accuracies = []
        all_diff_accuracies = []

        for output in self.test_outputs:
            if 'test_accuracy' in output:
                all_accuracies.append(output['test_accuracy'])
            if 'test_diff_accuracy' in output:
                all_diff_accuracies.append(output['test_diff_accuracy'])

        avg_accuracy = sum(all_accuracies) / len(all_accuracies) if all_accuracies else 0
        avg_diff_accuracy = sum(all_diff_accuracies) / len(all_diff_accuracies) if all_diff_accuracies else 0

        self.log('avg_test_loss', total_loss, prog_bar=True)
        self.log('avg_test_accuracy', avg_accuracy, prog_bar=True)
        self.log('avg_test_diff_accuracy', avg_diff_accuracy, prog_bar=True)

        print(f"DEBUG: Test epoch end - Avg loss: {total_loss}, Avg accuracy: {avg_accuracy}, Avg diff accuracy: {avg_diff_accuracy}")

    def compute_accuracy(self, outputs, targets):
        predictions = outputs.argmax(dim=-1)
        # Reshape predictions to match the target shape
        predictions = predictions.view(targets.size())
        # Calculate accuracy over all elements
        accuracy = (predictions == targets).float().mean()
        print(f"DEBUG: compute_accuracy - Accuracy: {accuracy.item()}")
        return accuracy

    def compute_diff_accuracy(self, inputs, targets, outputs):
        predictions = outputs.argmax(dim=-1)
        diff_accuracies, _, _ = differential_pixel_accuracy(inputs, targets, predictions)
        return diff_accuracies
        predictions = outputs.argmax(dim=-1)
        # Reshape predictions to match the target shape
        predictions = predictions.view(targets.size())
        # Calculate accuracy for each sample in the batch
        accuracies = (predictions == targets).float().mean(dim=[1, 2, 3])
        return accuracies
        
    def on_validation_epoch_end(self):
        # Compute average validation loss
        val_loss = self.trainer.callback_metrics.get('val_loss')
        if val_loss is not None:
            avg_val_loss = val_loss.item()
        else:
            avg_val_loss = float('inf')  # Default to a high value if val_loss is not available

        # Update best_val_loss and best_epoch
        if avg_val_loss &lt; self.best_val_loss:
            self.best_val_loss = avg_val_loss
            self.best_epoch = self.current_epoch

        # Log validation metrics
        self.log('val_loss', avg_val_loss)
        self.log('best_val_loss', self.best_val_loss)
        self.log('best_epoch', self.best_epoch)

        # Update the results collector
        self.results_collector.update_val_metrics(self.current_epoch, {
            "avg_loss": avg_val_loss,
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch
        })

        # Log additional information
        self.log('epoch', self.current_epoch)

    def configure_optimizers(self):
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        lr_scheduler = {
            'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95),
            'name': 'learning_rate',
        }
        return [optimizer], [lr_scheduler]

    def on_fit_end(self):
        self.results_collector.save_to_json(f"results/experiment_{self.results_collector.experiment_id}.json")
        try:
            self.writer.close()
            print("DEBUG: TensorBoard writer closed successfully.")
        except Exception as e:
            print(f"DEBUG: Error closing TensorBoard writer: {str(e)}")
        print("DEBUG: Results saved and TensorBoard writer closed.")

    def train_dataloader(self):
        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=0)

    def val_dataloader(self):
        loader = DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=0)
        logger.debug(f"DEBUG: Test dataloader created with {len(loader)} batches")
        return loader

    def test_dataloader(self):
        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4)

    def compute_loss(self, outputs, labels):
        loss = nn.CrossEntropyLoss()(
            outputs.view(-1, outputs.size(-1)), labels.view(-1)
        )

        logger.debug(f"Computed loss: {loss.item()}")
        return loss

    def forward(self, input_ids, attention_mask=None):
        return self.model(input_ids, attention_mask)
    def log_hyperparameters(self):
        hparams = {
            'learning_rate': self.config.training.learning_rate,
            'batch_size': self.config.training.batch_size,
            'n_embd': self.config.model.n_embd,
            'n_head': self.config.model.n_head,
            'n_layer': self.config.model.n_layer,
        }
        metric_dict = {
            'train_loss': 0,
            'val_loss': 0,
            'test_accuracy': 0,
        }
        try:
            self.writer.add_hparams(hparams, metric_dict)
            print(f"DEBUG: Successfully logged hyperparameters: {hparams}")
        except Exception as e:
            print(f"DEBUG: Error logging hyperparameters: {str(e)}")

</file>
<file name="src/training/__init__.py">

</file>
<file name="src/training/train.py">
# gpt2_arc/src/training/train.py
import argparse
import multiprocessing
import sys
import logging
import os
import json
import datetime
from unittest.mock import MagicMock, patch
import optuna
import arckit
import numpy as np
import torch
from lightning.pytorch.profilers import PyTorchProfiler
from pytorch_lightning.callbacks import Callback
from torch.profiler import ProfilerActivity
from torch.utils.data import DataLoader, WeightedRandomSampler

# Define the base directory for the arc-neural-reasoning-model
arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))

# Add the root directory of the project to the PYTHONPATH
project_root = arc_model_dir
sys.path.insert(0, project_root)

import pytorch_lightning as pl
#import torch.autograd.profiler as profiler
import torch
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.utils.experiment_tracker import ExperimentTracker
from gpt2_arc.src.utils.results_collector import ResultsCollector
from gpt2_arc.src.utils import GrokfastCallback

def get_num_workers():
    try:
        return multiprocessing.cpu_count() // 2  # Use half of the available CPUs
    except NotImplementedError:
        return 4  # Default fallback
logger = logging.getLogger(__name__)

class ConfigSavingModelCheckpoint(ModelCheckpoint):
    def __init__(self, config, trial_num='NA', task_id='NA', iter_num='NA', *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.config = config
        self.trial_num = trial_num
        self.task_id = task_id
        self.iter_num = iter_num
        self.timestamp = datetime.datetime.now().strftime("%Y%m%dT%H%M%S")  # e.g., 20240308T153045

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        # Add custom metadata to the checkpoint
        checkpoint['model_config'] = self.config.model.__dict__
        checkpoint['trial_num'] = self.trial_num
        checkpoint['task_id'] = self.task_id
        checkpoint['iter_num'] = self.iter_num
        checkpoint['timestamp'] = self.timestamp

        # Add the current epoch to the checkpoint
        checkpoint['epoch'] = trainer.current_epoch

        super().on_save_checkpoint(trainer, pl_module, checkpoint)

    def format_checkpoint_name(self, metrics):
        """
        Override the method to include custom placeholders in the filename.
        """
        return self.filename.format(
            trial_num=self.trial_num,
            task_id=self.task_id,
            iter_num=self.iter_num,
            val_loss=metrics.get("val_loss", 0.0),
            epoch=metrics.get("epoch", 0),
            timestamp=self.timestamp
        )

class ModelConfigSaver(Callback):
    def __init__(self, config):
        """
        Initialize the ModelConfigSaver callback with the current configuration.

        Args:
            config (Config): The configuration object containing model parameters.
        """
        super().__init__()
        self.config = config

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        """
        Override the checkpoint saving to include the model configuration.

        Args:
            trainer (pl.Trainer): The Trainer instance.
            pl_module (pl.LightningModule): The LightningModule being trained.
            checkpoint (dict): The checkpoint dictionary to be modified.
        """
        checkpoint['model_config'] = self.config.model.__dict__
def main(args):
    # Set float32 matrix multiplication precision
    torch.set_float32_matmul_precision(args.matmul_precision)
    logger.info(f"Set float32 matmul precision to: {args.matmul_precision}")
    log_level = getattr(logging, args.log_level.upper() if hasattr(args, 'log_level') else 'DEBUG', logging.DEBUG)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    profiler = PyTorchProfiler(
        dirpath=args.profiler_dirpath,
        filename=args.profiler_filename,
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],  # Include CUDA activities
        record_shapes=True,
        with_stack=True  # Enable stack tracing
    ) if args.use_profiler else None
    
    logger.setLevel(logging.DEBUG)  # Ensure logger is set to DEBUG
    
    logger.info("Starting main function")
    logger.debug(f"Command line arguments: {args}")

    trainer = None  # Initialize trainer to None

    try:
        if args.use_optuna:
            logger.info("Loading best hyperparameters from Optuna study")
            study_name = args.optuna_study_name

            if study_name is None:
                # Retrieve all study summaries from the storage
                study_summaries = optuna.get_all_study_summaries(storage=args.optuna_storage)
                study_names = [summary.study_name for summary in study_summaries]
                
                if len(study_names) == 1:
                    study_name = study_names[0]
                    logger.info(f"Automatically selected the only available study: {study_name}")
                elif len(study_names) == 0:
                    logger.error("No studies found in the specified Optuna storage.")
                    sys.exit(1)
                else:
                    logger.error("Multiple studies found in the specified Optuna storage. Please specify the study name using --optuna-study-name.")
                    sys.exit(1)

            study = optuna.load_study(study_name=study_name, storage=args.optuna_storage)
            best_params = study.best_params
            logger.debug(f"Loaded best parameters: {best_params}")
            
            n_head = 2 ** best_params['n_head_exp']
            n_embd = n_head * best_params['n_embd_multiplier']
            n_embd = 2 ** int(np.log2(n_embd))
            model_config = ModelConfig(
                n_embd=n_embd,
                n_head=n_head,
                n_layer=best_params['n_layer'],
                dropout=best_params['dropout']
            )
            training_config = TrainingConfig(
                batch_size=best_params['batch_size'],
                learning_rate=best_params['learning_rate'],
                max_epochs=args.max_epochs,
                use_gpu=args.use_gpu,
                log_level=args.log_level,
                use_synthetic_data=args.use_synthetic_data,
                synthetic_data_path=args.synthetic_data_path
            )
            training_config = TrainingConfig(
                batch_size=best_params['batch_size'],
                learning_rate=best_params['learning_rate'],
                max_epochs=args.max_epochs  # Always use the user-provided max_epochs
            )
        else:
            logger.info("Using provided or default hyperparameters")
            model_config = ModelConfig(
                n_embd=args.n_embd,
                n_head=args.n_head,
                n_layer=args.n_layer,
                mamba_ratio=args.mamba_ratio,
                d_state=args.d_state,
                d_conv=args.d_conv,
                dropout=args.dropout,
                mamba_depth=args.mamba_depth,
                mamba_expand=args.mamba_expand
            )
            training_config = TrainingConfig(
                batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                max_epochs=args.max_epochs,
                use_gpu=args.use_gpu,
                log_level=args.log_level,
                use_synthetic_data=args.use_synthetic_data,
                synthetic_data_path=args.synthetic_data_path,
                use_grokfast=args.use_grokfast,
                grokfast_type=args.grokfast_type,
                grokfast_alpha=args.grokfast_alpha,
                grokfast_lamb=args.grokfast_lamb,
                grokfast_window_size=args.grokfast_window_size
            )
        
        config = Config(model=model_config, training=training_config)
        logger.debug(f"Configuration: {config}")

        # Load data
        logger.info("Loading data")
        if args.use_synthetic_data:
            if not args.synthetic_data_path:
                raise ValueError("Synthetic data path not provided")
            logger.info(f"Loading synthetic data from {args.synthetic_data_path}")
            synthetic_files = os.listdir(args.synthetic_data_path)
            logger.debug(f"Total files in synthetic data path: {len(synthetic_files)}")
            logger.debug(f"Sample files: {synthetic_files[:5]}... (total {len(synthetic_files)})")
            train_data = ARCDataset(args.synthetic_data_path)
            synthetic_files = os.listdir(args.synthetic_data_path)
            logger.debug(f"Listing files in synthetic data path for validation: {synthetic_files[:5]}... (total {len(synthetic_files)})")
            val_data = ARCDataset(args.synthetic_data_path, is_test=True)
        else:
            logger.info("Loading ARC dataset")
            train_set, eval_set = arckit.load_data()
            train_data = ARCDataset(train_set)
            val_data = ARCDataset(eval_set)

        # Access dataset statistics
        train_grid_stats = train_data.get_grid_size_stats()
        train_symbol_freq = train_data.get_symbol_frequencies()

        val_grid_stats = val_data.get_grid_size_stats()
        val_symbol_freq = val_data.get_symbol_frequencies()

        # Convert train_symbol_freq from numpy array to dictionary with string keys
        train_symbol_freq_dict = {str(idx): float(freq) for idx, freq in enumerate(train_symbol_freq)}

        # Update the TrainingConfig with the symbol_freq dictionary
        training_config.symbol_freq = train_symbol_freq_dict

        logger.info(f"Train Grid Size Stats: {train_grid_stats}")
        logger.info(f"Train Symbol Frequencies: {train_symbol_freq_dict}")
        logger.info(f"Validation Grid Size Stats: {val_grid_stats}")
        logger.info(f"Validation Symbol Frequencies: {val_symbol_freq}")

        # Initialize experiment tracker
        tracker = ExperimentTracker(config, project=args.project)

        # Log dataset statistics to ExperimentTracker
        tracker.log_metric("train_max_grid_height", train_grid_stats.get("max_height", 0))
        tracker.log_metric("train_max_grid_width", train_grid_stats.get("max_width", 0))
        tracker.log_metric("train_symbol_frequencies", train_symbol_freq)

        tracker.log_metric("val_max_grid_height", val_grid_stats.get("max_height", 0))
        tracker.log_metric("val_max_grid_width", val_grid_stats.get("max_width", 0))
        tracker.log_metric("val_symbol_frequencies", val_symbol_freq)

        # Example: Adjust model configuration based on grid size stats
        max_grid_height = max(train_grid_stats.get("max_height", 30), val_grid_stats.get("max_height", 30))
        max_grid_width = max(train_grid_stats.get("max_width", 30), val_grid_stats.get("max_width", 30))
        logger.debug(f"Adjusted max grid size - Height: {max_grid_height}, Width: {max_grid_width}")

        # Set the number of classes
        num_classes = 10
        logger.info(f"Number of classes set to: {num_classes}")

        num_train_samples = train_data.get_num_samples()
        num_val_samples = val_data.get_num_samples()
        logger.info(f"Number of training examples: {num_train_samples}")
        logger.info(f"Number of validation examples: {num_val_samples}")
        
        if num_train_samples == 0 or num_val_samples == 0:
            logger.error("The dataset is empty. Please check the synthetic data path or dataset contents.")
            return

        logger.debug(f"Train data size: {train_data.get_num_samples()}, Validation data size: {val_data.get_num_samples()}")

        # Set the number of classes
        num_classes = 10
        logger.info(f"Number of classes set to: {num_classes}")

        # Create DataLoader instances
        logger.info("Creating DataLoader instances")
        # Create DataLoader instances
        logger.info("Creating DataLoader instances")
        if config.training.balance_symbols:
            if config.training.balancing_method == "weighting":
                # Compute class weights (inverse of frequencies)
                class_weights = 1.0 / torch.tensor(train_symbol_freq, dtype=torch.float)
                # Removed WeightedRandomSampler as it is not appropriate for multi-class samples
                train_loader = DataLoader(
                    train_data,
                    batch_size=config.training.batch_size,
                    num_workers=get_num_workers(),
                    shuffle=True,  # Enable shuffle
                    pin_memory=True if args.use_gpu else False,
                    prefetch_factor=config.training.prefetch_factor,
                    persistent_workers=config.training.persistent_workers
                )
                logger.debug("Class weights applied in loss function. WeightedRandomSampler removed.")
            elif config.training.balancing_method == "oversampling":
                # Placeholder for oversampling implementation
                logger.info("Oversampling method selected, but not yet implemented.")
                # Implement oversampling logic here if desired
                train_loader = DataLoader(
                    train_data,
                    batch_size=config.training.batch_size,
                    num_workers=get_num_workers(),
                    shuffle=True,  # Enable shuffle if not using a sampler
                    pin_memory=True if args.use_gpu else False,
                    prefetch_factor=config.training.prefetch_factor,
                    persistent_workers=config.training.persistent_workers
                )
            else:
                logger.warning(f"Unknown balancing method: {config.training.balancing_method}. Skipping balancing.")
                train_loader = DataLoader(
                    train_data,
                    batch_size=config.training.batch_size,
                    num_workers=get_num_workers(),
                    shuffle=True,  # Enable shuffle
                    pin_memory=True if args.use_gpu else False,
                    prefetch_factor=config.training.prefetch_factor,
                    persistent_workers=config.training.persistent_workers
                )
        else:
            train_loader = DataLoader(
                train_data,
                batch_size=config.training.batch_size,
                num_workers=get_num_workers(),
                shuffle=True,  # Enable shuffle
                pin_memory=True if args.use_gpu else False,
                prefetch_factor=config.training.prefetch_factor,
                persistent_workers=config.training.persistent_workers
            )
        val_loader = DataLoader(
            val_data,
            batch_size=config.training.batch_size,
            num_workers=get_num_workers(),
            pin_memory=True if args.use_gpu else False,
            prefetch_factor=config.training.prefetch_factor,
            persistent_workers=config.training.persistent_workers
        )
        logger.debug(f"DataLoaders created with batch size {args.batch_size}")

        # Initialize model
        logger.info("Initializing model")
        model = GPT2ARC(config=config, num_classes=num_classes, symbol_freq=train_symbol_freq_dict)
        logger.debug(f"Model initialized with config: {model_config}")

        # Load the checkpoint if specified
        if args.model_checkpoint:
            logger.info(f"Loading model from checkpoint: {args.model_checkpoint}")
            checkpoint = torch.load(args.model_checkpoint)
            if 'model_config' in checkpoint:
                model_config = ModelConfig(**checkpoint['model_config'])
                model = GPT2ARC(config=model_config)
            model.load_state_dict(checkpoint['state_dict'])

        # Initialize results collector
        results_collector = ResultsCollector(config)

        # Initialize experiment tracker
        tracker = ExperimentTracker(config, project=args.project)

        logger.debug("Initializing ExperimentTracker")
        tracker = ExperimentTracker(config, project=args.project)

        logger.debug("Initializing ARCTrainer")
        trainer = ARCTrainer(
            model=model,
            train_dataset=train_data,
            val_dataset=val_data,
            config=config
        )
        trainer.log_hyperparameters()

        # Determine accelerator parameters based on the --accelerator argument
        if args.accelerator == "tpu":
            accelerator = 'tpu'
            devices = 'xla:1'  # Use 'xla:8' for TPU v3-8 pods
            strategy = 'tpu_spawn'  # Recommended strategy for TPU
        elif args.accelerator == "gpu":
            if torch.cuda.is_available():
                accelerator = 'gpu'
                devices = 1
            else:
                accelerator = 'cpu'
                devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
        else:
            accelerator = 'cpu'
            devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
        
        # Initialize callbacks list                                                                                  
        callbacks = []

        # Initialize GrokfastCallback if enabled
        if config.training.use_grokfast:
            grokfast_callback = GrokfastCallback(
                filter_type=config.training.grokfast_type,  # 'ema' or 'ma'
                alpha=config.training.grokfast_alpha,
                lamb=config.training.grokfast_lamb,
                window_size=config.training.grokfast_window_size if config.training.grokfast_type == 'ma' else 100,  # default for ma
                warmup=True,
                trigger=False
            )
            callbacks.append(grokfast_callback)
            logger.info("GrokfastCallback added to the training callbacks.")
        else:
            logger.info("Grokfast is disabled; no callback added.")

        # Add the standard ModelCheckpoint callback
        if not args.no_checkpointing:
            checkpoint_callback = ModelCheckpoint(
                dirpath="checkpoints",
                filename="checkpoint-{epoch:02d}-{val_loss:.4f}",
                save_top_k=3,
                monitor="val_loss",
                mode="min",
            )
            callbacks.append(checkpoint_callback)

            # Instantiate and add the ModelConfigSaver callback
            model_config_saver = ModelConfigSaver(config)
            callbacks.append(model_config_saver)
            logger.info("ModelConfigSaver callback added to the training callbacks.")



        logger.info("Setting up PyTorch Lightning trainer")

        # Define trial_num, task_id, and iter_num
        trial_num = 0  # Initialize to 0 or another appropriate default
        task_id = "default_task"  # Replace with dynamic task identification if necessary
        iter_num = 1  # Initialize to 1; increment as needed within your training loop

        # Removed the custom ConfigSavingModelCheckpoint as it's not needed

        if not args.no_logging:
            tb_logger = TensorBoardLogger(
                save_dir="runs",
                name=f"experiment_{trainer.results_collector.experiment_id}"
            )
            logger.debug(f"TensorBoard logger initialized. Log dir: {tb_logger.log_dir}")
        else:
            tb_logger = False
            logger.debug("Logging is disabled")

        pl_trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            logger=tb_logger,
            callbacks=callbacks if callbacks else None,  # This now includes ModelCheckpoint
            enable_checkpointing=not args.no_checkpointing,
            enable_progress_bar=not args.no_progress_bar,
            fast_dev_run=args.fast_dev_run,  # Use the command-line argument
            gradient_clip_val=1.0,    # Add gradient clipping
            precision=16,             # Enable Automatic Mixed Precision
            accelerator=accelerator,
            devices=devices,
            strategy=strategy,
            profiler=profiler
        )

        if tb_logger:
            trainer.results_collector.set_tensorboard_log_path(tb_logger.log_dir)
            logger.debug(f"TensorBoard log path set in results collector: {tb_logger.log_dir}")

        # Log initial memory usage
        if args.use_gpu and torch.cuda.is_available():
            logger.info(f"Initial CUDA memory allocated: {torch.cuda.memory_allocated()} bytes")
            logger.info(f"Initial CUDA memory reserved: {torch.cuda.memory_reserved()} bytes")

        # Train the model
        logger.info("Starting model training")
        pl_trainer.fit(trainer, train_dataloaders=train_loader, val_dataloaders=val_loader)

        # Log memory usage after training
        if args.use_gpu and torch.cuda.is_available():
            logger.info(f"CUDA memory allocated after training: {torch.cuda.memory_allocated()} bytes")
            logger.info(f"CUDA memory reserved after training: {torch.cuda.memory_reserved()} bytes")

        # After training, run test
        logger.info("Running model evaluation")
        test_results = pl_trainer.test(trainer)
        if test_results:
            avg_test_loss = sum(result['avg_test_loss'] for result in test_results) / len(test_results)
            avg_test_accuracy = sum(result['avg_test_accuracy'] for result in test_results) / len(test_results)
            avg_test_diff_accuracy = sum(result['avg_test_diff_accuracy'] for result in test_results) / len(test_results)

            logger.info(f"Test results - Loss: {avg_test_loss}, Accuracy: {avg_test_accuracy}, Diff Accuracy: {avg_test_diff_accuracy}")

            results = {
                "avg_test_loss": avg_test_loss,
                "avg_test_accuracy": avg_test_accuracy,
                "avg_test_diff_accuracy": avg_test_diff_accuracy,
            }

            # Add task-specific results
            for result in test_results:
                for key, value in result.items():
                    if key.endswith('_test_accuracy') or key.endswith('_test_diff_accuracy'):
                        results[key] = value

            trainer.results_collector.set_test_results(results)

        trainer.results_collector.set_final_metrics({
            "best_val_loss": trainer.best_val_loss,
            "best_epoch": trainer.best_epoch,
            "final_test_loss": avg_test_loss,
            "final_test_accuracy": avg_test_accuracy
        })

        # Save the final model with configuration
        logger.info("Saving final model with configuration")
        model_path = f"final_model_{trainer.results_collector.experiment_id}.pth"
        os.makedirs("checkpoints", exist_ok=True)
        torch.save({
            'state_dict': trainer.model.state_dict(),
            'model_config': trainer.config.model.__dict__
        }, model_path)
        trainer.results_collector.set_checkpoint_path(model_path)
        logger.debug(f"Model and configuration saved to: {model_path}")

        # Save results
        logger.info("Saving experiment results")
        os.makedirs("results", exist_ok=True)
        results_path = f"results/experiment_{trainer.results_collector.experiment_id}.json"
        trainer.results_collector.save_to_json(results_path)
        logger.debug(f"Results saved to: {results_path}")

    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            logger.error("CUDA out of memory error occurred.")
            logger.error("Consider reducing the batch size or model complexity.")
            raise RuntimeError("CUDA out of memory error occurred.")
        else:
            logger.error(f"A runtime error occurred: {str(e)}", exc_info=True)
            raise RuntimeError(f"A runtime error occurred: {str(e)}")
    except Exception as e:
        logger.error(f"An unexpected error occurred: {str(e)}", exc_info=True)
        sys.exit(1)  # Exit the program after logging the error
    finally:
        if 'tracker' in locals():
            tracker.finish()

    if trainer is not None:
        # ... proceed with training ...
        pass
    else:
        logger.error("Trainer was not initialized. Exiting the training loop.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train the ARC Neural Reasoning Model")
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--use-profiler", action="store_true", help="Enable the custom profiler")
    group.add_argument("--fast-dev-run", action="store_true", help="Run a fast development test")
    
    parser.add_argument(
        "--optuna-study-name",
        type=str,
        default=None,
        help="Name of the Optuna study to load. If not provided and only one study exists in storage, it will be used automatically."
    )
    parser.add_argument("--optuna-storage", type=str, default="sqlite:///optuna_results.db", help="Storage URL for the Optuna study")
    parser.add_argument("--n-embd", type=int, default=4, help="Embedding dimension for profiling")
    parser.add_argument("--n-head", type=int, default=1, help="Number of attention heads for profiling")
    parser.add_argument("--n-layer", type=int, default=1, help="Number of transformer layers for profiling")
    parser.add_argument("--batch-size", type=int, default=32, help="Batch size for profiling")
    parser.add_argument("--learning-rate", type=float, default=1e-4, help="Learning rate")
    parser.add_argument("--max-epochs", type=int, required=True, help="Maximum number of epochs")
    parser.add_argument("--mamba-ratio", type=float, default=0.0, help="Mamba ratio (float value)")

    parser.add_argument("--dropout", type=float, default=0.05, help="Dropout rate")
    parser.add_argument("--d-state", type=int, default=4, help="Mamba state dimension")
    parser.add_argument("--d-conv", type=int, default=1, help="Mamba convolution dimension")
    parser.add_argument("--mamba-depth", type=int, default=1, help="Depth of each Mamba layer")
    parser.add_argument("--mamba-expand", type=int, default=2, help="Expand factor for each Mamba layer")
    parser.add_argument("--use-gpu", action="store_true", help="Use GPU for training if available")
    parser.add_argument("--use-grokfast", action="store_true", help="Enable Grokfast for gradient filtering.")
    parser.add_argument(
        "--grokfast-type",
        type=str,
        default="ema",
        choices=["ema", "ma"],
        help="Type of Grokfast filter to use: 'ema' or 'ma'."
    )
    parser.add_argument(
        "--grokfast-alpha",
        type=float,
        default=0.98,
        help="Alpha parameter for Grokfast-EMA."
    )
    parser.add_argument(
        "--grokfast-lamb",
        type=float,
        default=2.0,
        help="Lambda parameter for Grokfast filters."
    )
    parser.add_argument(
        "--grokfast-window_size",
        type=int,
        default=100,
        help="Window size for Grokfast-MA."
    )
    parser.add_argument("--no-logging", action="store_true", help="Disable logging")
    parser.add_argument("--no-checkpointing", action="store_true", help="Disable checkpointing")
    parser.add_argument("--no-progress-bar", action="store_true", help="Disable progress bar")
    parser.add_argument("--model_checkpoint", type=str, help="Path to the model checkpoint to resume training")
    parser.add_argument("--project", type=str, default="gpt2-arc", help="W&amp;B project name")
    parser.add_argument("--results-dir", type=str, default="./results", help="Directory to save results")
    parser.add_argument("--run-name", type=str, default="default_run", help="Name of the run for saving results")
    parser.add_argument("--use-synthetic-data", action="store_true", help="Use synthetic data for training")
    parser.add_argument(
        "--matmul-precision",
        type=str,
        default="medium",
        choices=["highest", "high", "medium"],
        help="Set the internal precision of float32 matrix multiplications. Options: 'highest', 'high', 'medium'. Defaults to 'medium'."
    )
    parser.add_argument("--synthetic-data-path", type=str, help="Path to synthetic data directory")
    parser.add_argument("--log-level", type=str, default="INFO", help="Logging level")
    parser.add_argument("--use-optuna", action="store_true", help="Use best hyperparameters from Optuna study")
    parser.add_argument(
        "--accelerator",
        type=str,
        default="gpu",
        choices=["cpu", "gpu", "tpu"],
        help="Accelerator to use for training: 'cpu', 'gpu', or 'tpu'. Defaults to 'gpu'."
    )
    parser.add_argument(
        "--profiler-dirpath",
        type=str,
        default="./profiler_logs",
        help="Directory path for profiler output files."
    )
    parser.add_argument(
        "--profiler-filename",
        type=str,
        default="profile",
        help="Filename for profiler output."
    )
    
    args = parser.parse_args()

    # Validate mamba_ratio
    if args.mamba_ratio &lt; 0.0:
        logger.error("Invalid value for --mamba-ratio: must be non-negative.")
        sys.exit(1)
    main(args)


</file>
<file name="src/UNKNOWN.egg-info/dependency_links.txt">


</file>
<file name="src/UNKNOWN.egg-info/SOURCES.txt">
README.md
setup.py
src/__init__.py
src/checkpoint_evaluator.py
src/config.py
src/evaluate.py
src/optimize_hyperparameters.py
src/UNKNOWN.egg-info/PKG-INFO
src/UNKNOWN.egg-info/SOURCES.txt
src/UNKNOWN.egg-info/dependency_links.txt
src/UNKNOWN.egg-info/top_level.txt
src/data/__init__.py
src/data/arc_dataset.py
src/models/__init__.py
src/models/gpt2.py
src/training/__init__.py
src/training/train.py
src/training/trainer.py
src/utils/__init__.py
src/utils/experiment_tracker.py
src/utils/grokfast.py
src/utils/grokfast_callback.py
src/utils/helpers.py
src/utils/model_memory_estimator.py
src/utils/performance_metrics.py
src/utils/results_collector.py
tests/test_arc_dataset.py
tests/test_arc_trainer.py
tests/test_balancing.py
tests/test_benchmark.py
tests/test_checkpoint_loading.py
tests/test_class_weights.py
tests/test_dataset.py
tests/test_differential_pixel_accuracy.py
tests/test_end_to_end.py
tests/test_evaluate.py
tests/test_experiment_tracker.py
tests/test_gpt2.py
tests/test_grokfast.py
tests/test_hyperparameter_optimization.py
tests/test_integration_experiment.py
tests/test_mamba_integration.py
tests/test_metrics.py
tests/test_model_evaluation.py
tests/test_models.py
tests/test_optimize_hyperparameters.py
tests/test_pytest_error_fixer.py
tests/test_pytorch_lightning_integration.py
tests/test_results_collector.py
tests/test_synthetic_arc_dataset.py
tests/test_synthetic_data.py
tests/test_train.py
tests/test_trainer.py
</file>
<file name="src/UNKNOWN.egg-info/top_level.txt">
__init__
checkpoint_evaluator
config
data
evaluate
models
optimize_hyperparameters
training
utils

</file>
<file name="src/models/__init__.py">

</file>
<file name="src/models/gpt2.py">
# gpt2_arc/src/models/gpt2.py

import logging

import torch
import torch.nn.functional as F
import pytorch_lightning as pl
from torch import nn
from typing import Dict
import torch.nn.init as init
from bitnet import BitLinearNew

from zeta.nn import MambaBlock

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)


class Attention(nn.Module):
    def __init__(self, n_embd, n_head, dropout):
        super().__init__()
        self.n_head = n_head
        self.n_embd = n_embd
        self.key = BitLinearNew(n_embd, n_embd)
        self.query = BitLinearNew(n_embd, n_embd)
        self.value = BitLinearNew(n_embd, n_embd)
        self.proj = BitLinearNew(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)  # Add this line
        logger.debug(f"Initialized Attention with n_embd={n_embd}, n_head={n_head}")

    def forward(self, x, mask=None):
        B, T, C = x.size()
        if not torch._dynamo.is_compiling():
            logger.debug(f"Attention input shape: {x.shape}")
        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32)))
        if mask is not None:
            att = att.masked_fill(mask[:, None, None, :] == 0, float("-inf"))
        att = F.softmax(att, dim=-1)

        # Apply dropout to attention probabilities
        att = self.dropout(att)  # Add this line
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        output = self.proj(y)
        if not torch._dynamo.is_compiling():
            logger.debug(f"Attention output shape: {output.shape}")
        return output


class FeedForward(nn.Module):
    def __init__(self, n_embd, dropout):
        super().__init__()
        self.net = nn.Sequential(
            BitLinearNew(n_embd, 4 * n_embd), nn.ReLU(), nn.Dropout(dropout), BitLinearNew(4 * n_embd, n_embd)
        )
        logger.debug(f"Initialized FeedForward with n_embd={n_embd}")

    def forward(self, x):
        if not torch._dynamo.is_compiling():
            logger.debug(f"FeedForward input shape: {x.shape}")
        output = self.net(x)
        if not torch._dynamo.is_compiling():
            logger.debug(f"FeedForward output shape: {output.shape}")
        return output


class TransformerBlock(nn.Module):
    def __init__(self, n_embd, n_head, dropout):
        super().__init__()
        self.attention = Attention(n_embd, n_head, dropout)
        self.feed_forward = FeedForward(n_embd, dropout)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        self.dropout = nn.Dropout(dropout)
        logger.debug(
            f"Initialized TransformerBlock with n_embd={n_embd}, n_head={n_head}"
        )

    def forward(self, x, mask=None):
        if not torch._dynamo.is_compiling():
            logger.debug(f"TransformerBlock input shape: {x.shape}")
        # Attention sublayer with residual dropout
        attn_output = self.attention(self.ln1(x), mask)
        attn_output = self.dropout(attn_output)  # Apply dropout
        x = x + attn_output

        # Feed-forward sublayer with residual dropout
        ff_output = self.feed_forward(self.ln2(x))
        ff_output = self.dropout(ff_output)  # Apply dropout
        x = x + ff_output
        if not torch._dynamo.is_compiling():
            logger.debug(f"TransformerBlock output shape: {x.shape}")
        return x


class MambaLayer(nn.Module):
    def __init__(self, n_embd, d_state, d_conv, dropout, depth, expand):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.mamba_block = MambaBlock(
            dim=n_embd,
            depth=depth,           # Use depth from config
            d_state=d_state,
            d_conv=d_conv,
            expand=expand,         # Use expand from config
            conv_bias=True,        # Default value
            bias=False             # Default value
        )
        self.layer_norm = nn.LayerNorm(n_embd)
        logger.debug(
            f"Initialized MambaLayer with n_embd={n_embd}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}"
        )

    def forward(self, x):
        if not torch._dynamo.is_compiling():
            logger.debug(f"MambaLayer input shape: {x.shape}")
        x_norm = self.layer_norm(x)
        x_mamba = self.mamba_block(x_norm)
        x_mamba = self.dropout(x_mamba)
        output = x + x_mamba
        if not torch._dynamo.is_compiling():
            logger.debug(f"MambaLayer output shape: {output.shape}")
        return output

from gpt2_arc.src.config import Config

class GPT2ARC(pl.LightningModule):
    def __init__(self, config: Config, num_classes: int, symbol_freq: Dict[str, float] = None):
        # Define an example input array for model summary
        self.example_input_array = torch.zeros(1, 1, 6, 6)  # Adjust dimensions as needed
        super().__init__()
        self.config = config
        self.symbol_freq = symbol_freq
        # Replace token embedding with a convolutional layer
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=self.config.model.n_embd,  # Accessing the 'model' attribute within Config
            kernel_size=3,
            padding=1
        ).to(torch.float32)
        # Initialize blocks with interleaved TransformerBlocks and MambaLayer(s)
        self.blocks = nn.ModuleList()
        num_transformer_blocks = self.config.model.n_layer
        total_mamba_layers = int(num_transformer_blocks * self.config.model.mamba_ratio)
        
        logger.debug(f"Total TransformerBlocks: {num_transformer_blocks}")
        logger.debug(f"Total MambaLayers to add: {total_mamba_layers}")

        # Distribute MambaLayers across TransformerBlocks
        mamba_layer_positions = []
        if total_mamba_layers &gt; 0:
            step = num_transformer_blocks / total_mamba_layers
            mamba_layer_positions = [int(i * step) for i in range(total_mamba_layers)]

        current_mamba_index = 0
        for layer_idx in range(num_transformer_blocks):
            # Add a TransformerBlock
            self.blocks.append(TransformerBlock(self.config.model.n_embd, self.config.model.n_head, self.config.model.dropout))
            logger.debug(f"Layer {len(self.blocks)}: Added TransformerBlock")

            # Check if we should add a MambaLayer after this TransformerBlock
            if current_mamba_index &lt; len(mamba_layer_positions) and layer_idx == mamba_layer_positions[current_mamba_index]:
                # Add a MambaLayer
                self.blocks.append(
                    MambaLayer(
                        n_embd=self.config.model.n_embd,
                        d_state=self.config.model.d_state,
                        d_conv=self.config.model.d_conv,
                        dropout=self.config.model.dropout,
                        depth=self.config.model.mamba_depth,
                        expand=self.config.model.mamba_expand
                    )
                )
                logger.debug(f"Layer {len(self.blocks)}: Added MambaLayer after TransformerBlock {layer_idx + 1}")
                current_mamba_index += 1
        self.ln_f = nn.LayerNorm(self.config.model.n_embd)
        assert isinstance(self.config.model.n_embd, int), "model.n_embd must be an integer"
        assert isinstance(num_classes, int), "num_classes must be an integer"
        self.fc_out = BitLinearNew(int(self.config.model.n_embd), int(num_classes))  # Add final linear layer

        # Initialize loss function with class weights if needed
        if self.config.training.balance_symbols and self.config.training.balancing_method == "weighting":
            if not self.symbol_freq:
                raise ValueError("symbol_freq must be provided when balance_symbols is True and balancing_method is 'weighting'")
            class_weights = 1.0 / torch.tensor(list(symbol_freq.values()), dtype=torch.float)
            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights)
            logger.debug(f"Class Weights: {class_weights}")  # Added line
        else:
            self.loss_fn = nn.CrossEntropyLoss()

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Conv2d):
            # Calculate fan_in for Conv2d
            fan_in = module.in_channels * module.kernel_size[0] * module.kernel_size[1]
            std = 1.0 / fan_in**0.5
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        elif isinstance(module, BitLinearNew):
            fan_in = module.in_features
            std = 1.0 / fan_in**0.5
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        # No initialization for nn.LayerNorm, using default

    def forward(self, input_ids, attention_mask=None):
        if not torch._dynamo.is_compiling():
            logger.debug(f"GPT2ARC input shape: {input_ids.shape}, dtype: {input_ids.dtype}")
        
        # Check if input_ids is already in the correct shape
        if input_ids.dim() == 4:
            x = input_ids.float()
        else:
            # Reshape input_ids to [batch_size, 1, height, width]
            batch_size = input_ids.size(0)
            seq_length = input_ids.size(1)
            height = width = int(seq_length ** 0.5)
            x = input_ids.float().view(batch_size, 1, height, width)
        
        x = self.conv1(x)
        logger.debug(f"After conv1 shape: {x.shape}")
        b, c, h, w = x.size()
        x = x.view(b, c, h * w)  # Flatten spatial dimensions
        x = x.permute(0, 2, 1)  # Rearrange to (batch_size, sequence_length, channels)
        logger.debug(f"Reshaped for transformer blocks: {x.shape}")

        for i, block in enumerate(self.blocks):
            # Check if the block is a TransformerBlock or MambaLayer
            if isinstance(block, TransformerBlock):
                x = block(x, attention_mask)
                logger.debug(f"After TransformerBlock {i + 1}: shape {x.shape}")
            else:
                x = block(x)
                logger.debug(f"After MambaLayer {i + 1}: shape {x.shape}")
        
        x = self.ln_f(x)
        x = self.fc_out(x)  # Apply final linear layer
        return x

</file>
<file name="debug tips/test_train.md">
When encountering an error in your `gpt2_arc/tests/test_train.py` test suite, the most relevant files to examine for debugging and resolving the issue are those that are directly imported and utilized within the test cases. Here's a breakdown of the key files you should focus on:

1. **Source Files Under Test:**
   
   - **`gpt2_arc/src/training/train.py`**
     - **Role:** Contains the `main` function that orchestrates the training process.
     - **Why Check:** Since your tests are invoking `main(args)`, any issues in how training is initiated or handled would likely originate here.
   
   - **`gpt2_arc/src/training/trainer.py`**
     - **Role:** Defines the `ARCTrainer` class, which is a subclass of `pl.LightningModule` responsible for managing the training loop.
     - **Why Check:** Errors related to the training logic, such as training steps, validation steps, or integration with PyTorch Lightning, would stem from this file.
   
   - **`gpt2_arc/src/models/gpt2.py`**
     - **Role:** Implements the `GPT2ARC` model, including its architecture and forward pass.
     - **Why Check:** If the error pertains to model initialization, forward propagation, or any layer-specific issues, this is the primary file to inspect.
   
   - **`gpt2_arc/src/data/arc_dataset.py`**
     - **Role:** Contains the `ARCDataset` class responsible for data loading and preprocessing.
     - **Why Check:** Issues related to data handling, such as dataset initialization, data preprocessing, or data loader configuration, would originate here.
   
   - **`gpt2_arc/src/config.py`**
     - **Role:** Defines configuration dataclasses like `Config`, `ModelConfig`, and `TrainingConfig`.
     - **Why Check:** Misconfigurations or incorrect parameter settings that affect training behavior would be defined in this file.

2. **Utility and Support Files:**
   
   - **`gpt2_arc/src/utils/results_collector.py`**
     - **Role:** Implements the `ResultsCollector` class for aggregating and managing training results.
     - **Why Check:** Errors related to result logging, metric collection, or summary generation would be found here.
   
   - **`gpt2_arc/src/utils/experiment_tracker.py`**
     - **Role:** Manages experiment tracking, possibly integrating with tools like Weights &amp; Biases.
     - **Why Check:** If the error involves experiment tracking, logging configurations, or integrations with external tracking tools, this file is pertinent.

3. **Additional Considerations:**
   
   - **`gpt2_arc/benchmark.py` and `gpt2_arc/src/evaluate.py`**
     - **Role:** While these files are more focused on benchmarking and evaluation, respectively, they might still interact with training components.
     - **Why Check:** If the error indirectly involves evaluation metrics or benchmarking during training, reviewing these files could provide insights.
   
   - **Mock and Fixture Implementations in `test_train.py`:**
     - **Role:** The test file itself uses fixtures and mocks extensively to simulate different components.
     - **Why Check:** Ensure that the mocks correctly mimic the behavior of the actual classes and that fixtures are set up appropriately. Errors in the test setup can lead to misleading test failures.

4. **Logging and Configuration:**
   
   - **Logging Configuration in `test_train.py`:**
     - **Role:** The test file sets up logging levels and configurations.
     - **Why Check:** Misconfigured logging can obscure error messages or lead to unexpected behaviors during testing.

5. **Dependencies and Environment:**
   
   - **External Libraries:**
     - Ensure that dependencies like `pytorch_lightning`, `torch`, and other libraries are correctly installed and compatible with your codebase.
   
   - **Environment Variables and Paths:**
     - Verify that the `sys.path` manipulations and environment settings in the test file correctly point to the necessary modules and that there are no path conflicts.

**Summary:**

To effectively debug and resolve errors in your `test_train.py`:

- **Start with the source files being tested** (`train.py`, `trainer.py`, `gpt2.py`, `arc_dataset.py`, and `config.py`) to identify any underlying issues in the training pipeline.
  
- **Examine utility files** (`results_collector.py` and `experiment_tracker.py`) for problems related to logging and result management.
  
- **Review the test setup itself**, ensuring that mocks and fixtures accurately represent the real components and that there are no setup-related errors.

By systematically inspecting these areas, you can pinpoint the root cause of the errors and implement effective fixes.
</file>
<file name="debug tips/test_trainer.md">
When encountering an error in your `gpt2_arc/tests/test_trainer.py` file, the most relevant files to examine for debugging and fixing the issue are those that the test file directly interacts with or depends upon. Here's a breakdown of the key files and their roles:

1. **`src/config.py`**
   - **Classes to Check:**
     - `Config`
     - `ModelConfig`
     - `TrainingConfig`
   - **Relevance:** This file defines the configuration classes used to initialize models and trainers. Errors related to configuration parameters, default values, or initialization logic likely originate here.

2. **`src/data/arc_dataset.py`**
   - **Classes and Functions to Check:**
     - `ARCDataset`
     - `set_debug_mode`
     - `_process_synthetic_data`
     - `_process_arckit_data`
     - `_preprocess_grid`
   - **Relevance:** This file handles data preprocessing and dataset creation. Issues related to data loading, preprocessing steps, or dataset structure (e.g., unexpected data formats) would be found here.

3. **`src/models/gpt2.py`**
   - **Classes to Check:**
     - `GPT2ARC`
     - `Attention`
     - `FeedForward`
     - `TransformerBlock`
     - `ModelConfig`
   - **Relevance:** This file contains the GPT-2 model architecture and its components. Errors in the model's forward pass, layer configurations, or parameter settings are likely rooted in this file.

4. **`src/training/trainer.py`**
   - **Classes and Methods to Check:**
     - `ARCTrainer`
     - `training_step`
     - `validation_step`
     - `configure_optimizers`
     - `train_dataloader`
     - `val_dataloader`
     - `test_step`
   - **Relevance:** This is the core training module that orchestrates the training and validation processes. Issues related to the training loop, optimizer configuration, data loaders, or logging mechanisms would be addressed here.

5. **`src/utils/experiment_tracker.py`**
   - **Classes and Methods to Check:**
     - `ExperimentTracker`
     - `log_metric`
     - `update_train_metrics`
     - `update_val_metrics`
     - `set_test_results`
     - `save_to_json`
   - **Relevance:** If your tests involve tracking experiments or logging metrics, any errors related to metric logging, experiment initialization, or result serialization would involve this file.

6. **`src/utils/results_collector.py`**
   - **Classes and Methods to Check:**
     - `ResultsCollector`
     - `update_train_metrics`
     - `update_val_metrics`
     - `set_test_results`
     - `add_task_specific_result`
     - `save_to_json`
   - **Relevance:** Similar to `experiment_tracker.py`, this file manages the collection and storage of results. Errors in aggregating or storing test results would be pertinent here.

### Steps to Debug:

1. **Identify the Error Message:**
   - Start by looking at the exact error message and stack trace. This will often point directly to the file and line number where the issue originated.

2. **Trace Dependencies:**
   - Understand how `test_trainer.py` interacts with the other modules. For instance, if there's an issue during model initialization, focus on `src/config.py` and `src/models/gpt2.py`.

3. **Check Configurations:**
   - Ensure that the configuration objects (`Config`, `ModelConfig`, `TrainingConfig`) are correctly set up and that all required parameters are provided.

4. **Validate Data Handling:**
   - If the error is related to data loading or preprocessing, review the methods in `arc_dataset.py` to ensure data is being processed as expected.

5. **Inspect Model Architecture:**
   - For issues in the forward pass or model outputs, delve into `gpt2.py` to verify layer configurations and data flow within the model.

6. **Examine Training Logic:**
   - If the error occurs during training or validation steps, scrutinize the `ARCTrainer` class in `trainer.py`, focusing on methods like `training_step` and `validation_step`.

7. **Review Utility Functions:**
   - For issues related to logging or result collection, check the utility files to ensure metrics are being recorded and stored correctly.

### Additional Tips:

- **Use Debugging Tools:**
  - Incorporate debugging statements or use tools like `pdb` to step through the code and inspect variable states at different execution points.

- **Isolate the Issue:**
  - Temporarily simplify your tests or mock certain components to isolate where the error is occurring.

- **Check Dependencies and Versions:**
  - Ensure that all dependencies (e.g., PyTorch, PyTest) are up to date and compatible with your codebase.

- **Consult Documentation:**
  - Review the documentation for any third-party libraries or frameworks you’re using to ensure you're adhering to best practices and usage patterns.

By systematically examining these files and following a structured debugging approach, you should be able to identify and resolve the error in your `test_trainer.py` code.
</file>
<file name="debug tips/test_arc_dataset.md">
To effectively troubleshoot and fix errors in your `gpt2_arc/tests/test_arc_dataset.py` test suite, you should focus on the following key files in your repository:

1. **`gpt2_arc/src/data/arc_dataset.py`**
   - **Why:** This is the primary file where the `ARCDataset` class and the `set_debug_mode` function are defined. Since your tests are directly interacting with these components, any issues related to dataset initialization, data preprocessing, or utility functions will likely originate here.
   - **What to Look For:**
     - **Initialization Logic:** Ensure that the `__init__` method correctly handles different types of `data_source` inputs (`str`, `List[Dict]`, `TaskSet`, etc.).
     - **Data Processing Methods:** Check methods like `_process_synthetic_data`, `_process_arckit_data`, and `_preprocess_grid` for any logical errors or incorrect handling of data.
     - **Debug Mode Handling:** Verify that the `set_debug_mode` function correctly toggles the debug state and that debug-related logging or behavior in `ARCDataset` is functioning as expected.

2. **`gpt2_arc/src/utils/experiment_tracker.py`**
   - **Why:** While not directly referenced in your test file, utility classes like `ExperimentTracker` can influence the behavior of your dataset, especially if they are used for logging or tracking metrics during dataset processing.
   - **What to Look For:**
     - **Logging Configuration:** Ensure that logging is correctly set up and that it doesn't interfere with dataset operations.
     - **Serialization Methods:** Check methods like `_make_serializable` and `_serialize_config` to ensure that configurations are correctly handled, which can affect dataset initialization if configurations are passed around.

3. **`gpt2_arc/src/models/gpt2.py`**
   - **Why:** Although your tests focus on the dataset, models often interact closely with datasets during training and evaluation. Issues in model configurations or data handling within the model can indirectly affect dataset behavior.
   - **What to Look For:**
     - **Data Expectations:** Ensure that the model correctly expects the data shapes and types provided by `ARCDataset`.
     - **Integration Points:** Verify that any integration points between the model and dataset (if present) are correctly implemented.

4. **Dependencies and External Libraries (`arckit`)**
   - **Why:** Your tests import `TaskSet` from `arckit.data`, which suggests that `arckit` is an external dependency. Issues within this library can propagate to your dataset tests.
   - **What to Look For:**
     - **Compatibility:** Ensure that the version of `arckit` you are using is compatible with your dataset and that there are no known bugs affecting `TaskSet`.
     - **Mock Implementations:** Since you use `unittest.mock.Mock` for `TaskSet`, ensure that your mock accurately reflects the structure and behavior expected by `ARCDataset`.

5. **Test File Itself (`gpt2_arc/tests/test_arc_dataset.py`)**
   - **Why:** Sometimes, the issue might reside within the test logic rather than the implementation. Reviewing the test file can help identify incorrect assumptions or faulty test setups.
   - **What to Look For:**
     - **Test Fixtures:** Ensure that fixtures like `sample_data` and `mock_taskset` provide the correct data structures expected by `ARCDataset`.
     - **Assertions:** Verify that all assertions correctly reflect the intended behavior and that they are not overly restrictive or incorrectly specified.
     - **Skipped Tests:** Review why certain tests are skipped and determine if they need to be updated or fixed to be included in the test suite.

6. **Additional Configuration Files (`gpt2_arc/src/config.py`)**
   - **Why:** Configuration files often dictate how datasets and models are initialized and interacted with. Errors in configurations can lead to unexpected behaviors during testing.
   - **What to Look For:**
     - **Model and Dataset Configurations:** Ensure that all necessary configurations for the dataset are correctly defined and accessible.
     - **Defaults and Overrides:** Check how default configurations are set and how they can be overridden, ensuring consistency across different test scenarios.

**Summary:**

- **Primary Focus:** `gpt2_arc/src/data/arc_dataset.py`
- **Secondary Focus:** Utility files like `experiment_tracker.py`, model definitions in `gpt2.py`, and configuration files in `config.py`
- **Dependencies:** Ensure external libraries like `arckit` are functioning as expected
- **Test Integrity:** Verify the correctness of the test setups and assertions within `test_arc_dataset.py`

By systematically reviewing these areas, you should be able to identify and resolve errors within your test suite effectively.
</file>
<file name="debug tips/test_end_to_end.md">
When troubleshooting errors in your `test_end_to_end.py` script, several files within your repository are likely to provide the most relevant information to help you identify and fix the issue. Here's a breakdown of the key files to examine based on different parts of your test script:

### 1. **Data Handling and Preprocessing**

- **`src/data/arc_dataset.py`**
  - **Relevance:** This file contains the `ARCDataset` class, which is crucial for loading and preprocessing your ARC dataset. Errors related to data loading, dataset splitting, or preprocessing steps (like `_process_synthetic_data` or `_preprocess_grid`) will likely originate here.
  - **What to Check:**
    - Ensure the dataset paths are correct.
    - Verify the data processing methods are handling the data as expected.
    - Check for any issues in the `collate_fn` used for batching data.

- **`arckit` Module**
  - **Relevance:** Your test script uses `arckit.load_data()` to load the dataset. Issues with the data loading process or the structure of the loaded data would be tied to this module.
  - **What to Check:**
    - Ensure `arckit` is correctly installed and accessible.
    - Verify that the `load_data` function returns data in the expected format.

### 2. **Model Definition**

- **`src/models/gpt2.py`**
  - **Relevance:** This file defines the `GPT2ARC` model and its components (`Attention`, `FeedForward`, `TransformerBlock`). Errors related to model architecture, such as layer mismatches or incorrect configurations, will likely originate here.
  - **What to Check:**
    - Ensure the model configuration (`ModelConfig`) matches the expected architecture.
    - Verify that all layers are correctly defined and initialized.
    - Check for any type mismatches or tensor dimension issues within the model.

### 3. **Training Logic**

- **`src/training/trainer.py`**
  - **Relevance:** This file contains the `ARCTrainer` class, which manages the training loop, loss computation, and metric tracking. Errors during training, such as issues with the optimizer, loss functions, or training steps, will likely stem from here.
  - **What to Check:**
    - Ensure that the training configurations (`TrainingConfig`) are correctly set.
    - Verify the implementation of training and validation steps.
    - Check for any runtime errors during the forward or backward passes.

### 4. **Configuration Management**

- **`src/config.py`**
  - **Relevance:** This file defines the configuration data classes (`Config`, `ModelConfig`, `TrainingConfig`). Misconfigurations, such as incorrect hyperparameters or mismatched settings, can lead to errors during model initialization or training.
  - **What to Check:**
    - Ensure all configuration parameters are correctly set and passed to other components.
    - Verify that default values are appropriate and that any overrides are correctly applied.

### 5. **Utility Functions and Experiment Tracking**

- **`src/utils/experiment_tracker.py`**
  - **Relevance:** If your test script involves experiment tracking or logging metrics, issues here could affect the logging and tracking of your experiments.
  - **What to Check:**
    - Ensure that the experiment tracker is correctly initialized and configured.
    - Verify that metrics are being logged and saved as expected.

- **`src/utils/results_collector.py`**
  - **Relevance:** This file manages the collection and storage of results from training and evaluation. Errors related to result aggregation or storage will likely originate here.
  - **What to Check:**
    - Ensure that results are correctly collected and serialized.
    - Verify that there are no issues with saving or loading result data.

### 6. **Evaluation Process**

- **`src/evaluate.py`**
  - **Relevance:** Although not directly referenced in your test script, if evaluation logic is invoked or shared between scripts, issues here could affect the evaluation metrics.
  - **What to Check:**
    - Ensure that evaluation metrics are correctly computed.
    - Verify that the evaluation data is correctly processed and fed into the model.

### 7. **Other Potential Sources**

- **`benchmark.py` and `train.py`**
  - **Relevance:** While these are more likely related to running benchmarks or training outside of tests, any shared components or configurations could indirectly affect your tests.
  - **What to Check:**
    - Ensure that any shared utilities or configurations used by these scripts are consistent and error-free.

### **General Debugging Tips:**

1. **Logging:** Your test script has extensive logging enabled (`logging.basicConfig(level=logging.DEBUG)`). Review the debug logs to pinpoint where the error occurs. The logs provide step-by-step insights into the test execution flow.

2. **Assertions and Error Messages:** Pay close attention to the assertion statements and any error messages they produce. These can guide you to the exact point of failure.

3. **Dependencies:** Ensure all dependencies (like `arckit`, `torch`, `pytorch_lightning`, etc.) are correctly installed and compatible with each other.

4. **Environment Issues:** Sometimes, errors arise from the environment (e.g., incorrect CUDA setup, incompatible library versions). Verify that your environment matches the expected setup.

5. **Isolate the Issue:** If possible, try running individual components or smaller tests to isolate where the error is occurring. This can help narrow down the problematic file or section of code.

### **Next Steps:**

If after reviewing the above files you're still unable to identify the issue, consider the following:

- **Provide Specific Error Messages:** Sharing the exact error messages or stack traces can help in diagnosing the problem more accurately.
  
- **Add Relevant Files:** If the issue seems to originate from a specific file not listed here, feel free to add its content to the chat for a more in-depth analysis.

By systematically reviewing these files and following the debugging tips, you should be able to identify and resolve the errors in your end-to-end test script effectively.
</file>
<file name="debug tips/test_differential_pixel_accuracy.md">
To identify and fix errors in your `test_differential_pixel_accuracy.py` test script, it's essential to examine the files and modules that the test interacts with. Based on your provided code and the summaries of your repository files, the following files are the most likely candidates to investigate:

1. **`gpt2_arc/src/utils/helpers.py`**
   - **Reason:** This file contains the `differential_pixel_accuracy` function, which is central to all your test cases. If there's an error related to the accuracy computation, its implementation here is the first place to check.
   - **Action:** Review the implementation of `differential_pixel_accuracy` for potential bugs or inconsistencies. Ensure that it correctly handles different tensor shapes, data types, and edge cases like empty tensors.

2. **`gpt2_arc/src/models/gpt2.py`**
   - **Reason:** The `GPT2ARC` model is instantiated and used in one of your tests (`test_differential_pixel_accuracy_with_arckit_data`). Errors related to model initialization, prediction generation, or tensor shapes are likely rooted here.
   - **Action:** 
     - Verify that the `GPT2ARC` model is correctly defined, especially the forward pass.
     - Ensure that the model's output dimensions match the expected shapes used in the test.
     - Check for any potential issues in the model's layers (e.g., `Attention`, `FeedForward`, `TransformerBlock`) that might affect predictions.

3. **`gpt2_arc/src/config.py`**
   - **Reason:** The `ModelConfig` dataclass is used to configure the `GPT2ARC` model. Misconfigurations here can lead to unexpected behaviors or mismatches in model parameters.
   - **Action:** 
     - Ensure that all necessary configuration parameters are correctly defined and passed.
     - Check for consistency between the configuration used in tests and the model's requirements.

4. **`gpt2_arc/src/data/arc_dataset.py`**
   - **Reason:** The `ARCDataset` class is responsible for data loading and preprocessing, which are critical for generating valid input and target tensors for the tests.
   - **Action:** 
     - Verify that the data preprocessing methods (e.g., `_process_arckit_data`, `_preprocess_grid`) correctly handle the data.
     - Ensure that the dataset returns tensors of expected shapes and types.
     - Check the `reverse_scaling` method to confirm it accurately reverses any scaling applied during preprocessing.

5. **External Dependency: `arckit`**
   - **Reason:** Your test `test_differential_pixel_accuracy_with_arckit_data` relies on the `arckit` library to load task data. Issues with data loading or compatibility can stem from here.
   - **Action:** 
     - Ensure that the `arckit` library is correctly installed and compatible with your project.
     - Verify that the `task_id` used (`"007bbfb7"`) exists and that `arckit.load_single(task_id)` returns the expected data structure.
     - Check for any updates or changes in the `arckit` API that might affect data loading.

6. **Additional Considerations:**
   - **Environment and Dependencies:**
     - Ensure that all dependencies (e.g., PyTorch, `arckit`) are up-to-date and compatible with each other.
     - Verify that the Python environment has all necessary packages installed.
   - **Test Environment:**
     - Confirm that the test is being run in an environment where all relative paths and module imports are correctly resolved.
     - Check for any recent changes in the project structure that might affect import statements.

7. **Debugging Tips:**
   - **Verbose Logging:** Enhance your test functions with more detailed logging to pinpoint where the error occurs. For example, print shapes and data types of tensors before and after each operation.
   - **Isolate Tests:** Run individual test functions separately to identify which specific test is failing.
   - **Use Assertions Carefully:** Ensure that your assertions accurately reflect the expected outcomes. For example, floating-point comparisons might require a tolerance level instead of exact equality.

8. **If the Error Persists:**
   - **Provide Error Messages:** Sharing specific error messages or stack traces can help in diagnosing the issue more effectively.
   - **Check Version Control:** Review recent commits to identify changes that might have introduced the error.
   - **Consult Documentation:** Refer to the documentation of external libraries like `arckit` for any breaking changes or known issues.

By systematically examining these files and following the debugging steps, you should be able to identify and resolve the error in your test code effectively. If you need further assistance with specific files or error messages, feel free to share the relevant code snippets or details.
</file>
<file name="debug tips/test_integration_experiment.md">
When debugging errors in your `test_integration_experiment.py` test file, several parts of your codebase are likely to provide valuable insights. Based on the imports and the structure of your project, the following files are the most relevant for diagnosing and fixing potential issues:

1. **Data Handling and Preprocessing:**
   
   - **`gpt2_arc/src/data/arc_dataset.py`**
     - **Why:** This file contains the `ARCDataset` class, which is crucial for data loading and preprocessing. Errors related to data formatting, missing fields, or incorrect data types often originate here.
     - **Key Sections to Review:**
       - `__init__` method: Ensure that the dataset is being initialized correctly with the provided data sources.
       - `_process_arckit_data` and `_process_synthetic_data` methods: Verify that data from `arckit` is being processed as expected.
       - Any debug or logging statements that might help trace data issues.

2. **Model Definition:**
   
   - **`gpt2_arc/src/models/gpt2.py`**
     - **Why:** This file defines the `GPT2ARC` model and related components like `Attention`, `FeedForward`, and `TransformerBlock`. Errors related to model architecture, layer configurations, or forward passes will stem from here.
     - **Key Sections to Review:**
       - `GPT2ARC` class initialization: Check that all layers are correctly instantiated with the right configurations.
       - Forward methods: Ensure that data flows correctly through the model without shape mismatches or other issues.
       - Any custom configurations or modifications to the standard GPT-2 architecture.

3. **Training Logic:**
   
   - **`gpt2_arc/src/training/trainer.py`**
     - **Why:** This file contains the `ARCTrainer` class, which manages the training loop, loss calculations, and metric updates. Issues like improper training steps, incorrect loss functions, or metric logging problems will be found here.
     - **Key Sections to Review:**
       - `__init__` method: Ensure that datasets, model, and configurations are correctly set up.
       - Training step methods: Verify that loss calculations and backpropagation are implemented correctly.
       - Integration with PyTorch Lightning: Check compatibility and correct usage of Lightning's `Trainer`.

4. **Configuration Management:**
   
   - **`gpt2_arc/src/config.py`**
     - **Why:** This file defines configuration classes like `Config`, `ModelConfig`, and `TrainingConfig`. Misconfigurations, such as incorrect hyperparameters or missing configuration fields, can lead to errors during data loading, model initialization, or training.
     - **Key Sections to Review:**
       - Default values and data types for all configuration parameters.
       - Any methods that manipulate or validate configurations.
       - Integration points where configurations are passed to other components like the model or trainer.

5. **Results Collection and Logging:**
   
   - **`gpt2_arc/src/utils/results_collector.py`**
     - **Why:** This file manages the collection and summarization of training and validation results. Errors related to metric logging, result storage, or summary generation will originate here.
     - **Key Sections to Review:**
       - Methods for updating and retrieving metrics.
       - Serialization and saving of results.
       - Integration with other components to ensure that metrics are correctly passed and stored.

6. **Additional Considerations:**
   
   - **`arckit` Library:**
     - **Why:** Your test setup relies on the `arckit` library to load task data. If there are issues with how tasks are loaded or structured, it could affect your tests.
     - **Action:** Ensure that `arckit` is correctly installed and that the task IDs used in tests (`"007bbfb7"`) are valid and accessible.

   - **PyTorch Lightning Integration:**
     - **Files Involved:** While not listed explicitly, your test uses PyTorch Lightning's `Trainer`. Ensure that all integrations with Lightning are correctly implemented in your `ARCTrainer` class and that callbacks like `ModelCheckpoint` are properly configured.

   - **Logging and Debug Statements:**
     - **Why:** Your test includes several `print` statements for debugging. Ensure that these logs provide meaningful information and that they don't interfere with the test flow.

7. **Common Error Scenarios and File Associations:**

   - **Import Errors:**
     - **Files to Check:** `arc_dataset.py`, `gpt2.py`, `trainer.py`, `config.py`, `results_collector.py`
     - **Reason:** Missing or incorrect imports usually point to issues in these modules.

   - **Attribute Errors or Missing Methods:**
     - **Files to Check:** `gpt2.py`, `trainer.py`, `arc_dataset.py`
     - **Reason:** Ensure that all necessary methods and attributes are defined and correctly named.

   - **Data Shape Mismatches:**
     - **Files to Check:** `arc_dataset.py`, `gpt2.py`
     - **Reason:** Verify that the data shapes are consistent throughout the data pipeline and model.

   - **Configuration Mismatches:**
     - **Files to Check:** `config.py`, `trainer.py`, `gpt2.py`
     - **Reason:** Ensure that all components receive and use configurations correctly.

8. **Next Steps:**

   - **Review the Relevant Files:** Start by examining the files listed above, focusing on the sections most likely related to your error.
   
   - **Add Detailed Logging:** If not already present, consider adding more detailed logging within these files to trace the flow of data and identify where things might be going wrong.
   
   - **Isolate the Issue:** Determine whether the error is related to data loading, model initialization, training steps, or configuration. This will help narrow down which file to focus on.
   
   - **Run Tests Incrementally:** Use PyTest's verbose mode or selectively run tests to get more context about where the failure occurs.

If you identify that a specific file or section is causing the issue and need further assistance, feel free to share the relevant code snippets by adding those files to the chat. This will allow for more targeted help in resolving the problem.
</file>
<file name="debug tips/test_gpt2.md">
When encountering an error in your `gpt2_arc/tests/test_gpt2.py` test suite, the most relevant files to examine for troubleshooting are those that define the components being tested. Here's a breakdown of the primary files you should investigate:

1. **`gpt2_arc/src/models/gpt2.py`**
   - **Why:** This file contains the definitions for the `GPT2ARC` class as well as its constituent modules like `Attention`, `FeedForward`, and `TransformerBlock`. Since your tests are directly interacting with these classes (e.g., initializing `GPT2ARC`, performing forward passes, etc.), any issues related to model architecture, initialization, or forward computations would likely originate here.
   - **Key Sections to Check:**
     - `GPT2ARC` class initialization and attributes (`conv1`, `blocks`, `ln_f`, `config`).
     - Implementation details of `Attention`, `FeedForward`, and `TransformerBlock` modules.
     - Any custom methods or overrides that might affect the model's behavior during testing.

2. **`gpt2_arc/src/config.py`**
   - **Why:** The `ModelConfig` class from this file is used to configure the `GPT2ARC` model during initialization in your tests. Errors related to configuration parameters, default values, or the structure of the configuration can lead to issues in model instantiation or behavior.
   - **Key Sections to Check:**
     - Definition of `ModelConfig` and its fields.
     - Any methods or default values that set up the model's configuration.
     - Interactions between `ModelConfig` and other parts of the model (e.g., ensuring all necessary configuration parameters are correctly passed and utilized).

3. **Additional Files to Consider:**
   - **`gpt2_arc/src/utils/experiment_tracker.py` &amp; `gpt2_arc/src/utils/results_collector.py`:**
     - **Why:** If your tests involve tracking experiments or collecting results, issues in these utility classes might indirectly affect your tests. For instance, incorrect logging or result serialization could lead to unexpected behavior or errors during test execution.
   - **`gpt2_arc/src/data/arc_dataset.py`:**
     - **Why:** If your tests rely on specific data preprocessing or dataset structures, any bugs or changes in data handling could impact the inputs your tests use. Ensuring that data is correctly processed and fed into the model is crucial for accurate testing.

4. **Test-Specific Considerations:**
   - **Duplicate Test Function:**
     - **Issue:** In your `test_gpt2.py`, there are two functions named `test_gpt2arc_forward_pass`. Python does not support function overloading, so the second definition will overwrite the first. This could lead to unexpected test behaviors or skipped tests.
     - **Solution:** Rename one of the test functions to ensure each test has a unique name, such as `test_gpt2arc_forward_pass_with_mask` and `test_gpt2arc_forward_pass_without_mask`.

5. **Logging Output:**
   - **Why:** Since your test file is configured with `logging.DEBUG`, reviewing the log outputs can provide detailed insights into where the error might be occurring. Ensure that the logs are being captured and review them to identify any anomalies or error messages during test execution.

6. **Dependencies and Environment:**
   - **Why:** Ensure that all dependencies (like `torch`, `pytest`, etc.) are correctly installed and compatible with your codebase. Sometimes, version mismatches can lead to unexpected errors.
   - **Solution:** Verify your `requirements.txt` or environment configuration and ensure all packages are up-to-date and compatible.

7. **Version Control and Recent Changes:**
   - **Why:** If the error is new, reviewing recent commits or changes in the related files can help pinpoint the source of the issue.
   - **Solution:** Use `git` commands like `git blame` or `git diff` to identify recent modifications that might have introduced the error.

**Next Steps:**

1. **Identify the Specific Error:**
   - If you encounter an error message or traceback, use it to determine which part of the code is failing.
   
2. **Inspect Relevant Files:**
   - Focus on the files highlighted above, especially `gpt2_arc/src/models/gpt2.py` and `gpt2_arc/src/config.py`.

3. **Request Additional File Contents if Needed:**
   - If you determine that a specific section of a file might be causing the issue but need more details, feel free to ask me to include that file's content for further analysis.

By systematically reviewing these areas, you should be able to identify and resolve the error in your test suite effectively.
</file>
<file name="debug tips/test_benchmark.md">

When encountering an error in the `gpt2_arc/tests/test_benchmark.py` test suite, it's essential to identify the most relevant files that could provide insights into the issue. Here's a prioritized list of files to examine, along with brief explanations of why they are likely to be involved:

1. **`gpt2_arc/benchmark.py`**
   - **Reason:** This is the primary module being tested. Functions like `benchmark_model` and `main` are directly imported and invoked in your test cases. Any issues with these functions (e.g., logic errors, incorrect handling of inputs/outputs) will likely manifest during testing.
   
2. **`gpt2_arc/src/models/gpt2.py`**
   - **Reason:** The `GPT2ARC` class is a core component being mocked and used in the tests. Errors related to model configuration, initialization, or methods (like `forward`) can affect the benchmark tests.
   
3. **`gpt2_arc/src/config.py`**
   - **Reason:** The `ModelConfig` dataclass is imported and potentially used within both the `benchmark.py` and model modules. Misconfigurations or incorrect parameter settings here can lead to unexpected behaviors during benchmarking.
   
4. **`gpt2_arc/src/data/arc_dataset.py`**
   - **Reason:** The dataset (`ARCDataset`) is mocked in the tests, but any underlying issues with data processing, loading, or preprocessing in the actual implementation can cause tests to fail or behave unpredictably.
   
5. **`gpt2_arc/src/utils/experiment_tracker.py`**
   - **Reason:** If `benchmark_model` or related functions utilize the `ExperimentTracker` for logging or tracking experiments, any bugs or exceptions within this utility can propagate to your tests.
   
6. **`gpt2_arc/src/utils/results_collector.py`**
   - **Reason:** Similar to `experiment_tracker.py`, if results collection is part of the benchmarking process, issues in `ResultsCollector` can affect the output and validation in your tests.
   
7. **`gpt2_arc/src/training/train.py` &amp; `gpt2_arc/src/training/trainer.py`**
   - **Reason:** While not directly invoked in the provided test code, these modules may be indirectly involved if `benchmark_model` interacts with training routines or utilizes components from these scripts.
   
8. **External Dependencies (e.g., `torch`, `pytest`, `unittest.mock`)**
   - **Reason:** Although less likely, issues with the external libraries or how they are mocked in the tests can also lead to errors. Ensure that the versions are compatible and that mocks are correctly set up.

### Steps to Diagnose the Error:

1. **Examine the Error Message:**
   - Start by looking at the exact error message and traceback provided when the test fails. This will often point directly to the problematic file and line number.

2. **Check `benchmark.py`:**
   - Since this is the main module under test, review the functions `benchmark_model` and `main` for any logical errors or incorrect handling of inputs and outputs.

3. **Validate Mocks and Fixtures:**
   - Ensure that your mocks (e.g., `mock_model`, `mock_dataset`, `mock_dataloader`) accurately represent the behavior of the real objects. Incorrect mocking can lead to misleading test results.

4. **Review Dependencies in `gpt2_arc/src/models/gpt2.py`:**
   - Look for any issues in the `GPT2ARC` class, especially in methods that are invoked during benchmarking, such as `forward`.

5. **Inspect Configuration in `gpt2_arc/src/config.py`:**
   - Verify that all necessary configurations are correctly set and that there are no mismatches between expected and actual parameters.

6. **Analyze Data Handling in `gpt2_arc/src/data/arc_dataset.py`:**
   - Ensure that data loading and preprocessing steps are functioning as intended. Errors here can lead to incorrect inputs being fed into the model during benchmarking.

7. **Evaluate Utility Modules:**
   - Check `experiment_tracker.py` and `results_collector.py` for any bugs or exceptions that might interfere with the benchmarking process.

8. **Run Isolated Tests:**
   - Consider running individual tests or components in isolation to pinpoint where the failure occurs.

9. **Check for Environment Issues:**
   - Sometimes, errors arise from the testing environment, such as incompatible library versions or insufficient resources (e.g., GPU availability). Ensure that the environment matches the expectations set in your tests.

### Additional Tips:

- **Enable Verbose Logging:**
  - Add logging statements within `benchmark.py` and related modules to trace the flow of execution and identify where things might be going wrong.

- **Use Debugging Tools:**
  - Utilize debugging tools like `pdb` to step through the test execution and inspect the state of variables at different points.

- **Review Recent Changes:**
  - If the tests were passing previously, review recent changes to the codebase that might have introduced the error.

By systematically examining these files and following the diagnostic steps, you should be able to identify and resolve the error in your test suite effectively.
</file>
<file name="debug tips/test_results_collector.md">
When debugging errors in the `test_results_collector.py` test suite, the most relevant files to examine are those directly involved in the functionality being tested. Here's a prioritized list of files that are most likely to provide information to help fix any issues:

1. **`gpt2_arc/src/utils/results_collector.py`**
   - **Reason:** This is the primary module being tested. Any errors in initialization, metric updates, or result handling are likely originating from here.
   - **Key Components to Check:**
     - `ResultsCollector` class implementation.
     - Methods like `update_train_metrics`, `update_val_metrics`, `set_test_results`, `add_task_specific_result`, and `get_summary`.
     - Initialization logic, especially how `experiment_id`, `timestamp`, and `config` are set up.

2. **`gpt2_arc/src/config.py`**
   - **Reason:** The test initializes `ResultsCollector` using configurations defined in this file. Errors related to configuration attributes (e.g., `n_embd`, `n_head`, `n_layer`, `batch_size`, etc.) may stem from issues in the configuration classes.
   - **Key Components to Check:**
     - `Config`, `ModelConfig`, and `TrainingConfig` dataclasses.
     - Any methods or default values that manipulate or validate configuration parameters.

3. **`gpt2_arc/src/utils/experiment_tracker.py`**
   - **Reason:** `ResultsCollector` may internally utilize `ExperimentTracker` for logging and tracking experiments. Issues in experiment tracking could affect the results collection process.
   - **Key Components to Check:**
     - `ExperimentTracker` class methods, especially those related to logging metrics and handling experiment IDs.
     - Initialization and any interactions with external services like WandB (if `use_wandb` is enabled).

4. **Dependencies and External Modules:**
   - **`gpt2_arc/src/utils/results_collector.py` Dependencies:**
     - Ensure that any utility functions or classes used within `ResultsCollector` are functioning correctly.
   - **Environment and Configuration Files:**
     - Check for any environment-specific configurations or dependencies that might affect the test execution.

5. **Test Environment Setup:**
   - Although the test file itself is primarily for testing, ensure that the `setUp` method correctly initializes all necessary components. Misconfigurations or incorrect setups here can lead to misleading test failures.

**Steps to Diagnose the Issue:**

1. **Identify the Error Message:**
   - Start by examining the exact error message or traceback from the failed test. This will often point directly to the problematic line of code.

2. **Trace the Source:**
   - Use the traceback to trace back to the source file and line number where the error originated. This will help you determine whether the issue is within `results_collector.py`, `config.py`, or another related module.

3. **Review Recent Changes:**
   - If the tests were previously passing, consider any recent changes made to the related modules that might have introduced the error.

4. **Check for Dependency Issues:**
   - Ensure that all dependencies are correctly installed and compatible with each other, especially if there have been updates to packages like `torch`, `pytorch_lightning`, or others used in the project.

5. **Isolate the Problem:**
   - Temporarily simplify or isolate parts of the `ResultsCollector` to identify which specific method or component is causing the failure.

By systematically reviewing these files and following the diagnostic steps, you should be able to identify and fix the error in your test suite.
</file>
<file name="debug tips/test_pytest_error_fixer.md">
To effectively diagnose and fix errors in your `test_pytest_error_fixer.py` test script, you'll want to focus on several key files within your repository. Here's a breakdown of the most relevant files that can provide the necessary information:

1. **Primary Module Under Test:**
   - **`pytest_error_fixer.py`**: This is the main module being tested by your `test_pytest_error_fixer.py` script. Any errors in your tests are likely related to the implementation details within this file. Since you didn't list this file in your summaries, ensure it's available and consider sharing its contents if you need detailed assistance.

2. **Configuration Files:**
   - **`gpt2_arc/src/config.py`**: This file contains configuration classes (`ModelConfig` and `Config`) that might be used by `PytestErrorFixer`. Misconfigurations here can lead to issues in initializing or running the fixer.

3. **Utility Modules:**
   - **`gpt2_arc/src/utils/experiment_tracker.py`** and **`gpt2_arc/src/utils/results_collector.py`**: These utility classes (`ExperimentTracker` and `ResultsCollector`) might be dependencies for `PytestErrorFixer`. Errors in these utilities can propagate to your tests.

4. **Data Handling:**
   - **`gpt2_arc/src/data/arc_dataset.py`**: If `PytestErrorFixer` interacts with datasets or relies on data preprocessing, issues in this module can affect your tests.

5. **Model and Training Modules:**
   - **`gpt2_arc/src/models/gpt2.py`**: This file defines the `GPT2ARC` model and related classes. If `PytestErrorFixer` interacts with model components, ensure that there are no issues here.
   - **`gpt2_arc/src/training/train.py`** and **`gpt2_arc/src/training/trainer.py`**: These modules handle the training process. Any integration between `PytestErrorFixer` and the training pipeline should be verified.

6. **Evaluation and Benchmarking:**
   - **`gpt2_arc/src/evaluate.py`** and **`gpt2_arc/benchmark.py`**: These scripts are essential for evaluating model performance. Ensure that `PytestErrorFixer` correctly interacts with evaluation metrics if applicable.

7. **Experiment Tracking:**
   - **`gpt2_arc/src/utils/experiment_tracker.py`**: This utility is crucial for logging and tracking experiments. Any issues here can affect how errors and progress are logged by `PytestErrorFixer`.

8. **Results Collection:**
   - **`gpt2_arc/src/utils/results_collector.py`**: Similar to the experiment tracker, this module handles the collection and storage of results, which might be integral to how `PytestErrorFixer` operates.

### Next Steps:

- **Check `pytest_error_fixer.py`**: Start by reviewing the implementation of the `PytestErrorFixer` class in `pytest_error_fixer.py`. Look for any obvious issues or dependencies that might not be properly handled.
  
- **Verify Dependencies**: Ensure that all dependencies (`experiment_tracker.py`, `results_collector.py`, etc.) are correctly implemented and free from errors.
  
- **Review Configuration**: Double-check the configurations in `config.py` to ensure they align with the requirements of `PytestErrorFixer`.
  
- **Mock External Interactions**: In your tests, you’re using mocks for subprocess calls and the `coder` object. Ensure that these mocks accurately represent the behavior of the actual components.

- **Add Missing Files if Needed**: If you encounter issues that trace back to files not listed (like `pytest_error_fixer.py`), please add them to the chat so I can provide more targeted assistance.

By focusing on these files, you should be able to identify and resolve the errors in your test script effectively. If you need more detailed help, feel free to share the contents of `pytest_error_fixer.py` or any other relevant files.
</file>
<file name="debug tips/test_model_evaluation.md">
To identify and fix errors in your `test_model_evaluation.py` file, it's essential to focus on the dependencies and modules that this test script interacts with. Here's a breakdown of the most relevant files in your repository that are likely to provide useful information for debugging:

1. **`src/models/gpt2.py`**
   - **Why:** This file defines the `GPT2ARC` class, which is a core component being tested. Any issues related to model architecture, forward pass, or specific layers (like `Attention`, `FeedForward`, or `TransformerBlock`) will originate here.
   - **What to Check:**
     - Initialization of the `GPT2ARC` model.
     - Implementation of the `forward` method.
     - Any custom layers or operations that might affect model outputs.

2. **`src/config.py`**
   - **Why:** This file contains the configuration classes (`Config`, `ModelConfig`, `TrainingConfig`) used to instantiate models and training parameters. Misconfigurations here can lead to unexpected behaviors or initialization errors in your tests.
   - **What to Check:**
     - Correct definitions and default values in the dataclasses.
     - Any dependencies or validations within the configuration classes.
     - Ensure that all required fields are being correctly passed and utilized.

3. **`src/training/trainer.py`**
   - **Why:** The `ARCTrainer` class is imported and used in your fixtures. Issues related to the training loop, validation steps, or how the trainer interacts with the model can manifest in your tests.
   - **What to Check:**
     - Initialization and setup of the `ARCTrainer`.
     - Implementation of methods like `validation_step`, which is explicitly tested.
     - Handling of incorrect batch formats and error raising mechanisms.

4. **`src/utils/helpers.py`**
   - **Why:** This file includes utility functions like `differential_pixel_accuracy`, which are directly used in your tests. Any bugs or unexpected behaviors in these helper functions can cause test failures.
   - **What to Check:**
     - Correct implementation of `differential_pixel_accuracy`.
     - Edge case handling and input validations within the helper functions.

5. **`src/data/arc_dataset.py`**
   - **Why:** Although not directly imported in your test script, the `DataLoader` relies on the `ARCDataset` class defined here. Issues with data preprocessing, batching, or dataset splitting can indirectly affect your tests.
   - **What to Check:**
     - Data loading and preprocessing logic.
     - Handling of different data sources and formats.
     - Any transformations applied to the data before it's fed into the model.

6. **Checkpoint Files (`checkpoints/arc_model-epoch=00-val_loss=0.73.ckpt`)**
   - **Why:** Your tests involve loading model checkpoints. Problems with checkpoint integrity, missing keys, or incompatible configurations can lead to errors during model loading and evaluation.
   - **What to Check:**
     - Ensure that the checkpoint file exists and is accessible.
     - Verify that the checkpoint contains all necessary keys (`config`, `state_dict`, etc.).
     - Confirm that the `ModelConfig` in the checkpoint matches the expected configuration in your code.

7. **`src/utils/experiment_tracker.py` &amp; `src/utils/results_collector.py`**
   - **Why:** These utilities handle experiment tracking and results collection, which can influence how metrics and configurations are logged and stored. Issues here can affect the integrity of the metrics being tested.
   - **What to Check:**
     - Correct logging of metrics and configurations.
     - Proper serialization and deserialization of experiment data.
     - Error handling and edge case management in tracking methods.

8. **Logging Configuration in `test_model_evaluation.py`**
   - **Why:** Since your test script sets up logging, any misconfigurations here can obscure error messages or make debugging more challenging.
   - **What to Check:**
     - Ensure that the logging level is appropriately set (`DEBUG` in your case).
     - Verify that log messages are correctly formatted and informative.

### Steps to Diagnose and Fix Errors:

1. **Identify the Error Source:**
   - Look at the error message and traceback to pinpoint where the error originates. This will guide you to the relevant file(s).

2. **Check Dependencies:**
   - Once you know which part of the code is failing, inspect the corresponding file(s) mentioned above for potential issues.

3. **Validate Configurations:**
   - Ensure that all configurations passed to models and trainers are correct and complete.

4. **Verify Data Integrity:**
   - Make sure that the data being loaded and processed matches the expected format and structure required by the models and trainers.

5. **Inspect Checkpoints:**
   - Confirm that the checkpoint files are not corrupted and contain all necessary components to reconstruct the model and its state.

6. **Enhance Logging:**
   - Utilize the debug logs you've set up to gain more insights into the internal states and data flow during test execution.

By systematically reviewing these files and following the diagnostic steps, you should be able to identify and resolve errors in your `test_model_evaluation.py` script effectively.
</file>
</source>