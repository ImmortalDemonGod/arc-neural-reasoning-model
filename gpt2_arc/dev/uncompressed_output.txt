<source type="local_directory" path="/workspaces/arc-neural-reasoning-model/gpt2_arc">
<file name="requirements.txt">
cysimdjson
absl-py==2.1.0
accelerate==0.33.0
aider-chat==0.59.1
aiohappyeyeballs==2.4.0
aiohttp==3.10.5
aiosignal==1.3.1
alembic==1.13.3
altair==5.3.0
annotated-types==0.7.0
anyio==4.6.0
arckit==0.1.0
argon2-cffi==23.1.0
argon2-cffi-bindings==21.2.0
arrow==1.3.0
asttokens==2.4.1
async-lru==2.0.4
attrs==24.2.0
babel==2.16.0
backoff==2.2.1
beartype==0.18.5
beautifulsoup4==4.12.3
bitnet==0.2.5
bitsandbytes==0.44.1
black==24.8.0
bleach==6.1.0
blinker==1.8.2
bottle==0.13.1
build==1.2.2
CacheControl==0.14.0
cachetools==5.3.3
certifi==2024.8.30
cffi==1.17.1
chardet==5.2.0
charset-normalizer==3.3.2
cleo==2.1.0
click==8.1.7
colorama==0.4.6
colorlog==6.8.2
CoLT5-attention==0.10.19
comm==0.2.2
commonmark==0.9.1
ConfigArgParse==1.7
contourpy==1.3.0
coverage==7.6.1
crashtest==0.4.1
cryptography==43.0.1
cycler==0.12.1
datasets==2.14.4
debugpy==1.8.5
decorator==5.1.1
defusedxml==0.7.1
diff-match-patch==20230430
dill==0.3.7
diskcache==5.6.3
distlib==0.3.8
distro==1.9.0
docker-pycreds==0.4.0
drawsvg==2.4.0
dulwich==0.21.7
e==1.4.5
einops==0.8.0
einops-exts==0.0.4
einx==0.3.0
entrypoints==0.4
executing==2.1.0
fairscale==0.4.13
fastjsonschema==2.20.0
filelock==3.16.1
flake8==7.1.1
fonttools==4.54.0
fqdn==1.5.1
frozendict==2.4.4
frozenlist==1.4.1
fsspec==2024.9.0
gitdb==4.0.11
GitPython==3.1.43
google-ai-generativelanguage==0.6.2
google-api-core==2.19.0
google-api-python-client==2.128.0
google-auth==2.29.0
google-auth-httplib2==0.2.0
google-generativeai==0.5.2
googleapis-common-protos==1.63.0
greenlet==3.0.3
grep-ast==0.3.3
grpcio==1.63.0
grpcio-status==1.62.2
h11==0.14.0
httpcore==1.0.5
httplib2==0.22.0
httpx==0.27.2
huggingface-hub==0.25.0
hypothesis==6.115.0
idna==3.10
importlib_metadata==7.2.1
importlib_resources==6.4.5
iniconfig==2.0.0
installer==0.7.0
ipykernel==6.29.5
ipython==8.27.0
isoduration==20.11.0
isort==5.13.2
jaraco.classes==3.4.0
jedi==0.19.1
jeepney==0.8.0
Jinja2==3.1.4
jiter==0.5.0
joblib==1.4.2
json5==0.9.25
jsonpointer==3.0.0
jsonschema==4.23.0
jsonschema-specifications==2023.12.1
jupyter-events==0.10.0
jupyter-lsp==2.2.5
jupyter-server-mathjax==0.2.6
jupyter_client==8.6.3
jupyter_core==5.7.2
jupyter_server==2.14.2
jupyter_server_terminals==0.5.3
jupyterlab==4.2.5
jupyterlab_git==0.50.1
jupyterlab_pygments==0.3.0
jupyterlab_server==2.27.3
keyring==24.3.1
kiwisolver==1.4.7
libcst==1.1.0
lightning==2.4.0
lightning-utilities==0.11.7
lion-pytorch==0.2.2
litellm==1.47.0
local-attention==1.9.15
loguru==0.7.2
Mako==1.3.5
Markdown==3.7
markdown-it-py==3.0.0
MarkupSafe==2.1.5
matplotlib==3.9.2
matplotlib-inline==0.1.7
mccabe==0.7.0
mdurl==0.1.2
memory-profiler==0.61.0
mistune==0.8.4
more-itertools==10.5.0
mpmath==1.3.0
msgpack==1.1.0
multidict==6.1.0
multiprocess==0.70.15
mypy==1.11.2
mypy-extensions==1.0.0
nbclient==0.10.0
nbconvert==6.5.0
nbdime==4.0.2
nbformat==5.4.0
nest-asyncio==1.6.0
networkx==3.2.1
nltk==3.7
notebook_shim==0.2.4
numpy==1.26.4
nvidia-cublas-cu12==12.1.3.1
nvidia-cuda-cupti-cu12==12.1.105
nvidia-cuda-nvrtc-cu12==12.1.105
nvidia-cuda-runtime-cu12==12.1.105
nvidia-cudnn-cu12==9.1.0.70
nvidia-cufft-cu12==11.0.2.54
nvidia-curand-cu12==10.3.2.106
nvidia-cusolver-cu12==11.4.5.107
nvidia-cusparse-cu12==12.1.0.106
nvidia-nccl-cu12==2.20.5
nvidia-nvjitlink-cu12==12.6.77
nvidia-nvtx-cu12==12.1.105
openai==1.47.0
optuna==4.0.0
optuna-dashboard==0.16.Text preprocessing completed with XML structure preserved.

Compressed Token Count: 49057
Uncompressed Token Count: 58479

compressed_output.txt and uncompressed_output.txt have been created in the working directory.

An error occurred: Pyperclip could not find a copy/paste mechanism for your system. For more information, please visit 
https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error
On Linux, you can run `sudo apt-get install xclip` or `sudo apt-get install xselect` to install a copy/paste mechanism.

Please check your input and try again.
Processing... ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 0:00:00
Traceback (most recent call last):
  File "/workspaces/arc-neural-reasoning-model/tmp/1filellm/onefilellm.py", line 678, in <module>
    main()
  File "/workspaces/arc-neural-reasoning-model/tmp/1filellm/onefilellm.py", line 669, in main
    pyperclip.copy(uncompressed_text)
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyperclip/__init__.py", line 622, in lazy_load_stub_copy
    return copy(text)
           ^^^^^^^^^^
  File "/usr/local/python/3.12.1/lib/python3.12/site-packages/pyperclip/__init__.py", line 295, in __call__
    raise PyperclipException('Pyperclip could not find a copy/paste mechanism for your system. For more information, please visit https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error' + additionalInfo)
pyperclip.PyperclipException: Pyperclip could not find a copy/paste mechanism for your system. For more information, please visit https://pyperclip.readthedocs.io/en/latest/index.html#not-implemented-error
On Linux, you can run `sudo apt-get install xclip` or `sudo apt-get install xselect` to install a copy/paste mechanism.
024.9.12
types-python-dateutil==2.9.0.20240906
typing==3.7.4.3
typing-inspect==0.9.0
typing_extensions==4.12.2
tzdata==2024.1
ultralytics-thop==2.0.8
uri-template==1.3.0
uritemplate==4.1.1
urllib3==2.2.3
vector-quantize-pytorch==1.12.0
virtualenv==20.26.6
wandb==0.18.3
watchdog==5.0.3
wcwidth==0.2.13
webcolors==24.8.0
webencodings==0.5.1
websocket-client==1.8.0
Werkzeug==3.0.4
wget==3.2
xxhash==3.5.0
yarl==1.11.1
youtube-transcript-api==0.4.1
zetascale==0.9.1
zipp==3.20.2
</file>
<file name="benchmark.py">
# gp2_arc/benchmark.py
import sys
import os

# Add the project root directory to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import torch._dynamo
import csv
import uuid
from datetime import datetime
import os
import torch
from torch.utils.data import DataLoader
import arckit
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import ModelConfig
import time
from torch.amp import autocast
import psutil
import logging
import argparse
import statistics
import numpy as np
from scipy import stats

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Dynamically adjustable baseline values for CPU, GPU, and MPS
BASELINES = {
    'cpu': {'total_time': 1.6391, 'grids_per_second': 199.27},
    'cuda': {'total_time': 0.0481, 'grids_per_second': 13774.98},
    'mps': {'total_time': 0.0481, 'grids_per_second': 13774.98}  # Updated baselines for MPS
}

def benchmark_model(model, dataset, batch_size=1, num_batches=1, num_runs=1, device_type='cpu', precision='medium', model_checkpoint=None):
    print(f"Starting benchmark_model with parameters: batch_size={batch_size}, num_batches={num_batches}, device_type={device_type}, precision={precision}, model_checkpoint={model_checkpoint}")

    if device_type not in ['cpu', 'cuda', 'mps']:
        raise ValueError("Invalid device type")

    if len(dataset) == 0:
        raise ValueError("Dataset is empty")

    checkpoint_used = False
    checkpoint_info = {}
    if model_checkpoint:
        checkpoint = torch.load(model_checkpoint)
        if 'state_dict' in checkpoint:
            state_dict = checkpoint['state_dict']
        else:
            state_dict = checkpoint
        model.load_state_dict(state_dict, strict=False)
        model.to(device_type)
        model.eval()
        checkpoint_used = True
        checkpoint_info = {
            'state_dict_keys': list(state_dict.keys())
        }
    run_id = str(uuid.uuid4())
    current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    practical_threshold = 20.0  # Define a threshold for practical significance
    total_time_runs = []
    grids_per_second_runs = []

    cpu_usages = []
    memory_usages = []

    run_results = []  # Initialize run_results to store each run's data
    gpu_usages = []  # Initialize gpu_usages to store GPU utilization data

    # Set the float32 matmul precision
    torch.set_float32_matmul_precision(precision)

    # Select device based on the argument (including support for MPS)
    device = torch.device("cuda" if device_type == "cuda" and torch.cuda.is_available() else
                          "mps" if device_type == "mps" and torch.backends.mps.is_available() else "cpu")
    model = model.to(device)
    torch._dynamo.config.suppress_errors = True
    if device.type == "cpu":
        compiled_model = model  # Use the model directly for CPU
    else:
        try:
            if device.type != "mps":
                compiled_model = torch.compile(model, mode="reduce-overhead", fullgraph=True)
            else:
                compiled_model = model  # Use the model directly for MPS
        except ImportError as e:
            logger.warning(f"Compilation failed with error: {e}. Falling back to eager execution.")
            compiled_model = model

    try:
        dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=ARCDataset.collate_fn)
        total_time = 0.0
        total_grids = 0

        for i, batch in enumerate(dataloader):
            if i &gt;= num_batches:
                break
            print(f"Processing batch {i+1}/{num_batches}")

            logger.debug(f"Batch content before unpacking: {batch}")
            if len(batch) != 3:
                raise ValueError(f"Unexpected batch format. Expected 3 items, got {len(batch)}")
            inputs, outputs, task_ids = batch

            print(f"Inputs type: {type(inputs)}")
            if hasattr(inputs, 'shape'):
                print(f"Inputs shape: {inputs.shape}")
            else:
                print("Inputs shape: N/A")
            print(f"Outputs type: {type(outputs)}, shape: {outputs.shape if torch.is_tensor(outputs) else 'N/A'}")
            print(f"Task IDs: {task_ids}")

            if inputs is None or not isinstance(inputs, torch.Tensor):
                raise ValueError(f"Expected inputs to be a torch.Tensor, got {type(inputs)}")

            if inputs.numel() == 0:
                raise ValueError("Inputs tensor is empty")
            print(f"Inputs shape: {inputs.shape}, Outputs shape: {outputs.shape}, Task IDs: {task_ids}")
            
            if inputs.dim() == 2:
                # If inputs is 2D (batch_size, sequence_length), reshape it to 4D
                height = width = int(inputs.size(1)**0.5)
                inputs = inputs.view(inputs.size(0), 1, height, width)
            elif inputs.dim() == 3:
                # If inputs is 3D (batch_size, height, width), add a channel dimension
                inputs = inputs.unsqueeze(1)
            elif inputs.dim() != 4:
                raise ValueError(f"Unexpected input dimensions: {inputs.dim()}. Expected 2, 3, or 4 dimensions.")
            
            attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32)
            inputs, attention_mask = inputs.to(device), attention_mask.to(device)

            # Log system load and system state before processing the batch
            cpu_percent = psutil.cpu_percent(interval=None)
            memory_info = psutil.virtual_memory()
            cpu_usages.append(cpu_percent)
            memory_usages.append(memory_info.percent)
            if device.type == 'cuda':
                gpu_utilization = torch.cuda.utilization(device.index)
                gpu_usages.append(gpu_utilization)
                logger.info(f"Batch {i+1}: CPU Usage: {cpu_percent}%, Memory Usage: {memory_info.percent}%, GPU Utilization: {gpu_utilization}%")
            else:
                logger.info(f"Batch {i+1}: CPU Usage: {cpu_percent}%, Memory Usage: {memory_info.percent}%")

                # Measure the time taken to process the batch
                start_time = time.time()

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                logger.debug("Invoking the model with inputs and attention_mask")
                with torch.no_grad():
                    if device.type == 'cuda':
                        with autocast(device_type=device.type, dtype=torch.float16):
                            compiled_model(inputs, attention_mask)
                    else:
                        compiled_model(inputs, attention_mask)

                if torch.cuda.is_available():
                    torch.cuda.synchronize()

                end_time = time.time()

                batch_time = end_time - start_time
                print(f"Batch time: {batch_time}")
                if batch_time &lt;= 0:
                    print(f"WARNING: Invalid batch time: {batch_time}. Skipping this batch.")
                    continue
                total_time += batch_time
                total_grids += len(inputs)
    except Exception as e:
        logger.error(f"An error occurred during benchmarking: {e}")
        raise

    print(f"Benchmark completed. Total time: {total_time}, Total grids: {total_grids}")
    # Calculate average and standard deviation for the runs
    num_runs = len(total_time_runs)
    avg_total_time = np.mean(total_time_runs)
    std_total_time = np.std(total_time_runs)
    avg_grids_per_second = np.mean(grids_per_second_runs)
    std_grids_per_second = np.std(grids_per_second_runs)
    if total_time &gt; 0:
        grids_per_second = total_grids / total_time
    else:
        grids_per_second = 0.0  # Avoid division by zero
        logger.warning("Total time is zero. Setting grids_per_second to 0.0 to avoid division by zero.")

    logger.info(f"Total Time: {total_time:.4f} seconds, Grids per Second: {grids_per_second:.2f}")
    
    # Store the results of the run
    run_results.append({
        'run_id': run_id,
        'datetime': current_time,
        'total_time': total_time,
        'grids_per_second': grids_per_second,
        'cpu_usage': np.mean(cpu_usages),
        'memory_usage': np.mean(memory_usages),
        'gpu_usage': np.mean(gpu_usages) if gpu_usages else None,
        'batch_size': batch_size,
        'num_batches': num_batches,
        'device': device.type,
        'n_embd': model.config.n_embd,
        'n_head': model.config.n_head,
        'n_layer': model.config.n_layer,
        'precision': precision,  # Add precision here
        'checkpoint_used': checkpoint_used,
        'checkpoint_info': checkpoint_info
    })

    total_time_runs.append(total_time)
    grids_per_second_runs.append(grids_per_second)

    if total_time &lt;= 0 or total_grids &lt;= 0:
        logger.warning(f"ERROR: Invalid total time ({total_time}) or total grids ({total_grids}). Check the benchmark implementation.")
        return 0.0, 0.0  # Return sensible defaults instead of infinity

    avg_total_time = total_time
    avg_grids_per_second = total_grids / total_time if total_time &gt; 0 else 0.0

    logger.info(f"Total Time: {avg_total_time:.4f} seconds, Grids per Second: {avg_grids_per_second:.2f}")


    # Perform statistical analysis (confidence intervals, effect size, etc.)
    confidence_level = 0.95
    z_score = stats.norm.ppf((1 + confidence_level) / 2)

    ci_total_time = z_score * (std_total_time / np.sqrt(num_runs))
    ci_grids_per_second = z_score * (std_grids_per_second / np.sqrt(num_runs))

    effect_size_time = (avg_total_time - BASELINES[device.type]['total_time']) / std_total_time
    effect_size_grids = (avg_grids_per_second - BASELINES[device.type]['grids_per_second']) / std_grids_per_second

    # Calculate improvements and regressions based on averages
    time_improvement = BASELINES[device.type]['total_time'] - avg_total_time
    time_improvement_percent = (time_improvement / BASELINES[device.type]['total_time']) * 100
    time_regression = avg_total_time - BASELINES[device.type]['total_time']
    time_regression_percent = (time_regression / BASELINES[device.type]['total_time']) * 100

    grids_per_second_improvement = avg_grids_per_second - BASELINES[device.type]['grids_per_second']
    grids_per_second_improvement_percent = (grids_per_second_improvement / BASELINES[device.type]['grids_per_second']) * 100
    grids_per_second_regression = BASELINES[device.type]['grids_per_second'] - avg_grids_per_second
    grids_per_second_regression_percent = (grids_per_second_regression / BASELINES[device.type]['grids_per_second']) * 100

    # Determine if there was an improvement
    improvement_time = avg_total_time &lt; BASELINES[device.type]['total_time']
    improvement_grids = avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']

    # Log improvements or regressions based on averages
    if avg_total_time &lt; BASELINES[device.type]['total_time']:
        logger.info(f"Improvement in average total time: -{time_improvement:.4f} seconds ({time_improvement_percent:.2f}%)")
    else:
        logger.info(f"Regression in average total time: +{time_regression:.4f} seconds ({time_regression_percent:.2f}%)")

    if avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']:
        logger.info(f"Improvement in average grids per second: +{grids_per_second_improvement:.2f} ({grids_per_second_improvement_percent:.2f}%)")
    else:
        logger.info(f"Regression in average grids per second: -{grids_per_second_regression:.2f} ({grids_per_second_regression_percent:.2f}%)")

    # Update practical significance checks
    practical_significance_time = time_improvement_percent &gt;= practical_threshold
    practical_significance_grids = grids_per_second_improvement_percent &gt;= practical_threshold

    # Log practical significance
    if improvement_time:
        if practical_significance_time:
            logger.info("The improvement in average total time is practically significant.")
        else:
            logger.info("The improvement in average total time is not practically significant.")
    else:
        if practical_significance_time:
            logger.info("The regression in average total time is practically significant.")
        else:
            logger.info("The regression in average total time is not practically significant.")

    if improvement_grids:
        if practical_significance_grids:
            logger.info("The improvement in average grids per second is practically significant.")
        else:
            logger.info("The improvement in average grids per second is not practically significant.")
    else:
        if practical_significance_grids:
            logger.info("The regression in average grids per second is practically significant.")
        else:
            logger.info("The regression in average grids per second is not practically significant.")

    # Perform a one-sample t-test
    t_stat_time, p_value_time = stats.ttest_1samp(total_time_runs, BASELINES[device.type]['total_time'])
    t_stat_grids, p_value_grids = stats.ttest_1samp(grids_per_second_runs, BASELINES[device.type]['grids_per_second'])

    logger.info(f"T-Test for total time: t-statistic = {t_stat_time:.4f}, p-value = {p_value_time:.4f}")
    logger.info(f"T-Test for grids per second: t-statistic = {t_stat_grids:.4f}, p-value = {p_value_grids:.4f}")

    # Log the results including confidence intervals
    logger.info(f"Run Summary:")
    logger.info(f" • Avg Total Time: {avg_total_time:.4f}s (CI 95%: ±{ci_total_time:.4f}s)")
    logger.info(f" • Avg Grids per Second: {avg_grids_per_second:.2f} (CI 95%: ±{ci_grids_per_second:.2f})")
    logger.info(f" • Effect Size (Total Time): {effect_size_time:.4f}, Effect Size (Grids per Second): {effect_size_grids:.4f}")

    # Determine if there was an improvement
    improvement_time = avg_total_time &lt; BASELINES[device.type]['total_time']
    improvement_grids = avg_grids_per_second &gt; BASELINES[device.type]['grids_per_second']
    csv_file_path = 'benchmark_results.csv'
    file_exists = os.path.isfile(csv_file_path)
    with open(csv_file_path, 'a', newline='') as csvfile:
        fieldnames = [
            'run_id', 'datetime', 'run', 'total_time', 'grids_per_second', 'cpu_usage', 'memory_usage',
            'batch_size', 'num_batches', 'device', 'n_embd', 'n_head', 'n_layer', 'gpu_usage', 'precision',
            'checkpoint_used', 'checkpoint_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not file_exists:
            writer.writeheader()
        for result in run_results:
            writer.writerow(result)

    # Write statistical summary to CSV
    stats_csv_file_path = 'benchmark_statistics.csv'
    stats_file_exists = os.path.isfile(stats_csv_file_path)
    with open(stats_csv_file_path, 'a', newline='') as csvfile:
        fieldnames = [
            'run_id', 'datetime', 'avg_total_time', 'std_total_time', 'ci_total_time',
            'avg_grids_per_second', 'std_grids_per_second', 'ci_grids_per_second',
            'effect_size_time', 'effect_size_grids', 'percent_change_time', 'percent_change_grids',
            't_stat_time', 'p_value_time', 't_stat_grids', 'p_value_grids',
            'improvement_time', 'improvement_grids',
            'practical_significance_time', 'practical_significance_grids', 'precision',
            'checkpoint_used', 'checkpoint_info'
        ]
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        if not stats_file_exists:
            writer.writeheader()
        writer.writerow({
            'run_id': run_id,
            'datetime': current_time,
            'avg_total_time': avg_total_time,
            'std_total_time': std_total_time,
            'ci_total_time': ci_total_time,
            'avg_grids_per_second': avg_grids_per_second,
            'std_grids_per_second': std_grids_per_second,
            'ci_grids_per_second': ci_grids_per_second,
            'effect_size_time': effect_size_time,
            'effect_size_grids': effect_size_grids,
            'percent_change_time': time_improvement_percent if improvement_time else time_regression_percent,
            'percent_change_grids': grids_per_second_improvement_percent if improvement_grids else grids_per_second_regression_percent,
            't_stat_time': t_stat_time,
            'p_value_time': p_value_time,
            't_stat_grids': t_stat_grids,
            'p_value_grids': p_value_grids,
            'improvement_time': improvement_time,
            'improvement_grids': improvement_grids,
            'practical_significance_time': practical_significance_time,
            'practical_significance_grids': practical_significance_grids,
            'precision': precision,  # Add precision here
            'checkpoint_used': checkpoint_used,
            'checkpoint_info': checkpoint_info
        })

    print(f"Benchmark completed. Final results - avg_time: {avg_total_time}, avg_grids: {avg_grids_per_second}")
    return avg_total_time, avg_grids_per_second


def main(args):
    print(f"Starting main function with args: {args}")
    # Set the float32 matmul precision
    torch.set_float32_matmul_precision(args.precision)
    train_set, _ = arckit.load_data()
    full_dataset = ARCDataset(train_set, is_test=False)

    # Create the model configuration
    model_config = ModelConfig(
        n_embd=args.n_embd,
        n_head=args.n_head,
        n_layer=args.n_layer,
        mamba_ratio=args.mamba_ratio,
        d_state=args.d_state,
        d_conv=args.d_conv
    )
    model = GPT2ARC(model_config, num_classes=args.num_classes)

    # Run the benchmark for different configurations
    for run_num in range(args.num_full_runs):
        logger.info(f"Starting full benchmark run {run_num + 1}/{args.num_full_runs}")
        avg_time, avg_grids = benchmark_model(
            model, full_dataset, batch_size=args.batch_size, num_batches=args.num_batches, num_runs=args.num_runs, device_type=args.device, precision=args.precision, model_checkpoint=args.model_checkpoint
        )
        logger.info(f"Full run {run_num + 1} - Avg Time: {avg_time:.4f}s, Avg Grids per Second: {avg_grids:.2f}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Benchmark the GPT2ARC model.")
    parser.add_argument('--model_checkpoint', type=str, help='Path to the model checkpoint')
    parser.add_argument('--num-runs', type=int, default=20, help='Number of runs for each configuration')
    parser.add_argument('--num-full-runs', type=int, default=1, help='Number of full configurations to run')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size for each run')
    parser.add_argument('--num-batches', type=int, default=10, help='Number of batches per run')
    parser.add_argument('--n-embd', type=int, default=64, help='Number of embeddings for the model')
    parser.add_argument('--n-head', type=int, default=2, help='Number of attention heads')
    parser.add_argument('--n-layer', type=int, default=1, help='Number of layers')
    parser.add_argument('--mamba-ratio', type=int, default=7, help='Number of Mamba layers per Transformer layer')
    parser.add_argument('--d-state', type=int, default=16, help='Mamba state dimension')
    parser.add_argument('--d-conv', type=int, default=4, help='Mamba convolution dimension')
    parser.add_argument('--device', choices=['cpu', 'cuda', 'mps'], default='cpu', help='Device to run the benchmark on (cpu, cuda, or mps)')
    parser.add_argument('--precision', choices=['highest', 'high', 'medium'], default='highest', help='Precision level for float32 matrix multiplications')
    parser.add_argument('--num-classes', type=int, default=10, help='Number of classes for the model')
    
    args = parser.parse_args()
    main(args)

</file>
<file name="README.md">
# GPT-2 ARC Neural Reasoning Model

This project implements a neural reasoning model based on the GPT-2 architecture to solve tasks from the Abstraction and Reasoning Corpus (ARC) challenge.

## Features

- **Data Handling**: Utilizes a custom `ArcDataset` class for handling and preprocessing ARC data.
- **Model Architecture**: Implements a `GPT2ARC` model leveraging the pre-trained GPT-2 architecture.
- **Training**: Includes a `train.py` script for training the model using PyTorch Lightning, with support for logging and checkpointing.
- **Testing**: Comprehensive test suite using `pytest` to ensure model and data integrity.

## Installation

Clone the repository and install the required packages:

```bash
git clone https://github.com/yourusername/arc-neural-reasoning-model.git
cd arc-neural-reasoning-model
pip install -e .
```

For development, install the extra dependencies:

```bash
pip install -e ".[dev]"
```

## Usage

### Training the Model

To train the model, use the following command:

```
python src/train.py --train_data path/to/train_data --val_data path/to/val_data --batch_size 32 --learning_rate 1e-4 --max_epochs 10 --use_gpu
```

Adjust the parameters as needed. The trained model checkpoints will be saved in the `checkpoints` directory.

### Evaluating the Model

To evaluate a trained model on a test set, use the following command:

```
python src/evaluate.py --test_data path/to/test_data --model_checkpoint path/to/model_checkpoint.ckpt --batch_size 32
```

This will output the evaluation metrics for the model on the test dataset.

## Running Tests

To run the tests, use the following command:

```
pytest -v
```

This will run all tests and display the results, including test coverage.

## Contributing

[Add contribution guidelines here]

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

</file>
<file name="setup.py">
from setuptools import setup

setup()

</file>
<file name="src/evaluate.py">
# gpt2_arc/src/evaluate.py
import sys
import sys
import os
import json
import re
from typing import Dict, Any, List
import argparse
import pytorch_lightning as pl
import os
import torch
import wandb
import numpy as np
from datetime import datetime
from pytorch_lightning.utilities.model_summary import ModelSummary
from torchsummary import summary
from pytorch_lightning.utilities.model_summary import ModelSummary

# Define the base directory for the arc-neural-reasoning-model
arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))

# Add the root directory of the project to the PYTHONPATH
project_root = arc_model_dir
sys.path.insert(0, project_root)

from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig, EvaluationConfig
import arckit
import logging
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.utils.training_helpers import get_num_workers
from gpt2_arc.src.utils.helpers import differential_pixel_accuracy

# Set up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def evaluate(model, test_dataset, config, batch_size=32, args=None):
    trainer = ARCTrainer(
        model=model,
        train_dataset=None,
        val_dataset=None,
        config=config,
        args=args,
        test_dataset=test_dataset
    )
    pl_trainer = pl.Trainer(accelerator='gpu' if torch.cuda.is_available() else 'cpu')
    results = pl_trainer.test(trainer)
    logger.debug(f"DEBUG: Raw results from test: {results}")

    avg_test_loss = pl_trainer.callback_metrics.get('avg_test_loss')
    avg_test_accuracy = pl_trainer.callback_metrics.get('avg_test_accuracy')
    avg_test_diff_accuracy = pl_trainer.callback_metrics.get('avg_test_diff_accuracy')

    # Convert tensors to Python floats if necessary
    if avg_test_loss is not None:
        avg_test_loss = avg_test_loss.item()
    if avg_test_accuracy is not None:
        avg_test_accuracy = avg_test_accuracy.item() if isinstance(avg_test_accuracy, torch.Tensor) else avg_test_accuracy
    if avg_test_diff_accuracy is not None:
        avg_test_diff_accuracy = avg_test_diff_accuracy.item() if isinstance(avg_test_diff_accuracy, torch.Tensor) else avg_test_diff_accuracy

    aggregated_results = {
        'test_loss': avg_test_loss,
        'test_accuracy': avg_test_accuracy,
        'test_diff_accuracy': avg_test_diff_accuracy,
    }

    print(f"DEBUG: Logged metrics - Avg test loss: {avg_test_loss}, Avg test accuracy: {avg_test_accuracy}, Avg diff accuracy: {avg_test_diff_accuracy}")

    # Collect individual task metrics from ResultsCollector
    individual_metrics = trainer.results_collector.get_task_specific_results()

    # Optional: Log individual_metrics for debugging
    logger.debug(f"DEBUG: Individual metrics retrieved: {individual_metrics}")
    print(f"DEBUG: Individual metrics retrieved: {individual_metrics}")

    # Compute complete task accuracy (fraction of tasks with perfect accuracy)
    num_tasks = len(individual_metrics)
    perfect_accuracy_threshold = config.evaluation.perfect_accuracy_threshold / 100.0  # Convert percentage to fraction

    num_complete_accuracy = 0
    for task_id, metrics in individual_metrics.items():
        test_accuracy = metrics.get('test_accuracy', 0)
        # Determine if the task is completely solved
        completely_solved = test_accuracy &gt;= perfect_accuracy_threshold
        metrics['completely_solved'] = completely_solved
        if completely_solved:
            num_complete_accuracy += 1

    complete_task_accuracy = num_complete_accuracy / num_tasks if num_tasks &gt; 0 else 0.0
    aggregated_results['complete_task_accuracy'] = complete_task_accuracy

    print(f"DEBUG: Computed complete task accuracy: {complete_task_accuracy}")

    return aggregated_results, individual_metrics

def load_config_from_json(json_path):
    with open(json_path, 'r') as f:
        data = json.load(f)
    return data['config']

import json
import re


def parse_model_summary(model_summary: str, model_checkpoint: str) -&gt; Dict[str, Any]:
    """
    Parses the model summary string into a structured JSON format and adds model file size.

    Args:
        model_summary (str): The raw model summary string.
        model_checkpoint (str): Path to the model checkpoint file.

    Returns:
        Dict[str, Any]: A dictionary containing 'header', 'layers', 'summary', and 'filesize'.
    """
    # Split the model summary into lines
    lines = model_summary.strip().split('\n')

    if len(lines) &lt; 2:
        print("Model summary does not contain sufficient lines.")
        return {"layers": [], "summary": {}, "header": []}

    # Find the header line and the separator line
    header_line = None
    separator_line = None
    for idx, line in enumerate(lines):
        if 'Name' in line and 'Type' in line:
            header_line = line
            separator_line = lines[idx + 1] if idx + 1 &lt; len(lines) else None
            data_start_idx = idx + 2  # Data starts after header and separator
            break

    if header_line is None or separator_line is None:
        print("Header or separator line not found.")
        return {"layers": [], "summary": {}, "header": []}

    # Use the positions of '|' to determine the column boundaries
    positions = [match.start() for match in re.finditer(r'\|', header_line)]

    # Function to parse a line into columns based on '|' positions
    def parse_line(line: str, positions: List[int]) -&gt; List[str]:
        cols = []
        for i in range(len(positions) - 1):
            start = positions[i] + 1
            end = positions[i + 1]
            col = line[start:end].strip()
            cols.append(col)
        # Last column after the last '|'
        start = positions[-1] + 1
        col = line[start:].strip()
        cols.append(col)
        return cols

    # Get the header columns
    header_columns = parse_line(header_line, positions)

    # Initialize list to hold layer details
    layers = []

    # Iterate over the data lines until a non-data line is encountered
    for line in lines[data_start_idx:]:
        # Stop if we reach the separator line (line with dashes)
        if set(line.strip()) == {'-'}:
            # Summary section starts after this line
            summary_start_idx = lines.index(line) + 1
            break

        # Skip empty lines
        if not line.strip():
            continue

        # Parse the line into columns
        cols = parse_line(line, positions)

        # Ensure the number of columns matches the header
        if len(cols) != len(header_columns):
            continue  # Skip lines that don't match the expected format

        # Create a dictionary for the current layer
        layer_dict = dict(zip(header_columns, cols))
        layers.append(layer_dict)
    else:
        # If we didn't break out of the loop, set summary_start_idx to the end
        summary_start_idx = len(lines)

    # Extract summary metrics from the remaining lines
    summary_lines = lines[summary_start_idx:]
    summary_dict = {}
    for line in summary_lines:
        line = line.strip()
        if not line:
            continue
        # Match lines like "195       Trainable params"
        match = re.match(r"(\d+\.?\d*)\s+(.+)", line)
        if match:
            value, key = match.groups()
            # Convert numeric values to float or int where appropriate
            try:
                if '.' in value:
                    value = float(value)
                else:
                    value = int(value)
            except ValueError:
                pass  # Keep as string if conversion fails
            summary_dict[key.strip()] = value
        else:
            # Handle lines that may have the key and value in reverse order
            match = re.match(r"(.+)\s+(\d+\.?\d*)", line)
            if match:
                key, value = match.groups()
                try:
                    if '.' in value:
                        value = float(value)
                    else:
                        value = int(value)
                except ValueError:
                    pass
                summary_dict[key.strip()] = value

    # Get model file size
    try:
        filesize = os.path.getsize(model_checkpoint)
        summary_dict["Model File Size (bytes)"] = filesize
    except Exception as e:
        print(f"Error getting model file size: {e}")

    # Combine header, layers, and summary into the final output
    output = {
        "header": header_columns,
        "layers": layers,
        "summary": summary_dict
    }

    return output



def save_results(results, individual_metrics, output_dir, model_name, model_summary, model_checkpoint):
    """
    Saves the evaluation results along with the parsed model summary to a JSON file.

    Args:
        results (Dict[str, Any]): Aggregate evaluation metrics.
        individual_metrics (Dict[str, Dict[str, Any]]): Per-task evaluation metrics.
        output_dir (str): Directory to save the results.
        model_name (str): Name of the model for file naming.
        model_summary (str): Raw model summary string.
    
    Returns:
        str: Path to the saved JSON file.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    filename = f"{model_name}_eval_results_{timestamp}.json"
    output_path = os.path.join(output_dir, filename)

    # Parse the model_summary string into JSON
    parsed_model_summary = parse_model_summary(model_summary, model_checkpoint)

    data_to_save = {
        "aggregate_results": results,
        "individual_metrics": {task_id: metrics for task_id, metrics in individual_metrics.items()},
        "model_summary": parsed_model_summary  # Use the parsed JSON
    }

    logger.debug(f"DEBUG: Data to be saved: {data_to_save}")

    os.makedirs(output_dir, exist_ok=True)
    with open(output_path, 'w') as f:
        json.dump(data_to_save, f, indent=2)

    logger.info(f"Results saved to {output_path}")
    return output_path

def main(args):
    if args.use_wandb:
        api_key = os.getenv("WANDB_API_KEY")
        if api_key:
            wandb.login(key=api_key)
            wandb.init(project=args.wandb_project, name=args.wandb_run_name)
        else:
            print("WARNING: WANDB_API_KEY not found in environment variables.")
            print("Weights &amp; Biases logging is disabled.")
            args.use_wandb = False
    else:
        print("Weights &amp; Biases logging is disabled.")

    # Load the test data using arckit
    _, test_set = arckit.load_data()
    test_data = ARCDataset(test_set)

    # Compute symbol frequencies from the test dataset
    symbol_freq_array = test_data.get_symbol_frequencies()
    symbol_freq = {str(i): float(freq) for i, freq in enumerate(symbol_freq_array)}
    checkpoint = torch.load(args.model_checkpoint, map_location='cpu')

    # Extract and convert the model configuration from the checkpoint
    if 'model_config' in checkpoint:
        model_config_dict = checkpoint['model_config']
        # Convert dict to ModelConfig object
        model_config = ModelConfig(**model_config_dict)
    else:
        logger.error("Model configuration not found in checkpoint. Please ensure the checkpoint includes 'model_config'.")
        raise ValueError("Model configuration not found in checkpoint. Ensure that the training process includes the ModelConfigSaver callback.")

    # Create configuration
    config = Config(
        model=model_config,
        training=TrainingConfig(),
        evaluation=EvaluationConfig()
    )

    # Determine the number of classes from the test dataset
    max_label_test = max([sample[1].max().item() for sample in test_data])
    num_classes = int(max_label_test) + 1  # Ensure num_classes is an integer
    config.training.symbol_freq = symbol_freq

    # Initialize the model with the complete Config object and symbol frequencies
    model = GPT2ARC(config, num_classes=num_classes, symbol_freq=symbol_freq)
    try:
        # Remove the "model." prefix from state dict keys
        state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items()}
        missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)

        # Check if 'loss_fn.weight' is missing and initialize it if necessary
        if "loss_fn.weight" in missing_keys:
            logger.debug("'loss_fn.weight' not found in state_dict. Initializing with default weights.")
            num_classes = config.training.num_classes  # Ensure this is correctly retrieved from your config
            default_weights = torch.ones(num_classes)
            model.loss_fn.weight = default_weights
            logger.debug(f"'loss_fn.weight' initialized with weights: {default_weights}")

        # Optionally, log any unexpected keys for further debugging
        if unexpected_keys:
            logger.warning(f"Unexpected keys in state_dict: {unexpected_keys}")

        # Print all keys in the model's state dictionary
        print("Model state_dict keys:", list(model.state_dict().keys()))
    except Exception as e:
        logger.error(f"Error while loading state_dict: {e}")
        logger.error(f"Available keys in checkpoint: {list(checkpoint.keys())}")
        raise

    model.eval()

    # Generate model summary
    print("DEBUG: Attempting to generate model summary")
    try:
        model_summary = str(ModelSummary(model, max_depth=-1))
        print("DEBUG: Model summary generated successfully")
    except Exception as e:
        print(f"DEBUG: Error generating model summary - {str(e)}")
        model_summary = "Error generating model summary"

    print("DEBUG: Model summary:")
    print(model_summary)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    logger.info(f"Model moved to device: {device}")

    # Define the input size based on your model's expected input
    sequence_length = 100  # Example value; adjust as needed
    input_size = (1, 1, sequence_length)  # Adjusted to match (batch_size, channels, sequence_length)
    logger.info(f"Defined input_size for summary: {input_size}")

    # Extract model name from the checkpoint path and sanitize it
    model_name = os.path.basename(args.model_checkpoint).split('.')[0]
    # Sanitize model_name to contain only valid characters
    model_name = ''.join(c if c.isalnum() or c in '-_.' else '_' for c in model_name)

    # Debugging statements
    logger.debug(f"Sanitized model_name: {model_name}")
    print(f"DEBUG: Sanitized model_name: {model_name}")

    # Verify that model_name contains only allowed characters
    allowed_chars = set("abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789-_.")
    invalid_chars = set(model_name) - allowed_chars
    if invalid_chars:
        logger.error(f"Model name contains invalid characters after sanitization: {invalid_chars}")
        print(f"ERROR: Model name contains invalid characters after sanitization: {invalid_chars}")
    else:
        logger.debug("Model name contains only valid characters.")
        print("DEBUG: Model name contains only valid characters.")


    # Create configuration
    config = Config(
        model=model_config,
        training=TrainingConfig(),
        evaluation=EvaluationConfig()
    )

    # Evaluate the model
    results, individual_metrics = evaluate(model, test_data, config, args.batch_size, args)

    logger.debug(f"DEBUG: Evaluation results: {results}")
    logger.debug(f"DEBUG: Individual metrics: {individual_metrics}")

    logger.info("Evaluation Results:")
    for metric, value in results.items():
        if metric != 'complete_task_accuracy':
            print(f"{metric}: {value}")
            if args.use_wandb:
                wandb.log({f"eval/{metric}": value})

    # Print complete_task_accuracy at the bottom
    if 'complete_task_accuracy' in results:
        print(f"complete_task_accuracy: {results['complete_task_accuracy']}")
        if args.use_wandb:
            wandb.log({"eval/complete_task_accuracy": results['complete_task_accuracy']})

    # Log individual task metrics
    for task_id, metrics in individual_metrics.items():
        # Ensure metrics are not already floats
        if isinstance(metrics['test_accuracy'], list):
            metrics['test_accuracy'] = sum(metrics['test_accuracy']) / len(metrics['test_accuracy'])
        if isinstance(metrics['test_diff_accuracy'], list):
            metrics['test_diff_accuracy'] = sum(metrics['test_diff_accuracy']) / len(metrics['test_diff_accuracy'])
        logger.info(f"Task {task_id}: Accuracy = {metrics['test_accuracy']:.4f}, Diff Accuracy = {metrics['test_diff_accuracy']:.4f}")

    # Save results regardless of wandb usage
    results_path = save_results(results, individual_metrics, args.output_dir, model_name, model_summary, args.model_checkpoint)

    if args.use_wandb:
        # Wandb artifact creation and logging
        logger.debug(f"Creating wandb Artifact with name: {model_name}")
        print(f"DEBUG: Creating wandb Artifact with name: {model_name}")

        try:
            artifact = wandb.Artifact(name=model_name, type='evaluation')
            artifact.add_file(results_path)
            wandb.log_artifact(artifact)
            logger.debug("Artifact created and logged successfully.")
            print("DEBUG: Artifact created and logged successfully.")
        except ValueError as ve:
            logger.error(f"Failed to create wandb Artifact: {ve}")
            print(f"ERROR: Failed to create wandb Artifact: {ve}")
            raise ve

        wandb.finish()

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Evaluate the ARC Neural Reasoning Model")
    parser.add_argument("--model_checkpoint", type=str, required=True, help="Path to the model checkpoint")
    parser.add_argument("--batch_size", type=int, default=32, help="Batch size for evaluation")
    parser.add_argument("--output_dir", type=str, default="./evaluation_results", help="Directory to save evaluation results")
    parser.add_argument("--log-level", type=str, default="INFO", help="Set the logging level (e.g., DEBUG, INFO, WARNING)")
    parser.add_argument("--wandb_project", type=str, default="arc-evaluation", help="Weights &amp; Biases project name")
    parser.add_argument("--wandb_run_name", type=str, default=None, help="Weights &amp; Biases run name")

    parser.add_argument("--use_wandb", action='store_true', help="Use Weights &amp; Biases for logging")
    args = parser.parse_args()

    # Create output directory if it doesn't exist
    os.makedirs(args.output_dir, exist_ok=True)

    # Set logging level
    logging.basicConfig(level=getattr(logging, args.log_level.upper(), None))
    
    main(args)

</file>
<file name="src/optimize_hyperparameters.py">
# gpt2_arc/src/optimize_hyperparameters.py
import argparse
import multiprocessing
import random
import optuna
import logging
import sys
import os
import torch
from torch.utils.data import DataLoader
import gc
import pytorch_lightning as pl
import numpy as np
from pytorch_lightning.utilities.model_summary import ModelSummary
from optuna.pruners import PercentilePruner
from optuna.samplers import TPESampler
from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint, Callback
from pytorch_lightning.loggers import TensorBoardLogger
#from gpt2_arc.src.utils.training_helpers import get_num_workers
from gpt2_arc.src.training.trainer import NanLossPruningCallback
from functools import partial  # Import partial
from gpt2_arc.src.training.train import load_dataset, load_and_split_synthetic_data
from gpt2_arc.src.training.train import ModelConfigSaver
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.utils.results_collector import ResultsCollector

class BestEpochTrackerCallback(Callback):
    def __init__(self):
        super().__init__()
        self.best_epoch = 0

    def on_validation_end(self, trainer, pl_module):
        current_val_loss = trainer.callback_metrics.get("val_loss")
        if current_val_loss is not None:
            if not hasattr(self, 'best_val_loss') or current_val_loss &lt; self.best_val_loss:
                self.best_val_loss = current_val_loss
                self.best_epoch = trainer.current_epoch
                logger.debug(f"New best_val_loss: {self.best_val_loss} at epoch {self.best_epoch}")
from gpt2_arc.src.utils.model_memory_estimator import (
    calculate_params,
    estimate_memory_usage,
    get_available_memory,
    can_fit_model
)

class CustomPruningCallback(pl.Callback):
    def __init__(self, trial, monitor="val_loss"):
        super().__init__()
        self.trial = trial
        self.monitor = monitor

    def on_validation_end(self, trainer, pl_module):
        epoch = trainer.current_epoch
        current_score = trainer.callback_metrics.get(self.monitor)
        if current_score is None:
            return
        self.trial.report(current_score, step=epoch)
        if self.trial.should_prune():
            raise optuna.TrialPruned()

# Define the base directory for the arc-neural-reasoning-model
arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../.."))

# Add the project root to the Python path
project_root = arc_model_dir
sys.path.insert(0, project_root)

from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.training.trainer import ARCTrainer
from gpt2_arc.src.data.arc_dataset import ARCDataset
import arckit
from gpt2_arc.src.utils.performance_metrics import calculate_mamba_efficiency



def validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout):
    """Validate that hyperparameters meet necessary constraints."""
    logger.debug(f"Validating hyperparameters: n_embd={n_embd}, n_head={n_head}, n_layer={n_layer}, "
                 f"mamba_ratio={mamba_ratio}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}")
    assert n_embd % n_head == 0, f"n_embd ({n_embd}) must be divisible by n_head ({n_head})"
    assert n_embd &gt;= n_head, f"n_embd ({n_embd}) must be greater than or equal to n_head ({n_head})"
    assert n_layer &gt; 0, f"n_layer ({n_layer}) must be positive"
    assert d_state &gt; 0, f"d_state ({d_state}) must be positive"
    assert d_conv &gt; 0, f"d_conv ({d_conv}) must be positive"
    assert 0.0 &lt;= dropout &lt;= 1.0, f"dropout ({dropout}) must be between 0.0 and 1.0"
    logger.debug("Hyperparameters validated successfully")
    return True





def objective(trial, args, all_synthetic_data):
    model = None
    trainer = None
    arc_trainer = None
    logger.info(f"Starting trial {trial.number}")
    try:
        # Initialize config
        model_config = ModelConfig()
        training_config = TrainingConfig()
        config = Config(model=model_config, training=training_config)
        # Use pre-loaded synthetic data for training
        train_data = all_synthetic_data['train_dataset'] if args.use_synthetic_data else load_dataset(args, config, dataset_type='train', max_samples=args.max_train_samples)

        # Load validation and test data from arckit
        val_data = load_dataset(args, config, dataset_type='val')
        test_data = load_dataset(args, config, dataset_type='test')

        # Log dataset sizes
        logger.debug(f"Trial {trial.number}: Loaded {len(train_data)} training samples.")
        logger.debug(f"Trial {trial.number}: Loaded {len(val_data)} validation samples.")
        logger.debug(f"Trial {trial.number}: Loaded {len(test_data)} test samples.")
        fixed_hyperparams = {}

        # Initialize config and symbol_freq_dict
        model_config = ModelConfig()
        training_config = TrainingConfig()
        config = Config(model=model_config, training=training_config)
        symbol_freq_dict = {}


        # Create configuration
        model_config = ModelConfig()
        training_config = TrainingConfig()
        config = Config(model=model_config, training=training_config)


        if args.fast_dev_run:
            # Disable symbol frequency balancing for fast development run
            symbol_freq_dict = {}
            balance_symbols = False
            balancing_method = "none"
            logger.debug("fast_dev_run is enabled. Disabling symbol frequency balancing.")
        else:
            if args.enable_symbol_freq:
                # Calculate Symbol Frequencies
                if args.use_synthetic_data:
                    logger.debug("Calculating symbol frequencies for synthetic training set")
                    symbol_freq = train_data.get_symbol_frequencies()
                else:
                    logger.debug("Calculating symbol frequencies for ARC training set")
                    symbol_freq = train_data.get_symbol_frequencies()

                logger.debug(f"Computed symbol frequencies: {symbol_freq}")

                # Directly copy symbol_freq to symbol_freq_dict
                # Ensure symbol_freq_dict is a dictionary
                if isinstance(symbol_freq, np.ndarray):
                    # If symbol_freq is a NumPy array, convert it to a dictionary
                    symbol_freq_dict = {i: float(freq) for i, freq in enumerate(symbol_freq)}
                    logger.debug("Converted symbol_freq from NumPy array to dictionary.")
                elif isinstance(symbol_freq, dict):
                    symbol_freq_dict = symbol_freq.copy()
                    logger.debug("Copied symbol_freq as a dictionary.")
                else:
                    raise TypeError(f"Unexpected type for symbol_freq: {type(symbol_freq)}. Expected dict or np.ndarray.")

                # Assert that symbol_freq_dict is indeed a dictionary
                assert isinstance(symbol_freq_dict, dict), f"symbol_freq_dict must be a dict, but got {type(symbol_freq_dict)}."

                # Remove the padding symbol from symbol_freq_dict
                pad_symbol_idx = config.training.pad_symbol_idx
                symbol_freq_dict.pop(pad_symbol_idx, None)
                logger.debug(f"Removed pad_symbol_idx ({pad_symbol_idx}) from symbol_freq_dict. New length: {len(symbol_freq_dict)}")

                # Debugging: Check keys and their types
                logger.debug(f"Keys in symbol_freq_dict after popping padding symbol: {list(symbol_freq_dict.keys())}")
                logger.debug(f"Types of keys in symbol_freq_dict: {set(type(k) for k in symbol_freq_dict.keys())}")

                # Ensure the length of symbol_freq_dict matches num_classes - 1
                assert len(symbol_freq_dict) == config.training.num_classes - 1, (
                    f"Length of symbol_freq_dict ({len(symbol_freq_dict)}) does not match num_classes minus padding ({config.training.num_classes - 1})."
                )
                balance_symbols = True
                balancing_method = "weighting"
            else:
                symbol_freq_dict = {}
                logger.debug("Symbol frequency calculation is disabled. Using empty symbol_freq_dict.")
                balance_symbols = False
                balancing_method = "none"
        include_pad_in_loss = args.include_pad_in_loss
        torch.set_float32_matmul_precision(args.matmul_precision)
        logger.info(f"Trial {trial.number}: Set float32 matmul precision to: {args.matmul_precision}")

        if not args.model_checkpoint:
            # Only suggest hyperparameters that are not fixed by the checkpoint
            # Suggest n_head exponent and calculate n_head
            n_head_exp = trial.suggest_int("n_head_exp", args.n_head_exp_min, args.n_head_exp_max)
            n_head = 2 ** n_head_exp
            logger.debug(f"Suggested n_head: {n_head} (2^{n_head_exp})")

            # Suggest n_embd as a multiple of n_head and ensure it's a power of 2
            n_embd_multiplier = trial.suggest_int("n_embd_multiplier", args.n_embd_multiplier_min, args.n_embd_multiplier_max)
            n_embd = n_head * n_embd_multiplier
            n_embd = 2 ** int(np.log2(n_embd))
            logger.debug(f"Adjusted n_embd: {n_embd}")

            # Suggest n_layer
            n_layer = trial.suggest_int("n_layer", args.n_layer_min, args.n_layer_max)
            logger.debug(f"Suggested n_layer: {n_layer}")

            # Suggest Mamba-specific hyperparameters
            mamba_ratio = trial.suggest_float("mamba_ratio", args.mamba_ratio_min, args.mamba_ratio_max, step=args.mamba_ratio_step)
            d_state = trial.suggest_int("d_state", args.d_state_min, args.d_state_max)
            d_conv = trial.suggest_int("d_conv_min", args.d_conv_min, args.d_conv_max)

            # Suggest dropout rate
            dropout = trial.suggest_float("dropout", args.dropout_min, args.dropout_max, step=args.dropout_step)
            mamba_depth = trial.suggest_int("mamba_depth", args.mamba_depth_min, args.mamba_depth_max)
            logger.debug(f"Suggested mamba_depth: {mamba_depth}")

            mamba_expand = trial.suggest_int("mamba_expand", args.mamba_expand_min, args.mamba_expand_max)
            logger.debug(f"Suggested mamba_expand: {mamba_expand}")

            logger.debug(f"Using suggested hyperparameters: n_head={n_head}, n_embd={n_embd}, "
                         f"n_layer={n_layer}, mamba_ratio={mamba_ratio}, d_state={d_state}, "
                         f"d_conv={d_conv}, dropout={dropout}, mamba_depth={mamba_depth}, "
                         f"mamba_expand={mamba_expand}")

            # Ensure the Config uses the fixed hyperparameters
            config = Config(model=model_config, training=training_config)


        # Use grokfast based on command line argument
        use_grokfast = args.use_grokfast

        if use_grokfast:
            # Suggest Grokfast type based on command-line choices
            grokfast_type = trial.suggest_categorical("grokfast_type", args.grokfast_type_choices)

            # Suggest Grokfast alpha within specified range
            grokfast_alpha = trial.suggest_float("grokfast_alpha", args.grokfast_alpha_min, args.grokfast_alpha_max)

            # Suggest Grokfast lambda within specified range
            grokfast_lamb = trial.suggest_float("grokfast_lamb", args.grokfast_lamb_min, args.grokfast_lamb_max)

            # If using 'ma', suggest window_size within specified range
            if grokfast_type == "ma":
                grokfast_window_size = trial.suggest_int("grokfast_window_size", args.grokfast_window_size_min, args.grokfast_window_size_max)
            else:
                grokfast_window_size = None
        else:
            grokfast_type = None
            grokfast_alpha = None
            grokfast_lamb = None
            grokfast_window_size = None
        batch_size = trial.suggest_int("batch_size", args.batch_size_min, args.batch_size_max)
        learning_rate = trial.suggest_float("learning_rate", args.learning_rate_min, args.learning_rate_max, log=True)
        max_epochs = trial.suggest_int("max_epochs", args.max_epochs_min, args.max_epochs_max)
        
        # If a checkpoint is used, set fixed values and do not suggest architecture-related hyperparameters
        if args.model_checkpoint:
            n_head = model_config.n_head
            n_embd = model_config.n_embd
            n_layer = model_config.n_layer
            mamba_ratio = model_config.mamba_ratio
            d_state = model_config.d_state
            d_conv = model_config.d_conv
            dropout = model_config.dropout
            mamba_depth = model_config.mamba_depth
            mamba_expand = model_config.mamba_expand

        # Validate hyperparameters
        validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout)

        # Check if the model will fit in memory
        # Adjust the total number of layers to include Mamba layers
        total_mamba_layers = int(n_layer * mamba_ratio)
        total_layers = n_layer + total_mamba_layers

        # Recalculate total parameters based on total_layers
        total_params = calculate_params(
            n_layers=total_layers,
            n_heads=n_head,
            d_model=n_embd,
            mamba_ratio=mamba_ratio,
            d_state=d_state,
            d_conv=d_conv,
            mamba_depth=mamba_depth,
            mamba_expand=mamba_expand
        )
        # Improve memory estimation by considering additional factors like optimizer state and activation memory
        safety_margin = 0.1  # 10% safety margin
        estimated_memory = estimate_memory_usage(
            total_params=total_params,
            batch_size=batch_size,
            height=30,  # Adjust as necessary based on your data
            width=30,   # Adjust as necessary
            d_model=n_embd
        )
        available_memory = get_available_memory()
        estimated_memory *= (1 + safety_margin)

        logger.debug(f"Trial {trial.number}: Estimated memory usage: {estimated_memory:.2f} GB")
        logger.debug(f"Trial {trial.number}: Available memory: {available_memory:.2f} GB")

        # Prune the trial if estimated memory exceeds 80% of available memory
        if not can_fit_model(estimated_memory, available_memory * 0.8):
            logger.warning(f"Trial {trial.number}: Model too large for available memory. Skipping.")
            raise optuna.exceptions.TrialPruned()

        logger.debug(f"Suggested dropout rate: {dropout}")


        if args.model_checkpoint:
            # Use fixed hyperparameters from the checkpoint
            model_config = ModelConfig(**fixed_hyperparams)
            training_config = TrainingConfig(
                num_classes=config.model.num_classes,
                batch_size=batch_size,
                learning_rate=learning_rate,
                max_epochs=max_epochs,
                use_grokfast=use_grokfast,
                grokfast_type=grokfast_type,
                grokfast_alpha=grokfast_alpha,
                grokfast_lamb=grokfast_lamb,
                grokfast_window_size=grokfast_window_size,
                include_pad_in_loss=include_pad_in_loss,
                symbol_freq=symbol_freq_dict,
                balance_symbols=balance_symbols,
                balancing_method="weighting" if balance_symbols else "none"
            )
        else:
            # Use suggested hyperparameters
            model_config = ModelConfig(
                n_embd=n_embd,
                n_head=n_head,
                n_layer=n_layer,
                dropout=dropout,
                mamba_ratio=mamba_ratio,
                d_state=d_state,
                d_conv=d_conv,
                mamba_depth=mamba_depth,
                mamba_expand=mamba_expand
            )
            training_config = TrainingConfig(
                batch_size=batch_size,
                learning_rate=learning_rate,
                max_epochs=max_epochs,
                use_grokfast=use_grokfast,
                grokfast_type=grokfast_type,
                grokfast_alpha=grokfast_alpha,
                grokfast_lamb=grokfast_lamb,
                grokfast_window_size=grokfast_window_size,
                include_pad_in_loss=include_pad_in_loss,
                symbol_freq=symbol_freq_dict,
                balance_symbols=balance_symbols,
                balancing_method=balancing_method,
                num_workers=args.num_workers if args.num_workers is not None else multiprocessing.cpu_count(),
                prefetch_factor=args.prefetch_factor,
                persistent_workers=not args.no_persistent_workers,
                pin_memory=not args.no_pin_memory
            )

        config = Config(model=model_config, training=training_config)
        config.estimated_memory = estimated_memory
        config.available_memory = available_memory
        logger.debug(f"Suggested Mamba parameters - mamba_ratio: {mamba_ratio}, d_state: {d_state}, d_conv: {d_conv}")
        trial.set_user_attr("mamba_ratio", mamba_ratio)
        trial.set_user_attr("d_state", d_state)
        trial.set_user_attr("d_conv", d_conv)
        trial.set_user_attr("mamba_depth", mamba_depth)
        trial.set_user_attr("mamba_expand", mamba_expand)
        logger.debug(f"Full config: {config}")

        # Instantiate the ModelConfigSaver callback with the current config
        model_config_saver = ModelConfigSaver(config)


        # Calculate Symbol Frequencies
        if args.use_synthetic_data:
            logger.debug("Calculating symbol frequencies for synthetic training set")
            symbol_freq = train_data.get_symbol_frequencies()
        else:
            logger.debug("Calculating symbol frequencies for ARC training set")
            symbol_freq = train_data.get_symbol_frequencies()

        logger.debug(f"Computed symbol frequencies: {symbol_freq}")


        # Create model and trainer
        logger.debug("Creating model and trainer")
        num_classes = config.training.num_classes
        # Instantiate the GPT2ARC model with the constructed Config
        if args.model_checkpoint:
            # Load the checkpoint
            logger.info(f"Loading model from checkpoint: {args.model_checkpoint}")
            checkpoint = torch.load(args.model_checkpoint, map_location="cpu")

            # Extract model configuration from the checkpoint
            if 'model_config' in checkpoint:
                model_config_dict = checkpoint['model_config']
                model_config = ModelConfig(**model_config_dict)
                logger.debug("Extracted model configuration from checkpoint.")
            else:
                logger.error("Model configuration not found in checkpoint. Cannot proceed.")
                raise ValueError("Model configuration not found in checkpoint.")

            # Set hyperparameters from the extracted model_config
            n_head = model_config.n_head
            n_embd = model_config.n_embd
            n_layer = model_config.n_layer
            mamba_ratio = model_config.mamba_ratio
            d_state = model_config.d_state
            d_conv = model_config.d_conv
            dropout = model_config.dropout
            mamba_depth = model_config.mamba_depth
            mamba_expand = model_config.mamba_expand

            # Validate hyperparameters
            validate_hyperparameters(n_embd, n_head, n_layer, mamba_ratio, d_state, d_conv, dropout)

            # Reconstruct the config with the extracted model_config and training_config
            config = Config(model=model_config, training=training_config)

            # Instantiate the model with the exact configuration used during training
            model = GPT2ARC(config=config, num_classes=model_config.num_classes, symbol_freq=symbol_freq_dict, pad_symbol_idx=config.training.pad_symbol_idx)

            # Load the state_dict from the checkpoint
            if 'state_dict' in checkpoint:
                # Remove the "model." prefix from state dict keys
                state_dict = {k.replace('model.', ''): v for k, v in checkpoint['state_dict'].items()}
            else:
                state_dict = checkpoint  # Adjust based on how you saved your model

            # Load the state_dict into the model with strict=False
            try:
                missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
                logger.debug(f"Successfully loaded state_dict from checkpoint: {args.model_checkpoint}")
                if missing_keys:
                    logger.warning(f"Missing keys when loading state_dict: {missing_keys}")
                if unexpected_keys:
                    logger.warning(f"Unexpected keys when loading state_dict: {unexpected_keys}")

                # Handle specific missing keys if necessary
                if "loss_fn.weight" in missing_keys:
                    logger.debug("'loss_fn.weight' not found in state_dict. Initializing with default weights.")
                    default_weights = torch.ones(config.model.num_classes)
                    model.loss_fn.weight = default_weights
                    logger.debug(f"'loss_fn.weight' initialized with weights: {default_weights}")
            except RuntimeError as e:
                logger.error(f"Error loading state_dict: {e}")
                raise e
            
        else:
            model = GPT2ARC(config=config, num_classes=num_classes, symbol_freq=symbol_freq_dict)
        
        # Generate model summary
        print("DEBUG: Attempting to generate model summary")
        try:
            model_summary = str(ModelSummary(model, max_depth=-1))
            print("DEBUG: Model summary generated successfully")
        except Exception as e:
            print(f"DEBUG: Error generating model summary - {str(e)}")
            model_summary = "Error generating model summary"

        # Save model summary to trial user attributes
        print("DEBUG: Attempting to save model summary to trial user attributes")
        try:
            trial.set_user_attr("model_summary", model_summary)
            print("DEBUG: Model summary saved to trial user attributes")
        except Exception as e:
            print(f"DEBUG: Error saving model summary to trial - {str(e)}")

        print("DEBUG: Model summary:")
        print(model_summary)

        # Calculate Mamba efficiency metrics
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        logger.debug("Calculating Mamba efficiency metrics")
        sample_input = torch.randn(1, 1, 6, 6).to(device)
        model.to(device)
        mamba_metrics = calculate_mamba_efficiency(model, sample_input)
        for key, value in mamba_metrics.items():
            trial.set_user_attr(key, value)
            logger.debug(f"Mamba metric - {key}: {value}")

        # Initialize the ResultsCollector
        results_collector = ResultsCollector(config)

        arc_trainer = ARCTrainer(
            model=model,
            train_dataset=train_data,
            val_dataset=val_data,
            config=config,
            args=args,  # Add this line to pass args
            compile_model=False,  # Disable model compilation
            results_collector=results_collector  # Pass ResultsCollector to ARCTrainer
        )

        # Set up PyTorch Lightning trainer with custom pruning callback
        if args.no_progress_bar:
            logger.info("Disabling progress bar.")
        else:
            logger.info("Enabling progress bar.")
        pruning_callback = CustomPruningCallback(trial, monitor="val_loss")
        early_stop_callback = EarlyStopping(monitor="val_loss", min_delta=0.00, patience=3, verbose=False, mode="min")
        # Determine accelerator parameters based on the --accelerator argument
        if args.accelerator == "tpu":
            accelerator = 'tpu'
            devices = 'xla:1'  # Use 'xla:8' for TPU v3-8 pods
            strategy = 'tpu_spawn'  # Recommended strategy for TPU
        elif args.accelerator == "gpu":
            if torch.cuda.is_available():
                accelerator = 'gpu'
                devices = 1
            else:
                accelerator = 'cpu'
                devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
        else:
            accelerator = 'cpu'
            devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
            
        nan_loss_pruning_callback = NanLossPruningCallback()
        #callbacks.append(nan_loss_pruning_callback)
        logger.info("NanLossPruningCallback added to the training callbacks.")
        experiment_id = f"optuna_trial_{trial.number}"
        tb_logger = TensorBoardLogger(save_dir="runs", name=f"experiment_{experiment_id}")
        print(f"DEBUG: Optuna trial TensorBoard logger initialized. Log dir: {tb_logger.log_dir}")
        
        # Extract trial number
        trial_num = trial.number

        # Define task_id (assuming a single task; modify as needed for multiple tasks)
        task_id = "main_task"  # Replace with dynamic task identification if necessary

        # Define iter_num (e.g., based on trial.number or another tracking mechanism)
        iter_num = 1  # Initialize to 1; increment as needed within your optimization loop

        # Initialize the checkpoint callback with descriptive filename
        checkpoint_callback = ModelCheckpoint(
            dirpath=f"checkpoints/trial_{trial.number}",
            filename=f"{'tuning-' if args.model_checkpoint else ''}step_{{step}}-val_loss_{{val_loss:.4f}}",
            save_top_k=3,
            monitor="val_loss",
            mode="min",
        )
        logger.info("Standard ModelCheckpoint callback added to the training callbacks.")

        # Initialize the BestEpochTrackerCallback
        best_epoch_tracker = BestEpochTrackerCallback()

        # Initialize PyTorch Lightning Trainer with the checkpoint callback
        trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            callbacks=[pruning_callback, early_stop_callback, nan_loss_pruning_callback, checkpoint_callback, model_config_saver, best_epoch_tracker],
            logger=tb_logger,
            gradient_clip_val=1.0,    # Add gradient clipping
            val_check_interval=args.val_check_interval,  # Added line
            precision=16,             # Enable Automatic Mixed Precision
            enable_checkpointing=True,
            accelerator=accelerator,
            devices=devices,
            strategy=strategy,
            enable_progress_bar=not args.no_progress_bar
        )
        print("DEBUG: Trainer created for Optuna trial with TensorBoard logger")
        logger.debug(f"Trainer created with config: {trainer.state}")

        # Ensure model is in train mode before training
        model.train()
        logger.debug("Model set to train mode before training")

        # Enhanced Logging: Log model mode before training
        logger.info("Before training:")
        for name, module in model.named_modules():
            logger.debug(f"{name}: {'train' if module.training else 'eval'}")

        logger.debug("Starting training")
        trainer.fit(arc_trainer)

        # Retrieve the best validation loss from the ModelCheckpoint callback
        if checkpoint_callback.best_model_score is not None:
            best_val_loss = checkpoint_callback.best_model_score.item()
            logger.info(f"Trial {trial.number}: Best validation loss: {best_val_loss}")
        else:
            logger.warning(f"Trial {trial.number}: No checkpoints were saved. Assigning a high validation loss.")
            best_val_loss = float('inf')

        logger.info(f"Trial {trial.number} completed. Best validation loss: {best_val_loss}")

        # Enhanced Logging: Log model mode after training
        logger.info("After training:")
        for name, module in model.named_modules():
            logger.debug(f"{name}: {'train' if module.training else 'eval'}")

        # Define DataLoader for test data
        test_loader = DataLoader(
            test_data,
            batch_size=config.training.batch_size,
            shuffle=False,
            num_workers=config.training.num_workers,
            pin_memory=config.training.pin_memory
        )

        # Evaluate the model on test data
        logger.info("Evaluating model on test dataset.")
        test_results = trainer.test(model=arc_trainer, dataloaders=test_loader)

        # Process test results
        if test_results:
            avg_test_loss = sum(result['test_loss'] for result in test_results) / len(test_results)
            avg_test_accuracy = sum(result['test_accuracy'] for result in test_results) / len(test_results)
            avg_test_diff_accuracy = sum(result['test_diff_accuracy'] for result in test_results) / len(test_results)

            logger.info(f"Test results - Loss: {avg_test_loss:.4f}, Accuracy: {avg_test_accuracy:.4f}, Diff Accuracy: {avg_test_diff_accuracy:.4f}")

            # Update final metrics with actual test results
            arc_trainer.results_collector.set_final_metrics({
                "best_val_loss": best_val_loss,
                "best_epoch": trainer.current_epoch,
                "final_test_loss": avg_test_loss,
                "final_test_accuracy": avg_test_accuracy,
                "final_test_diff_accuracy": avg_test_diff_accuracy
            })

        return best_val_loss

    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            logger.error(f"Trial {trial.number}: CUDA out of memory error.")
            logger.error("Pruning trial and suggesting to adjust hyperparameters.")
            trial.set_user_attr('failed_reason', 'CUDA out of memory')
            raise optuna.exceptions.TrialPruned()
        else:
            logger.error(f"Trial {trial.number}: A runtime error occurred: {str(e)}", exc_info=True)
            raise RuntimeError(f"Trial {trial.number}: A runtime error occurred: {str(e)}")
    except Exception as e:
        # Improved exception handling for symbol frequency issues
        if "symbol_freq" in str(e):
            logger.error(f"Trial {trial.number}: 'symbol_freq' is missing or invalid. Ensure it is calculated and passed correctly.", exc_info=True)
            raise optuna.exceptions.TrialPruned(f"Trial {trial.number}: 'symbol_freq' is missing or invalid.")
        else:
            logger.error(f"Trial {trial.number}: An unexpected error occurred: {str(e)}", exc_info=True)
            raise optuna.exceptions.TrialPruned(f"Trial {trial.number}: An unexpected error occurred: {str(e)}")
    finally:
        # Ensure Proper Cleanup Between Trials
        logger.debug(f"Cleaning up after trial {trial.number}")
        if model is not None:
            del model
        if trainer is not None:
            del trainer
        if arc_trainer is not None:
            del arc_trainer
        gc.collect()
        torch.cuda.empty_cache()
        logger.debug(f"Cleanup completed for trial {trial.number}")
        gc.collect()
        torch.cuda.empty_cache()



from functools import partial

def run_optimization(n_trials=100, storage_name="sqlite:///optuna_results.db", n_jobs=-1, args=None, study_name="gpt2_arc_optimization_v2"):

    if n_trials &lt; 10:
        n_startup_trials = 1
    else:
        n_startup_trials = 5

    pruner = PercentilePruner(
        percentile=25, 
        n_startup_trials=n_startup_trials, 
        n_warmup_steps=2, 
        interval_steps=1
    )
    sampler = TPESampler(n_startup_trials=5)

    # Initialize configuration
    model_config = ModelConfig()
    training_config = TrainingConfig()
    config = Config(model=model_config, training=training_config)

    all_synthetic_data = load_and_split_synthetic_data(args, config) if args.use_synthetic_data else None
    if all_synthetic_data:
        logger.info(f"Synthetic data loaded with {len(all_synthetic_data['train_dataset'])} samples.")

    # Create a partial objective function that includes all_synthetic_data
    objective_partial = partial(objective, args=args, all_synthetic_data=all_synthetic_data)

    study = optuna.create_study(
        study_name=study_name,
        storage=storage_name,
        load_if_exists=True,
        direction="minimize",
        pruner=pruner,
        sampler=sampler
    )

    logger.info(f"Starting optimization with {n_trials} trials using {n_jobs} parallel jobs")
    logger.info(f"Data Splitting Ratios - Train: 80%, Validation: 10%, Test: 10%")
    if args.use_gpu:
        available_gpus = torch.cuda.device_count()
        if available_gpus &gt; 1:
            n_jobs = max(n_jobs, available_gpus)
        else:
            n_jobs = 1  # Limit to 1 to prevent memory issues
    study.optimize(objective_partial, n_trials=n_trials, n_jobs=n_jobs)  # Use the partial function

    logger.info("Optimization completed")

    if study.best_trial and study.best_trial.state == optuna.trial.TrialState.COMPLETE:
        print("DEBUG: Best trial found, attempting to retrieve model summary")
        best_model_summary = study.best_trial.user_attrs.get("model_summary")
        if best_model_summary:
            print("DEBUG: Model summary retrieved successfully")
            logger.info("Model summary for the best trial:")
            logger.info(best_model_summary)
        else:
            print("DEBUG: No model summary found for the best trial")
    else:
        logger.warning("No successful trials found. Please check the trial configurations and constraints.")
        if study.best_trial:
            logger.info(f"Best trial: {study.best_trial.number}")
            logger.info(f"Best value: {study.best_trial.value}")
            
            best_trial = study.best_trial
            best_trial.set_user_attr("mamba_ratio", best_trial.params.get("mamba_ratio"))
            best_trial.set_user_attr("d_state", best_trial.params.get("d_state"))
            best_trial.set_user_attr("d_conv", best_trial.params.get("d_conv"))
    
            logger.info("Best Mamba metrics:")
            for key in ['mamba_forward_pass_time', 'mamba_params', 'mamba_params_ratio']:
                value = study.best_trial.user_attrs.get(key)
                if value is not None:
                    logger.info(f"  {key}: {value}")
            
            logger.info("Best hyperparameters:")
            for key, value in study.best_trial.params.items():
                logger.info(f"  {key}: {value}")
        else:
            logger.info("No trials have been completed successfully.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Optimize hyperparameters for GPT2ARC model.")
    parser.add_argument("--max_train_samples", type=int, default=None, help="Maximum number of training samples to load. Use None to load all samples.")
    parser.add_argument("--n_trials", type=int, default=10, help="Number of trials for optimization.")
    parser.add_argument("--n_jobs", type=int, default=1, help="Number of parallel jobs. -1 means using all available cores.")
    parser.add_argument("--batch_size_min", type=int, default=1, help="Minimum value for batch_size.")
    parser.add_argument("--batch_size_max", type=int, default=1, help="Maximum value for batch_size.")
    parser.add_argument(
        "--fast_dev_run",
        action="store_true",
        help="Run a fast development test."
    )
    parser.add_argument(
        "--use_grokfast",
        action="store_true",
        help="Enable Grokfast for gradient filtering."
    )
    parser.add_argument("--storage", type=str, default="sqlite:///optuna_results.db", help="Storage path for Optuna results.")
    parser.add_argument("--random_seed", type=int, default=42, help="Random seed for reproducibility.")
    parser.add_argument(
        "--val_check_interval",
        type=float,
        default=0.01,
        help=(
            "How often to perform validation. "
            "If a float, represents the fraction of an epoch (e.g., 0.5 for halfway through each epoch). "
            "If an integer, represents the number of training steps."
        )
    )
    parser.add_argument(
        "--model_checkpoint",
        type=str,
        default=None,
        help="Path to the model checkpoint to resume optimization from."
    )
    parser.add_argument(
        "--include_pad_in_loss",
        type=lambda x: (str(x).lower() in ['true', '1', 't', 'y', 'yes']),
        default=True,
        help="Whether to include the padding class in the loss calculation. (True/False)"
    )
    parser.add_argument(
        "--study_name",
        type=str,
        default="gpt2_arc_optimization_v3",
        help="Name of the Optuna study."
    )

    parser.add_argument("--train_split", type=float, default=0.8, help="Proportion of data to use for training")
    parser.add_argument("--val_split", type=float, default=0.1, help="Proportion of data to use for validation")
    parser.add_argument("--test_split", type=float, default=0.1, help="Proportion of data to use for testing")
    parser.add_argument("--n_embd_max", type=int, default=1, help="Maximum value for n_embd")
    parser.add_argument(
        "--num_workers",
        type=int,
        default=None,
        help="Number of worker threads for DataLoader. If not set, uses configuration default (total CPU count)."
    )
    parser.add_argument(
        "--prefetch_factor",
        type=int,
        default=2,
        help="Number of batches to prefetch per worker."
    )
    parser.add_argument(
        "--no_persistent_workers",
        action="store_true",
        help="Disable persistent workers in DataLoader."
    )
    parser.add_argument(
        "--no_pin_memory",
        action="store_true",
        help="Disable pin_memory in DataLoader."
    )
    parser.add_argument("--n_head_min", type=int, default=1, help="Minimum value for n_head")
    parser.add_argument("--n_head_max", type=int, default=1, help="Maximum value for n_head")
    parser.add_argument("--n_head_exp_min", type=int, default=1, help="Minimum exponent for n_head (2^x)")
    parser.add_argument("--n_head_exp_max", type=int, default=1, help="Maximum exponent for n_head (2^x)")
    parser.add_argument("--n_embd_multiplier_min", type=int, default=1, help="Minimum multiplier for n_embd")
    parser.add_argument("--n_embd_multiplier_max", type=int, default=1, help="Maximum multiplier for n_embd")
    parser.add_argument("--n_layer_min", type=int, default=1, help="Minimum value for n_layer")
    parser.add_argument("--n_layer_max", type=int, default=1, help="Maximum value for n_layer")
    parser.add_argument("--learning_rate_min", type=float, default=1e-5, help="Minimum value for learning_rate")
    parser.add_argument("--learning_rate_max", type=float, default=1e-2, help="Maximum value for learning_rate")
    parser.add_argument("--max_epochs_min", type=int, default=1, help="Minimum value for max_epochs")
    parser.add_argument("--max_epochs_max", type=int, default=10, help="Maximum value for max_epochs")

    parser.add_argument("--mamba_ratio_min", type=float, default=1.0, help="Minimum value for mamba_ratio")
    parser.add_argument("--mamba_ratio_max", type=float, default=8.0, help="Maximum value for mamba_ratio")
    parser.add_argument("--mamba_ratio_step", type=float, default=0.25, help="Step size for mamba_ratio")
    parser.add_argument("--d_state_min", type=int, default=1, help="Minimum value for d_state")
    parser.add_argument("--d_state_max", type=int, default=1, help="Maximum value for d_state")
    parser.add_argument("--d_conv_min", type=int, default=1, help="Minimum value for d_conv")
    parser.add_argument("--d_conv_max", type=int, default=1, help="Maximum value for d_conv")

    parser.add_argument("--dropout_min", type=float, default=0.0, help="Minimum value for dropout")
    parser.add_argument("--mamba_depth_min", type=int, default=1, help="Minimum value for mamba_depth")
    parser.add_argument("--mamba_depth_max", type=int, default=1, help="Maximum value for mamba_depth")
    parser.add_argument("--mamba_expand_min", type=int, default=2, help="Minimum value for mamba_expand")
    parser.add_argument("--mamba_expand_max", type=int, default=2, help="Maximum value for mamba_expand")
    parser.add_argument(
        "--enable_symbol_freq",
        action="store_true",
        help="Enable the calculation of symbol frequencies."
    )
    parser.set_defaults(enable_symbol_freq=False)
    parser.add_argument("--dropout_max", type=float, default=0.5, help="Maximum value for dropout")
    parser.add_argument("--dropout_step", type=float, default=0.1, help="Step size for dropout")
    parser.add_argument("--use_gpu", action="store_true", help="Flag to indicate whether to use GPU for training.")
    parser.add_argument(
        "--no_progress_bar",
        action="store_true",
        help="Disable the progress bar during training."
    )
    parser.add_argument("--use_synthetic_data", action="store_true", help="Flag to indicate whether to use synthetic data for training.")
    parser.add_argument(
        "--matmul_precision",
        type=str,
        default="medium",
        choices=["highest", "high", "medium"],
        help="Set the internal precision of float32 matrix multiplications for optimization trials. Options: 'highest', 'high', 'medium'. Defaults to 'medium'."
    )
    parser.add_argument("--synthetic_data_path", type=str, default="", help="Path to synthetic data for training.")
    parser.add_argument("--log_level", type=str, default="INFO", help="Logging level (e.g., DEBUG, INFO, WARNING, ERROR, CRITICAL).")
    parser.add_argument(
        "--accelerator",
        type=str,
        default="gpu",
        choices=["cpu", "gpu", "tpu"],
        help="Accelerator to use for training: 'cpu', 'gpu', or 'tpu'. Defaults to 'gpu'."
    )

    # Grokfast parameter ranges
    parser.add_argument("--grokfast_alpha_min", type=float, default=0.9, help="Minimum value for grokfast_alpha.")
    parser.add_argument("--grokfast_alpha_max", type=float, default=0.99, help="Maximum value for grokfast_alpha.")
    parser.add_argument("--grokfast_lamb_min", type=float, default=1.0, help="Minimum value for grokfast_lamb.")
    parser.add_argument("--grokfast_lamb_max", type=float, default=3.0, help="Maximum value for grokfast_lamb.")
    parser.add_argument("--grokfast_window_size_min", type=int, default=50, help="Minimum value for grokfast_window_size.")
    parser.add_argument("--grokfast_window_size_max", type=int, default=200, help="Maximum value for grokfast_window_size.")
    parser.add_argument("--grokfast_type_choices", type=str, nargs='+', default=["ema", "ma"], choices=["ema", "ma"], help="List of Grokfast types to consider during tuning.")


    args = parser.parse_args()
    
    log_level = getattr(logging, args.log_level.upper(), logging.INFO)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler()
        ]
    )
    logger = logging.getLogger(__name__)
    logger.setLevel(log_level)

    # Log parsed arguments for debugging
    logger.debug(f"Parsed arguments: {vars(args)}")
    logger.setLevel(log_level)

    # Ensure the storage_name has the correct SQLite prefix and handle relative paths
    import os  # Ensure os is imported at the top of the file

    if not args.storage.startswith("sqlite:///"):
        if os.path.isabs(args.storage):
            args.storage = f"sqlite:////{args.storage}"
        else:
            args.storage = f"sqlite:///{os.path.abspath(args.storage)}"
    
    logger.debug(f"Optuna storage URL set to: {args.storage}")
    
    # Validate val_check_interval
    if args.val_check_interval &lt;= 0:
        logger.error("The --val_check_interval must be a positive number.")
        sys.exit(1)

    # Set random seeds for reproducibility
    random.seed(args.random_seed)
    np.random.seed(args.random_seed)
    torch.manual_seed(args.random_seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(args.random_seed)
    
    logger.debug(f"Random seed set to: {args.random_seed}")

    run_optimization(
        n_trials=args.n_trials,
        storage_name=args.storage,
        n_jobs=args.n_jobs,
        args=args,
        study_name=args.study_name
    )


</file>
<file name="src/__init__.py">
# This file allows the src directory to be recognized as a package.

</file>
<file name="src/checkpoint_evaluator.py">
#!/usr/bin/env python3
"""
checkpoint_evaluator.py

A script to monitor a directory for new model checkpoints and evaluate them using a specified evaluation script.
Logs are maintained, and resource usage is monitored.

Usage:
    python checkpoint_evaluator.py \
        --output_dir /path/to/output \
        --arc_model_dir /path/to/arc_model \
        --date_folder 2024-10-07 \
        --wandb_project arc-evaluation \
        [optional arguments]
"""

import os
import sys
import time
import subprocess
import threading
import shutil
import logging
import argparse
from datetime import datetime

import psutil  # For resource monitoring
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

def parse_arguments():
    parser = argparse.ArgumentParser(description="Monitor directories for model checkpoints and evaluate them.")
    
    parser.add_argument('--output_dir', type=str, required=True,
                        help='Directory to store logs and evaluation results.')
    parser.add_argument('--arc_model_dir', type=str, required=True,
                        help='Directory containing the arc model scripts.')
    parser.add_argument('--date_folder', type=str, required=True,
                        help='Date folder name (e.g., 2024-10-07) to organize outputs.')
    parser.add_argument('--wandb_project', type=str, default='arc-evaluation',
                        help='Weights &amp; Biases project name.')
    parser.add_argument('--batch_size', type=int, default=32,
                        help='Batch size for evaluation.')
    parser.add_argument('--log_level', type=str, default='DEBUG',
                        choices=['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                        help='Logging level.')
    parser.add_argument('--resource_monitor_interval', type=int, default=60,
                        help='Interval in seconds for resource monitoring logs.')
    
    return parser.parse_args()

def setup_logging(output_dir, log_level):
    os.makedirs(output_dir, exist_ok=True)
    
    logging.basicConfig(
        level=getattr(logging, log_level),
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(os.path.join(output_dir, "checkpoint_evaluator.log")),
            logging.StreamHandler(sys.stdout)
        ]
    )
    logger = logging.getLogger("CheckpointEvaluator")
    return logger

def load_evaluated_models(evaluated_models_file, logger):
    evaluated_models = set()
    if os.path.exists(evaluated_models_file):
        try:
            with open(evaluated_models_file, "r") as f:
                evaluated_models.update(line.strip() for line in f)
            logger.info(f"Loaded evaluated models from {evaluated_models_file}")
        except Exception as e:
            logger.error(f"Error loading evaluated models from {evaluated_models_file}: {e}")
    else:
        logger.info("No previously evaluated models found. Starting fresh.")
    return evaluated_models

def save_evaluated_model(evaluated_models_file, model_path, logger):
    try:
        with open(evaluated_models_file, "a") as f:
            f.write(model_path + "\n")
        logger.debug(f"Recorded evaluation of {model_path} in {evaluated_models_file}")
    except Exception as e:
        logger.error(f"Error writing to evaluated models file {evaluated_models_file}: {e}")

def wait_for_file_stable(file_path, wait_time=1.0, max_retries=10, logger=None):
    """Wait until the file is stable (not changing size)"""
    previous_size = -1
    retries = 0
    while retries &lt; max_retries:
        if not os.path.exists(file_path):
            if logger:
                logger.warning(f"File {file_path} does not exist.")
            return False
        current_size = os.path.getsize(file_path)
        if current_size == previous_size:
            return True
        else:
            previous_size = current_size
            time.sleep(wait_time)
            retries += 1
    if logger:
        logger.warning(f"File {file_path} is not stable after {max_retries} retries.")
    return False

class CheckpointHandler(FileSystemEventHandler):
    def __init__(self, evaluated_models, temp_checkpoint_dir, evaluate_callback, logger):
        super().__init__()
        self.evaluated_models = evaluated_models
        self.temp_checkpoint_dir = temp_checkpoint_dir
        self.evaluate_callback = evaluate_callback
        self.logger = logger

    def on_created(self, event):
        if event.is_directory:
            return
        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):
            self.evaluate_model(event.src_path)

    def evaluate_model(self, model_path):
        model_file = os.path.basename(model_path)
        if model_path in self.evaluated_models:
            self.logger.info(f"Skipping already evaluated model: {model_file}")
            return  # Skip if the model was already evaluated

        # Wait for the file to be stable
        if not wait_for_file_stable(model_path, logger=self.logger):
            self.logger.warning(f"File {model_file} is not stable. Skipping evaluation.")
            return

        # Copy the checkpoint file to temp_checkpoint_dir
        temp_model_path = os.path.join(self.temp_checkpoint_dir, model_file)
        try:
            shutil.copy2(model_path, temp_model_path)
            self.logger.info(f"Copied {model_file} to temporary directory.")
        except Exception as e:
            self.logger.error(f"Error copying {model_file} to temporary directory: {e}")
            return

        # Extract epoch and val_loss from the filename for run_name
        try:
            parts = model_file.replace('.ckpt', '').replace('.pth', '').split('-')
            epoch = None
            val_loss = None
            for part in parts:
                if part.startswith('epoch='):
                    epoch = part.split('=')[1]
                elif part.startswith('val_loss='):
                    val_loss = part.split('=')[1]
            if epoch is not None and val_loss is not None:
                run_name = f"evaluation-epoch{epoch}-val_loss{val_loss}"
            else:
                run_name = f"evaluation-{model_file}"
            self.logger.debug(f"Parsed run name: {run_name}")
        except Exception as e:
            self.logger.error(f"Error parsing run name from filename {model_file}: {e}")
            run_name = f"evaluation-{model_file}"

        # Define the evaluation command
        eval_command = [
            "python", os.path.join(args.arc_model_dir, "gpt2_arc/src/evaluate.py"),
            "--model_checkpoint", temp_model_path,
            "--batch_size", str(args.batch_size),
            "--output_dir", args.output_dir,
            "--wandb_project", args.wandb_project,
            "--wandb_run_name", run_name
        ]
        self.logger.info(f"Evaluating model: {model_file} with run name: {run_name}")

        # Define the log file path
        log_file_path = os.path.join(args.output_dir, f"{model_file}_evaluation.log")
        try:
            with open(log_file_path, "w") as log_file:
                # Run the evaluation command and redirect stdout and stderr to the log file
                subprocess.run(
                    eval_command,
                    check=True,
                    stdout=log_file,
                    stderr=subprocess.STDOUT,
                    text=True  # Automatically decode bytes to string
                )
            self.logger.info(f"Successfully evaluated model: {model_file}. Logs at {log_file_path}")
        except subprocess.CalledProcessError as e:
            self.logger.error(f"Error during evaluation of {model_file}. See log at {log_file_path}")
        except Exception as ex:
            self.logger.exception(f"An unexpected error occurred while evaluating {model_file}: {ex}")

        self.evaluated_models.add(model_path)
        save_evaluated_model(os.path.join(args.output_dir, "evaluated_models.txt"), model_path, self.logger)

        # Delete the temp model file
        try:
            os.remove(temp_model_path)
            self.logger.debug(f"Deleted temporary model file: {temp_model_path}")
        except Exception as e:
            self.logger.error(f"Error deleting temp model file {temp_model_path}: {e}")

def get_all_checkpoint_files(directory):
    checkpoint_files = []
    for root, _, files in os.walk(directory):
        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])
    return checkpoint_files

def start_observer(model_dir, handler, logger):
    # Set up and start the watchdog observer
    observer = Observer()
    observer.schedule(handler, model_dir, recursive=True)
    observer.start()

    logger.info("Watching for new checkpoints and final models in all subdirectories...")
    logger.info("This script will continue running in the background.")

    try:
        while True:
            time.sleep(10)
            # Optionally, you can implement additional periodic checks here
    except KeyboardInterrupt:
        observer.stop()
        logger.info("Observer stopped by user.")
    except FileNotFoundError as fnf_error:
        logger.error(f"FileNotFoundError: {fnf_error}")
        logger.error(f"Please ensure that the directory '{model_dir}' exists.")
    except Exception as e:
        logger.exception(f"An error occurred in the observer: {e}")
    finally:
        observer.join()
        logger.info("Checkpoint and final model evaluation completed.")

def monitor_resources(logger, interval=60):
    while True:
        try:
            memory = psutil.virtual_memory()
            cpu = psutil.cpu_percent(interval=1)
            logger.debug(f"Memory Usage: {memory.percent}%")
            logger.debug(f"CPU Usage: {cpu}%")
            time.sleep(interval)
        except Exception as e:
            logger.error(f"Error in resource monitoring: {e}")

def main(args):
    logger = setup_logging(args.output_dir, args.log_level)
    logger.info("Starting Checkpoint Evaluator Script")
    logger.debug(f"Arguments: {args}")

    model_dir = os.path.join(args.date_folder, "checkpoints")
    logger.debug(f"Watching for new models in directory: {model_dir}")

    # Create necessary directories
    os.makedirs(model_dir, exist_ok=True)
    temp_checkpoint_dir = os.path.join(args.output_dir, "temp_checkpoints")
    os.makedirs(temp_checkpoint_dir, exist_ok=True)

    # Load previously evaluated models
    evaluated_models_file = os.path.join(args.output_dir, "evaluated_models.txt")
    evaluated_models = load_evaluated_models(evaluated_models_file, logger)

    # Set up the event handler
    handler = CheckpointHandler(evaluated_models, temp_checkpoint_dir, None, logger)

    # Initialize watchdog event handler with the ability to evaluate models
    event_handler = CheckpointHandler(evaluated_models, temp_checkpoint_dir, None, logger)

    # Start the observer in a separate thread
    observer_thread = threading.Thread(target=start_observer, args=(model_dir, event_handler, logger))
    observer_thread.daemon = True  # Ensures the thread will exit when the main program exits
    observer_thread.start()
    logger.info("Background checkpoint observer started.")

    # Start the resource monitor in a background thread
    resource_monitor_thread = threading.Thread(target=monitor_resources, args=(logger, args.resource_monitor_interval))
    resource_monitor_thread.daemon = True
    resource_monitor_thread.start()
    logger.info("Background resource monitor started.")

    # Keep the main thread alive
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        logger.info("Script terminated by user.")
    except Exception as e:
        logger.exception(f"An unexpected error occurred: {e}")

if __name__ == "__main__":
    args = parse_arguments()
    main(args)

</file>
<file name="src/config.py">
# gpt2_arc/src/config.py
from typing import Optional, Dict
from dataclasses import dataclass, asdict, field
import multiprocessing
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)  # Set to DEBUG for detailed logs

@dataclass
class ModelConfig:
    n_embd: int = 256          # Reduced from 768 to 256
    n_head: int = 2            # Increased from 1 to 2
    n_layer: int = 2           # Increased from 12 to 2
    num_classes: int = field(default=11, metadata={"description": "Number of output classes for the model."})
    dropout: float = 0.1
    mamba_ratio: float = 0.0
    d_state: int = 4
    d_conv: int = 1
    mamba_depth: int = 1
    mamba_expand: int = 2

    def __post_init__(self):
        assert self.n_embd % self.n_head == 0, f"n_embd ({self.n_embd}) must be divisible by n_head ({self.n_head})"
        assert self.n_embd &gt;= self.n_head, f"n_embd ({self.n_embd}) must be greater than or equal to n_head ({self.n_head})"
        assert self.n_layer &gt; 0, f"n_layer ({self.n_layer}) must be positive"
        assert self.d_state &gt;= 1, f"d_state ({self.d_state}) must be at least 1"
        assert self.d_conv &gt;= 1, f"d_conv ({self.d_conv}) must be at least 1"
        assert self.mamba_depth &gt;= 1, f"mamba_depth ({self.mamba_depth}) must be at least 1"
        assert self.mamba_expand &gt;= 2, f"mamba_expand ({self.mamba_expand}) must be at least 2"
        logger.debug("ModelConfig initialized successfully")

@dataclass
class TrainingConfig:
    batch_size: int = 16             # Reduced from 32 to 16
    learning_rate: float = 1e-4      # Keep as is or adjust if necessary
    max_epochs: int = 1              # Already set for fast dev run
    num_classes: int = field(default=11, metadata={"description": "Number of output classes for the model."})
    num_symbols: int = 11  # Ensure num_symbols is set to 11
    num_workers: int = 1             # Reduced from multiprocessing.cpu_count() to 1
    symbol_freq: Optional[Dict[int, float]] = None
    pin_memory: bool = False         # Disable if not using GPU
    prefetch_factor: int = 1         # Reduced from 2 to 1
    persistent_workers: bool = False # Ensure workers do not stay alive
    use_gpu: bool = True
    log_level: str = "INFO"
    use_synthetic_data: bool = False
    use_grokfast: bool = False
    grokfast_type: Optional[str] = field(default=None)  # 'ema' or 'ma'
    grokfast_alpha: float = field(default=0.98)
    grokfast_lamb: float = field(default=2.0)
    grokfast_window_size: Optional[int] = field(default=100)  # Only relevant if grokfast_type == 'ma'
    balance_symbols: bool = True
    balancing_method: str = "weighting"
    synthetic_data_path: Optional[str] = None
    include_pad_in_loss: bool = True  # Whether to include the padding class in the loss calculation
    include_pad_in_accuracy: bool = True  # Whether to include the padding class in accuracy calculations
    tensorboard_log_path: Optional[str] = None  # Default to None if not set

    # New fields for padding symbol
    pad_symbol: str = "&lt;PAD&gt;"
    pad_symbol_idx: int = field(default=10)  # Add this line

    def __post_init__(self):
        # Dynamically set num_classes based on symbol_freq
        self.pad_symbol_idx = 10  # Set to the default padding index
        print(f"TrainingConfig initialized with {self.num_classes} classes and PAD symbol index {self.pad_symbol_idx}")
        print(f"include_pad_in_loss: {self.include_pad_in_loss}")  # Added debug statement
        print(f"include_pad_in_accuracy: {self.include_pad_in_accuracy}")  # Added debug statement

@dataclass
class EvaluationConfig:
    perfect_accuracy_threshold: float = 99.9  # Set to 99.9 for near-perfect accuracy

@dataclass
class Config:
    model: ModelConfig = field(default_factory=ModelConfig)
    training: TrainingConfig = field(default_factory=TrainingConfig)
    evaluation: EvaluationConfig = field(default_factory=EvaluationConfig)
    estimated_memory: Optional[float] = None
    available_memory: Optional[float] = None

    def to_dict(self):
        return asdict(self)

</file>
<file name="src/utils/helpers.py">
# gpt2_arc/src/utils/helpers.py
import torch
import logging

logger = logging.getLogger(__name__)

def differential_pixel_accuracy(input, target, prediction, pad_symbol_idx=10):
    logger.debug(f"Differential pixel accuracy - Input shape: {input.shape}, Target shape: {target.shape}, Prediction shape: {prediction.shape}")
    
    assert isinstance(input, torch.Tensor) and isinstance(target, torch.Tensor) and isinstance(prediction, torch.Tensor), "All inputs must be torch.Tensor"
    assert input.numel() == target.numel() == prediction.numel(), "Input, target, and prediction must have the same number of elements"

    """
    Compute differential pixel accuracy, excluding padding tokens.

    Args:
        input (torch.Tensor): Input tensor for the model.
        target (torch.Tensor): Ground truth labels.
        prediction (torch.Tensor): Model predictions.
        pad_symbol_idx (int): Index of the padding token to exclude from calculations.

    Returns:
        tuple: A tuple containing:
            - accuracy (float): Differential pixel accuracy.
            - input_target_diff (torch.Tensor): Tensor indicating where input differs from target.
            - correct_diff_predictions (torch.Tensor): Tensor indicating correct predictions of differing pixels.
    """
    device = input.device
    target = target.to(device)
    prediction = prediction.to(device)
    prediction = prediction.view_as(target)
    
    logger.debug(f"Reshaped - Input: {input.shape}, Target: {target.shape}, Prediction: {prediction.shape}")

    # Exclude padding tokens by creating a valid mask
    valid_mask = target != pad_symbol_idx
    input_target_diff = (input != target) &amp; valid_mask
    correct_diff_predictions = (prediction == target) &amp; input_target_diff

    total_diff_pixels = input_target_diff.sum().item()
    correct_diff_pixels = correct_diff_predictions.sum().item()

    logger.debug(f"Total different pixels: {total_diff_pixels}")
    logger.debug(f"Correctly predicted different pixels: {correct_diff_pixels}")

    if total_diff_pixels &gt; 0:
        accuracy = correct_diff_pixels / total_diff_pixels
    else:
        accuracy = 1.0  # If no pixels differ, consider it 100% accurate

    logger.debug(f"Calculated accuracy: {accuracy}")
    return accuracy, input_target_diff, correct_diff_predictions

</file>
<file name="src/utils/grokfast_callback.py">

from pytorch_lightning.callbacks import Callback
from typing import Optional, Dict
import torch.nn as nn
import logging
from .grokfast import gradfilter_ema, gradfilter_ma

logger = logging.getLogger(__name__)


class GrokfastCallback(Callback):
    def __init__(
        self,
        filter_type: str = 'ema',  # 'ema' or 'ma'
        alpha: float = 0.98,
        lamb: float = 2.0,
        window_size: int = 100,
        warmup: bool = True,
        trigger: bool = False,  # For ablation study.
    ):
        """
        Initializes the Grokfast callback.

        Args:
            filter_type (str): Type of Grokfast filter ('ema' or 'ma').
            alpha (float): Momentum parameter for EMA.
            lamb (float): Amplifying factor.
            window_size (int): Window size for MA.
            warmup (bool): Whether to use warmup for MA.
            trigger (bool): For ablation studies.
        """
        super().__init__()
        self.filter_type = filter_type
        self.alpha = alpha
        self.lamb = lamb
        self.window_size = window_size
        self.warmup = warmup
        self.trigger = trigger
        self.grads = None  # Will hold the state across batches

    def on_after_backward(self, trainer, pl_module):
        """
        Called after the backward pass and before the optimizer step.

        Args:
            trainer: The trainer instance.
            pl_module: The LightningModule instance.
        """
        model = pl_module.model  # Adjust if your model is nested differently

        if self.filter_type == 'ema':
            self.grads = gradfilter_ema(
                m=model,  # Pass the actual model
                grads=self.grads,
                alpha=self.alpha,
                lamb=self.lamb
            )
            logger.debug("Applied Grokfast-EMA filter.")
        elif self.filter_type == 'ma':
            self.grads = gradfilter_ma(
                m=model,  # Pass the actual model
                grads=self.grads,
                window_size=self.window_size,
                lamb=self.lamb,
                filter_type='mean',  # or 'sum' based on preference
                warmup=self.warmup,
                trigger=self.trigger
            )
            logger.debug("Applied Grokfast-MA filter.")
        else:
            logger.warning(f"Unknown Grokfast filter type: {self.filter_type}. Skipping gradient filtering.")

</file>
<file name="src/utils/grokfast.py">
from collections import deque
from typing import Dict, Optional, Literal
import torch
import torch.nn as nn


def gradfilter_ma(
    m: nn.Module,
    grads: Optional[Dict[str, deque]] = None,
    window_size: int = 100,
    lamb: float = 5.0,
    filter_type: Literal['mean', 'sum'] = 'mean',
    warmup: bool = True,
    trigger: bool = False, # For ablation study.
) -&gt; Dict[str, deque]:
    if grads is None:
        grads = {n: deque(maxlen=window_size) for n, p in m.named_parameters() if p.requires_grad and p.grad is not None}

    for n, p in m.named_parameters():
        if p.requires_grad and p.grad is not None:
            grads[n].append(p.grad.data.detach().clone())

            # Modify the gradients.
            if not warmup or len(grads[n]) == window_size and not trigger:
                if filter_type == "mean":
                    avg = sum(grads[n]) / len(grads[n])
                elif filter_type == "sum":
                    avg = sum(grads[n])
                else:
                    raise ValueError(f"Unrecognized filter_type {filter_type}")
                p.grad.data = p.grad.data + avg * lamb

    return grads


def gradfilter_ema(
    m: nn.Module,
    grads: Optional[Dict[str, torch.Tensor]] = None,
    alpha: float = 0.98,
    lamb: float = 2.0,
) -&gt; Dict[str, torch.Tensor]:
    if grads is None:
        grads = {n: p.grad.data.detach().clone() for n, p in m.named_parameters() if p.requires_grad and p.grad is not None}

    for n, p in m.named_parameters():
        if p.requires_grad and p.grad is not None:
            grads[n] = grads[n] * alpha + p.grad.data.detach() * (1 - alpha)
            p.grad.data = p.grad.data + grads[n] * lamb

    return grads

</file>
<file name="src/utils/performance_metrics.py">
import torch
import time

def calculate_mamba_efficiency(model, input_data):
    """
    Calculates performance metrics specific to Mamba layers in the model.

    Args:
        model: The GPT2ARC model instance.
        input_data: A sample input tensor.

    Returns:
        A dictionary containing Mamba-specific performance metrics.
    """
    metrics = {}
    model.eval()  # Set model to evaluation mode

    # Ensure model and input data are on the same device
    device = next(model.parameters()).device
    input_data = input_data.to(device)

    # Measure the forward pass time
    with torch.no_grad():
        start_time = time.time()
        _ = model(input_data)
        total_time = time.time() - start_time

    metrics['mamba_forward_pass_time'] = total_time

    # Count the number of parameters in Mamba layers
    mamba_params = 0
    total_params = 0
    for name, param in model.named_parameters():
        param_count = param.numel()
        total_params += param_count
        if 'mamba_block' in name:
            mamba_params += param_count

    metrics['mamba_params'] = mamba_params
    metrics['total_params'] = total_params
    metrics['mamba_params_ratio'] = mamba_params / total_params if total_params &gt; 0 else 0

    return metrics

</file>
<file name="src/utils/results_collector.py">
# gpt2_arc/src/utils/results_collector.py
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
import logging
from typing import Dict, Any

logger = logging.getLogger(__name__)

class ResultsCollector:
    def __init__(self, config):
        """Initialize the ResultsCollector with a given configuration."""
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = asdict(config)
        self.symbol_freq = self.config['training']['symbol_freq'] if 'symbol_freq' in self.config['training'] else {}
        logger.debug(f"Symbol frequencies set in ResultsCollector: {self.symbol_freq}")
        self.results = {
            "train": {},
            "validation": {},
            "test": {}
        }
        self.metrics = {}
        self.task_specific_results = {}
        self.tensorboard_log_path = getattr(config.training, 'tensorboard_log_path', None)
        self.used_synthetic_data = getattr(config.training, 'use_synthetic_data', False)
        self.environment = self._get_environment_info()
        self.checkpoint_path = None
        print(f"DEBUG: Initialized self.results['train'] as {type(self.results['train'])}")
        self._log_results_type("After initialization")

    def set_tensorboard_log_path(self, path):
        self.tensorboard_log_path = path
        print(f"DEBUG: Set TensorBoard log path in ResultsCollector: {path}")

    def _get_environment_info(self) -&gt; Dict[str, str]:
        """Retrieve environment information such as Python and PyTorch versions."""
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    def _log_results_type(self, context: str):
        """Log the type of self.results['train'] for debugging."""
        logger.debug(f"{context}: self.results['train'] is of type {type(self.results['train'])}")
    
    def update_train_metrics(self, epoch: int, metrics: Dict[str, float]):
        """Update training metrics for a specific epoch."""
        if "train" not in self.results:
            self.results["train"] = {}
        if not isinstance(self.results["train"], dict):
            raise TypeError(f"Expected self.results['train'] to be a dict, but got {type(self.results['train'])}")
        self.results["train"].setdefault(epoch, {})
        self.results["train"][epoch].update(metrics)
        logger.debug(f"Updated train metrics for epoch {epoch}: {metrics}")

    def update_val_metrics(self, epoch: int, metrics: Dict[str, float]):
        """Update validation metrics for a specific epoch."""
        if "validation" not in self.results:
            self.results["validation"] = {}
        if not isinstance(self.results["validation"], dict):
            raise TypeError(f"Expected self.results['validation'] to be a dict, but got {type(self.results['validation'])}")
        self.results["validation"].setdefault(epoch, {})
        self.results["validation"][epoch].update(metrics)
        logger.debug(f"Updated validation metrics for epoch {epoch}: {metrics}")

    def set_test_results(self, metrics: Dict[str, float]):
        """Set the test results metrics."""
        self.results["test"] = metrics
        
    def add_task_specific_result(self, task_id: str, metrics: Dict[str, float]):
        """Add task-specific results for a given task ID."""
        if task_id == "default_task":
            logger.error("Attempted to add metrics for 'default_task'. This should be avoided.")
            raise ValueError("Cannot add metrics for 'default_task'. Ensure that task_id is correctly assigned.")
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        for key, value in metrics.items():
            if key not in self.task_specific_results[task_id]:
                self.task_specific_results[task_id][key] = []
            self.task_specific_results[task_id][key].append(value)

    def get_task_specific_results(self) -&gt; Dict[str, Dict[str, float]]:
        """Retrieve aggregated task-specific metrics."""
        aggregated = {}
        for task_id, metrics in self.task_specific_results.items():
            aggregated[task_id] = {}
            for metric_name, values in metrics.items():
                aggregated[task_id][metric_name] = sum(values) / len(values) if values else 0.0
        return aggregated
    
    def set_final_metrics(self, metrics: Dict[str, float]):                                                                                          
        """Set the final metrics after training."""
        self.metrics = metrics

    def set_checkpoint_path(self, path: str):
        """Set the path to the model checkpoint."""
        self.checkpoint_path = path

    def save_to_json(self, filepath: str):
        """Save the collected results to a JSON file."""
        try:
            self._ensure_directory_exists(os.path.dirname(filepath))
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path,
                "used_synthetic_data": self.used_synthetic_data
            }
            if self.symbol_freq:
                data["symbol_freq"] = self.symbol_freq
            else:
                data["symbol_freq"] = {}
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    def _ensure_directory_exists(self, directory: str):
        """Ensure that the directory exists; create it if it does not."""
        if not os.path.exists(directory):
            os.makedirs(directory)

    def get_summary(self) -&gt; Dict[str, Any]:
        """
        Get a summary of the results.
        
        Returns:
            Dict[str, Any]: Summary of key metrics.
        """
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_loss": self.results["test"].get("avg_loss"),
            "test_acc_with_pad": self.results["test"].get("avg_acc_with_pad"),
            "test_acc_without_pad": self.results["test"].get("avg_acc_without_pad"),
            "best_val_loss": self.results.get("best_val_loss"),
            "best_val_epoch": self.results.get("best_val_epoch"),
            "learning_rate": self.config['training']['learning_rate'],
            "batch_size": self.config['training']['batch_size'],
            "training_duration": self.results.get("training_duration"),
            "config": self._serialize_config(self.config),
            "tensorboard_log_path": self.tensorboard_log_path
        }
        logger.debug(f"DEBUG: Added TensorBoard log path to results: {summary['tensorboard_log_path']}")
        return {k: self._make_serializable(v) for k, v in summary.items()}

    def _make_serializable(self, obj):
        """Ensure the value is serializable, handling non-serializable objects."""
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

</file>
<file name="src/utils/experiment_tracker.py">
# gpt2_arc/src/utils/experiment_tracker.py
import logging
import wandb
import json
import time
import uuid
import torch
import platform
import os
from dataclasses import asdict
from typing import Dict, Any, Optional

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

class ExperimentTracker:
    def __init__(self, config: Dict[str, Any], project: str, entity: Optional[str] = None, use_wandb: bool = False):
        self.experiment_id = str(uuid.uuid4())
        self.timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
        self.config = config.to_dict() if hasattr(config, 'to_dict') else self._config_to_dict(config)
        self.project = project
        self.entity = entity
        self.run = None
        self.use_wandb = use_wandb
        self.metrics = {}
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False

        self.results = {
            "train": [],
            "validation": [],
            "test": {}
        }
        self.metrics = {}
        self.task_specific_results = {}
        self.environment = self._get_environment_info()
        self.checkpoint_path = None

        # Add debug logging
        logger.debug(f"ExperimentTracker initialized with config: {json.dumps(self.config, indent=2)}")
        logger.debug(f"Project: {project}, Entity: {entity}")
        logger.debug(f"use_wandb: {self.use_wandb}")

    def _get_environment_info(self) -&gt; Dict[str, str]:
        return {
            "python_version": platform.python_version(),
            "torch_version": torch.__version__,
            "gpu_info": torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        }

    def _config_to_dict(self, config):
        if isinstance(config, dict):
            return {k: self._config_to_dict(v) for k, v in config.items()}
        elif hasattr(config, '__dict__'):
            return {k: self._config_to_dict(v) for k, v in config.__dict__.items() if not k.startswith('_')}
        else:
            return config
        if self.use_wandb:
            try:
                self.run = wandb.init(project=self.project, entity=self.entity, config=self.config)
                print(f"Wandb run initialized: {self.run.id}")
            except Exception as e:
                print(f"Error initializing wandb: {str(e)}")
                self.use_wandb = False
        
        if not self.use_wandb:
            print("Using local logging only")

    def finish(self):
        if self.use_wandb and self.run:
            try:
                wandb.finish()
                print("Wandb run finished")
            except Exception as e:
                print(f"Error finishing wandb run: {str(e)}")
        else:
            print("Experiment finished. Metrics:", self.metrics)

    def log_metric(self, name: str, value: float, step: Optional[int] = None):
        if self.use_wandb:
            try:
                wandb.log({name: value}, step=step)
                print(f"Logged metric to wandb: {name}={value}, step={step}")
            except Exception as e:
                print(f"Error logging metric to wandb: {str(e)}")
        
        # Always log locally as a fallback
        print(f"Logged metric locally: {name}={value}, step={step}")

    def update_train_metrics(self, epoch: int, metrics: Dict[str, float]):
        if "train" not in self.results:
            self.results["train"] = []
        while len(self.results["train"]) &lt;= epoch:
            self.results["train"].append({})
        self.results["train"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"train": metrics}, step=epoch)

    def update_val_metrics(self, epoch: int, metrics: Dict[str, float]):
        if "validation" not in self.results:
            self.results["validation"] = []
        while len(self.results["validation"]) &lt;= epoch:
            self.results["validation"].append({})
        self.results["validation"][epoch] = metrics
        if self.use_wandb:
            wandb.log({"validation": metrics}, step=epoch)

    def set_test_results(self, metrics: Dict[str, float]):
        self.results["test"] = metrics
        if self.use_wandb:
            wandb.log({"test": metrics})

    def add_task_specific_result(self, task_id: str, metrics: Dict[str, float]):
        if task_id not in self.task_specific_results:
            self.task_specific_results[task_id] = {}
        self.task_specific_results[task_id].update(metrics)
        logger.debug(f"Added task-specific result for task_id {task_id}: {metrics}")
        if self.use_wandb:
            wandb.log({f"task_{task_id}": metrics})

    def set_final_metrics(self, metrics: Dict[str, float]):
        self.metrics = metrics
        if self.use_wandb:
            wandb.log(metrics)

    def set_checkpoint_path(self, path: str):
        self.checkpoint_path = path
        if self.use_wandb:
            wandb.save(path)

    def save_to_json(self, filepath: str):
        try:
            directory = os.path.dirname(filepath)
            if directory and not os.path.exists(directory):
                os.makedirs(directory)
            data = {
                "experiment_id": self.experiment_id,
                "timestamp": self.timestamp,
                "config": self.config,
                "results": self.results,
                "metrics": self.metrics,
                "task_specific_results": self.task_specific_results,
                "environment": self.environment,
                "checkpoint_path": self.checkpoint_path
            }
            with open(filepath, 'w') as f:
                json.dump(data, f, indent=2)
            print(f"Results saved to {filepath}")
        except IOError as e:
            print(f"Error saving results to {filepath}: {e}")

    def _ensure_directory_exists(self, directory: str):
        if not os.path.exists(directory):
            os.makedirs(directory)

    def get_summary(self) -&gt; Dict[str, Any]:
        summary = {
            "experiment_id": self.experiment_id,
            "timestamp": self.timestamp,
            "final_train_loss": self.results["train"][-1]["loss"] if self.results["train"] else None,
            "final_val_loss": self.results["validation"][-1]["loss"] if self.results["validation"] else None,
            "test_loss": self.results["test"].get("avg_loss"),
            "test_acc_with_pad": self.results["test"].get("avg_acc_with_pad"),
            "test_acc_without_pad": self.results["test"].get("avg_acc_without_pad"),
            "best_val_loss": self.results.get("best_val_loss"),
            "best_val_epoch": self.results.get("best_val_epoch"),
            "learning_rate": self.config.get("training", {}).get("learning_rate"),
            "batch_size": self.config.get("training", {}).get("batch_size"),
            "training_duration": self.results.get("training_duration"),
            "config": self._serialize_config(self.config),
            "tensorboard_log_path": self.tensorboard_log_path
        }
        self.logger.debug(f"DEBUG: Added TensorBoard log path to results: {summary['tensorboard_log_path']}")
        return {k: self._make_serializable(v) for k, v in summary.items()}

    def _make_serializable(self, obj):
        if isinstance(obj, (int, float, str, bool, type(None))):
            return obj
        elif isinstance(obj, (list, tuple)):
            return [self._make_serializable(item) for item in obj]
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        else:
            return str(obj)

    def _serialize_config(self, config):
        return {k: self._make_serializable(v) for k, v in config.items()}

    def log_metric(self, name: str, value: float, step: Optional[int] = None):
        if name.startswith("default_task_"):
            logger.error("Attempted to log metric under 'default_task'. This should be avoided.")
            raise ValueError("Cannot log metrics under 'default_task'. Ensure that task_id is correctly assigned.")

# Add a simple test
if __name__ == "__main__":
    config = {"learning_rate": 0.01, "batch_size": 32, "use_wandb": True}
    tracker = ExperimentTracker(config, project="test-project")
    tracker.start()
    tracker.log_metric("accuracy", 0.85, step=1)
    tracker.update_train_metrics(0, {"loss": 0.5, "accuracy": 0.8})
    tracker.update_val_metrics(0, {"loss": 0.6, "accuracy": 0.75})
    tracker.set_test_results({"loss": 0.55, "accuracy": 0.82})
    tracker.add_task_specific_result("task_1", {"accuracy": 0.9})
    tracker.set_final_metrics({"best_accuracy": 0.85})
    tracker.set_checkpoint_path("model_checkpoint.pth")
    tracker.save_to_json("results.json")
    tracker.finish()

</file>
<file name="src/utils/__init__.py">
from .grokfast import (
    gradfilter_ma,
    gradfilter_ema
)
from .grokfast_callback import GrokfastCallback

</file>
<file name="src/utils/training_helpers.py">
from typing import Optional
from gpt2_arc.src.config import TrainingConfig

def get_num_workers(config: TrainingConfig) -&gt; int:
    """Determine the number of DataLoader workers based on the configuration."""
    return config.num_workers

</file>
<file name="src/utils/model_memory_estimator.py">
# gpt2_arc/src/utils/model_memory_estimator.py
import torch
import math
import psutil
import logging

logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

def calculate_params(n_layers, n_heads, d_model, mamba_ratio, d_state=16, d_conv=4, mamba_depth=1, mamba_expand=2):
    logger.debug(f"Executing calculate_params with mamba_ratio = {mamba_ratio}")
    transformer_params_per_layer = (
        12 * d_model * d_model + 13 * d_model
    )
    
    # Calculate the number of Mamba layers
    total_mamba_layers = int(n_layers * mamba_ratio)
    
    # Calculate parameters for Mamba layers
    # Assuming MambaBlock has parameters based on d_state, d_conv, depth, and expand
    mamba_params_per_layer = (
        d_state * d_conv * mamba_expand * mamba_depth  # Example calculation
    )
    total_mamba_params = total_mamba_layers * mamba_params_per_layer
    
    # Total parameters
    total_params = n_layers * transformer_params_per_layer + total_mamba_params
    logger.debug(f"Total parameters calculated: {total_params}")
    return total_params

def estimate_memory_usage(total_params, batch_size, height, width, d_model, dtype_size=4):
    model_memory = total_params * dtype_size  # Model parameters
    optimizer_memory = model_memory * 2  # Adam optimizer uses 2x model size
    input_memory = batch_size * height * width * dtype_size  # Input tensors
    conv_output_memory = batch_size * height * width * d_model * dtype_size  # After conv layer
    activations_memory = batch_size * (height * width) * d_model * dtype_size * 2  # Forward &amp; backward pass
    total_memory = model_memory + optimizer_memory + input_memory + conv_output_memory + activations_memory
    return total_memory / (1024**3)  # Convert to GB

def get_available_memory():
    if torch.cuda.is_available():
        return torch.cuda.get_device_properties(0).total_memory / (1024**3)  # Convert to GB
    else:
        return psutil.virtual_memory().total / (1024**3)  # Get actual system memory for CPU

def get_device_info():
    if torch.cuda.is_available():
        return {
            "device": "GPU",
            "name": torch.cuda.get_device_name(0),
            "compute_capability": torch.cuda.get_device_capability(0),
            "total_memory": torch.cuda.get_device_properties(0).total_memory / (1024**3),
            "cuda_version": torch.version.cuda
        }
    else:
        return {
            "device": "CPU",
            "name": "System CPU",
            "total_memory": psutil.virtual_memory().total / (1024**3),
            "cpu_count": psutil.cpu_count(logical=False),
            "cpu_freq": psutil.cpu_freq().max if psutil.cpu_freq() else "N/A"
        }

def can_fit_model(estimated_memory, available_memory, threshold=0.9):
    return estimated_memory &lt; available_memory * threshold

def estimate_single_configuration(n_layers, n_heads, d_model, batch_size, height, width):
    device_info = get_device_info()
    available_memory = get_available_memory()
    
    print(f"Device Information:")
    for key, value in device_info.items():
        print(f"  {key}: {value}")
    print(f"Available memory: {available_memory:.2f} GB")

    total_params = calculate_params(n_layers, n_heads, d_model)
    estimated_memory = estimate_memory_usage(total_params, batch_size, height, width, d_model)
    
    print(f"\nConfiguration:")
    print(f"  n_layers: {n_layers}")
    print(f"  n_heads: {n_heads}")
    print(f"  d_model: {d_model}")
    print(f"  batch_size: {batch_size}")
    print(f"  input_height: {height}")
    print(f"  input_width: {width}")
    print(f"Total parameters: {total_params:,}")
    print(f"Estimated memory usage: {estimated_memory:.2f} GB")
    
    if can_fit_model(estimated_memory, available_memory):
        print(f"Model should fit in {device_info['device']} memory.")
    else:
        print(f"Warning: Model may be too large for available {device_info['device']} memory!")
    
    print(f"Memory utilization: {(estimated_memory / available_memory) * 100:.2f}%")

</file>
<file name="src/data/__init__.py">

</file>
<file name="src/data/arc_dataset.py">
# gp2_arc/src/data/arc_dataset.py
import os
import json
import random
from typing import Union, List, Dict, Tuple, Any, Optional
from typing import List, Dict, Union, Optional
import logging

logger = logging.getLogger(__name__)
import numpy as np
import pickle
import hashlib
import torch
import torch.nn.functional as F
from torch.utils.data import Dataset, Subset, WeightedRandomSampler
import logging
import ijson  # Import ijson for streaming JSON parsing
import cysimdjson  # New import added
from tqdm import tqdm  # Import tqdm for progress bars
import sys  # Import sys for handling tqdm output
from concurrent.futures import ThreadPoolExecutor, as_completed  # For parallel processing
from concurrent.futures import ThreadPoolExecutor, as_completed  # For parallel processing
import multiprocessing  # To determine CPU count
from threading import Lock
from jsonschema import validate, ValidationError
from torch.utils.data import get_worker_info

try:
    from arckit.data import TaskSet, Task
except ImportError:
    TaskSet = None

logger = logging.getLogger(__name__)

# Create a handler that writes to stderr
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)

# Add the handler to the logger
logger.addHandler(handler)

# Determine debug mode
DEBUG_MODE = os.getenv("DEBUG_MODE", "False") == "True"

# Set logging levels based on DEBUG_MODE
if DEBUG_MODE:
    logger.setLevel(logging.DEBUG)
    handler.setLevel(logging.DEBUG)
else:
    logger.setLevel(logging.INFO)
    handler.setLevel(logging.INFO)

# Define JSON Schema for task validation
TASK_SCHEMA = {
    "type": "object",
    "properties": {
        "id": {"type": "string"},
        "input": {
            "type": "array",
            "items": {"type": "number"}
        },
        "output": {
            "type": "array",
            "items": {"type": "number"}
        }
    },
    "required": ["input", "output"],
    "additionalProperties": False
}
def set_debug_mode(debug=False):
    if debug:
        logger.setLevel(logging.DEBUG)
        handler.setLevel(logging.DEBUG)
    else:
        logger.setLevel(logging.ERROR)
        handler.setLevel(logging.ERROR)

class ARCDataset(Dataset):
    def __init__(
        self,
        data_source: Union[str, List[Dict], 'TaskSet', Tuple[Union[List, 'TaskSet'], str]],
        is_test: bool = False,
        max_samples: Optional[int] = None,  # Add this parameter
        num_symbols: int = 11,
        test_split: float = 0.2,
        pad_symbol_idx: int = 10,
        symbol_freq: Optional[Dict[int, float]] = None,
        debug: bool = False,
    ):
        self.is_test = is_test
        self.num_symbols = num_symbols
        self.test_split = test_split
        self.pad_symbol_idx = pad_symbol_idx
        self.symbol_freq = symbol_freq if symbol_freq is not None else {}

        logger.debug(f"Initialized ARCDataset with pad_symbol_idx: {self.pad_symbol_idx}")
        if self.symbol_freq:
            logger.debug("Symbol frequencies provided; initializing WeightedRandomSampler.")
        else:
            logger.debug("No symbol frequencies provided.")
        self.data_files = []
        self.data_source = data_source
        self.num_samples = 0

        # Initialize cysimdjson JSONParser
        self.json_parser = cysimdjson.JSONParser()
        logger.debug("Initialized cysimdjson.JSONParser instance")
        
        self.cache_path = self._generate_cache_path(
            data_source=self.data_source,
            num_symbols=self.num_symbols,
            is_test=self.is_test,
            test_split=self.test_split
        )
        
        if not self._load_cache(self.cache_path):
            # Load data only if not loaded from cache
            try:
                logger.debug("Loading data from data source as cache was not found or failed.")
                self.data = self._load_data(data_source)

                if not self.data:
                    logger.error("No valid samples loaded. Ensure that all samples have 'input' and 'output' keys.")
                    raise ValueError("No valid samples loaded. Ensure that all samples have 'input' and 'output' keys.")
                
                logger.debug(f"Data loaded successfully from data source with {len(self.data)} samples.")
            except Exception as e:
                logger.error(f"Failed to load data: {e}", exc_info=True)
                raise

            # Apply sample limit if specified
            if max_samples is not None:
                self.data = self.data[:max_samples]
                logger.debug(f"Limited dataset to {max_samples} samples.")
            self.num_samples = len(self.data)
            self._compute_and_cache_statistics()
            self._save_cache(self.cache_path)

        if self.symbol_freq:
            # Calculate weights for each sample based on symbol frequencies
            weights = []
            for sample in self.data:
                input_freq = torch.tensor([self.symbol_freq.get(symbol.item(), 0.0) for symbol in sample["input"].flatten()])
                output_freq = torch.tensor([self.symbol_freq.get(symbol.item(), 0.0) for symbol in sample["output"].flatten()])
                sample_freq = torch.cat((input_freq, output_freq)).mean()
                weights.append(1.0 / (sample_freq + 1e-8))  # Add epsilon to avoid division by zero

            self.sample_weights = torch.tensor(weights, dtype=torch.float)
            self.sampler = WeightedRandomSampler(self.sample_weights, num_samples=len(self.sample_weights), replacement=True)
            logger.debug("WeightedRandomSampler initialized based on symbol frequencies.")
        else:
            self.sampler = None
            logger.debug("No symbol frequencies provided; sampler not initialized.")

        if debug:
            logger.setLevel(logging.DEBUG)
            handler.setLevel(logging.DEBUG)
            logger.debug("Debug mode is enabled for ARCDataset.")
        else:
            logger.setLevel(logging.INFO)
            handler.setLevel(logging.INFO)

        logger.debug("ARCDataset initialization completed.")

    def _process_single_task(self, task: Union[Dict, List], task_id: str) -&gt; List[Dict]:
        """
        Processes a single task dictionary or list and returns a list of samples.
        
        Args:
            task (Union[Dict, List]): The task data containing 'input' and 'output', or a list of such dictionaries.
            task_id (str): Identifier for the task.
        
        Returns:
            List[Dict]: List of processed sample dictionaries.
        """
        samples = []
        try:
            if isinstance(task, dict):
                # Existing processing for dictionary tasks
                for ex in task.get('train', []):
                    logger.debug(f"Processing training example keys: {ex.keys()}")
                    logger.debug(f"Processing example keys: {ex.keys()}")
                    input_tensor = self._preprocess_grid(ex['input'])
                    output_tensor = self._preprocess_grid(ex['output'])
                    samples.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task_id
                    })
                
                for ex in task.get('test', []):
                    input_tensor = self._preprocess_grid(ex['input'])
                    output_tensor = self._preprocess_grid(ex['output'])
                    samples.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task_id
                    })
            elif isinstance(task, list):
                # New processing for list-type tasks
                for ex in task:
                    input_tensor = self._preprocess_grid(ex['input'])
                    output_tensor = self._preprocess_grid(ex['output'])
                    samples.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task_id
                    })
            else:
                raise ValueError(f"Unsupported task type: {type(task)}")
        except Exception as e:
            logger.error(f"Error processing task {task_id}: {e}", exc_info=True)
        return samples
    
    
    def _process_single_file_streaming(self, file_path: str) -&gt; List[Dict]:
        """
        Processes a single JSON file using streaming parsing and returns the list of samples.
        
        Args:
            file_path (str): Path to the JSON file.
        
        Returns:
            List[Dict]: List of processed samples from the file.
        """
        samples = []
        samples = []
        sample_count = 0  # Initialize sample counter
        missing_id_logged = False  # Flag to track if warning has been logged for this file

        # Skip empty files early
        if os.path.getsize(file_path) == 0:
            logger.warning(f"Empty JSON file detected: {file_path}. Skipping.")
            return samples

        try:
            with open(file_path, 'rb') as f:  # Open in binary mode
                # Use cysimdjson for efficient parsing
                parsed_json = self.json_parser.parse(f.read())
                # Convert parsed_json to native Python structures
                parsed_py = self._cysimdjson_to_native(parsed_json)
                
                if isinstance(parsed_py, list):
                    for ex in parsed_py:
                        if 'input' in ex and 'output' in ex:
                            try:
                                input_tensor = self._preprocess_grid(ex['input'])
                                output_tensor = self._preprocess_grid(ex['output'])
                                task_id = ex.get('id', f"default_task_{sample_count}")
                                if not isinstance(task_id, str) or not task_id:
                                    if not missing_id_logged:
                                        task_id = f"default_task_{sample_count}"
                                        logger.warning(f"Sample missing valid 'id'. Assigned task_id: {task_id}")
                                        missing_id_logged = True
                                    else:
                                        task_id = f"default_task_{sample_count}"
                                samples.append({
                                    "input": input_tensor,
                                    "output": output_tensor,
                                    "task_id": task_id
                                })
                                sample_count += 1
                            except Exception as e:
                                logger.error(
                                    f"Error preprocessing sample {sample_count} (Task ID: {task_id}) in file {file_path}: {e}",
                                    exc_info=True
                                )
                        else:
                            logger.warning(f"Sample missing 'input' or 'output' keys in file {file_path}. Skipping.")
                elif isinstance(parsed_py, dict):
                    # Existing processing for dictionary tasks
                    for ex in parsed_py.get('train', []):
                        try:
                            logger.debug(f"Processing training example keys: {ex.keys()}")
                            input_tensor = self._preprocess_grid(ex['input'])
                            output_tensor = self._preprocess_grid(ex['output'])
                            task_id = parsed_py.get('id', f"default_task_{sample_count}")
                            if not isinstance(task_id, str) or not task_id:
                                if not missing_id_logged:
                                    task_id = f"default_task_{sample_count}"
                                    logger.warning(f"Task missing valid 'id'. Assigned task_id: {task_id}")
                                    missing_id_logged = True
                                else:
                                    task_id = f"default_task_{sample_count}"
                            samples.append({
                                "input": input_tensor,
                                "output": output_tensor,
                                "task_id": task_id
                            })
                            sample_count += 1
                        except Exception as e:
                            logger.error(
                                f"Error preprocessing training sample {sample_count} (Task ID: {task_id}) in file {file_path}: {e}",
                                exc_info=True
                            )
                    
                    for ex in parsed_py.get('test', []):
                        try:
                            input_tensor = self._preprocess_grid(ex['input'])
                            output_tensor = self._preprocess_grid(ex['output'])
                            task_id = parsed_py.get('id', f"default_task_{sample_count}")
                            if not isinstance(task_id, str) or not task_id:
                                if not missing_id_logged:
                                    task_id = f"default_task_{sample_count}"
                                    logger.warning(f"Task missing valid 'id'. Assigned task_id: {task_id}")
                                    missing_id_logged = True
                                else:
                                    task_id = f"default_task_{sample_count}"
                            samples.append({
                                "input": input_tensor,
                                "output": output_tensor,
                                "task_id": task_id
                            })
                            sample_count += 1
                        except Exception as e:
                            logger.error(
                                f"Error preprocessing testing sample {sample_count} (Task ID: {task_id}) in file {file_path}: {e}",
                                exc_info=True
                            )
                else:
                    logger.warning(f"Unexpected JSON structure in file {file_path}. Skipping.")
            logger.info(f"Finished processing synthetic data file: {file_path}. Extracted {len(samples)} samples.")
        except Exception as e:  # Catch all exceptions related to parsing
            logger.error(f"cysimdjson failed to parse file {file_path}: {e}. Skipping.")

        return samples

    def _cysimdjson_to_native(self, parsed_json):
        """
        Recursively converts cysimdjson parsed objects to native Python lists and dicts.
        
        Args:
            parsed_json (cysimdjson.cysimdjson.JSONValue): Parsed JSON object.
        
        Returns:
            Union[dict, list, primitive]: Native Python data structure.
        """
        if isinstance(parsed_json, cysimdjson.JSONObject):
            return {k: self._cysimdjson_to_native(v) for k, v in parsed_json.items()}
        elif isinstance(parsed_json, cysimdjson.JSONArray):
            return [self._cysimdjson_to_native(item) for item in parsed_json]
        elif isinstance(parsed_json, (int, float, str, bool)):
            return parsed_json
        elif parsed_json is None:
            return None
        else:
            logger.warning(f"Unknown JSON type encountered: {type(parsed_json)}")
            return None
        """
        Wrapper method to process a single file in parallel.
        
        Args:
            file_path (str): Path to the JSON file.
            
        Returns:
            List[Dict]: List of processed samples from the file.
        """
        """
        Wrapper method to process a single file in parallel.
        
        Args:
            file_path (str): Path to the JSON file.
            
        Returns:
            List[Dict]: List of processed samples from the file.
        """
        return self._process_single_file_streaming
    
    
    def _save_cache(self, cache_path: str, data_only=False):
        """
        Saves the dataset and its statistics to the specified cache path using pickle.
    
        Args:
            cache_path (str): The file path where the cache will be saved.
            data_only (bool): If True, only save the data without statistics.
        """
        logger.debug(f"Attempting to save {'data only ' if data_only else ''}cache to {cache_path}")
        try:
            if data_only:
                cache_data = {
                    "data": self.data
                }
            else:
                cache_data = {
                    "data": self.data,
                    "statistics": self.statistics
                }
            with open(cache_path, 'wb') as f:
                pickle.dump(cache_data, f)
            logger.info(f"Successfully saved {'data only ' if data_only else ''}cache to {cache_path}")
        except Exception as e:
            logger.error(f"Failed to save cache to {cache_path}: {e}", exc_info=True)

        # Add data validation
        self._validate_data()

    def _validate_data(self):
        """
        Validates the dataset to ensure each sample contains the required keys and correct data types.
        Raises:
            ValueError: If any sample is missing required keys or has incorrect types.
        """
        required_keys = {"input", "output", "task_id"}
        for idx, sample in enumerate(self.data):
            # Check for required keys
            if not required_keys.issubset(sample.keys()):
                missing = required_keys - sample.keys()
                raise KeyError(f"Sample at index {idx} is missing keys: {missing}")
            
            # Validate 'input' and 'output' types
            for key in ["input", "output"]:
                if not isinstance(sample[key], torch.Tensor):
                    raise TypeError(f"Sample at index {idx} has '{key}' of type {type(sample[key])}, expected torch.Tensor.")
                
                if sample[key].ndimension() != 3 or sample[key].shape[0] != 1:
                    raise ValueError(f"Sample at index {idx} has '{key}' with shape {sample[key].shape}, expected shape (1, H, W).")
                
                # Validate that symbols are within the allowed range based on num_symbols
                max_symbol_allowed = self.num_symbols - 1
                max_symbol = sample[key].max()
                if max_symbol &gt; max_symbol_allowed:
                    logger.error(
                        f"Sample at index {idx} has symbol {max_symbol.item()} exceeding the allowed maximum ({max_symbol_allowed})."
                    )
                    raise ValueError(
                        f"Sample at index {idx} has symbol {max_symbol.item()} exceeding the allowed maximum ({max_symbol_allowed})."
                    )
            
            # Validate 'task_id' type
            if not isinstance(sample["task_id"], str):
                raise TypeError(f"Sample at index {idx} has 'task_id' of type {type(sample['task_id'])}, expected str.")
        
        logger.debug("All samples passed validation.")
    
    def __len__(self):
        return len(self.data)

    def get_num_samples(self):
        return self.num_samples
    
    
    def __getitem__(self, idx):
        sample = self.data[idx]
        task_id = sample["task_id"]
        assert task_id != "default_task", f"Sample at index {idx} has 'default_task' as task_id."
        input_tensor = sample["input"]  # Already padded
        output_tensor = sample["output"]  # Already padded
        if idx &lt; 5:  # Log only the first 5 samples to avoid clutter
            logger.debug(f"Sample {idx} - Task ID: {task_id}")
        return input_tensor, output_tensor, task_id


    @staticmethod
    def _generate_cache_path(data_source: Union[str, List[Dict], 'TaskSet', Tuple[Union[List, 'TaskSet'], str]], num_symbols: int, is_test: bool, test_split: float) -&gt; str:
        dataset_version = "v1"
        
        # Create a stable representation of data_source based on its type
        if isinstance(data_source, str):
            data_source_str = os.path.abspath(data_source)  # Use absolute path for consistency
        elif isinstance(data_source, TaskSet):
            data_source_str = f"TaskSet:{len(data_source.tasks)}"  # Use number of tasks as identifier
        elif isinstance(data_source, list):
            data_source_str = f"List:{len(data_source)}"  # Use length of the list
        else:
            data_source_str = str(data_source)  # Fallback to string representation
        
        # Create a JSON string with stable identifiers
        hash_input = json.dumps({
            'version': dataset_version,
            'data_source': data_source_str,
            'num_symbols': num_symbols,
            'is_test': is_test,
            'test_split': test_split
        }, sort_keys=True).encode('utf-8')
        
        # Generate MD5 hash for the cache filename
        hash_digest = hashlib.md5(hash_input).hexdigest()
        cache_filename = f"arc_dataset_cache_{hash_digest}.pkl"
        
        # Define the cache directory relative to the current file
        cache_dir = os.path.join(os.path.dirname(__file__), 'cache')
        os.makedirs(cache_dir, exist_ok=True)
        
        return os.path.join(cache_dir, cache_filename)

    def _load_cache(self, cache_path: str) -&gt; bool:
        logger.debug(f"Attempting to load cache from: {cache_path}")
        if os.path.exists(cache_path):
            try:
                with open(cache_path, 'rb') as f:
                    cache_data = pickle.load(f)
                self.data = cache_data.get("data", [])
                self.statistics = cache_data.get("statistics", {})
                self.num_samples = len(self.data)
                logger.info(f"Successfully loaded cache from {cache_path} with {self.num_samples} samples.")
                return True
            except Exception as e:
                logger.error(f"Failed to load cache from {cache_path}: {e}", exc_info=True)
        else:
            logger.warning(f"Cache file does not exist at: {cache_path}. Proceeding without cache.")
        return False

    def _compute_and_cache_statistics(self):
        """
        Computes dataset statistics and caches them alongside the dataset cache.
        """
        logger.info("Starting computation of dataset statistics.")
        try:
            grid_size_stats = self._compute_grid_size_stats()
            logger.debug(f"Computed grid size statistics: {grid_size_stats}")
            
            symbol_frequencies = self._compute_symbol_frequencies()
            logger.debug(f"Computed symbol frequencies: {symbol_frequencies}")
            
            statistics = {
                "grid_size_stats": grid_size_stats,
                "symbol_frequencies": symbol_frequencies
            }
            
            # Update the cache dictionary with statistics
            self.statistics = statistics
            logger.info("Completed computation of dataset statistics.")
            
            self._save_cache(self.cache_path)
            logger.info("Dataset statistics have been cached successfully.")
            
            if self.symbol_freq:
                logger.debug(f"Sampling weights - min: {self.sample_weights.min().item()}, "
                             f"max: {self.sample_weights.max().item()}, "
                             f"mean: {self.sample_weights.mean().item()}")
        except Exception as e:
            logger.error(f"Failed to compute and cache dataset statistics: {e}", exc_info=True)
            raise


    def _process_list_data(self, data_list: List[Dict], task_id: Optional[str] = None) -&gt; List[Dict]:
        processed_data = []
        logger.debug(f"Processing list data with {len(data_list)} items")
        for idx, example in enumerate(data_list):
            if 'input' in example and 'output' in example:
                input_grid = self._preprocess_grid(example['input'])
                output_grid = self._preprocess_grid(example['output'])
                
                task_id_sample = task_id if task_id else example.get('task_id')
                if not task_id_sample or task_id_sample == "default_task":
                    task_id_sample = f"default_task_{idx}"
                    logger.warning(f"Sample at index {idx} has invalid 'task_id'. Assigning new task_id: {task_id_sample}")
                
                processed_data.append({
                    "input": input_grid,
                    "output": output_grid,
                    "task_id": task_id_sample
                })
            else:
                logger.warning(f"Example at index {idx} missing 'input' or 'output' keys. Skipping.")
        logger.debug(f"Processed {len(processed_data)} samples")
        return processed_data


    def _combine_data(self, official_data, synthetic_data_path):
        official_processed = self._process_arckit_data(official_data) if TaskSet is not None and isinstance(official_data, TaskSet) else official_data
        synthetic_processed = self._process_synthetic_data(synthetic_data_path)
        return official_processed + synthetic_processed

    def _process_synthetic_data(self, directory: str):
        self.data_files = []
        for filename in os.listdir(directory):
            if filename.endswith('.json'):
                file_path = os.path.join(directory, filename)
                self.data_files.append(file_path)
                logger.debug(f"Processing file: {file_path}")
                with open(file_path, 'r') as f:
                    try:
                        task_data = json.load(f)
                        # Assign task_id from filename
                        task_id = os.path.splitext(filename)[0]
                        processed_samples = self._process_single_task(task_data, task_id=task_id)
                        self.data.extend(processed_samples)
                    except json.JSONDecodeError as e:
                        logger.error(f"Error decoding JSON from file {file_path}: {e}")

    def _process_arckit_data(self, taskset: 'TaskSet') -&gt; List[Dict]:
        """
        Processes data from an arckit TaskSet and returns a list of samples.
        
        Args:
            taskset (TaskSet): The TaskSet object containing tasks.
            
        Returns:
            List[Dict]: List of processed sample dictionaries.
        """
        processed_data = []
        logger.debug(f"Processing TaskSet with {len(taskset.tasks)} tasks")
        for task in taskset.tasks:
            logger.debug(f"Processing task: {task.id}")
            logger.debug(f"Train samples: {len(task.train)}, Test samples: {len(task.test)}")
            # Process training samples
            for ex in task.train:
                try:
                    input_tensor = self._preprocess_grid(ex[0])
                    output_tensor = self._preprocess_grid(ex[1])
                    processed_data.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task.id
                    })
                except Exception as e:
                    logger.error(f"Error processing training example in task {task.id}: {e}", exc_info=True)
            
            # Process testing samples
            for ex in task.test:
                try:
                    input_tensor = self._preprocess_grid(ex[0])
                    output_tensor = self._preprocess_grid(ex[1])
                    processed_data.append({
                        "input": input_tensor,
                        "output": output_tensor,
                        "task_id": task.id
                    })
                except Exception as e:
                    logger.error(f"Error processing testing example in task {task.id}: {e}", exc_info=True)
            
            logger.debug(f"Processed task {task.id}: Total samples added: {len(task.train) + len(task.test)}")
        
        logger.debug(f"Total samples processed from TaskSet: {len(processed_data)}")
        return processed_data


    def get_grid_size_stats(self) -&gt; Dict[str, Any]:
        """
        Returns the precomputed grid size statistics.
        
        Returns:
            Dict[str, Any]: A dictionary containing grid size statistics.
        """
        if hasattr(self, 'statistics') and 'grid_size_stats' in self.statistics:
            return self.statistics['grid_size_stats']
        else:
            logger.warning("Grid size statistics not available.")
            return {}
    
    def get_symbol_frequencies(self) -&gt; Dict[int, float]:
        """
        Returns the precomputed symbol frequencies.
        
        Returns:
            Dict[int, float]: A dictionary mapping symbols to their frequencies.
        """
        if hasattr(self, 'statistics') and 'symbol_frequencies' in self.statistics:
            return self.statistics['symbol_frequencies']
        else:
            logger.warning("Symbol frequencies not available.")
            return {}

    def _compute_grid_size_stats(self):
        max_height, max_width = 0, 0
        for sample in self.data:
            # Assuming sample["input"] and sample["output"] have shape [C, H, W]
            max_height = max(max_height, sample["input"].shape[1], sample["output"].shape[1])
            max_width = max(max_width, sample["input"].shape[2], sample["output"].shape[2])
        grid_size_stats = {"max_height": max_height, "max_width": max_width}
        self.max_grid_size = (max_height, max_width)
        return grid_size_stats

    def _compute_symbol_frequencies(self):
        symbol_counts = np.zeros(self.num_symbols, dtype=int)
        max_symbol_in_data = 0
        for sample in self.data:
            input_symbols = sample["input"].flatten().numpy().astype(int)
            output_symbols = sample["output"].flatten().numpy().astype(int)
            if input_symbols.size &gt; 0:
                max_symbol_in_data = max(max_symbol_in_data, input_symbols.max())
            if output_symbols.size &gt; 0:
                max_symbol_in_data = max(max_symbol_in_data, output_symbols.max())
            symbol_counts += np.bincount(input_symbols, minlength=self.num_symbols)
            symbol_counts += np.bincount(output_symbols, minlength=self.num_symbols)
        
        logger.debug(f"Maximum symbol index in data: {max_symbol_in_data}")
        logger.debug(f"Symbol counts length: {len(symbol_counts)}")
        
        if max_symbol_in_data &gt;= self.num_symbols:
            logger.error(f"Found symbol index {max_symbol_in_data} exceeding num_symbols - 1 ({self.num_symbols - 1}).")
            raise ValueError(f"Symbol index {max_symbol_in_data} exceeds the allowed range.")
        
        total_symbols = symbol_counts.sum()
        if total_symbols == 0:
            logger.warning("Total de símbolos es 0. Evitando la división por cero.")
            symbol_freq = np.zeros_like(symbol_counts, dtype=float)
        else:
            symbol_freq = symbol_counts / total_symbols
        
        return symbol_freq
    
    def _preprocess_grid(self, grid: Union[Dict, List, np.ndarray, torch.Tensor], pad_value: int = 0) -&gt; torch.Tensor:
        logger.debug(f"Preprocessing grid with initial type: {type(grid)}")
        
        # Convert grid to torch.Tensor if it's a list or numpy array
        if isinstance(grid, list):
            grid_tensor = torch.as_tensor(grid, dtype=torch.float32)
            logger.debug(f"Converted list to tensor with shape: {grid_tensor.shape}")
        elif isinstance(grid, np.ndarray):
            grid_tensor = torch.as_tensor(grid, dtype=torch.float32)
            logger.debug(f"Converted numpy array to tensor with shape: {grid_tensor.shape}")
        elif isinstance(grid, torch.Tensor):
            grid_tensor = grid.float()
            logger.debug(f"Using existing tensor with shape: {grid_tensor.shape}")
        elif isinstance(grid, int):
            logger.debug("Grid is of type int. Converting to 1x1 grid.")
            grid = [[grid]]
            grid_tensor = torch.as_tensor(grid, dtype=torch.float32)
            logger.debug(f"Converted int to tensor with shape: {grid_tensor.shape}")
        else:
            raise ValueError(f"Unexpected grid type: {type(grid)}")
    
        # Ensure grid_tensor has three dimensions [C, H, W]
        if grid_tensor.ndim == 2:
            logger.debug("Grid tensor is 2D. Adding channel dimension.")
            grid_tensor = grid_tensor.unsqueeze(0)  # Add channel dimension
            logger.debug(f"Grid tensor shape after unsqueeze: {grid_tensor.shape}")
        elif grid_tensor.ndim != 3:
            raise ValueError(f"Unexpected grid tensor dimensions: {grid_tensor.ndim}. Expected 2D or 3D tensor.")

        logger.debug(f"Grid shape before padding: {grid_tensor.shape}")

        # Apply padding using PyTorch's built-in functions with correct pad_value
        padded_grid = self._pad_grid_torch(grid_tensor, height=30, width=30, pad_value=self.pad_symbol_idx)

        logger.debug(f"Grid shape after padding: {padded_grid.shape}")
        logger.debug(f"Padded grid with pad_symbol_idx: {self.pad_symbol_idx}, resulting shape: {padded_grid.shape}")
        return padded_grid
    
    
    def kronecker_scale(self, X, target_height=30, target_width=30):
        logger.debug(f"Kronecker scaling input shape: {X.shape}")
        h, w = X.shape
        scale_h = target_height / h
        scale_w = target_width / w
        d = int(np.floor(min(scale_h, scale_w)))
        
        X_scaled = np.kron(X, np.ones((d, d)))
        logger.debug(f"Kronecker scaled output shape: {X_scaled.shape}")
        return X_scaled


    def reverse_scaling(self, X_orig, X_pred):
        logger.debug(f"Reverse scaling - Original shape: {X_orig.shape}, Prediction shape: {X_pred.shape}")
        h, w = X_orig.shape
        # Reshape X_pred to 2D if it's 1D
        if X_pred.ndim == 1:
            X_pred = X_pred.reshape((int(np.sqrt(X_pred.size)), -1))
        
        X_pred_cropped = X_pred[:h, :w]  # Crop to original size
        
        if h == X_pred.shape[0] and w == X_pred.shape[1]:
            logger.debug("No rescaling needed")
            return X_pred_cropped
        
        # Calculate the downscale factor
        d_h = X_pred_cropped.shape[0] // h
        d_w = X_pred_cropped.shape[1] // w
        
        # Ensure the dimensions are compatible for reshaping
        if d_h &gt; 0 and d_w &gt; 0:
            try:
                X_rev = X_pred_cropped.reshape(h, d_h, w, d_w).mean(axis=(1, 3))
            except ValueError as e:
                logger.error(f"Error during reshaping: {e}")
                logger.debug(f"X_pred_cropped shape: {X_pred_cropped.shape}, h: {h}, w: {w}, d_h: {d_h}, d_w: {d_w}")
                raise
        else:
            logger.warning(f"Invalid downscale factors: d_h={d_h}, d_w={d_w}")
            raise ValueError("Invalid dimensions for reverse scaling")
        # Resize the result to match the original target shape
        result = np.resize(X_rev.round().astype(int), X_orig.shape)
        logger.debug(f"Reverse scaled output shape: {result.shape}")
        return result

    def _scale_grid(self, grid: np.ndarray, height: int, width: int) -&gt; np.ndarray:
        return grid  # No scaling, preserve original size

    def _pad_grid_torch(self, grid: torch.Tensor, height: int, width: int, pad_value: int = 0) -&gt; torch.Tensor:
        """
        Pads the input grid tensor to the specified height and width using PyTorch's functional padding.
        
        Args:
            grid (torch.Tensor): The input grid tensor with shape [C, H, W].
            height (int): The target height after padding.
            width (int): The target width after padding.
        
        Returns:
            torch.Tensor: The padded grid tensor.
        """
        _, h, w = grid.shape
        pad_h = max((height - h) // 2, 0)
        pad_w = max((width - w) // 2, 0)

        # Calculate padding for top, bottom, left, and right
        padding = (pad_w, width - w - pad_w, pad_h, height - h - pad_h)  # (left, right, top, bottom)
        logger.debug(f"Padding applied: left={pad_w}, right={width - w - pad_w}, top={pad_h}, bottom={height - h - pad_h}")
    
        # Apply padding using PyTorch's functional pad
        padded_grid = F.pad(grid, padding, mode='constant', value=pad_value)
        return padded_grid


    @staticmethod
    def collate_fn(batch):
        # Debugging: Check batch size
        logger.debug(f"Collating batch of size: {len(batch)}")
        
        if not batch:
            logger.warning("Empty batch received")
            return torch.tensor([]), torch.tensor([]), []

        inputs, outputs, task_ids = zip(*batch)
    
        # Since all samples are already padded to 30x30, no additional padding is required here.
        # However, to ensure consistency, you can verify the shapes.
    
        padded_inputs = torch.stack(inputs)
        padded_outputs = torch.stack(outputs)
    
        # Debugging: Verify shapes after stacking
        #print(f"Padded inputs shape: {padded_inputs.shape}")
        #print(f"Padded outputs shape: {padded_outputs.shape}")
    

        return padded_inputs, padded_outputs, list(task_ids)
    
    def _load_data(self, data_source):
        logger.debug(f"Loading data from source type: {type(data_source)}")
        if isinstance(data_source, list):
            return self._process_list_data(data_source)
        elif isinstance(data_source, TaskSet):
            return self._process_arckit_data(data_source)
        elif isinstance(data_source, str):
            if os.path.isdir(data_source):
                logger.debug(f"Loading data from directory: {data_source}")
                return self._load_directory(data_source)
            elif os.path.isfile(data_source):
                logger.debug(f"Loading data from file: {data_source}")
                # Since synthetic data is preloaded and passed via 'all_synthetic_data', avoid reloading here
                # Instead, assume 'all_synthetic_data' contains the necessary datasets
                samples = self._process_single_file_parallel(data_source)
                return samples
            else:
                raise ValueError(f"Invalid data source path: {data_source}")
        else:
            raise ValueError(f"Unsupported data_source type: {type(data_source)}")
    def _load_directory(self, directory_path: str) -&gt; List[Dict]:
        """
        Loads all JSON files from the specified directory and processes them.

        Args:
            directory_path (str): Path to the directory containing JSON files.

        Returns:
            List[Dict]: List of processed samples from all JSON files in the directory.
        """
        all_samples = []
        file_paths = []

        # Collect all JSON file paths
        for root, _, files in os.walk(directory_path):
            for file in files:
                if file.endswith('.json'):
                    file_path = os.path.join(root, file)
                    file_paths.append(file_path)
                    logger.debug(f"Queued file for processing: {file_path}")

        # Define the number of threads (adjust based on your system's resources)
        max_workers = min(32, os.cpu_count() + 4)  # Example adjustment

        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit all file processing tasks to the executor
            future_to_file = {executor.submit(self._process_single_file_streaming, fp): fp for fp in file_paths}

            with tqdm(total=len(file_paths), desc="Loading synthetic data", unit="file") as pbar:
                for future in as_completed(future_to_file):
                    fp = future_to_file[future]
                    try:
                        samples = future.result()
                        all_samples.extend(samples)
                        logger.debug(f"Completed processing file: {fp} with {len(samples)} samples")
                    except Exception as e:
                        logger.error(f"Error processing file {fp}: {e}", exc_info=True)
                    pbar.update(1)
        logger.debug(f"Loaded {len(all_samples)} samples from directory {directory_path}")
        return all_samples

    def _load_single_file(self, file_path: str) -&gt; List[Dict]:
        """
        Loads and processes a single JSON file.

        Args:
            file_path (str): Path to the JSON file.

        Returns:
            List[Dict]: List of processed samples from the JSON file.
        """
        try:
            return self._process_single_file_streaming(file_path)
        except Exception as e:
            logger.error(f"Error processing file {file_path}: {e}", exc_info=True)
            return []

</file>
<file name="src/training/trainer.py">
# gpt2_arc/src/training/trainer.py
import pytorch_lightning as pl
import torch
import logging
from torch import nn, optim
import time
from typing import Any, Dict, Optional
from collections import deque
from torch.optim.lr_scheduler import LambdaLR
from ..config import Config
from gpt2_arc.src.utils.training_helpers import get_num_workers
from gpt2_arc.src.utils.helpers import differential_pixel_accuracy
from ..utils.results_collector import ResultsCollector
from torch.utils.data import DataLoader
from gpt2_arc.src.data.arc_dataset import ARCDataset

logger = logging.getLogger(__name__)
from torch.utils.tensorboard import SummaryWriter
from pytorch_lightning.loggers import TensorBoardLogger
from gpt2_arc.src.utils.training_helpers import get_num_workers
import os
from optuna.exceptions import TrialPruned
from pytorch_lightning.callbacks import Callback
import torch

logger = logging.getLogger(__name__)

class NanLossPruningCallback(Callback):
    def on_train_batch_end(self, trainer, pl_module, outputs, batch, batch_idx):
        # Extract loss from outputs
        loss = outputs.get('loss') if isinstance(outputs, dict) else outputs
        if loss is not None:
            if torch.isnan(loss) or torch.isinf(loss):
                logger.warning(f"Invalid loss detected at epoch {trainer.current_epoch}, batch {batch_idx}: {loss.item()}")
                raise TrialPruned("Invalid loss encountered, pruning this trial.")


class ARCTrainer(pl.LightningModule):
    def __init__(self, model, train_dataset, val_dataset, config: Config, args, compile_model: bool = True, results_collector=None, test_dataset=None):
        logger.debug("Initializing ARCTrainer")
        super().__init__()
        logger.debug(f"ARCTrainer received args.accelerator: {args.accelerator}")
        self.model = model
        # Determine the device type based on the model's parameters
        device = next(self.model.parameters()).device
        logger.debug(f"ARCTrainer initialization on device: {device}")
        if compile_model and not args.fast_dev_run and device.type != "cpu":
            logger.info("Compiling the model with torch.compile for improved performance.")
            self.model = torch.compile(self.model, mode="reduce-overhead")
        else:
            logger.info("torch.compile not applied (compile_model=False, fast_dev_run=True, or using CPU).")
        logger.debug(f"Model is on device: {device}")
        self.train_dataset = train_dataset
        self.val_dataset = val_dataset
        self.config = config
        self.test_dataset = test_dataset  # Add this line
        self.batch_size = config.training.batch_size
        self.lr = config.training.learning_rate
        self.train_losses = []
        self.logged_metrics = {}
        self.test_outputs = []  # Initialize an empty list to store test outputs
        self.test_results = []  # Initialize test results for storing test outcomes
        self.best_val_loss = float('inf')
        self.best_epoch = 0
        self.results_collector = results_collector if results_collector else ResultsCollector(config)
        self.writer = SummaryWriter(f"runs/experiment_{self.results_collector.experiment_id}")
        self.args = args  # Add this line to assign args
    
    
    def train_dataloader(self):
        logger.info("Creating training DataLoader with centralized num_workers")

        if self.config.training.balance_symbols:
            if self.config.training.balancing_method == "weighting":
                # Compute class weights (inverse of frequencies)
                class_weights = 1.0 / torch.tensor(
                    list(self.config.training.symbol_freq.values()), dtype=torch.float
                )

                train_loader = DataLoader(
                    self.train_dataset,
                    batch_size=self.config.training.batch_size,
                    num_workers=get_num_workers(self.config.training),
                    sampler=self.train_dataset.sampler,  # Use sampler instead of shuffle
                    pin_memory=True if self.args.use_gpu else False,
                    prefetch_factor=self.config.training.prefetch_factor,
                    persistent_workers=self.config.training.persistent_workers,
                    collate_fn=self.train_dataset.dataset.collate_fn if isinstance(self.train_dataset, torch.utils.data.Subset) else ARCDataset.collate_fn
                )
            elif self.config.training.balancing_method == "oversampling":
                # Placeholder for oversampling implementation
                logger.info("Oversampling method selected, but not yet implemented.")

                train_loader = DataLoader(
                    self.train_dataset,
                    batch_size=self.config.training.batch_size,
                    num_workers=get_num_workers(self.config.training),
                    shuffle=True,  # Enable shuffle if not using a sampler
                    pin_memory=True if self.args.use_gpu else False,
                    prefetch_factor=self.config.training.prefetch_factor,
                    persistent_workers=self.config.training.persistent_workers,
                    collate_fn=self.train_dataset.dataset.collate_fn if isinstance(self.train_dataset, torch.utils.data.Subset) else self.train_dataset.collate_fn
                )
            else:
                logger.warning(f"Unknown balancing method: {self.config.training.balancing_method}. Skipping balancing.")

                train_loader = DataLoader(
                    self.train_dataset,
                    batch_size=self.config.training.batch_size,
                    num_workers=get_num_workers(self.config.training),
                    shuffle=True,  # Enable shuffle
                    pin_memory=True if self.args.use_gpu else False,
                    prefetch_factor=self.config.training.prefetch_factor,
                    persistent_workers=self.config.training.persistent_workers,
                    collate_fn=self.train_dataset.dataset.collate_fn if isinstance(self.train_dataset, torch.utils.data.Subset) else self.train_dataset.collate_fn
                )
        else:
            train_loader = DataLoader(
                self.train_dataset,
                batch_size=self.config.training.batch_size,
                num_workers=get_num_workers(self.config.training),
                shuffle=True,  # Enable shuffle
                pin_memory=self.config.training.pin_memory,
                prefetch_factor=self.config.training.prefetch_factor,
                persistent_workers=self.config.training.persistent_workers,
                collate_fn=self.train_dataset.dataset.collate_fn if isinstance(self.train_dataset, torch.utils.data.Subset) else self.train_dataset.collate_fn
            )

        logger.debug(f"Training DataLoader created with num_workers={get_num_workers(self.config.training)}")
        return train_loader

    def val_dataloader(self):
        logger.debug("Entering ARCTrainer.val_dataloader")
        collate_fn = self.val_dataset.dataset.collate_fn if isinstance(self.val_dataset, torch.utils.data.Subset) else ARCDataset.collate_fn
        dataloader = DataLoader(
            self.val_dataset,
            batch_size=self.config.training.batch_size,
            num_workers=self.config.training.num_workers,  # Updated num_workers
            pin_memory=self.config.training.pin_memory,    # Updated pin_memory
            prefetch_factor=self.config.training.prefetch_factor,
            persistent_workers=self.config.training.persistent_workers,
            collate_fn=collate_fn
        )
        logger.debug("Exiting ARCTrainer.val_dataloader")
        return dataloader

    def test_dataloader(self):
        if self.test_dataset is None:
            logger.error("Test dataset is not provided. Please ensure that the test dataset is correctly loaded.")
            raise ValueError("Test dataset is not provided.")
        collate_fn = self.test_dataset.dataset.collate_fn if isinstance(self.test_dataset, torch.utils.data.Subset) else ARCDataset.collate_fn
        return DataLoader(
            self.test_dataset,
            batch_size=self.config.training.batch_size,
            num_workers=get_num_workers(self.config.training),
            shuffle=False,
            pin_memory=self.config.training.pin_memory,
            prefetch_factor=self.config.training.prefetch_factor,
            persistent_workers=self.config.training.persistent_workers,
            collate_fn=collate_fn
        )

    
    def get_tensorboard_logger(self):
        for logger in self.trainer.loggers:
            if isinstance(logger, TensorBoardLogger):
                return logger.experiment
        logger.debug("DEBUG: No TensorBoardLogger found in trainer.loggers")
        return None

    def training_step(self, batch, batch_idx):
        logger.debug(f"Starting training step {batch_idx}")
        inputs, targets, _ = batch
        logger.debug(f"  Inputs shape: {inputs.shape}, dtype: {inputs.dtype}")
        logger.debug(f"  Targets shape: {targets.shape}, dtype: {targets.dtype}")
        
        outputs = self(inputs)
        logger.debug(f"  Outputs shape: {outputs.shape}, dtype: {outputs.dtype}")
        
        loss = self.compute_loss(outputs, targets)
        logger.debug(f"Loss computed: {loss.item()}")
        
        preds = torch.argmax(outputs, dim=-1)
        logger.debug(f"  Preds shape: {preds.shape}, dtype: {preds.dtype}")
        
        # Reshape preds to match targets if necessary
        try:
            preds = preds.view(targets.shape)
            logger.debug(f"  Reshaped preds to match targets shape: {preds.shape}")
        except RuntimeError as e:
            logger.error(f"  Failed to reshape preds: {e}")
            raise e
        
        accuracy = (preds == targets).float().mean()
        logger.debug(f"  Training accuracy: {accuracy.item()}")

        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log('train_accuracy', accuracy, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        
        return loss

    def validation_step(self, batch, batch_idx):
        logger.debug(f"Starting validation step {batch_idx}")
        inputs, targets, _ = batch
        logger.debug(f"Validation Inputs shape: {inputs.shape}, Targets shape: {targets.shape}, Targets dtype: {targets.dtype}")
        logger.debug(f"Validation batch input shape: {batch[0].shape}, Validation batch target shape: {batch[1].shape}")
        targets = targets.long()  # Ensure targets are of type Long
        logger.debug(f"Targets dtype after casting: {targets.dtype}")
        
        outputs = self(inputs)
        logger.debug(f"Validation Outputs shape: {outputs.shape}")
        
        loss = self.compute_loss(outputs, targets)
        logger.debug(f"Validation loss computed: {loss.item()}")
        
        preds = torch.argmax(outputs, dim=-1)
        logger.debug(f"  Preds shape before reshape: {preds.shape}, dtype: {preds.dtype}")

        # Reshape preds to match targets shape using view_as for safety
        try:
            preds = preds.view_as(targets)
            logger.debug(f"  Reshaped preds to match targets shape: {preds.shape}, dtype: {preds.dtype}")
        except RuntimeError as e:
            logger.error(f"  Failed to reshape preds: {e}")
            raise e

        accuracy = (preds == targets).float().mean()
        logger.debug(f"  Validation accuracy: {accuracy.item()}")

        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('val_accuracy', accuracy, on_step=False, on_epoch=True, prog_bar=True, logger=True)

        return loss

    def on_test_epoch_start(self):
        self.test_outputs = []  # Clear previous test outputs

    def test_step(self, batch, batch_idx):
        logger.debug(f"DEBUG: test_step input - batch: {batch}, batch_idx: {batch_idx}")
        logger.debug(f"Test batch input shape: {batch[0].shape}, Test batch target shape: {batch[1].shape}")
        logger.debug(f"DEBUG: Test step - Batch type: {type(batch)}, length: {len(batch)}")

        # Unpack batch
        if len(batch) == 3:
            inputs, outputs, task_ids = batch
        elif len(batch) == 2:
            inputs, outputs = batch
            logger.error("Batch does not contain 'task_ids'. Ensure the dataset provides 'task_ids'.")
            raise ValueError("Batch does not contain 'task_ids'. Ensure the dataset provides 'task_ids'.")
        else:
            raise ValueError(f"Unexpected batch format with length {len(batch)}")
        logger.debug(f"Received task_ids: {task_ids}")

        inputs = inputs.float()
        outputs = outputs.long()  # Ensure outputs are of type Long

        attention_mask = torch.ones(inputs.size(0), inputs.size(2) * inputs.size(3), dtype=torch.float32, device=inputs.device)

        model_outputs = self(inputs, attention_mask)
        loss = self.compute_loss(model_outputs, outputs)

        accuracies = []
        diff_accuracies = []
        
        # Compute batch-wise accuracy
        accuracy = self.compute_accuracy(model_outputs, outputs)
        diff_accuracy = self.compute_diff_accuracy(inputs, outputs, model_outputs)

        # Append batch metrics
        accuracies.append(accuracy)
        diff_accuracies.append(diff_accuracy)

        logger.debug(f"DEBUG: Batch accuracy: {accuracy}, Batch diff_accuracy: {diff_accuracy}")

        result = {
            'test_loss': loss.item(),
            'task_ids': task_ids,
            'test_accuracy': sum(accuracies) / len(accuracies) if accuracies else 0,
            'test_diff_accuracy': sum(diff_accuracies) / len(diff_accuracies) if diff_accuracies else 0,
        }
        logger.debug(f"DEBUG: Test loss: {result['test_loss']}, Avg accuracy: {result['test_accuracy']}, Avg diff accuracy: {result['test_diff_accuracy']}")

        # Log task-specific metrics if task_ids are available
        if task_ids is not None:
            # Aggregate task-specific metrics across the batch
            for task_id in set(task_ids):  # Remove .tolist()
                if task_id == "default_task":
                    logger.error(f"'default_task' detected in task_id: {task_id}")
                    raise ValueError(f"'default_task' detected in task_id: {task_id}")

        try:
            self.writer.add_scalar('test/loss', result['test_loss'], self.current_epoch)
            self.writer.add_scalar('test/avg_accuracy', result['test_accuracy'], self.current_epoch)
            self.writer.add_scalar('test/diff_accuracy', result['test_diff_accuracy'], self.current_epoch)
            logger.debug(f"DEBUG: Logged test metrics for epoch {self.current_epoch}: loss={result['test_loss']}, avg_accuracy={result['test_accuracy']}, diff_accuracy={result['test_diff_accuracy']}")
        except Exception as e:
            logger.error(f"DEBUG: Error logging test step: {str(e)}")

        logger.debug(f"DEBUG: test_step output - result: {result}")
        logger.debug(f"DEBUG: Test step result: {result}")

        # Add per-task metrics to ResultsCollector
        for i, task_id in enumerate(task_ids):
            task_accuracy = self.compute_accuracy(model_outputs[i], outputs[i])
            task_diff_accuracy = self.compute_diff_accuracy(inputs[i], outputs[i], model_outputs[i])
            self.results_collector.add_task_specific_result(task_id, {
                "test_accuracy": task_accuracy,
                "test_diff_accuracy": task_diff_accuracy
            })

        # Append the result to self.test_outputs
        self.test_outputs.append(result)

        return result

    def on_test_epoch_end(self):
        total_loss = torch.stack([torch.tensor(x['test_loss']) for x in self.test_outputs]).mean()
        all_accuracies = []
        all_diff_accuracies = []

        per_task_accuracy = {}
        per_task_diff_accuracy = {}

        for output in self.test_outputs:
            if 'test_accuracy' in output:
                all_accuracies.append(output['test_accuracy'])
            if 'test_diff_accuracy' in output:
                all_diff_accuracies.append(output['test_diff_accuracy'])

            # Collect per-task metrics
            for key, value in output.items():
                if key.endswith('_test_accuracy'):
                    per_task_accuracy[key] = value
                elif key.endswith('_test_diff_accuracy'):
                    per_task_diff_accuracy[key] = value

        avg_accuracy = sum(all_accuracies) / len(all_accuracies) if all_accuracies else 0
        avg_diff_accuracy = sum(all_diff_accuracies) / len(all_diff_accuracies) if all_diff_accuracies else 0

        self.log('avg_test_loss', total_loss, prog_bar=True)
        self.log('avg_test_accuracy', avg_accuracy, prog_bar=True)
        self.log('avg_test_diff_accuracy', avg_diff_accuracy, prog_bar=True)

        print(f"DEBUG: Test epoch end - Avg loss: {total_loss}, Avg accuracy: {avg_accuracy}, Avg diff accuracy: {avg_diff_accuracy}")

        # Prepare final metrics including per-task metrics
        final_metrics = {
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch,
            "final_test_loss": total_loss.item(),
            "final_test_accuracy": avg_accuracy,
            "final_test_diff_accuracy": avg_diff_accuracy
        }

        # Retrieve per-task metrics from ResultsCollector and include them in final_metrics
        for task_id, metrics in self.results_collector.task_specific_results.items():
            final_metrics.update({
                f"{task_id}_test_accuracy": metrics.get("test_accuracy", 0.0),
                f"{task_id}_test_diff_accuracy": metrics.get("test_diff_accuracy", 0.0)
            })

        logger.debug(f"DEBUG: Final metrics including per-task metrics: {final_metrics}")
        self.results_collector.set_final_metrics(final_metrics)

    def compute_accuracy(self, outputs, targets):
        predictions = outputs.argmax(dim=-1)
        # Reshape predictions to match the target shape
        predictions = predictions.view(targets.size())
        # Calculate accuracy over all elements
        accuracy = (predictions == targets).float().mean()
        logger.debug(f"DEBUG: compute_accuracy - Accuracy: {accuracy.item()}")
        return accuracy.item()

    def compute_diff_accuracy(self, inputs, targets, outputs):
        pad_symbol_idx = self.config.training.pad_symbol_idx  # Retrieve pad_symbol_idx from config
        predictions = outputs.argmax(dim=-1)
        diff_accuracy, _, _ = differential_pixel_accuracy(inputs, targets, predictions, pad_symbol_idx=pad_symbol_idx)
        logger.debug(f"Computed differential pixel accuracy (excluding padding tokens): {diff_accuracy}")
        return diff_accuracy
        
    def on_validation_epoch_end(self):
        # Compute average validation loss
        val_loss = self.trainer.callback_metrics.get('val_loss')
        if val_loss is not None:
            avg_val_loss = val_loss.item()
        else:
            avg_val_loss = float('inf')  # Default to a high value if val_loss is not available

        # Update best_val_loss and best_epoch
        if avg_val_loss &lt; self.best_val_loss:
            self.best_val_loss = avg_val_loss
            self.best_epoch = self.current_epoch

        # Log validation metrics
        self.log('val_loss', avg_val_loss)
        self.log('best_val_loss', self.best_val_loss)
        self.log('best_epoch', self.best_epoch)

        # Update the results collector
        self.results_collector.update_val_metrics(self.current_epoch, {
            "avg_loss": avg_val_loss,
            "best_val_loss": self.best_val_loss,
            "best_epoch": self.best_epoch
        })

        # Log additional information
        self.log('epoch', self.current_epoch)

    def configure_optimizers(self):
        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)
        lr_scheduler = {
            'scheduler': optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95),
            'name': 'learning_rate',
        }
        return [optimizer], [lr_scheduler]

    def on_fit_end(self):
        try:
            self.writer.close()
            print("DEBUG: TensorBoard writer closed successfully.")
        except Exception as e:
            print(f"DEBUG: Error closing TensorBoard writer: {str(e)}")
        logger.debug("DEBUG: Results saved and TensorBoard writer closed.")


    def compute_loss(self, outputs, labels):
        labels = labels.long()  # Ensure labels are of type Long
        loss = self.model.loss_fn(
            outputs.view(-1, outputs.size(-1)), labels.view(-1)
        )
        logger.debug(f"Using model's loss function with ignore_index={self.model.loss_fn.ignore_index}")
        logger.debug(f"Computed loss: {loss.item()}")
        return loss

    def forward(self, input_ids, attention_mask=None):
        return self.model(input_ids, attention_mask)
    def log_hyperparameters(self):
        hparams = {
            'learning_rate': self.config.training.learning_rate,
            'batch_size': self.config.training.batch_size,
            'n_embd': self.config.model.n_embd,
            'n_head': self.config.model.n_head,
            'n_layer': self.config.model.n_layer,
        }
        metric_dict = {
            'train_loss': 0,
            'val_loss': 0,
            'test_accuracy': 0,
        }
        try:
            self.writer.add_hparams(hparams, metric_dict)
            print(f"DEBUG: Successfully logged hyperparameters: {hparams}")
        except Exception as e:
            print(f"DEBUG: Error logging hyperparameters: {str(e)}")

</file>
<file name="src/training/__init__.py">

</file>
<file name="src/training/train.py">
# gpt2_arc/src/training/train.py
import argparse
import logging

logging.basicConfig(
    level=logging.DEBUG,  # Set logging level to DEBUG
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
from typing import Optional
import multiprocessing
import sys
import logging
import os
import json
import datetime
from unittest.mock import MagicMock, patch
import optuna
import arckit
import numpy as np
import torch
from concurrent.futures import ThreadPoolExecutor
from lightning.pytorch.profilers import PyTorchProfiler
from pytorch_lightning.callbacks import Callback
from torch.profiler import ProfilerActivity
from torch.utils.data import DataLoader, WeightedRandomSampler, Subset
import concurrent.futures
import random
from tqdm import tqdm

# Define the base directory for the arc-neural-reasoning-model
arc_model_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../.."))

# Add the root directory of the project to the PYTHONPATH
project_root = arc_model_dir
sys.path.insert(0, project_root)

import pytorch_lightning as pl
#import torch.autograd.profiler as profiler
import torch
from pytorch_lightning.callbacks import ModelCheckpoint
from pytorch_lightning.loggers import TensorBoardLogger

from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.models.gpt2 import GPT2ARC
from gpt2_arc.src.config import Config, ModelConfig, TrainingConfig
from gpt2_arc.src.training.trainer import ARCTrainer, get_num_workers
logger = logging.getLogger(__name__)
logger.debug(f"Imported get_num_workers from training_helpers: {get_num_workers}")
from gpt2_arc.src.utils.experiment_tracker import ExperimentTracker
from gpt2_arc.src.utils.results_collector import ResultsCollector
from gpt2_arc.src.utils import GrokfastCallback

logger = logging.getLogger(__name__)

class ConfigSavingModelCheckpoint(ModelCheckpoint):
    def __init__(self, config, trial_num='NA', task_id='NA', iter_num='NA', *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.config = config
        self.trial_num = trial_num
        self.task_id = task_id
        self.iter_num = iter_num
        self.timestamp = datetime.datetime.now().strftime("%Y%m%dT%H%M%S")  # e.g., 20240308T153045

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        # Add custom metadata to the checkpoint
        checkpoint['model_config'] = self.config.model.__dict__
        checkpoint['trial_num'] = self.trial_num
        checkpoint['task_id'] = self.task_id
        checkpoint['iter_num'] = self.iter_num
        checkpoint['timestamp'] = self.timestamp

        # Add the current epoch to the checkpoint
        checkpoint['epoch'] = trainer.current_epoch

        super().on_save_checkpoint(trainer, pl_module, checkpoint)

    def format_checkpoint_name(self, metrics):
        """
        Override the method to include custom placeholders in the filename.
        """
        return self.filename.format(
            trial_num=self.trial_num,
            task_id=self.task_id,
            iter_num=self.iter_num,
            val_loss=metrics.get("val_loss", 0.0),
            epoch=metrics.get("epoch", 0),
            timestamp=self.timestamp
        )

class ModelConfigSaver(Callback):
    def __init__(self, config):
        """
        Initialize the ModelConfigSaver callback with the current configuration.

        Args:
            config (Config): The configuration object containing model parameters.
        """
        super().__init__()
        self.config = config

    def on_save_checkpoint(self, trainer, pl_module, checkpoint):
        """
        Override the checkpoint saving to include the model configuration.

        Args:
            trainer (pl.Trainer): The Trainer instance.
            pl_module (pl.LightningModule): The LightningModule being trained.
            checkpoint (dict): The checkpoint dictionary to be modified.
        """
        checkpoint['model_config'] = self.config.model.__dict__

        
def prepare_val_or_test_data(eval_set, args, is_validation=True):
    """
    Prepare validation or test data from the arckit evaluation set.

    Args:
        eval_set: The evaluation TaskSet from arckit.load_data().
        args: Parsed command-line arguments.
        is_validation: Boolean indicating whether it's validation data.

    Returns:
        List of dictionaries with keys 'input', 'output', and 'task_id'.
    """
    logger.debug(f"Preparing {'validation' if is_validation else 'test'} data from arckit evaluation set")
    samples = []
    for task in tqdm(eval_set.tasks, desc=f"Processing tasks for {'validation' if is_validation else 'test'} dataset"):
        for ex in task.train if is_validation else task.test:
            sample = {'input': ex[0], 'output': ex[1], 'task_id': task.id}
            samples.append(sample)
    logger.debug(f"Prepared {len(samples)} samples for {'validation' if is_validation else 'test'} dataset")
    return samples

def load_dataset(args, config, dataset_type='train', all_synthetic_data=None):
    logger.debug(f"load_dataset called with dataset_type='{dataset_type}', args.use_synthetic_data={args.use_synthetic_data}")

    if dataset_type.lower() == 'train':
        dataset = all_synthetic_data['train_dataset'] if args.use_synthetic_data else ARCDataset(
            data_source=arckit.load_data()[0],
            is_test=False,
            max_samples=args.max_train_samples,
            num_symbols=config.training.num_symbols,
            pad_symbol_idx=config.training.pad_symbol_idx,
            symbol_freq=config.training.symbol_freq if args.enable_symbol_freq else None
        )
    else:
        _, eval_set = arckit.load_data()
        data_source = prepare_val_or_test_data(eval_set, args, is_validation=(dataset_type.lower() == 'val'))
        dataset = ARCDataset(
            data_source=data_source,
            is_test=(dataset_type.lower() == 'test'),
            num_symbols=config.training.num_symbols,
            pad_symbol_idx=config.training.pad_symbol_idx,
            symbol_freq=config.training.symbol_freq if args.enable_symbol_freq else None
        )
    logger.debug(f"{dataset_type.capitalize()} dataset loaded with {len(dataset)} samples")

    return dataset


def load_and_split_synthetic_data(args, config):
    """
    Load synthetic data using ARCDataset. Returns a dictionary containing only 'train_dataset'.

    Args:
        args: Parsed command-line arguments.
        config: Configuration object.

    Returns:
        dict: {'train_dataset': synthetic_train_dataset}
    """
    logger.debug("Entering load_and_split_synthetic_data function")
    synthetic_dataset = ARCDataset(
        data_source=args.synthetic_data_path,
        is_test=False,
        max_samples=args.max_train_samples,  # Pass the new argument here
        num_symbols=config.training.num_symbols,
        pad_symbol_idx=config.training.pad_symbol_idx,
        symbol_freq=config.training.symbol_freq if args.enable_symbol_freq else None
    )
    total_samples = len(synthetic_dataset)
    logger.debug(f"Total synthetic samples loaded: {total_samples}")

    return {'train_dataset': synthetic_dataset}


def main(args):
    if args.use_synthetic_data and not args.synthetic_data_path:
        raise ValueError("--synthetic_data_path must be provided when using synthetic data.")

    total_split = args.train_split + args.val_split + args.test_split
    if not abs(total_split - 1.0) &lt; 1e-6:
        raise ValueError("Train, validation, and test splits must sum to 1.0")
    torch.set_float32_matmul_precision(args.matmul_precision)
    logger.info(f"Set float32 matmul precision to: {args.matmul_precision}")
    logger.debug(f"Command line arguments: {args}")
    log_level = getattr(logging, args.log_level.upper() if hasattr(args, 'log_level') else 'DEBUG', logging.DEBUG)
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    profiler = PyTorchProfiler(
        dirpath=args.profiler_dirpath,
        filename=args.profiler_filename,
        activities=[ProfilerActivity.CPU, ProfilerActivity.CUDA],  # Include CUDA activities
        record_shapes=True,
        with_stack=True  # Enable stack tracing
    ) if args.use_profiler else None
    
    logger.setLevel(logging.DEBUG)  # Ensure logger is set to DEBUG
    
    logger.info("Starting main function")
    logger.debug("Initializing PyTorch Lightning Trainer")
    logger.debug(f"Command line arguments: {args}")

    trainer = None  # Initialize trainer to None

    try:
        if args.use_optuna:
            logger.info("Loading best hyperparameters from Optuna study")
            study_name = args.optuna_study_name

            if study_name is None:
                # Retrieve all study summaries from the storage
                study_summaries = optuna.get_all_study_summaries(storage=args.optuna_storage)
                study_names = [summary.study_name for summary in study_summaries]
                
                if len(study_names) == 1:
                    study_name = study_names[0]
                    logger.info(f"Automatically selected the only available study: {study_name}")
                elif len(study_names) == 0:
                    logger.error("No studies found in the specified Optuna storage.")
                    sys.exit(1)
                else:
                    logger.error("Multiple studies found in the specified Optuna storage. Please specify the study name using --optuna-study-name.")
                    sys.exit(1)

            study = optuna.load_study(study_name=study_name, storage=args.optuna_storage)
            best_params = study.best_params
            logger.debug(f"Loaded best parameters: {best_params}")
            
            n_head = 2 ** best_params['n_head_exp']
            n_embd = n_head * best_params['n_embd_multiplier']
            n_embd = 2 ** int(np.log2(n_embd))
            model_config = ModelConfig(
                n_embd=n_embd,
                n_head=n_head,
                n_layer=best_params['n_layer'],
                dropout=best_params['dropout'],
                num_workers=args.num_workers if args.num_workers is not None else multiprocessing.cpu_count(),
                prefetch_factor=args.prefetch_factor,
                persistent_workers=not args.no_persistent_workers,
                pin_memory=not args.no_pin_memory,
            )
            training_config = TrainingConfig(
                batch_size=best_params['batch_size'],
                learning_rate=best_params['learning_rate'],
                max_epochs=args.max_epochs,
                use_gpu=args.use_gpu,
                log_level=args.log_level,
                use_synthetic_data=args.use_synthetic_data,
                synthetic_data_path=args.synthetic_data_path,
                include_pad_in_loss=args.include_pad_in_loss,
                include_pad_in_accuracy=args.include_pad_in_accuracy,
                num_workers=args.num_workers,
                prefetch_factor=args.prefetch_factor,
                persistent_workers=not args.no_persistent_workers,
                pin_memory=args.pin_memory
            )
            training_config = TrainingConfig(
                batch_size=best_params['batch_size'],
                learning_rate=best_params['learning_rate'],
                max_epochs=args.max_epochs,  # Always use the user-provided max_epochs
            )
        else:
            logger.info("Using provided or default hyperparameters")
            model_config = ModelConfig(
                n_embd=args.n_embd,
                n_head=args.n_head,
                n_layer=args.n_layer,
                dropout=args.dropout,
                mamba_ratio=args.mamba_ratio,
                d_state=args.d_state,
                d_conv=args.d_conv,
                mamba_depth=args.mamba_depth,
                mamba_expand=args.mamba_expand,
            )
            training_config = TrainingConfig(
                batch_size=args.batch_size,
                learning_rate=args.learning_rate,
                max_epochs=args.max_epochs,
                use_gpu=args.use_gpu,
                log_level=args.log_level,
                use_synthetic_data=args.use_synthetic_data,
                synthetic_data_path=args.synthetic_data_path,
                use_grokfast=args.use_grokfast,
                grokfast_type=args.grokfast_type,
                grokfast_alpha=args.grokfast_alpha,
                grokfast_lamb=args.grokfast_lamb,
                grokfast_window_size=args.grokfast_window_size,
            )
        
        config = Config(model=model_config, training=training_config)
        logger.debug(f"Configuration: {config}")

        # Validate split ratios sum to 1.0
        total_split = args.train_split + args.val_split + args.test_split
        if not abs(total_split - 1.0) &lt; 1e-6:
            raise ValueError("Train, validation, and test splits must sum to 1.0")

        if args.use_synthetic_data:
            # Load and split synthetic data
            all_synthetic_data = load_and_split_synthetic_data(args, config)
        else:
            all_synthetic_data = None

        # Sequentially load datasets to avoid memory allocation issues
        logger.info("Loading datasets sequentially to avoid memory allocation issues")

        try:
            train_data = load_dataset(args, config, dataset_type='train', all_synthetic_data=all_synthetic_data)
            val_data = load_dataset(args, config, dataset_type='val')      # Removed all_synthetic_data
            test_data = load_dataset(args, config, dataset_type='test')    # Removed all_synthetic_data
        except Exception as e:
            logger.error(f"Error loading datasets: {e}")
            raise

        # Log the source of each dataset
        logger.info(f"Training dataset source: {'synthetic data' if args.use_synthetic_data else 'official ARC data'}")
        logger.info(f"Validation dataset source: official ARC data")
        logger.info(f"Test dataset source: official ARC data")
        
        # Log the number of samples in each dataset
        logger.debug(f"Number of training samples: {len(train_data)}")
        logger.debug(f"Number of validation samples: {len(val_data)}")
        logger.debug(f"Number of test samples: {len(test_data)}")

        if args.enable_symbol_freq:
            logger.debug("Calculating symbol frequencies as it is enabled.")
            if args.use_synthetic_data:
                logger.debug("Calculating symbol frequencies for synthetic training set")
                symbol_freq = train_data.get_symbol_frequencies()
            else:
                logger.debug("Calculating symbol frequencies for ARC training set")
                symbol_freq = train_data.get_symbol_frequencies()

            symbol_freq_dict = {i: float(freq) for i, freq in enumerate(symbol_freq)}
            pad_symbol_idx = config.training.pad_symbol_idx
            symbol_freq_dict.pop(pad_symbol_idx, None)
            logger.debug(f"Removed pad_symbol_idx ({pad_symbol_idx}) from symbol_freq_dict. New length: {len(symbol_freq_dict)}")

            assert len(symbol_freq_dict) == config.training.num_classes - 1, (
                f"Length of symbol_freq_dict ({len(symbol_freq_dict)}) does not match num_classes minus padding ({config.training.num_classes - 1})."
            )
            balance_symbols = True
            balancing_method = "weighting"
        else:
            symbol_freq_dict = {}
            logger.debug("Symbol frequency calculation is disabled. Using empty symbol_freq_dict.")
            balance_symbols = False
            balancing_method = "none"

        training_config = TrainingConfig(
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            max_epochs=args.max_epochs,
            use_grokfast=args.use_grokfast,
            grokfast_type=args.grokfast_type,
            grokfast_alpha=args.grokfast_alpha,
            grokfast_lamb=args.grokfast_lamb,
            grokfast_window_size=args.grokfast_window_size,
            include_pad_in_loss=args.include_pad_in_loss,
            symbol_freq=symbol_freq_dict,
            balance_symbols=balance_symbols,
            balancing_method=balancing_method,
            num_workers=args.num_workers,
            prefetch_factor=args.prefetch_factor,
            persistent_workers=not args.no_persistent_workers,
            pin_memory=args.pin_memory
        )
        config = Config(model=model_config, training=training_config)

        # Calculate symbol frequencies if enabled
        if args.enable_symbol_freq:
            if args.use_synthetic_data:
                logger.debug("Calculating symbol frequencies for synthetic training set")
                train_symbol_freq = train_data.get_symbol_frequencies()
            else:
                logger.debug("Calculating symbol frequencies for ARC training set")
                train_symbol_freq = train_data.get_symbol_frequencies()
        else:
            train_symbol_freq = {}

        # Set the number of classes based on TrainingConfig
        num_classes = config.training.num_classes
        logger.info(f"Number of classes set to: {num_classes}")
        logger.info("Creating DataLoader instances")
        from torch.utils.data import Subset


        # Ensure test_data is not None
        assert test_data is not None, "Test dataset is None after loading."

        # Initialize model
        logger.info("Initializing model")
        model = GPT2ARC(
            config=config,
            num_classes=config.training.num_classes,  # Use num_classes from config
            symbol_freq=symbol_freq_dict,
            pad_symbol_idx=config.training.pad_symbol_idx
        )
        logger.debug(f"Model initialized with config: {model_config}")

        # Load the checkpoint if specified
        if args.model_checkpoint:
            logger.info(f"Loading model from checkpoint: {args.model_checkpoint}")
            checkpoint = torch.load(args.model_checkpoint)
            if 'model_config' in checkpoint and 'training_config' in checkpoint:
                model_config = ModelConfig(**checkpoint['model_config'])
                training_config = TrainingConfig(**checkpoint['training_config'])
                config = Config(model=model_config, training=training_config)
                num_classes = config.training.num_classes
                symbol_freq_dict = config.training.symbol_freq
                model = GPT2ARC(config=config, num_classes=num_classes, symbol_freq=symbol_freq_dict, pad_symbol_idx=config.training.pad_symbol_idx)
                logger.debug(f"Loaded TrainingConfig with num_classes={num_classes} from checkpoint")
            else:
                logger.error("Checkpoint missing 'model_config' or 'training_config'.")
                raise KeyError("Checkpoint must contain 'model_config' and 'training_config'.")
            model.load_state_dict(checkpoint['state_dict'])

        # Log dataset source information
        if args.use_synthetic_data:
            logger.info("Using synthetic data for training, validation, and testing.")
        else:
            logger.info("Using official ARC datasets for training, validation, and testing.")
        results_collector = ResultsCollector(config)

        # Initialize experiment tracker
        tracker = ExperimentTracker(config, project=args.project)


        logger.info("Initializing ARCTrainer")
        trainer = ARCTrainer(
            model=model,
            train_dataset=train_data,
            val_dataset=val_data,
            config=config,
            args=args,
            results_collector=results_collector,
            test_dataset=test_data
        )
        trainer.log_hyperparameters()

        # Determine accelerator parameters based on the --accelerator argument
        if args.accelerator == "tpu":
            accelerator = 'tpu'
            devices = 'xla:1'  # Use 'xla:8' for TPU v3-8 pods
            strategy = 'tpu_spawn'  # Recommended strategy for TPU
        elif args.accelerator == "gpu":
            if torch.cuda.is_available():
                accelerator = 'gpu'
                devices = 1
            else:
                accelerator = 'cpu'
                devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
        else:
            accelerator = 'cpu'
            devices = 1
            strategy = 'auto'  # Changed from None to 'auto'
        
        # Initialize callbacks list                                                                                  
        callbacks = []

        # Initialize GrokfastCallback if enabled
        if config.training.use_grokfast:
            grokfast_callback = GrokfastCallback(
                filter_type=config.training.grokfast_type,  # 'ema' or 'ma'
                alpha=config.training.grokfast_alpha,
                lamb=config.training.grokfast_lamb,
                window_size=config.training.grokfast_window_size if config.training.grokfast_type == 'ma' else 100,  # default for ma
                warmup=True,
                trigger=False
            )
            callbacks.append(grokfast_callback)
            logger.info("GrokfastCallback added to the training callbacks.")
        else:
            logger.info("Grokfast is disabled; no callback added.")

        # Add the standard ModelCheckpoint callback
        if not args.no_checkpointing:
            checkpoint_filename = f"{'resume-' if args.model_checkpoint else ''}checkpoint-step_{{step}}-val_loss_{{val_loss:.4f}}"
            checkpoint_callback = ModelCheckpoint(
                dirpath="checkpoints",
                filename=checkpoint_filename,
                save_top_k=3,
                monitor="val_loss",
                mode="min",
            )
            callbacks.append(checkpoint_callback)

            # Instantiate and add the ModelConfigSaver callback
            model_config_saver = ModelConfigSaver(config)
            callbacks.append(model_config_saver)
            logger.info("ModelConfigSaver callback added to the training callbacks.")



        logger.info("Setting up PyTorch Lightning trainer")

        # Define trial_num, task_id, and iter_num
        trial_num = 0  # Initialize to 0 or another appropriate default
        task_id = "default_task"  # Replace with dynamic task identification if necessary
        iter_num = 1  # Initialize to 1; increment as needed within your training loop

        # Removed the custom ConfigSavingModelCheckpoint as it's not needed

        if not args.no_logging:
            tb_logger = TensorBoardLogger(
                save_dir="runs",
                name=f"experiment_{trainer.results_collector.experiment_id}"
            )
            logger.debug(f"TensorBoard logger initialized. Log dir: {tb_logger.log_dir}")
        else:
            tb_logger = False
            logger.debug("Logging is disabled")

        pl_trainer = pl.Trainer(
            max_epochs=config.training.max_epochs,
            logger=tb_logger,
            callbacks=callbacks if callbacks else None,  # This now includes ModelCheckpoint
            enable_checkpointing=not args.no_checkpointing,
            enable_progress_bar=not args.no_progress_bar,
            fast_dev_run=args.fast_dev_run,  # Use the command-line argument
            gradient_clip_val=1.0,    # Add gradient clipping
            precision=16,             # Enable Automatic Mixed Precision
            accelerator=accelerator,
            devices=devices,
            strategy=strategy,
            profiler=profiler,
            val_check_interval=args.val_check_interval  # Added line
        )

        if tb_logger:
            trainer.results_collector.set_tensorboard_log_path(tb_logger.log_dir)
            logger.debug(f"TensorBoard log path set in results collector: {tb_logger.log_dir}")

        # Log initial memory usage
        if args.use_gpu and torch.cuda.is_available():
            logger.info(f"Initial CUDA memory allocated: {torch.cuda.memory_allocated()} bytes")
            logger.info(f"Initial CUDA memory reserved: {torch.cuda.memory_reserved()} bytes")

        logger.info("Starting model training")
        logger.debug("Training parameters: ")
        logger.debug(f"Batch size: {config.training.batch_size}, Learning rate: {config.training.learning_rate}, Max epochs: {config.training.max_epochs}")

        # Train the model
        logger.info("Starting model training")
        # Update the fit call to exclude DataLoaders
        pl_trainer.fit(trainer)

        # Log memory usage after training
        if args.use_gpu and torch.cuda.is_available():
            logger.info(f"CUDA memory allocated after training: {torch.cuda.memory_allocated()} bytes")
            logger.info(f"CUDA memory reserved after training: {torch.cuda.memory_reserved()} bytes")

        logger.info("Model training completed successfully.")

        # Define DataLoader for test data
        test_loader = DataLoader(
            test_data,
            batch_size=config.training.batch_size,
            shuffle=False,
            num_workers=config.training.num_workers,
            pin_memory=config.training.pin_memory
        )

        # After training, run test
        logger.info("Starting model evaluation on test dataset.")
        logger.info("Running model evaluation")
        logger.debug("Preparing to run Trainer.test()")
        test_results = pl_trainer.test(model=trainer, dataloaders=test_loader)
        if test_results:
            avg_test_loss = sum(result['avg_test_loss'] for result in test_results) / len(test_results)
            avg_test_accuracy = sum(result['avg_test_accuracy'] for result in test_results) / len(test_results)
            avg_test_diff_accuracy = sum(result['avg_test_diff_accuracy'] for result in test_results) / len(test_results)

            logger.info(f"Test results - Loss: {avg_test_loss:.4f}, Accuracy: {avg_test_accuracy:.4f}, Diff Accuracy: {avg_test_diff_accuracy:.4f}")

            results = {
                "avg_test_loss": avg_test_loss,
                "avg_test_accuracy": avg_test_accuracy,
                "avg_test_diff_accuracy": avg_test_diff_accuracy,
            }

            # Add task-specific results
            for result in test_results:
                for key, value in result.items():
                    if key.endswith('_test_accuracy') or key.endswith('_test_diff_accuracy'):
                        results[key] = value

            trainer.results_collector.set_test_results(results)

        trainer.results_collector.set_final_metrics({
            "best_val_loss": trainer.best_val_loss,
            "best_epoch": trainer.best_epoch,
            "final_test_loss": avg_test_loss,
            "final_test_accuracy": avg_test_accuracy,
            "final_test_diff_accuracy": avg_test_diff_accuracy
        })

        # Save the final model with configuration
        logger.info("Saving final model with configuration")
        model_path = f"final_model_{trainer.results_collector.experiment_id}.pth"
        os.makedirs("checkpoints", exist_ok=True)
        torch.save({
            'state_dict': trainer.model.state_dict(),
            'model_config': trainer.config.model.__dict__,
            'training_config': trainer.config.training.__dict__,
            'pad_symbol_idx': trainer.config.training.pad_symbol_idx,
            'symbol_freq': trainer.config.training.symbol_freq
        }, model_path)
        trainer.results_collector.set_checkpoint_path(model_path)
        logger.debug(f"Model and configuration saved to: {model_path}")

        # Save results
        logger.info("Saving experiment results")
        os.makedirs("results", exist_ok=True)
        results_path = f"results/experiment_{trainer.results_collector.experiment_id}.json"
        trainer.results_collector.save_to_json(results_path)
        logger.debug(f"Results saved to: {results_path}")

    except RuntimeError as e:
        if 'CUDA out of memory' in str(e):
            logger.error("CUDA out of memory error occurred.")
            logger.error("Consider reducing the batch size or model complexity.")
            raise RuntimeError("CUDA out of memory error occurred.")
        else:
            logger.error(f"A runtime error occurred: {str(e)}", exc_info=True)
            raise RuntimeError(f"A runtime error occurred: {str(e)}")
    except Exception as e:
        logger.error(f"An unexpected error occurred: {str(e)}", exc_info=True)
        sys.exit(1)  # Exit the program after logging the error
    finally:
        if 'tracker' in locals():
            tracker.finish()

    if trainer is not None:
        # ... proceed with training ...
        pass
    else:
        logger.error("Trainer was not initialized. Exiting the training loop.")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Train the ARC Neural Reasoning Model")
    group = parser.add_mutually_exclusive_group()
    group.add_argument("--use_profiler", action="store_true", help="Enable the custom profiler")
    group.add_argument("--fast_dev_run", action="store_true", help="Run a fast development test")
    
    parser.add_argument(
        "--optuna_study_name",
        type=str,
        default=None,
        help="Name of the Optuna study to load. If not provided and only one study exists in storage, it will be used automatically."
    )
    parser.add_argument("--optuna_storage", type=str, default="sqlite:///optuna_results.db", help="Storage URL for the Optuna study")
    parser.add_argument(
        "--max_train_samples",
        type=int,
        default=None,
        help="Maximum number of training samples to load. Use None to load all samples."
    )
    parser.add_argument(
        "--num_workers",
        type=int,
        default=4,  # Increased from None/1 to 4
        help="Number of worker threads for DataLoader. Increasing this can speed up data loading."
    )
    parser.add_argument(
        "--prefetch_factor",
        type=int,
        default=2,
        help="Number of batches to prefetch per worker."
    )
    parser.add_argument(
        "--no_persistent_workers",
        action="store_true",
        help="Disable persistent workers in DataLoader."
    )
    parser.add_argument(
        "--pin_memory",
        action="store_true",
        help="Enable pin_memory in DataLoader for faster GPU data transfer."
    )
    parser.set_defaults(pin_memory=True)  # Enable by default if using GPU
    parser.add_argument("--n_embd", type=int, default=12, help="Embedding size for the model.")
    parser.add_argument("--n_head", type=int, default=1, help="Number of attention heads for profiling")
    parser.add_argument("--n_layer", type=int, default=1, help="Number of transformer layers for profiling")
    parser.add_argument("--batch_size", type=int, default=16, help="Batch size for profiling")  # Increased from 1 to 16
    parser.add_argument("--learning_rate", type=float, default=1e-4, help="Learning rate")
    parser.add_argument("--max_epochs", type=int, required=True, help="Maximum number of epochs")
    parser.add_argument("--mamba_ratio", type=float, default=0.0, help="Mamba ratio (float value)")

    parser.add_argument("--dropout", type=float, default=0.05, help="Dropout rate")
    parser.add_argument("--d_state", type=int, default=4, help="Mamba state dimension")
    parser.add_argument("--d_conv", type=int, default=1, help="Mamba convolution dimension")
    parser.add_argument("--mamba_depth", type=int, default=1, help="Depth of each Mamba layer")
    parser.add_argument("--mamba_expand", type=int, default=2, help="Expand factor for each Mamba layer")
    parser.add_argument("--use_gpu", action="store_true", help="Use GPU for training if available")
    parser.add_argument("--use_grokfast", action="store_true", help="Enable Grokfast for gradient filtering.")
    parser.add_argument(
        "--include_pad_in_loss",
        dest="include_pad_in_loss",
        action="store_true",
        help="Include the padding class in the loss calculation."
    )
    parser.add_argument(
        "--no_include_pad_in_loss",
        dest="include_pad_in_loss",
        action="store_false",
        help="Exclude the padding class from the loss calculation."
    )
    parser.set_defaults(include_pad_in_loss=True)
    parser.add_argument(
        "--grokfast_type",
        type=str,
        default="ema",
        choices=["ema", "ma"],
        help="Type of Grokfast filter to use: 'ema' or 'ma'."
    )
    parser.add_argument(
        "--grokfast_alpha",
        type=float,
        default=0.98,
        help="Alpha parameter for Grokfast-EMA."
    )
    parser.add_argument(
        "--grokfast_lamb",
        type=float,
        default=2.0,
        help="Lambda parameter for Grokfast filters."
    )
    parser.add_argument(
        "--grokfast_window_size",
        type=int,
        default=100,
        help="Window size for Grokfast-MA."
    )
    parser.add_argument("--no_logging", action="store_true", help="Disable logging")
    parser.add_argument("--no_checkpointing", action="store_true", help="Disable checkpointing")
    parser.add_argument("--no_progress-bar", action="store_true", help="Disable progress bar")
    parser.add_argument("--model_checkpoint", type=str, help="Path to the model checkpoint to resume training")
    parser.add_argument("--project", type=str, default="gpt2-arc", help="W&amp;B project name")
    parser.add_argument(
        "--val_check_interval",
        type=float,
        default=0.01,
        help=(
            "How often to perform validation. "
            "If a float, represents the fraction of an epoch (e.g., 0.5 for halfway through each epoch). "
            "If an integer, represents the number of training steps."
        )
    )
    parser.add_argument(
        "--enable_symbol_freq",
        action="store_true",
        help="Enable the calculation of symbol frequencies."
    )
    parser.set_defaults(enable_symbol_freq=False)
    parser.add_argument("--results_dir", type=str, default="./results", help="Directory to save results")
    parser.add_argument("--run_name", type=str, default="default_run", help="Name of the run for saving results")
    parser.add_argument("--use_synthetic_data", action="store_true", help="Use synthetic data for training")
    parser.add_argument("--train_split", type=float, default=0.8, help="Proportion of data to use for training")
    parser.add_argument("--val_split", type=float, default=0.1, help="Proportion of data to use for validation")
    parser.add_argument("--test_split", type=float, default=0.1, help="Proportion of data to use for testing")
    parser.add_argument(
        "--matmul_precision",
        type=str,
        default="medium",
        choices=["highest", "high", "medium"],
        help="Set the internal precision of float32 matrix multiplications. Options: 'highest', 'high', 'medium'. Defaults to 'medium'."
    )
    parser.add_argument("--synthetic_data_path", type=str, help="Path to synthetic data directory")
    parser.add_argument("--log_level", type=str, default="INFO", help="Logging level")
    parser.add_argument("--use_optuna", action="store_true", help="Use best hyperparameters from Optuna study")
    parser.add_argument(
        "--accelerator",
        type=str,
        default="gpu",
        choices=["cpu", "gpu", "tpu"],
        help="Accelerator to use for training: 'cpu', 'gpu', or 'tpu'. Defaults to 'gpu'."
    )
    parser.add_argument(
        "--profiler_dirpath",
        type=str,
        default="./profiler_logs",
        help="Directory path for profiler output files."
    )
    parser.add_argument(
        "--profiler_filename",
        type=str,
        default="profile",
        help="Filename for profiler output."
    )
    parser.add_argument(
        "--include_pad_in_accuracy",
        type=lambda x: (str(x).lower() in ['true', '1', 't', 'y', 'yes']),
        default=True,
        help="Whether to include the padding class in accuracy calculations. (True/False)"
    )
    
    args = parser.parse_args()

    # Validate mamba_ratio
    if args.mamba_ratio &lt; 0.0:
        logger.error("Invalid value for --mamba_ratio: must be non-negative.")
        sys.exit(1)
    # Validate the val_check_interval
    if args.val_check_interval &lt;= 0:
        logger.error("The --val_check_interval must be a positive number.")
        sys.exit(1)

    main(args)


</file>
<file name="src/models/mamba_block_internal.py">
from __future__ import annotations

import math

import torch
import torch.nn.functional as F
from einops import einsum, rearrange, repeat
from torch import Tensor, nn
from bitnet import BitLinearNew

from zeta.nn.modules.rms_norm import RMSNorm
from zeta.utils import exists


class MambaBlock(nn.Module):
    """
    Initialize a single Mamba block.

    Args:
        dim (int): The input dimension.
        dim_inner (Optional[int]): The inner dimension. If not provided, it is set to dim * expand.
        depth (int): The depth of the Mamba block.
        d_state (int): The state dimension. Default is 16.
        expand (int): The expansion factor. Default is 2.
        dt_rank (Union[int, str]): The rank of the temporal difference (Δ) tensor. Default is "auto".
        d_conv (int): The dimension of the convolutional kernel. Default is 4.
        conv_bias (bool): Whether to include bias in the convolutional layer. Default is True.
        bias (bool): Whether to include bias in the linear layers. Default is False.

    Examples:
        &gt;&gt;&gt; import torch
        &gt;&gt;&gt; from zeta.nn.modules.simple_mamba import MambaBlock
        &gt;&gt;&gt; block = MambaBlock(dim=64, depth=1)
        &gt;&gt;&gt; x = torch.randn(1, 10, 64)
        &gt;&gt;&gt; y = block(x)
        &gt;&gt;&gt; y.shape
        torch.Size([1, 10, 64])
    """

    def __init__(
        self,
        dim: int = None,
        depth: int = 5,
        d_state: int = 16,
        expand: int = 2,
        d_conv: int = 4,
        conv_bias: bool = True,
        bias: bool = False,
    ):
        """A single Mamba block, as described in Figure 3 in Section 3.4 in the Mamba paper [1]."""
        super().__init__()
        self.dim = dim
        self.depth = depth
        self.d_state = d_state
        self.expand = expand
        self.d_conv = d_conv
        self.conv_bias = conv_bias
        self.bias = bias

        # If dt_rank is not provided, set it to ceil(dim / d_state)
        dt_rank = math.ceil(self.dim / 16)
        self.dt_rank = dt_rank

        # If dim_inner is not provided, set it to dim * expand
        dim_inner = dim * expand
        self.dim_inner = dim_inner

        # If dim_inner is not provided, set it to dim * expand
        self.in_proj = BitLinearNew(dim, dim_inner * 2, bias=bias)

        self.conv1d = nn.Conv1d(
            in_channels=dim_inner,
            out_channels=dim_inner,
            bias=conv_bias,
            kernel_size=d_conv,
            groups=dim_inner,
            padding=d_conv - 1,
        )

        # x_proj takes in `x` and outputs the input-specific Δ, B, C
        self.x_proj = BitLinearNew(
            dim_inner, dt_rank + self.d_state * 2, bias=False
        )

        # dt_proj projects Δ from dt_rank to d_in
        self.dt_proj = BitLinearNew(dt_rank, dim_inner, bias=True)

        A = repeat(torch.arange(1, self.d_state + 1), "n -&gt; d n", d=dim_inner)
        self.A_log = nn.Parameter(torch.log(A))
        self.D = nn.Parameter(torch.ones(dim_inner))
        self.out_proj = BitLinearNew(dim_inner, dim, bias=bias)

    def forward(self, x: Tensor):
        """Mamba block forward. This looks the same as Figure 3 in Section 3.4 in the Mamba paper [1].

        Args:
            x: shape (b, l, d)    (See Glossary at top for definitions of b, l, d_in, n...)

        Returns:
            output: shape (b, l, d)


        Official Implementation:
            class Mamba, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/modules/mamba_simple.py#L119
            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311

        """
        (b, l, d) = x.shape

        x_and_res = self.in_proj(x)  # shape (b, l, 2 * d_in)
        x_and_res = rearrange(x_and_res, "b l x -&gt; b x l")
        (x, res) = x_and_res.split(
            split_size=[self.dim_inner, self.dim_inner], dim=1
        )

        x = self.conv1d(x)[:, :, :l]
        x = F.silu(x)

        y = self.ssm(x)

        y = y * F.silu(res)

        output = self.out_proj(rearrange(y, "b dim l -&gt; b l dim"))

        return output

    def ssm(self, x: Tensor):
        """Runs the SSM. See:
            - Algorithm 2 in Section 3.2 in the Mamba paper [1]
            - run_SSM(A, B, C, u) in The Annotated S4 [2]

        Args:
            x: shape (b, d_in, l)    (See Glossary at top for definitions of b, l, d_in, n...)

        Returns:
            output: shape (b, d_in, l)

        Official Implementation:
            mamba_inner_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L311

        """
        (d_in, n) = self.A_log.shape

        # Compute ∆ A B C D, the state space parameters.
        #     A, D are input independent
        #     ∆, B, C are input-dependent (this is a key difference between Mamba and the linear time invariant S4)

        A = -torch.exp(self.A_log.float())  # shape (d_in, n)
        D = self.D.float()

        x_dbl = rearrange(x, "b d l -&gt; b l d")
        x_dbl = self.x_proj(x_dbl)  # (b, l, dt_rank + 2*n)

        (delta, B, C) = x_dbl.split(
            split_size=[self.dt_rank, n, n], dim=-1
        )  # delta: (b, l, dt_rank). B, C: (b, l, n)
        delta = F.softplus(self.dt_proj(delta))  # (b, l, d_in)

        y = self.selective_scan(
            x, delta, A, B, C, D
        )  # This is similar to run_SSM(A, B, C, u) in The Annotated S4 [2]

        return y

    def selective_scan(self, u, delta, A, B, C, D):
        """Does selective scan algorithm. See:
            - Section 2 State Space Models in the Mamba paper [1]
            - Algorithm 2 in Section 3.2 in the Mamba paper [1]
            - run_SSM(A, B, C, u) in The Annotated S4 [2]

        This is the classic discrete state space formula:
            x(t + 1) = Ax(t) + Bu(t)
            y(t)     = Cx(t) + Du(t)
        except B and C (and the step size delta, which is used for discretization) are dependent on the input x(t).

        Args:
            u: shape (b, d_in, l)    (See Glossary at top for definitions of b, l, d_in, n...)
            delta: shape (b, l, d_in)
            A: shape (d_in, n)
            B: shape (b, l, n)
            C: shape (b, l, n)
            D: shape (d_in,)

        Returns:
            output: shape (b, d_in, l)

        Official Implementation:
            selective_scan_ref(), https://github.com/state-spaces/mamba/blob/main/mamba_ssm/ops/selective_scan_interface.py#L86
            Note: I refactored some parts out of `selective_scan_ref` out, so the functionality doesn't match exactly.

        """
        (b, d_in, l) = u.shape
        n = A.shape[1]

        # Discretize continuous parameters (Δ, A, B)  (see Section 2 Equation 4 in the Mamba paper [1])
        # Note that B is parameterized directly
        deltaA = torch.exp(einsum(delta, A, "b l d_in, d_in n -&gt; b d_in l n"))
        deltaB_u = einsum(
            delta, B, u, "b l d_in, b l n, b d_in l -&gt; b d_in l n"
        )

        # Perform selective scan (see scan_SSM() in The Annotated S4 [2])
        x = torch.zeros((b, d_in, n), device=next(self.parameters()).device)
        ys = []
        for i in range(l):
            x = deltaA[:, :, i] * x + deltaB_u[:, :, i]
            y = einsum(x, C[:, i, :], "b d_in n , b n -&gt; b d_in")
            ys.append(y)
        y = torch.stack(ys, dim=2)  # (b d_in l)

        if D is not None:
            y = y + u * rearrange(D, "d_in -&gt; d_in 1")

        return y


class Mamba(nn.Module):
    """Mamba model.

    Args:
        vocab_size (int): The size of the vocabulary.
        dim (int): The input dimension.
        depth (int): The depth of the Mamba block.
        d_state (int): The state dimension. Default is 16.
        expand (int): The expansion factor. Default is 2.
        dt_rank (Union[int, str]): The rank of the temporal difference (Δ) tensor. Default is "auto".
        d_conv (int): The dimension of the convolutional kernel. Default is 4.

    Examples:
    x = torch.randint(0, 16, (1, 64))
    model = Mamba(16, 64, 5, 16)
    out = model(x)
    print(out)
    """

    def __init__(
        self,
        vocab_size: int = None,
        dim: int = None,
        depth: int = 5,
        d_state: int = 16,
        img_dim: int = 64,
        *args,
        **kwargs,
    ):
        """Full Mamba model."""
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, dim)
        self.norm_f = RMSNorm(dim)
        self.lm_head = BitLinearNew(dim, vocab_size, bias=False)
        self.lm_head.weight = self.embedding.weight
        self.mamba_layers = nn.ModuleList(
            [
                MambaBlock(
                    dim=dim, depth=depth, d_state=d_state, *args, **kwargs
                )
                for _ in range(depth)
            ]
        )

        # Projection for img
        self.img_proj = BitLinearNew(img_dim, dim)

    def forward(
        self,
        x: Tensor,
        context: Tensor = None,
    ):
        """
        Args:
            x (long tensor): shape (b, l)    (See Glossary at top for definitions of b, l, d_in, n...)

        Returns:
            logits: shape (b, l, vocab_size)

        Official Implementation:
            class MambaLMHeadModel, https://github.com/state-spaces/mamba/blob/main/mamba_ssm/models/mixer_seq_simple.py#L173

        """
        x = self.embedding(x)

        if exists(context):
            # Project the image
            projected_img = self.img_proj(context)

            # Concatenate the image and text
            x = torch.cat([x, projected_img], dim=1)

        for layer in self.mamba_layers:
            x = layer(self.norm_f(x)) + x

        x = self.norm_f(x)
        logits = self.lm_head(x)

        return logits

</file>
<file name="src/models/__init__.py">

</file>
<file name="src/models/gpt2.py">
# gpt2_arc/src/models/gpt2.py

import logging

import torch
import torch.nn.functional as F
import pytorch_lightning as pl
from torch import nn
from typing import Dict, Optional
import torch.nn.init as init
from bitnet import BitLinearNew
from torch.utils.data import DataLoader

logger = logging.getLogger(__name__)
from gpt2_arc.src.data.arc_dataset import ARCDataset
from gpt2_arc.src.config import Config
#from zeta.nn import MambaBlock
from gpt2_arc.src.models.mamba_block_internal import MambaBlock


class Attention(nn.Module):
    def __init__(self, n_embd, n_head, dropout):
        super().__init__()
        self.n_head = n_head
        self.n_embd = n_embd
        self.key = BitLinearNew(n_embd, n_embd)
        self.query = BitLinearNew(n_embd, n_embd)
        self.value = BitLinearNew(n_embd, n_embd)
        self.proj = BitLinearNew(n_embd, n_embd)
        self.dropout = nn.Dropout(dropout)  # Add this line
        logger.debug(f"Initialized Attention with n_embd={n_embd}, n_head={n_head}")

    def forward(self, x, mask=None):
        B, T, C = x.size()
        logger.debug(f"Model input shape: {x.shape}")
        if not torch._dynamo.is_compiling():
            logger.debug(f"Attention input shape: {x.shape}")
        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)
        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)

        att = (q @ k.transpose(-2, -1)) * (1.0 / torch.sqrt(torch.tensor(k.size(-1), dtype=torch.float32)))
        if mask is not None:
            att = att.masked_fill(mask[:, None, None, :] == 0, float("-inf"))
        att = F.softmax(att, dim=-1)

        # Apply dropout to attention probabilities
        att = self.dropout(att)  # Add this line
        y = att @ v
        y = y.transpose(1, 2).contiguous().view(B, T, C)
        output = self.proj(y)
        if not torch._dynamo.is_compiling():
            logger.debug(f"Attention output shape: {output.shape}")
        return output


class FeedForward(nn.Module):
    def __init__(self, n_embd, dropout):
        super().__init__()
        self.net = nn.Sequential(
            BitLinearNew(n_embd, 4 * n_embd), nn.ReLU(), nn.Dropout(dropout), BitLinearNew(4 * n_embd, n_embd)
        )
        logger.debug(f"Initialized FeedForward with n_embd={n_embd}")

    def forward(self, x):
        if not torch._dynamo.is_compiling():
            logger.debug(f"FeedForward input shape: {x.shape}")
        output = self.net(x)
        if not torch._dynamo.is_compiling():
            logger.debug(f"FeedForward output shape: {output.shape}")
        return output


class TransformerBlock(nn.Module):
    def __init__(self, n_embd, n_head, dropout):
        super().__init__()
        self.attention = Attention(n_embd, n_head, dropout)
        self.feed_forward = FeedForward(n_embd, dropout)
        self.ln1 = nn.LayerNorm(n_embd)
        self.ln2 = nn.LayerNorm(n_embd)
        self.dropout = nn.Dropout(dropout)
        logger.debug(
            f"Initialized TransformerBlock with n_embd={n_embd}, n_head={n_head}"
        )

    def forward(self, x, mask=None):
        if not torch._dynamo.is_compiling():
            logger.debug(f"TransformerBlock input shape: {x.shape}")
        # Attention sublayer with residual dropout
        attn_output = self.attention(self.ln1(x), mask)
        attn_output = self.dropout(attn_output)  # Apply dropout
        x = x + attn_output

        # Feed-forward sublayer with residual dropout
        ff_output = self.feed_forward(self.ln2(x))
        ff_output = self.dropout(ff_output)  # Apply dropout
        x = x + ff_output
        if not torch._dynamo.is_compiling():
            logger.debug(f"TransformerBlock output shape: {x.shape}")
        logger.debug(f"Final output shape: {x.shape}")
        return x


class MambaLayer(nn.Module):
    def __init__(self, n_embd, d_state, d_conv, dropout, depth, expand):
        super().__init__()
        self.dropout = nn.Dropout(dropout)
        self.mamba_block = MambaBlock(
            dim=n_embd,
            depth=depth,           # Use depth from config
            d_state=d_state,
            d_conv=d_conv,
            expand=expand,         # Use expand from config
            conv_bias=True,        # Default value
            bias=False             # Default value
        )
        self.layer_norm = nn.LayerNorm(n_embd)
        logger.debug(
            f"Initialized MambaLayer with n_embd={n_embd}, d_state={d_state}, d_conv={d_conv}, dropout={dropout}"
        )

    def forward(self, x):
        if not torch._dynamo.is_compiling():
            logger.debug(f"MambaLayer input shape: {x.shape}")
        x_norm = self.layer_norm(x)
        x_mamba = self.mamba_block(x_norm)
        x_mamba = self.dropout(x_mamba)
        output = x + x_mamba
        if not torch._dynamo.is_compiling():
            logger.debug(f"MambaLayer output shape: {output.shape}")
        return output



class GPT2ARC(pl.LightningModule):
    def __init__(self, config: Config, num_classes: int, symbol_freq: Optional[Dict[int, float]] = None, pad_symbol_idx: int = 10):
        super().__init__()
        self.example_input_array = torch.zeros(1, 1, 6, 6)  # Adjust dimensions as needed
        self.config = config
        self.symbol_freq = symbol_freq if symbol_freq is not None else {}
        self.pad_symbol_idx = pad_symbol_idx  # Add this line
        self.include_pad_in_loss = self.config.training.include_pad_in_loss  # Reintroduced
        self.conv1 = nn.Conv2d(
            in_channels=1,
            out_channels=self.config.model.n_embd,  # Accessing the 'model' attribute within Config
            kernel_size=3,
            padding=1
        ).to(torch.float32)
        # Initialize blocks with interleaved TransformerBlocks and MambaLayer(s)
        self.blocks = nn.ModuleList()
        num_transformer_blocks = self.config.model.n_layer
        total_mamba_layers = int(num_transformer_blocks * self.config.model.mamba_ratio)
        
        logger.debug(f"Total TransformerBlocks: {num_transformer_blocks}")
        logger.debug(f"Total MambaLayers to add: {total_mamba_layers}")

        # Distribute MambaLayers across TransformerBlocks
        mamba_layer_positions = []
        if total_mamba_layers &gt; 0:
            step = num_transformer_blocks / total_mamba_layers
            mamba_layer_positions = [int(i * step) for i in range(total_mamba_layers)]

        current_mamba_index = 0
        for layer_idx in range(num_transformer_blocks):
            # Add a TransformerBlock
            self.blocks.append(TransformerBlock(self.config.model.n_embd, self.config.model.n_head, self.config.model.dropout))
            logger.debug(f"Layer {len(self.blocks)}: Added TransformerBlock")

            # Check if we should add a MambaLayer after this TransformerBlock
            if current_mamba_index &lt; len(mamba_layer_positions) and layer_idx == mamba_layer_positions[current_mamba_index]:
                # Add a MambaLayer
                self.blocks.append(
                    MambaLayer(
                        n_embd=self.config.model.n_embd,
                        d_state=self.config.model.d_state,
                        d_conv=self.config.model.d_conv,
                        dropout=self.config.model.dropout,
                        depth=self.config.model.mamba_depth,
                        expand=self.config.model.mamba_expand
                    )
                )
                logger.debug(f"Layer {len(self.blocks)}: Added MambaLayer after TransformerBlock {layer_idx + 1}")
                current_mamba_index += 1
        self.ln_f = nn.LayerNorm(self.config.model.n_embd)
        assert isinstance(self.config.model.n_embd, int), "model.n_embd must be an integer"
        assert isinstance(num_classes, int), "num_classes must be an integer"
        self.fc_out = BitLinearNew(int(self.config.model.n_embd), num_classes)  # Use num_classes directly

        # Initialize loss function with class weights if needed
        if self.symbol_freq:
            # Create a tensor of class weights based on symbol frequencies
            class_weights = torch.tensor([self.symbol_freq.get(i, 1.0) for i in range(num_classes)], dtype=torch.float32)
            class_weights = class_weights.to(self.device)  # Ensure weights are on the correct device
            self.loss_fn = nn.CrossEntropyLoss(weight=class_weights, ignore_index=self.pad_symbol_idx)
        else:
            self.loss_fn = nn.CrossEntropyLoss(ignore_index=self.pad_symbol_idx)
        

        # Initialize weights
        self.apply(self._init_weights)

    def _init_weights(self, module):
        if isinstance(module, nn.Conv2d):
            # Calculate fan_in for Conv2d
            fan_in = module.in_channels * module.kernel_size[0] * module.kernel_size[1]
            std = 1.0 / fan_in**0.5
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        elif isinstance(module, BitLinearNew):
            fan_in = module.in_features
            std = 1.0 / fan_in**0.5
            init.normal_(module.weight, mean=0.0, std=std)
            if module.bias is not None:
                init.zeros_(module.bias)
        # No initialization for nn.LayerNorm, using default

    def training_step(self, batch, batch_idx):
        inputs, targets, _ = batch
        outputs = self(inputs)
        loss = self.loss_fn(outputs.view(-1, self.config.training.num_classes), targets.view(-1))
        
        preds = torch.argmax(outputs, dim=-1)
        
        # Accuracy including padding
        correct_with_pad = (preds == targets).float()
        total_with_pad = torch.numel(targets)
        acc_with_pad = correct_with_pad.sum() / total_with_pad if total_with_pad &gt; 0 else torch.tensor(0.0)
        
        # Accuracy excluding padding
        mask = targets != self.pad_symbol_idx
        correct_without_pad = (preds == targets).float() * mask
        total_without_pad = mask.sum()
        acc_without_pad = correct_without_pad.sum() / total_without_pad if total_without_pad &gt; 0 else torch.tensor(0.0)
        
        # Log both accuracies: with padding and without padding
        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log('train_acc_with_pad', acc_with_pad, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        self.log('train_acc_without_pad', acc_without_pad, on_step=True, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def test_dataloader(self):
        # Initialize the test dataset
        test_dataset = ARCDataset(
            data_source=self.config.test_data_path,  # Ensure this path is correctly set in your configuration
            is_test=True,
            num_symbols=self.config.training.num_symbols,
            pad_symbol_idx=self.config.training.pad_symbol_idx,
            symbol_freq=self.config.training.symbol_freq,
            debug=self.config.debug
        )
        
        # Create and return the DataLoader
        logger.debug("Entering GPT2ARC.test_dataloader")
        dataloader = DataLoader(
            test_dataset,
            batch_size=self.config.training.batch_size,
            num_workers=self.config.training.num_workers,
            shuffle=False,  # Typically, shuffling is not needed for test data
            pin_memory=self.config.training.use_gpu  # Optimize memory usage based on GPU availability
        )

    def validation_step(self, batch, batch_idx):
        inputs, targets, _ = batch
        outputs = self(inputs)
        loss = self.loss_fn(outputs.view(-1, self.config.training.num_classes), targets.view(-1))
        
        preds = torch.argmax(outputs, dim=-1)
        
        # Accuracy including padding
        correct_with_pad = (preds == targets).float()
        total_with_pad = torch.numel(targets)
        acc_with_pad = correct_with_pad.sum() / total_with_pad if total_with_pad &gt; 0 else torch.tensor(0.0)
        
        # Accuracy excluding padding
        mask = targets != self.pad_symbol_idx
        correct_without_pad = (preds == targets).float() * mask
        total_without_pad = mask.sum()
        acc_without_pad = correct_without_pad.sum() / total_without_pad if total_without_pad &gt; 0 else torch.tensor(0.0)
        
        # Log both accuracies
        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('val_acc_with_pad', acc_with_pad, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('val_acc_without_pad', acc_without_pad, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return loss

    def test_step(self, batch, batch_idx):
        inputs, targets, _ = batch
        outputs = self(inputs)
        loss = self.loss_fn(outputs.view(-1, self.config.training.num_classes), targets.view(-1))
        
        preds = torch.argmax(outputs, dim=-1)
        
        # Accuracy including padding
        correct_with_pad = (preds == targets).float()
        total_with_pad = torch.numel(targets)
        acc_with_pad = correct_with_pad.sum() / total_with_pad if total_with_pad &gt; 0 else torch.tensor(0.0)
        
        # Accuracy excluding padding
        mask = targets != self.pad_symbol_idx
        correct_without_pad = (preds == targets).float() * mask
        total_without_pad = mask.sum()
        acc_without_pad = correct_without_pad.sum() / total_without_pad if total_without_pad &gt; 0 else torch.tensor(0.0)
        
        # Log both accuracies
        self.log('test_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('test_acc_with_pad', acc_with_pad, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        self.log('test_acc_without_pad', acc_without_pad, on_step=False, on_epoch=True, prog_bar=True, logger=True)
        return loss
    
    def forward(self, input_ids, attention_mask=None):
        logger.debug(f"GPT2ARC forward - Input shape: {input_ids.shape}, dtype: {input_ids.dtype}")
        
        if input_ids.dim() == 4:
            x = input_ids.float()
        else:
            batch_size = input_ids.size(0)
            seq_length = input_ids.size(1)
            height = width = int(seq_length ** 0.5)
            x = input_ids.float().view(batch_size, 1, height, width)
        
        x = self.conv1(x)
        logger.debug(f"After conv1 shape: {x.shape}")
        b, c, h, w = x.size()
        x = x.view(b, c, h * w)
        x = x.permute(0, 2, 1)
        logger.debug(f"Reshaped for transformer blocks: {x.shape}")

        for i, block in enumerate(self.blocks):
            if isinstance(block, TransformerBlock):
                x = block(x, attention_mask)
                logger.debug(f"After TransformerBlock {i + 1}: shape {x.shape}")
            else:
                x = block(x)
                logger.debug(f"After MambaLayer {i + 1}: shape {x.shape}")
        
        x = self.ln_f(x)
        x = self.fc_out(x)
        logger.debug(f"GPT2ARC forward - Final output shape: {x.shape}, dtype: {x.dtype}")
        return x

</file>
</source>