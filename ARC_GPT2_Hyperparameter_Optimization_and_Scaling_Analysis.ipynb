{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8-QMJgz1LmwE",
      "metadata": {
        "id": "8-QMJgz1LmwE"
      },
      "outputs": [],
      "source": [
        "#Required: https://youtu.be/IDxrMbXPVTA?si=PHfGry-HQj__3Xne\n",
        "# https://ngrok.com/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0L9IwEdxzd-H",
      "metadata": {
        "id": "0L9IwEdxzd-H"
      },
      "source": [
        "# STUFF YOU CHANGE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6okItopOumX_",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6okItopOumX_",
        "outputId": "44391386-b0da-4dee-e405-e4fb2f621a4a"
      },
      "outputs": [],
      "source": [
        "# ========== User-Defined Parameters (Top of Notebook) ==========\n",
        "\n",
        "dev_mode = True  # Set to True for development mode (Miguel's coding machine)\n",
        "\n",
        "\n",
        "# Replace with your actual authtoken\n",
        "ngrok_auth_token = \"2NCEuxuUBMj6zsdTokQHkYJ4AZz_3E7e2pyW87otgFg3UdSC3\"\n",
        "\n",
        "# 1. Base directory for storing date-based experiment result folders.\n",
        "import os\n",
        "\n",
        "# Define the base directory for the arc-neural-reasoning-model\n",
        "arc_model_dir = \"/content/arc-neural-reasoning-model/\"\n",
        "parent_dir = \"/content/drive/MyDrive/ArcGPT/\"\n",
        "#parent_dir = \"/content/\"\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"ARC Model Directory: {arc_model_dir}\")\n",
        "\n",
        "# 2. Boolean flag to choose between best hyperparameters from previous experiments (True) or manual parameters (False).\n",
        "use_best_params = True  # Set to False to use manual parameters\n",
        "perform_hyperparameter_tuning = True  # Set to True to perform hyperparameter tuning\n",
        "\n",
        "# 3. Dictionary of manually set hyperparameters used when use_best_params is False.\n",
        "#    Includes model architecture and training settings.\n",
        "manual_params = {\n",
        "    \"n_embd\": 4,     # Embedding dimension\n",
        "    \"n_head\": 4,       # Number of attention heads\n",
        "    \"n_layer\": 5,     # Number of transformer layers\n",
        "    \"batch_size\": 15,  # Batch size for training\n",
        "    \"learning_rate\": 0.005936106784234055,  # Learning rate\n",
        "    \"max_epochs\": 50   # Maximum number of epochs for training\n",
        "}\n",
        "\n",
        "# Hyperparameter tuning search space settings\n",
        "# 4. Number of Optuna trials for hyperparameter tuning.\n",
        "#    More trials can lead to better optimization but increase computation time.\n",
        "n_trials = 100\n",
        "\n",
        "# 5. Range for embedding dimension in hyperparameter search space.\n",
        "n_embd_min, n_embd_max = 64, 4096\n",
        "\n",
        "# 6. Range for number of attention heads in transformer model during tuning.\n",
        "n_head_min, n_head_max = 2, 16\n",
        "\n",
        "# 7. Range for number of transformer layers in model architecture during optimization.\n",
        "n_layer_min, n_layer_max = 2, 16\n",
        "\n",
        "# 8. Range of batch sizes to explore. Larger batches can speed up training but may require more memory.\n",
        "batch_size_min, batch_size_max = 1, 40\n",
        "\n",
        "# 9. Range for learning rate in hyperparameter search space. Crucial for model convergence and performance.\n",
        "learning_rate_min, learning_rate_max = 1e-4, 1e-1\n",
        "\n",
        "# 10. Range for number of training epochs to consider during hyperparameter optimization.\n",
        "max_epochs_min, max_epochs_max = 15, 40\n",
        "\n",
        "# 11. Range for n_head exponent in hyperparameter search space.\n",
        "n_head_exp_min, n_head_exp_max = 1, 10\n",
        "\n",
        "# 12. Range for n_embd multiplier in hyperparameter search space.\n",
        "n_embd_multiplier_min, n_embd_multiplier_max = 4, 256\n",
        "\n",
        "# These parameters allow flexible experimentation with different model configurations and training settings,\n",
        "# enabling comprehensive exploration of the hyperparameter space for optimal model performance.\n",
        "\n",
        "# Validate manual parameters\n",
        "def validate_manual_params(params):\n",
        "    assert params[\"n_embd\"] % params[\"n_head\"] == 0, f\"n_embd ({params['n_embd']}) must be divisible by n_head ({params['n_head']})\"\n",
        "    assert params[\"n_embd\"] >= params[\"n_head\"], f\"n_embd ({params['n_embd']}) must be greater than or equal to n_head ({params['n_head']})\"\n",
        "    assert params[\"n_layer\"] > 0, f\"n_layer ({params['n_layer']}) must be positive\"\n",
        "    print(\"Manual parameters validated successfully\")\n",
        "\n",
        "\n",
        "# Validate the manual parameters\n",
        "validate_manual_params(manual_params)\n",
        "\n",
        "# Print configurations for verification\n",
        "print(\"Current Configuration:\")\n",
        "print(f\"Parent Directory: {parent_dir}\")\n",
        "print(f\"Use Best Parameters: {use_best_params}\")\n",
        "print(f\"Manual Parameters: {manual_params}\")\n",
        "print(f\"Number of Optuna Trials: {n_trials}\")\n",
        "print(\"Hyperparameter Ranges:\")\n",
        "print(f\"  n_embd: {n_embd_min} to {n_embd_max}\")\n",
        "print(f\"  n_head: {n_head_min} to {n_head_max}\")\n",
        "print(f\"  n_layer: {n_layer_min} to {n_layer_max}\")\n",
        "print(f\"  batch_size: {batch_size_min} to {batch_size_max}\")\n",
        "print(f\"  learning_rate: {learning_rate_min} to {learning_rate_max}\")\n",
        "print(f\"  max_epochs: {max_epochs_min} to {max_epochs_max}\")\n",
        "\n",
        "# Save configuration\n",
        "import json\n",
        "\n",
        "def save_config(config, filename=\"config.json\"):\n",
        "    full_path = os.path.join(parent_dir, filename)\n",
        "    os.makedirs(parent_dir, exist_ok=True)\n",
        "    with open(full_path, 'w') as f:\n",
        "        json.dump(config, f, indent=4)\n",
        "\n",
        "config = {\n",
        "    \"parent_dir\": parent_dir,\n",
        "    \"use_best_params\": use_best_params,\n",
        "    \"manual_params\": manual_params,\n",
        "    \"tuning\": {\n",
        "        \"n_trials\": n_trials,\n",
        "        \"n_embd\": (n_embd_min, n_embd_max),\n",
        "        \"n_head\": (n_head_min, n_head_max),\n",
        "        \"n_layer\": (n_layer_min, n_layer_max),\n",
        "        \"batch_size\": (batch_size_min, batch_size_max),\n",
        "        \"learning_rate\": (learning_rate_min, learning_rate_max),\n",
        "        \"max_epochs\": (max_epochs_min, max_epochs_max)\n",
        "    }\n",
        "}\n",
        "\n",
        "# Check GPU availability\n",
        "import torch\n",
        "print(\"\\nGPU Available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU Name:\", torch.cuda.get_device_name(0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2-qicsswzi-S",
      "metadata": {
        "id": "2-qicsswzi-S"
      },
      "source": [
        "# CODE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DgxlLFBTg8Kt",
      "metadata": {
        "id": "DgxlLFBTg8Kt"
      },
      "source": [
        "### 1. Set up the Colab environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u8k0Yxzd4RCb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "u8k0Yxzd4RCb",
        "outputId": "e77118cc-921d-4b4d-f4b4-8fb0a72aabda"
      },
      "outputs": [],
      "source": [
        "if dev_mode != True:\n",
        "    print(\"Setting up Colab environment...\")\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        save_config(config)\n",
        "        print(f\"Configuration saved to config.json\")\n",
        "\n",
        "    #general error\n",
        "    except:\n",
        "        print(\"Google Colab not detected. Skipping drive mount.\")\n",
        "\n",
        "    %cd /content/\n",
        "    !rm -rf /content/arc-neural-reasoning-model/\n",
        "    !git clone https://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@github.com/ImmortalDemonGod/arc-neural-reasoning-model.git\n",
        "    !pip install -r /content/arc-neural-reasoning-model/gpt2_arc/requirements.txt\n",
        "    !pip install optuna\n",
        "    !pip install torchsummary\n",
        "    !pip install jupyterlab jupyterlab-optuna\n",
        "    # Install the required packages\n",
        "    !pip install optuna-dashboard pyngrok\n",
        "    #!ngrok config add-authtoken 2NCEuxuUBMj6zsdTokQHkYJ4AZz_3E7e2pyW87otgFg3UdSC3\n",
        "    !pip install tensorboard\n",
        "    !pip install watchdog\n",
        "    !pip install numpy\n",
        "    !pip install --upgrade jax jaxlib torch pytorch_lightning\n",
        "    #!pip uninstall tensorflow -y\n",
        "    #!find . -type d -name \"__pycache__\" -exec rm -r {} +\n",
        "    !rm -rf /tmp/libtpu_lockfile\n",
        "    !rm -rf /content/arc-neural-reasoning-model/arc_sat_solver\n",
        "    !rm -rf /content/arc-neural-reasoning-model/benchmark_results\n",
        "    !rm -rf /content/arc-neural-reasoning-model/checkpoints\n",
        "    !rm -rf /content/arc-neural-reasoning-model/tmp\n",
        "    %cd /content/arc-neural-reasoning-model/\n",
        "    !pip install -e .\n",
        "else:\n",
        "    print(\"Development mode is enabled. Using Miguel's coding machine.\")\n",
        "    arc_model_dir = \"/workspaces/arc-neural-reasoning-model/\"\n",
        "    parent_dir = \"/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\"\n",
        "    %cd /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/\n",
        "    save_config(config)\n",
        "    print(f\"Configuration saved to config.json\")\n",
        "\n",
        "# Setup ngrok for remote access (ensure you have your authtoken configured)\n",
        "from pyngrok import ngrok\n",
        "ngrok.set_auth_token(ngrok_auth_token)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iz4dsrstg8Ku",
      "metadata": {
        "id": "Iz4dsrstg8Ku"
      },
      "source": [
        "### 2. Run hyperparameter tuning (in the background) click the **links** to see the dashboards:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6zIQhT9m9_wv",
      "metadata": {
        "id": "6zIQhT9m9_wv"
      },
      "source": [
        "make sure to set the directory in your drive to save the files in"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DVIRPnKcPV92",
      "metadata": {
        "id": "DVIRPnKcPV92"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Get the current date in YYYYMMDD format\n",
        "current_date = datetime.now().strftime('%Y%m%d')\n",
        "\n",
        "\n",
        "# Create a folder named after the current date\n",
        "date_folder = os.path.join(parent_dir, current_date)\n",
        "if not os.path.exists(date_folder):\n",
        "    os.makedirs(date_folder)\n",
        "\n",
        "# Change into the newly created folder\n",
        "%cd {date_folder}\n",
        "\n",
        "# Now all your operations will save to /content/YYYYMMDD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sV0auRJ4OojK",
      "metadata": {
        "id": "sV0auRJ4OojK"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import os\n",
        "import time\n",
        "from pyngrok import ngrok\n",
        "from datetime import datetime\n",
        "import sys\n",
        "\n",
        "# Install TensorBoard if not already installed\n",
        "!pip install tensorboard\n",
        "\n",
        "# Function to kill processes by name\n",
        "def kill_process_by_name(process_name):\n",
        "    try:\n",
        "        # Find and kill the process using pkill (Linux/Unix/Mac) or taskkill (Windows)\n",
        "        if os.name == \"posix\":  # For Unix-based systems\n",
        "            subprocess.run([\"pkill\", \"-f\", process_name], check=True)\n",
        "        elif os.name == \"nt\":  # For Windows systems\n",
        "            subprocess.run([\"taskkill\", \"/IM\", process_name, \"/F\"], check=True)\n",
        "        print(f\"Successfully terminated any running {process_name} processes.\")\n",
        "    except subprocess.CalledProcessError:\n",
        "        print(f\"No {process_name} processes were found running.\")\n",
        "\n",
        "# Kill existing Optuna, TensorBoard, and ngrok processes\n",
        "kill_process_by_name(\"tensorboard\")\n",
        "kill_process_by_name(\"ngrok\")\n",
        "kill_process_by_name(\"optuna-dashboard\")\n",
        "\n",
        "# Start ngrok tunnel for TensorBoard\n",
        "print(\"Setting up ngrok tunnel for TensorBoard...\")\n",
        "public_url_tb = ngrok.connect(6006)\n",
        "print(f\"TensorBoard ngrok tunnel is accessible at: {public_url_tb}\")\n",
        "\n",
        "# Start ngrok tunnel for Optuna dashboard\n",
        "print(\"Setting up ngrok tunnel for Optuna dashboard...\")\n",
        "public_url_optuna = ngrok.connect(8081)\n",
        "print(f\"Optuna dashboard ngrok tunnel is accessible at: {public_url_optuna}\")\n",
        "\n",
        "# Define the log directory\n",
        "log_dir = os.path.join(date_folder, \"runs\")\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "# Now run TensorBoard in the background using Python module execution\n",
        "# Use 'tensorboard.main' to correctly invoke TensorBoard\n",
        "tensorboard_command = [\n",
        "    sys.executable, \"-m\", \"tensorboard.main\", \"--logdir\", log_dir, \"--port\", \"6006\"\n",
        "]\n",
        "print(\"Starting TensorBoard...\")\n",
        "subprocess.Popen(tensorboard_command)  # Run TensorBoard without blocking\n",
        "\n",
        "# Wait for TensorBoard to start\n",
        "time.sleep(10)\n",
        "\n",
        "print(\"\\nAll processes launched successfully. You can access them via ngrok URLs:\")\n",
        "print(f\"📊 TensorBoard: {public_url_tb}\")\n",
        "print(f\"📈 Optuna Dashboard: {public_url_optuna}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wV89eu4FipY8",
      "metadata": {
        "id": "wV89eu4FipY8"
      },
      "outputs": [],
      "source": [
        "if perform_hyperparameter_tuning:\n",
        "    print(\"Searching for best hyperparameters...\")\n",
        "    # Run the optimization command in a separate subprocess (so it's non-blocking too, if desired)\n",
        "    print(\"Starting hyperparameter tuning...\")\n",
        "    optimize_command = [\n",
        "        \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/optimize_hyperparameters.py\"),\n",
        "        \"--n_trials\", str(n_trials),\n",
        "        \"--storage\", f\"sqlite:///{date_folder}/optuna_results.db\",\n",
        "        \"--n_jobs\", \"1\",  # Use all available cores\n",
        "        \"--n_embd_min\", str(n_embd_min), \"--n_embd_max\", str(n_embd_max),\n",
        "        \"--n_head_min\", str(n_head_min), \"--n_head_max\", str(n_head_max),\n",
        "        \"--n_layer_min\", str(n_layer_min), \"--n_layer_max\", str(n_layer_max),\n",
        "        \"--batch_size_min\", str(batch_size_min), \"--batch_size_max\", str(batch_size_max),\n",
        "        \"--learning_rate_min\", str(learning_rate_min), \"--learning_rate_max\", str(learning_rate_max),\n",
        "        \"--max_epochs_min\", str(max_epochs_min), \"--max_epochs_max\", str(max_epochs_max),\n",
        "        \"--n_head_exp_min\", str(n_head_exp_min), \"--n_head_exp_max\", str(n_head_exp_max),\n",
        "        \"--n_embd_multiplier_min\", str(n_embd_multiplier_min), \"--n_embd_multiplier_max\",\n",
        "    str(n_embd_multiplier_max)\n",
        "    ]\n",
        "    subprocess.Popen(optimize_command)\n",
        "\n",
        "    # Start the Optuna dashboard after the optimization begins\n",
        "    print(\"Starting Optuna dashboard...\")\n",
        "    optuna_command = [\n",
        "        \"optuna-dashboard\", \"--port\", \"8081\", f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "    ]\n",
        "    subprocess.Popen(optuna_command)\n",
        "\n",
        "    print(\"All processes launched successfully. You can access them via ngrok URLs.\")\n",
        "else:\n",
        "    print(\"Hyperparameter tuning not performed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gb75DkIFg8Kw",
      "metadata": {
        "id": "gb75DkIFg8Kw"
      },
      "source": [
        "### 3. Get the best hyperparameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda44504",
      "metadata": {
        "id": "fda44504"
      },
      "outputs": [],
      "source": [
        "import optuna\n",
        "import numpy as np\n",
        "\n",
        "# Set Optuna storage and study details\n",
        "#storage_name = f\"sqlite:///{date_folder}/optuna_results.db\"\n",
        "storage_name = \"sqlite:////workspaces/arc-neural-reasoning-model/optuna_results.db\"\n",
        "study_name = \"gpt2_arc_optimization\"\n",
        "print(f\"Storage Name: {storage_name}\")\n",
        "print(f\"Study Name: {study_name}\")\n",
        "\n",
        "if use_best_params:\n",
        "    try:\n",
        "        # List all study names in the database\n",
        "        study_summaries = optuna.study.get_all_study_summaries(storage=storage_name)\n",
        "        print(\"Available studies in the database:\")\n",
        "        for study_summary in study_summaries:\n",
        "            print(f\"- {study_summary.study_name}\")\n",
        "\n",
        "        # Load the specified study\n",
        "        study = optuna.load_study(study_name=study_name, storage=storage_name)\n",
        "        best_params = study.best_params\n",
        "        print(\"Best hyperparameters:\")\n",
        "        print(json.dumps(best_params, indent=2))\n",
        "\n",
        "        # Save the best parameters to a JSON file\n",
        "        with open(f\"{date_folder}/best_hyperparameters.json\", \"w\") as f:\n",
        "            json.dump(best_params, f)\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(\"Error: The specified study does not exist in the database. Please ensure that the study name and storage path are correct.\")\n",
        "        print(f\"Details: {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5062c297",
      "metadata": {
        "id": "5062c297"
      },
      "source": [
        "### 4. Setup Evaluation of the trained model in the background:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0pwrd3HMBnhW",
      "metadata": {
        "id": "0pwrd3HMBnhW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checking directory for .ckpt and .pth files: /workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240928/checkpoints\n",
            "Found checkpoint files: ['/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240928/checkpoints/arc_model-epoch=00-val_loss=0.37.ckpt']\n",
            "Current models: {'/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240928/checkpoints/arc_model-epoch=00-val_loss=0.37.ckpt'}\n",
            "New models to evaluate: {'/workspaces/arc-neural-reasoning-model/EXPERIMENTAL/20240928/checkpoints/arc_model-epoch=00-val_loss=0.37.ckpt'}\n",
            "Skipping already evaluated model: arc_model-epoch=00-val_loss=0.37.ckpt\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import wandb\n",
        "import time\n",
        "from watchdog.observers import Observer\n",
        "from watchdog.events import FileSystemEventHandler\n",
        "import subprocess\n",
        "import threading\n",
        "from datetime import datetime\n",
        "\n",
        "# Set W&B API key (replace with your actual API key)\n",
        "wandb_api_key = \"2b06e99af167044b281668f6edd388c633aba1a0\"  # Replace with your W&B API key\n",
        "os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "\n",
        "print(f\"arc_model_dir is set to: {arc_model_dir}\")\n",
        "\n",
        "# Directory containing the model files\n",
        "model_dir = os.path.join(date_folder, \"checkpoints\")\n",
        "print(f\"Watching for new models in directory: {model_dir}\")\n",
        "\n",
        "# Create the model_dir if it doesn't exist\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "output_dir = \"evaluation_results\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "wandb_project = \"arc-evaluation\"\n",
        "\n",
        "# Set of evaluated models\n",
        "evaluated_models = set()\n",
        "\n",
        "# Load previously evaluated models from a file\n",
        "evaluated_models_file = os.path.join(output_dir, \"evaluated_models.txt\")\n",
        "if os.path.exists(evaluated_models_file):\n",
        "    with open(evaluated_models_file, \"r\") as f:\n",
        "        evaluated_models.update(line.strip() for line in f)\n",
        "    print(f\"Loaded evaluated models from {evaluated_models_file}\")\n",
        "else:\n",
        "    print(f\"No previously evaluated models found. Starting fresh.\")\n",
        "\n",
        "class CheckpointHandler(FileSystemEventHandler):\n",
        "    def on_created(self, event):\n",
        "        if event.is_directory:\n",
        "            return\n",
        "        if event.src_path.endswith('.ckpt') or event.src_path.endswith('.pth'):\n",
        "            print(f\"New checkpoint detected: {event.src_path}\")\n",
        "            self.evaluate_model(event.src_path)\n",
        "\n",
        "    def evaluate_model(self, model_path):\n",
        "        model_file = os.path.basename(model_path)\n",
        "\n",
        "        if model_file in evaluated_models:\n",
        "            print(f\"Skipping already evaluated model: {model_file}\")\n",
        "            return  # Skip if the model was already evaluated\n",
        "\n",
        "        # Extract epoch and val_loss from the filename for run_name\n",
        "        try:\n",
        "            parts = model_file.replace('.ckpt', '').replace('.pth', '').split('-')\n",
        "            epoch = None\n",
        "            val_loss = None\n",
        "            for part in parts:\n",
        "                if part.startswith('epoch='):\n",
        "                    epoch = part.split('=')[1]\n",
        "                elif part.startswith('val_loss='):\n",
        "                    val_loss = part.split('=')[1]\n",
        "            if epoch is not None and val_loss is not None:\n",
        "                run_name = f\"scaling-test-evaluation-epoch{epoch}-val_loss{val_loss}\"\n",
        "            else:\n",
        "                run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "        except Exception as e:\n",
        "            print(f\"Error parsing run name from filename {model_file}: {e}\")\n",
        "            run_name = f\"scaling-test-evaluation-{model_file}\"\n",
        "\n",
        "        eval_command = [\n",
        "            \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/evaluate.py\"),\n",
        "            \"--model_checkpoint\", model_path,\n",
        "            \"--batch_size\", \"32\",\n",
        "            \"--output_dir\", output_dir,\n",
        "            \"--wandb_project\", wandb_project,\n",
        "            \"--wandb_run_name\", run_name\n",
        "        ]\n",
        "        print(f\"Evaluating model: {model_file} with command: {' '.join(eval_command)}\")\n",
        "\n",
        "        try:\n",
        "            # Run the evaluation command and capture stdout and stderr\n",
        "            result = subprocess.run(\n",
        "                eval_command,\n",
        "                check=True,\n",
        "                stdout=subprocess.PIPE,\n",
        "                stderr=subprocess.PIPE,\n",
        "                text=True  # Automatically decode bytes to string\n",
        "            )\n",
        "            print(f\"Successfully evaluated model: {model_file}\")\n",
        "            print(\"Evaluation Output:\")\n",
        "            print(result.stdout)  # Print the standard output from evaluate.py\n",
        "            if result.stderr:\n",
        "                print(\"Evaluation Errors/Warnings:\")\n",
        "                print(result.stderr)  # Print any errors or warnings from evaluate.py\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error during evaluation of {model_file}: {e}\")\n",
        "            print(\"Standard Output:\")\n",
        "            print(e.stdout)\n",
        "            print(\"Standard Error:\")\n",
        "            print(e.stderr)\n",
        "        except Exception as ex:\n",
        "            print(f\"An unexpected error occurred while evaluating {model_file}: {ex}\")\n",
        "\n",
        "        evaluated_models.add(model_file)\n",
        "\n",
        "        # Save the evaluated model to the file\n",
        "        with open(evaluated_models_file, \"a\") as f:\n",
        "            f.write(model_file + \"\\n\")\n",
        "\n",
        "def get_all_checkpoint_files(directory):\n",
        "    print(f\"Checking directory for .ckpt and .pth files: {directory}\")\n",
        "    checkpoint_files = []\n",
        "    for root, _, files in os.walk(directory):\n",
        "        checkpoint_files.extend([os.path.join(root, f) for f in files if f.endswith('.ckpt') or f.endswith('.pth')])\n",
        "    print(f\"Found checkpoint files: {checkpoint_files}\")\n",
        "    return checkpoint_files\n",
        "\n",
        "def start_observer():\n",
        "    # Set up and start the watchdog observer\n",
        "    event_handler = CheckpointHandler()\n",
        "    observer = Observer()\n",
        "    observer.schedule(event_handler, model_dir, recursive=True)\n",
        "    observer.start()\n",
        "\n",
        "    print(\"Watching for new checkpoints and final models in all subdirectories...\")\n",
        "    print(\"This script will continue running in the background.\")\n",
        "\n",
        "    try:\n",
        "        while True:\n",
        "            time.sleep(10)\n",
        "            # Check for any new models\n",
        "            current_models = set(get_all_checkpoint_files(model_dir))\n",
        "            new_models = current_models - evaluated_models\n",
        "\n",
        "            print(f\"Current models: {current_models}\")\n",
        "            print(f\"New models to evaluate: {new_models}\")\n",
        "\n",
        "            for model_path in new_models:\n",
        "                event_handler.evaluate_model(model_path)\n",
        "    except KeyboardInterrupt:\n",
        "        observer.stop()\n",
        "        print(\"Observer stopped by user.\")\n",
        "    except FileNotFoundError as fnf_error:\n",
        "        print(f\"FileNotFoundError: {fnf_error}\")\n",
        "        print(f\"Please ensure that the directory '{model_dir}' exists.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred in the observer: {e}\")\n",
        "    finally:\n",
        "        observer.join()\n",
        "        print(\"Checkpoint and final model evaluation completed.\")\n",
        "\n",
        "# Function to start the observer in a background thread\n",
        "def run_observer():\n",
        "    observer_thread = threading.Thread(target=start_observer)\n",
        "    observer_thread.daemon = True  # Ensures the thread will exit when the main program exits\n",
        "    observer_thread.start()\n",
        "    print(\"Background checkpoint observer started.\")\n",
        "\n",
        "# Start the observer\n",
        "run_observer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2ba232c",
      "metadata": {
        "id": "f2ba232c"
      },
      "source": [
        "### 5. Use the best hyperparameters for longer training (manually set max epochs!):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "BWmLkQeNsxiX",
      "metadata": {
        "id": "BWmLkQeNsxiX"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import subprocess\n",
        "import os\n",
        "import math\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m8uLi0U5l6QL",
      "metadata": {
        "id": "m8uLi0U5l6QL"
      },
      "outputs": [],
      "source": [
        "# Load best hyperparameters from the JSON file if use_best_params is True\n",
        "if use_best_params:\n",
        "    try:\n",
        "        with open(f\"{date_folder}/best_hyperparameters.json\", \"r\") as f:\n",
        "            best_params = json.load(f)\n",
        "        print(\"Loaded best hyperparameters from JSON.\")\n",
        "\n",
        "        # Check if 'n_head_exp' and 'n_embd_multiplier' are present\n",
        "        if 'n_head_exp' in best_params and 'n_embd_multiplier' in best_params:\n",
        "            # Convert exponent to actual n_head\n",
        "            n_head_exp = best_params['n_head_exp']\n",
        "            n_head = 2 ** n_head_exp\n",
        "            # Convert multiplier to actual n_embd\n",
        "            n_embd_multiplier = best_params['n_embd_multiplier']\n",
        "            n_embd = n_head * n_embd_multiplier\n",
        "            # Ensure n_embd is a power of 2\n",
        "            n_embd = 2 ** int(math.log2(n_embd))\n",
        "            print(f\"Converted n_head_exp: {n_head_exp} to n_head: {n_head}\")\n",
        "            print(f\"Converted n_embd_multiplier: {n_embd_multiplier} to n_embd: {n_embd}\")\n",
        "        else:\n",
        "            # If conversion parameters are not present, use manual values or existing best_params\n",
        "            n_head = best_params.get(\"n_head\", manual_params[\"n_head\"])\n",
        "            n_embd = best_params.get(\"n_embd\", manual_params[\"n_embd\"])\n",
        "            print(\"n_head_exp and/or n_embd_multiplier not found in best_params. Using existing n_head and n_embd.\")\n",
        "\n",
        "        # Extract other hyperparameters but OVERRIDE max_epochs with manual setting\n",
        "        params = {\n",
        "            \"n_embd\": n_embd,\n",
        "            \"n_head\": n_head,\n",
        "            \"n_layer\": best_params.get(\"n_layer\", manual_params[\"n_layer\"]),\n",
        "            \"batch_size\": best_params.get(\"batch_size\", manual_params[\"batch_size\"]),\n",
        "            \"learning_rate\": best_params.get(\"learning_rate\", manual_params[\"learning_rate\"]),\n",
        "            \"max_epochs\": manual_params[\"max_epochs\"]  # Override max_epochs with manual setting\n",
        "        }\n",
        "        print(f\"Final Parameters for Training: {params}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Warning: {date_folder}/best_hyperparameters.json not found. Using manual parameters.\")\n",
        "        params = manual_params\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading best hyperparameters: {str(e)}. Using manual parameters.\")\n",
        "        params = manual_params\n",
        "else:\n",
        "    # Use manually defined parameters\n",
        "    params = manual_params\n",
        "    print(\"Using manual hyperparameters.\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ysTJm45AtuzN",
      "metadata": {
        "id": "ysTJm45AtuzN"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Build the arguments for the training command\n",
        "train_args = [\n",
        "    \"python\", os.path.join(arc_model_dir, \"gpt2_arc/src/training/train.py\"),\n",
        "    \"--n-embd\", str(params[\"n_embd\"]),\n",
        "    \"--n-head\", str(params[\"n_head\"]),\n",
        "    \"--n-layer\", str(params[\"n_layer\"]),\n",
        "    \"--batch-size\", str(params[\"batch_size\"]),\n",
        "    \"--learning-rate\", str(params[\"learning_rate\"]),\n",
        "    \"--max-epochs\", str(params[\"max_epochs\"]),\n",
        "    \"--use-gpu\",\n",
        "    \"--project\", \"arc-scaling-test\"\n",
        "]\n",
        "\n",
        "# Notify the user that training is starting\n",
        "print(\"Starting training process...\")\n",
        "\n",
        "# Start the training process and stream output to the notebook cell\n",
        "process = subprocess.Popen(\n",
        "    train_args,\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True  # Ensures that the output is treated as text\n",
        ")\n",
        "\n",
        "# Stream the output in real-time\n",
        "try:\n",
        "    for line in process.stdout:\n",
        "        print(line, end='')  # Print each line as it's received\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nTraining interrupted by user.\")\n",
        "    process.terminate()\n",
        "    process.wait()\n",
        "\n",
        "# Wait for the process to complete and get the return code\n",
        "return_code = process.wait()\n",
        "if return_code == 0:\n",
        "    print(\"Training completed successfully.\")\n",
        "else:\n",
        "    print(f\"Training failed with return code {return_code}.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "L25zvW0ig8K1",
      "metadata": {
        "id": "L25zvW0ig8K1"
      },
      "source": [
        "### 6. Analyze the results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJhLC5Ttg8K1",
      "metadata": {
        "id": "mJhLC5Ttg8K1"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import glob\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# 1. Define the directory containing evaluation results\n",
        "results_dir = Path(\"./evaluation_results/\")\n",
        "\n",
        "# 2. Retrieve all JSON files\n",
        "json_files = list(results_dir.glob(\"*.json\"))\n",
        "print(f\"Found {len(json_files)} JSON files.\")\n",
        "\n",
        "# 3. Function to extract timestamp from filename\n",
        "def extract_timestamp(filename):\n",
        "    \"\"\"\n",
        "    Extracts the timestamp from the filename.\n",
        "    Expected format: ..._YYYYMMDD_HHMMSS.json\n",
        "    \"\"\"\n",
        "    pattern = r\"_(\\d{8}_\\d{6})\\.json$\"\n",
        "    match = re.search(pattern, filename)\n",
        "    if match:\n",
        "        return pd.to_datetime(match.group(1), format=\"%Y%m%d_%H%M%S\")\n",
        "    else:\n",
        "        return pd.NaT  # Not a Time if pattern doesn't match\n",
        "\n",
        "# 4. Load and compile aggregate results\n",
        "data = []\n",
        "\n",
        "for file in json_files:\n",
        "    with open(file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    # Extract aggregate results\n",
        "    aggregate = results.get(\"aggregate_results\", {})\n",
        "\n",
        "    # Extract timestamp from filename\n",
        "    timestamp = extract_timestamp(file.name)\n",
        "\n",
        "    # Combine data\n",
        "    record = {\n",
        "        \"timestamp\": timestamp,\n",
        "        \"test_loss\": aggregate.get(\"test_loss\"),\n",
        "        \"test_accuracy\": aggregate.get(\"test_accuracy\"),\n",
        "        \"test_diff_accuracy\": aggregate.get(\"test_diff_accuracy\"),\n",
        "        \"complete_task_accuracy\": aggregate.get(\"complete_task_accuracy\")\n",
        "    }\n",
        "\n",
        "    data.append(record)\n",
        "\n",
        "print(f\"Compiled {len(data)} records.\")\n",
        "\n",
        "# 5. Create DataFrame for aggregate results\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Drop records with missing timestamps\n",
        "df = df.dropna(subset=[\"timestamp\"])\n",
        "\n",
        "# Sort by timestamp\n",
        "df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "print(\"Aggregate Results DataFrame:\")\n",
        "print(df.head())\n",
        "\n",
        "# 6. (Optional) Handle individual metrics\n",
        "individual_data = []\n",
        "\n",
        "for file in json_files:\n",
        "    with open(file, \"r\") as f:\n",
        "        results = json.load(f)\n",
        "\n",
        "    # Extract individual metrics\n",
        "    individual = results.get(\"individual_metrics\", {})\n",
        "\n",
        "    # Extract timestamp from filename\n",
        "    timestamp = extract_timestamp(file.name)\n",
        "\n",
        "    for metric_id, metrics in individual.items():\n",
        "        record = {\n",
        "            \"timestamp\": timestamp,\n",
        "            \"metric_id\": metric_id,\n",
        "            \"test_accuracy\": metrics.get(\"test_accuracy\"),\n",
        "            \"test_diff_accuracy\": metrics.get(\"test_diff_accuracy\")\n",
        "        }\n",
        "        individual_data.append(record)\n",
        "\n",
        "individual_df = pd.DataFrame(individual_data)\n",
        "\n",
        "# Drop records with missing timestamps\n",
        "individual_df = individual_df.dropna(subset=[\"timestamp\"])\n",
        "\n",
        "# Convert timestamp to datetime if not already\n",
        "if individual_df[\"timestamp\"].dtype == object:\n",
        "    individual_df[\"timestamp\"] = pd.to_datetime(individual_df[\"timestamp\"])\n",
        "\n",
        "print(\"Individual Metrics DataFrame:\")\n",
        "print(individual_df.head())\n",
        "\n",
        "# 7. Save DataFrames to CSV (Optional)\n",
        "df.to_csv(\"aggregate_evaluation_results.csv\", index=False)\n",
        "individual_df.to_csv(\"individual_evaluation_metrics.csv\", index=False)\n",
        "\n",
        "# 8. Plot Test Accuracy Over Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['timestamp'], df['test_accuracy'], marker='o', linestyle='-', label='Test Accuracy')\n",
        "plt.title('Test Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Test Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Test Diff Accuracy Over Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['timestamp'], df['test_diff_accuracy'], marker='o', linestyle='-', color='green', label='Test Diff Accuracy')\n",
        "plt.title('Test Diff Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Test Diff Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Plot Complete Task Accuracy Over Time\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['timestamp'], df['complete_task_accuracy'], marker='o', linestyle='-', color='red', label='Complete Task Accuracy')\n",
        "plt.title('Complete Task Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Complete Task Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Heatmap: Task Accuracy Over Time\n",
        "\n",
        "# Pivot the individual_df to have 'metric_id' as rows and 'timestamp' as columns, with 'test_accuracy' as values\n",
        "heatmap_data = individual_df.pivot_table(index='metric_id', columns='timestamp', values='test_accuracy')\n",
        "\n",
        "# Plot the heatmap\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(heatmap_data, cmap=\"coolwarm\", cbar_kws={'label': 'Test Accuracy'}, annot=False)\n",
        "plt.title('Task Accuracy Over Time')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Task (Metric ID)')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LOEvyN3LCVdV",
      "metadata": {
        "id": "LOEvyN3LCVdV"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "5062c297"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
